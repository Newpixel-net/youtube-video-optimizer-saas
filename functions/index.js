/**
 * YouTube Tools - Complete SaaS Backend
 * 20+ Cloud Functions with Authentication, Usage Limits, and Admin Panel
 *
 * SECURITY NOTES:
 * - All user-facing functions require authentication via verifyAuth()
 * - Admin functions require admin status via requireAdmin()
 * - Error messages are sanitized via sanitizeErrorMessage() to prevent info disclosure
 * - Rate limiting is implemented via quota system + burst protection
 *
 * RATE LIMITING RECOMMENDATIONS:
 * For production, consider implementing:
 * 1. Firebase App Check - https://firebase.google.com/docs/app-check
 * 2. Cloud Armor - for DDoS protection
 * 3. API Gateway rate limiting
 */

const functions = require('firebase-functions');
const admin = require('firebase-admin');
const { OpenAI } = require('openai');
const { google } = require('googleapis');
const axios = require('axios');
const { GoogleGenAI } = require('@google/genai');
const { fal } = require('@fal-ai/client');
const sharp = require('sharp');

// ==============================================
// IMAGE DIMENSION ENFORCEMENT
// Ensures consistent exact pixel dimensions for all aspect ratios
// ==============================================

/**
 * Get exact target dimensions for a given aspect ratio
 * Returns HD-quality dimensions that match the exact ratio
 */
function getTargetDimensions(aspectRatio, quality = 'hd') {
  // Define exact pixel dimensions for each aspect ratio
  const dimensionMap = {
    // HD quality (default for scene images)
    hd: {
      '16:9': { width: 1920, height: 1080 },  // Full HD
      '9:16': { width: 1080, height: 1920 },  // Full HD vertical
      '1:1': { width: 1080, height: 1080 },   // Square
      '4:3': { width: 1440, height: 1080 },   // Standard
      '3:4': { width: 1080, height: 1440 },   // Portrait
      '4:5': { width: 1080, height: 1350 },   // Instagram portrait
      '5:4': { width: 1350, height: 1080 },   // Landscape variant
      '3:2': { width: 1620, height: 1080 },   // Classic photo
      '2:3': { width: 1080, height: 1620 }    // Portrait photo
    },
    // Standard quality (lighter/faster)
    standard: {
      '16:9': { width: 1280, height: 720 },   // HD 720p
      '9:16': { width: 720, height: 1280 },
      '1:1': { width: 720, height: 720 },
      '4:3': { width: 960, height: 720 },
      '3:4': { width: 720, height: 960 },
      '4:5': { width: 720, height: 900 },
      '5:4': { width: 900, height: 720 },
      '3:2': { width: 1080, height: 720 },
      '2:3': { width: 720, height: 1080 }
    },
    // 4K quality (for upscaling)
    '4k': {
      '16:9': { width: 3840, height: 2160 },  // 4K UHD
      '9:16': { width: 2160, height: 3840 },
      '1:1': { width: 2160, height: 2160 },
      '4:3': { width: 2880, height: 2160 },
      '3:4': { width: 2160, height: 2880 },
      '4:5': { width: 2160, height: 2700 },
      '5:4': { width: 2700, height: 2160 },
      '3:2': { width: 3240, height: 2160 },
      '2:3': { width: 2160, height: 3240 }
    }
  };

  const qualityDimensions = dimensionMap[quality] || dimensionMap.hd;
  const dimensions = qualityDimensions[aspectRatio];

  // Fallback to 16:9 HD if aspect ratio not found
  if (!dimensions) {
    console.warn(`Unknown aspect ratio: ${aspectRatio}, defaulting to 16:9 HD`);
    return dimensionMap.hd['16:9'];
  }

  return dimensions;
}

/**
 * Enforce exact dimensions on an image buffer
 * Uses cover fit with center positioning for consistent cropping
 */
async function enforceImageDimensions(imageBuffer, aspectRatio, quality = 'hd') {
  const { width, height } = getTargetDimensions(aspectRatio, quality);

  try {
    const processedBuffer = await sharp(imageBuffer)
      .resize(width, height, {
        fit: 'cover',
        position: 'center'
      })
      .png({ quality: 95, compressionLevel: 6 })
      .toBuffer();

    console.log(`Image resized to exact dimensions: ${width}x${height} (${aspectRatio})`);
    return { buffer: processedBuffer, width, height, mimeType: 'image/png' };
  } catch (error) {
    console.error('Error enforcing image dimensions:', error);
    // Return original if resize fails
    return { buffer: imageBuffer, width: null, height: null, mimeType: 'image/png' };
  }
}

// Explicit initialization with project ID and storage bucket
admin.initializeApp({
  projectId: 'ytseo-6d1b0',
  storageBucket: 'ytseo-6d1b0.firebasestorage.app'
});
const db = admin.firestore();

// ==============================================
// RATE LIMITING - Burst Protection
// ==============================================

/**
 * Simple in-memory rate limiter for burst protection
 * Note: This resets on each function cold start, so it's only for burst protection.
 * For persistent rate limiting, use the quota system in Firestore.
 */
const rateLimitStore = new Map();

function checkRateLimit(userId, action, maxRequestsPerMinute = 10) {
  const key = `${userId}:${action}`;
  const now = Date.now();
  const windowMs = 60 * 1000; // 1 minute window

  // Get or create entry
  let entry = rateLimitStore.get(key);
  if (!entry || now - entry.windowStart > windowMs) {
    entry = { windowStart: now, count: 0 };
  }

  entry.count++;
  rateLimitStore.set(key, entry);

  // Clean old entries periodically (simple cleanup)
  if (rateLimitStore.size > 10000) {
    const cutoff = now - windowMs;
    for (const [k, v] of rateLimitStore.entries()) {
      if (v.windowStart < cutoff) rateLimitStore.delete(k);
    }
  }

  if (entry.count > maxRequestsPerMinute) {
    throw new functions.https.HttpsError(
      'resource-exhausted',
      'Too many requests. Please wait a moment and try again.'
    );
  }

  return true;
}

const openai = new OpenAI({
  apiKey: functions.config().openai?.key || process.env.OPENAI_API_KEY
});

const youtube = google.youtube({
  version: 'v3',
  auth: functions.config().youtube?.key || process.env.YOUTUBE_API_KEY
});

// ==============================================
// AUTH HELPERS
// ==============================================

async function verifyAuth(context) {
  if (!context.auth) {
    throw new functions.https.HttpsError('unauthenticated', 'User must be logged in');
  }
  return context.auth.uid;
}

async function isAdmin(uid) {
  const adminDoc = await db.collection('adminUsers').doc(uid).get();
  return adminDoc.exists;
}

async function requireAdmin(context) {
  const uid = await verifyAuth(context);
  const isUserAdmin = await isAdmin(uid);
  if (!isUserAdmin) {
    throw new functions.https.HttpsError('permission-denied', 'Admin access required');
  }
  return uid;
}

async function getUser(uid) {
  const userDoc = await db.collection('users').doc(uid).get();
  if (!userDoc.exists) {
    // Auto-create user profile if missing
    console.log('User document not found, creating profile for:', uid);
    const defaultPlan = 'free';
    const defaultLimits = {
      warpOptimizer: { dailyLimit: 3 },
      competitorAnalysis: { dailyLimit: 3 },
      trendPredictor: { dailyLimit: 3 },
      thumbnailGenerator: { dailyLimit: 3 }
    };

    // Use serverTimestamp for Firestore storage
    const newUserData = {
      uid: uid,
      email: '',
      displayName: '',
      photoURL: '',
      createdAt: admin.firestore.FieldValue.serverTimestamp(),
      lastLoginAt: admin.firestore.FieldValue.serverTimestamp(),
      isActive: true,
      isAdmin: false,
      subscription: {
        plan: defaultPlan,
        status: 'active',
        startDate: admin.firestore.FieldValue.serverTimestamp(),
        endDate: null,
        autoRenew: false
      },
      usage: {
        warpOptimizer: {
          usedToday: 0,
          limit: defaultLimits.warpOptimizer.dailyLimit,
          lastResetAt: admin.firestore.FieldValue.serverTimestamp(),
          cooldownUntil: null
        },
        competitorAnalysis: {
          usedToday: 0,
          limit: defaultLimits.competitorAnalysis.dailyLimit,
          lastResetAt: admin.firestore.FieldValue.serverTimestamp(),
          cooldownUntil: null
        },
        trendPredictor: {
          usedToday: 0,
          limit: defaultLimits.trendPredictor.dailyLimit,
          lastResetAt: admin.firestore.FieldValue.serverTimestamp(),
          cooldownUntil: null
        },
        thumbnailGenerator: {
          usedToday: 0,
          limit: defaultLimits.thumbnailGenerator.dailyLimit,
          lastResetAt: admin.firestore.FieldValue.serverTimestamp(),
          cooldownUntil: null
        }
      },
      notes: '',
      customLimits: {}
    };

    await db.collection('users').doc(uid).set(newUserData);
    console.log('User profile created successfully');

    // Re-read the document to get actual timestamp values (not sentinel objects)
    const createdDoc = await db.collection('users').doc(uid).get();
    return createdDoc.data();
  }
  return userDoc.data();
}

async function checkUsageLimit(uid, toolType) {
  const userDoc = await db.collection('users').doc(uid).get();
  const user = userDoc.data();
  if (!user) {
    throw new functions.https.HttpsError('not-found', 'User not found');
  }

  let usage = user.usage?.[toolType];

  // Auto-create usage data for new tools if missing (for existing users)
  if (!usage) {
    const defaultLimit = 2; // Free plan default
    const newUsageData = {
      usedToday: 0,
      limit: defaultLimit,
      lastResetAt: admin.firestore.FieldValue.serverTimestamp(),
      cooldownUntil: null
    };

    // Create the missing tool usage in Firestore
    await db.collection('users').doc(uid).update({
      [`usage.${toolType}`]: newUsageData
    });

    // Use default values for this request (serverTimestamp won't be resolved yet)
    usage = {
      usedToday: 0,
      limit: defaultLimit,
      lastResetAt: admin.firestore.Timestamp.now()
    };
  }

  const now = admin.firestore.Timestamp.now();
  const nowMs = now.toMillis();

  // Get custom reset time from settings (default 1440 minutes = 24 hours)
  let resetMinutes = 1440;
  try {
    const settingsDoc = await db.collection('settings').doc('quotaSettings').get();
    if (settingsDoc.exists && settingsDoc.data().resetTimeMinutes) {
      resetMinutes = settingsDoc.data().resetTimeMinutes;
    }
  } catch (e) {
    console.log('Using default reset time');
  }

  // Check if reset is due based on custom reset time
  const lastResetMs = usage.lastResetAt ? usage.lastResetAt.toMillis() : 0;
  const resetIntervalMs = resetMinutes * 60 * 1000;
  const nextResetMs = lastResetMs + resetIntervalMs;

  if (nowMs >= nextResetMs) {
    // Time to reset
    await db.collection('users').doc(uid).update({
      [`usage.${toolType}.usedToday`]: 0,
      [`usage.${toolType}.lastResetAt`]: admin.firestore.FieldValue.serverTimestamp()
    });
    usage.usedToday = 0;
  }

  // Calculate total available uses (regular limit + bonus)
  const bonusUses = user.bonusUses?.[toolType] || 0;
  const totalLimit = usage.limit + bonusUses;

  if (usage.usedToday >= totalLimit) {
    // Calculate time until next reset
    const currentLastReset = usage.lastResetAt ? usage.lastResetAt.toMillis() : nowMs;
    const resetAtMs = currentLastReset + resetIntervalMs;
    const remainingMs = Math.max(0, resetAtMs - nowMs);
    const remainingSeconds = Math.ceil(remainingMs / 1000);
    const remainingMinutes = Math.ceil(remainingSeconds / 60);

    // Get token bypass info for this tool
    let tokenBypassInfo = { available: false, balance: 0, cost: 0 };
    try {
      const tokenDoc = await db.collection('creativeTokens').doc(uid).get();
      const tokenBalance = tokenDoc.exists ? (tokenDoc.data().balance || 0) : 0;

      // Get bypass costs (use inline defaults if getTokenBypassCosts not yet defined)
      let bypassCosts;
      try {
        const costDoc = await db.collection('settings').doc('tokenBypassCosts').get();
        bypassCosts = costDoc.exists ? costDoc.data().costs : {};
      } catch (e) {
        bypassCosts = {};
      }
      const defaultCosts = {
        warpOptimizer: 3, competitorAnalysis: 3, trendPredictor: 3,
        thumbnailGenerator: 5, channelAudit: 3
      };
      const toolCost = bypassCosts[toolType] || defaultCosts[toolType] || 3;

      tokenBypassInfo = {
        available: true,
        balance: tokenBalance,
        cost: toolCost,
        canBypass: tokenBalance >= toolCost
      };
    } catch (tokenErr) {
      console.log('Could not get token bypass info:', tokenErr.message);
    }

    throw new functions.https.HttpsError(
      'resource-exhausted',
      `Quota exhausted (${usage.usedToday}/${totalLimit}). Resets in ${remainingMinutes} minutes.`,
      {
        limit: totalLimit,
        used: usage.usedToday,
        bonusUses: bonusUses,
        resetAtMs: resetAtMs,
        remainingMs: remainingMs,
        remainingSeconds: remainingSeconds,
        remainingMinutes: remainingMinutes,
        toolType: toolType,
        tokenBypass: tokenBypassInfo
      }
    );
  }

  return {
    allowed: true,
    remaining: totalLimit - usage.usedToday - 1,
    limit: totalLimit,
    bonusUses: bonusUses
  };
}

async function incrementUsage(uid, toolType) {
  await db.collection('users').doc(uid).update({
    [`usage.${toolType}.usedToday`]: admin.firestore.FieldValue.increment(1)
  });
}

async function logUsage(uid, action, metadata = {}) {
  await db.collection('usageLogs').add({
    userId: uid,
    action,
    timestamp: admin.firestore.FieldValue.serverTimestamp(),
    metadata
  });
}

/**
 * Helper: Enforce maximum projects per user
 * Deletes oldest projects if user exceeds limit (default 8)
 * Also cleans up associated storage files
 */
async function enforceMaxProjects(uid, maxProjects = 8) {
  // Use the default bucket (most reliable) - Firebase admin SDK knows the correct bucket
  const bucket = admin.storage().bucket();
  const STORAGE_BUCKET = bucket.name;

  try {
    // Get user's projects ordered by creation date (oldest first)
    const projectsSnapshot = await db.collection('wizardProjects')
      .where('userId', '==', uid)
      .orderBy('createdAt', 'asc')
      .get();

    const projectCount = projectsSnapshot.size;

    // If at or over limit, delete oldest projects to make room for new one
    if (projectCount >= maxProjects) {
      const projectsToDelete = projectCount - maxProjects + 1; // +1 to make room for new project
      const docs = projectsSnapshot.docs.slice(0, projectsToDelete);

      console.log(`[enforceMaxProjects] User ${uid} has ${projectCount} projects, deleting ${projectsToDelete} oldest`);

      for (const doc of docs) {
        const projectData = doc.data();
        const projectId = doc.id;

        // Clean up storage files associated with this project
        try {
          // Delete sourceAsset if exists
          if (projectData.sourceAsset?.storagePath) {
            await bucket.file(projectData.sourceAsset.storagePath).delete().catch(() => {});
          }
          // Delete uploaded video if exists
          if (projectData.uploadedVideoPath) {
            await bucket.file(projectData.uploadedVideoPath).delete().catch(() => {});
          }
          // Delete any clip-specific captures from extension-uploads/{videoId}/
          if (projectData.videoId) {
            const clipCapturesPath = `extension-uploads/${projectData.videoId}`;
            const [captureFiles] = await bucket.getFiles({ prefix: clipCapturesPath });
            for (const file of captureFiles) {
              await file.delete().catch(() => {});
            }
          }

          // Delete wizard-thumbnails for clips in this project
          // Thumbnails are stored as: wizard-thumbnails/{uid}/{timestamp}-{clipId}-{idx}.{ext}
          if (projectData.clips && Array.isArray(projectData.clips)) {
            for (const clip of projectData.clips) {
              if (clip.id) {
                // List and delete thumbnails containing this clipId
                const [thumbFiles] = await bucket.getFiles({ prefix: `wizard-thumbnails/${uid}/` });
                for (const file of thumbFiles) {
                  if (file.name.includes(clip.id)) {
                    await file.delete().catch(() => {});
                  }
                }
              }
            }
          }
        } catch (storageError) {
          console.log(`[enforceMaxProjects] Storage cleanup error for project ${projectId}:`, storageError.message);
        }

        // Delete related processing jobs
        try {
          const jobsSnapshot = await db.collection('processingJobs')
            .where('projectId', '==', projectId)
            .get();

          if (!jobsSnapshot.empty) {
            const batch = db.batch();
            jobsSnapshot.forEach(jobDoc => batch.delete(jobDoc.ref));
            await batch.commit();
            console.log(`[enforceMaxProjects] Deleted ${jobsSnapshot.size} processing jobs for project ${projectId}`);
          }
        } catch (jobsError) {
          console.log(`[enforceMaxProjects] Jobs cleanup error for project ${projectId}:`, jobsError.message);
        }

        // Delete the project document
        await db.collection('wizardProjects').doc(projectId).delete();
        console.log(`[enforceMaxProjects] Deleted old project ${projectId} (${projectData.videoId || 'uploaded'})`);
      }
    }

    return { deleted: projectCount >= maxProjects ? projectCount - maxProjects + 1 : 0 };
  } catch (error) {
    console.error('[enforceMaxProjects] Error:', error.message);
    // Don't throw - allow project creation to continue even if cleanup fails
    return { deleted: 0, error: error.message };
  }
}

/**
 * Helper: Get max projects setting from admin config
 */
async function getMaxProjectsLimit() {
  try {
    const configDoc = await db.collection('settings').doc('wizardConfig').get();
    if (configDoc.exists && configDoc.data().maxProjectsPerUser) {
      return configDoc.data().maxProjectsPerUser;
    }
  } catch (error) {
    console.log('[getMaxProjectsLimit] Using default:', error.message);
  }
  return 8; // Default max projects
}

/**
 * Helper: Get token configuration from admin settings
 * Default values MUST match admin panel defaults in admin-plans.html
 * This function is used across multiple token-related Cloud Functions
 */
async function getTokenConfigFromAdmin() {
  const tokenConfigDoc = await db.collection('settings').doc('tokenConfig').get();
  // These defaults match the admin panel UI defaults
  const defaultTokenConfig = {
    free: { monthlyTokens: 10, rolloverPercent: 0 },
    lite: { monthlyTokens: 50, rolloverPercent: 25 },
    pro: { monthlyTokens: 200, rolloverPercent: 50 },
    enterprise: { monthlyTokens: 1000, rolloverPercent: 100 }
  };

  return tokenConfigDoc.exists
    ? { ...defaultTokenConfig, ...tokenConfigDoc.data().plans }
    : defaultTokenConfig;
}

/**
 * SECURITY: Sanitize error messages to prevent information disclosure
 * Removes sensitive details like file paths, API keys, and internal structure
 */
function sanitizeErrorMessage(error, defaultMessage = 'An error occurred. Please try again.') {
  if (!error) return defaultMessage;

  const message = error.message || String(error);

  // Patterns that indicate sensitive information
  const sensitivePatterns = [
    /\/home\/[^\s]+/gi,              // File paths
    /\/var\/[^\s]+/gi,               // System paths
    /node_modules/gi,                 // Node internals
    /at\s+[^\s]+\s+\([^)]+\)/gi,     // Stack trace lines
    /sk-[a-zA-Z0-9]+/gi,             // OpenAI API keys
    /AIza[a-zA-Z0-9_-]+/gi,          // Google API keys
    /Bearer\s+[^\s]+/gi,             // Bearer tokens
    /password|secret|credential/gi,   // Sensitive terms
    /ECONNREFUSED|ETIMEDOUT/gi,      // Network internals
  ];

  // Check if the message contains sensitive info
  for (const pattern of sensitivePatterns) {
    if (pattern.test(message)) {
      console.error('Sanitized error (original logged):', message);
      return defaultMessage;
    }
  }

  // Known safe error messages that can be passed through
  const safeErrorPrefixes = [
    'Video not found',
    'Channel not found',
    'User not found',
    'Invalid YouTube URL',
    'Video URL is required',
    'Quota exhausted',
    'User must be logged in',
    'Admin access required',
    'Permission denied',
    'Invalid argument',
  ];

  for (const prefix of safeErrorPrefixes) {
    if (message.startsWith(prefix) || message.includes(prefix)) {
      return message;
    }
  }

  // If the message is short and doesn't look like a stack trace, allow it
  if (message.length < 100 && !message.includes('\n') && !message.includes('    at ')) {
    return message;
  }

  // Default: return generic message and log the original
  console.error('Sanitized error (original logged):', message);
  return defaultMessage;
}

// ==============================================
// USER LIFECYCLE
// ==============================================

exports.onUserCreate = functions.auth.user().onCreate(async (user) => {
  try {
    const settingsDoc = await db.collection('adminSettings').doc('config').get();
    const defaultPlan = settingsDoc.data()?.defaultPlan || 'free';
    const planDoc = await db.collection('subscriptionPlans').doc(defaultPlan).get();
    const planLimits = planDoc.data()?.limits || {};

    // Default limits for each tool (fallback if plan doesn't have new tool keys)
    const defaultToolLimit = 2;

    await db.collection('users').doc(user.uid).set({
      uid: user.uid,
      email: user.email,
      displayName: user.displayName || '',
      photoURL: user.photoURL || '',
      createdAt: admin.firestore.FieldValue.serverTimestamp(),
      lastLoginAt: admin.firestore.FieldValue.serverTimestamp(),
      isActive: true,
      isAdmin: false,
      subscription: {
        plan: defaultPlan,
        status: 'active',
        startDate: admin.firestore.FieldValue.serverTimestamp(),
        endDate: null,
        autoRenew: false
      },
      usage: {
        warpOptimizer: {
          usedToday: 0,
          limit: planLimits.warpOptimizer?.dailyLimit || defaultToolLimit,
          lastResetAt: admin.firestore.FieldValue.serverTimestamp(),
          cooldownUntil: null
        },
        competitorAnalysis: {
          usedToday: 0,
          limit: planLimits.competitorAnalysis?.dailyLimit || defaultToolLimit,
          lastResetAt: admin.firestore.FieldValue.serverTimestamp(),
          cooldownUntil: null
        },
        trendPredictor: {
          usedToday: 0,
          limit: planLimits.trendPredictor?.dailyLimit || defaultToolLimit,
          lastResetAt: admin.firestore.FieldValue.serverTimestamp(),
          cooldownUntil: null
        },
        thumbnailGenerator: {
          usedToday: 0,
          limit: planLimits.thumbnailGenerator?.dailyLimit || defaultToolLimit,
          lastResetAt: admin.firestore.FieldValue.serverTimestamp(),
          cooldownUntil: null
        }
      },
      notes: '',
      customLimits: {}
    });

    await logUsage(user.uid, 'user_created', { email: user.email });
    console.log(`User created: ${user.email}`);
  } catch (error) {
    console.error('Error creating user:', error);
  }
});

exports.updateLastLogin = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  await db.collection('users').doc(uid).update({
    lastLoginAt: admin.firestore.FieldValue.serverTimestamp()
  });
  return { success: true };
});

// ==============================================
// VIDEO HELPERS
// ==============================================

function extractVideoId(url) {
  const patterns = [
    /(?:youtube\.com\/watch\?v=|youtu\.be\/|youtube\.com\/embed\/)([^&\n?#]+)/,
    /^([a-zA-Z0-9_-]{11})$/
  ];
  for (const pattern of patterns) {
    const match = url.match(pattern);
    if (match) return match[1];
  }
  throw new Error('Invalid YouTube URL');
}

async function getVideoTranscript(videoId) {
  try {
    const response = await axios.get(`https://www.youtube.com/watch?v=${videoId}`);
    const html = response.data;
    const captionsRegex = /"captions":(\{.*?\}),"videoDetails"/;
    const match = html.match(captionsRegex);
    if (!match) return { segments: [], fullText: 'Transcript not available.' };
    
    const captionsData = JSON.parse(match[1]);
    const captionTracks = captionsData?.playerCaptionsTracklistRenderer?.captionTracks;
    if (!captionTracks || captionTracks.length === 0) {
      return { segments: [], fullText: 'No captions available.' };
    }
    
    const captionUrl = captionTracks[0].baseUrl;
    const transcriptResponse = await axios.get(captionUrl);
    const transcriptXml = transcriptResponse.data;
    const segments = [];
    const textMatches = transcriptXml.matchAll(/<text start="([^"]*)"[^>]*>(.*?)<\/text>/g);
    
    for (const match of textMatches) {
      const timestamp = parseFloat(match[1]);
      const text = match[2]
        .replace(/&amp;/g, '&')
        .replace(/&lt;/g, '<')
        .replace(/&gt;/g, '>')
        .replace(/&quot;/g, '"')
        .replace(/&#39;/g, "'")
        .replace(/\n/g, ' ')
        .trim();
      if (text) segments.push({ timestamp, text });
    }
    
    return { segments, fullText: segments.map(s => s.text).join(' ') };
  } catch (error) {
    return { segments: [], fullText: 'Transcript not available.' };
  }
}

/**
 * AI Virality Scoring System (OpusClip-style)
 *
 * This enhanced scoring system analyzes clips based on multiple factors:
 * - Hook Strength: How compelling are the first 3 seconds
 * - Emotional Impact: What emotions does the content trigger
 * - Trend Alignment: How relevant is the topic to current trends
 * - Engagement Potential: Likelihood of comments, shares, saves
 * - Watch-Through Rate: Will viewers watch to the end
 *
 * Returns a detailed score breakdown instead of just a single number
 */
async function calculateEnhancedViralityScore(clipData, videoContext) {
  const { transcript, duration, emotionalHook, reason, uniqueAngle } = clipData;
  const { title, channelTitle, viewCount, contentType } = videoContext;

  // Build analysis prompt for detailed scoring
  const scoringPrompt = `You are a viral content scoring AI like OpusClip's virality predictor.

Analyze this short-form video clip and provide detailed virality scores.

CLIP CONTENT:
- From video: "${title}" by ${channelTitle}
- Duration: ${duration} seconds
- Content: "${transcript || reason || 'No transcript available'}"
- Unique angle: "${uniqueAngle || 'Not specified'}"
- Emotional hook: "${emotionalHook || 'Not specified'}"
- Video views: ${(viewCount || 0).toLocaleString()}
- Content type: ${contentType || 'general'}

Score each factor from 0-100 and provide a DETAILED breakdown.

SCORING CRITERIA:

1. HOOK STRENGTH (0-100): Does the first 3 seconds grab attention?
   - 90-100: Irresistible hook, impossible to scroll past
   - 70-89: Strong hook, catches most viewers
   - 50-69: Decent hook, some engagement
   - Below 50: Weak hook, easy to scroll past

2. EMOTIONAL IMPACT (0-100): How much emotion does it trigger?
   - Consider: curiosity, surprise, inspiration, humor, controversy, relatability
   - 90-100: Strong emotional response guaranteed
   - 70-89: Noticeable emotional reaction
   - 50-69: Mild interest
   - Below 50: Low emotional engagement

3. SHAREABILITY (0-100): Will viewers share this?
   - Consider: "I need to show this to someone" factor
   - Quotable moments, relatable content, useful tips

4. TREND ALIGNMENT (0-100): How relevant to current trends?
   - Consider: trending topics, formats, sounds, themes
   - Evergreen content scores 60-70 (always relevant but not trending)

5. COMPLETION RATE (0-100): Will viewers watch until the end?
   - Consider: pacing, payoff, story arc, length

6. ENGAGEMENT BAIT (0-100): Will it generate comments?
   - Consider: controversial takes, questions, debate potential

RESPOND IN JSON:
{
  "overallScore": <weighted average, 0-100>,
  "breakdown": {
    "hookStrength": { "score": <0-100>, "reason": "brief explanation" },
    "emotionalImpact": { "score": <0-100>, "reason": "brief explanation" },
    "shareability": { "score": <0-100>, "reason": "brief explanation" },
    "trendAlignment": { "score": <0-100>, "reason": "brief explanation" },
    "completionRate": { "score": <0-100>, "reason": "brief explanation" },
    "engagementBait": { "score": <0-100>, "reason": "brief explanation" }
  },
  "viralPrediction": "HIGH/MEDIUM/LOW",
  "recommendedPlatform": "tiktok/instagram/youtube",
  "improvementTips": ["tip 1", "tip 2"],
  "bestPostingTime": "e.g., 'weekday evenings' or 'weekend mornings'"
}`;

  try {
    // Add 15 second timeout per API call to prevent hanging
    const response = await Promise.race([
      openai.chat.completions.create({
        model: 'gpt-4o-mini', // Use mini for cost efficiency on repeated calls
        messages: [{ role: 'user', content: scoringPrompt }],
        response_format: { type: 'json_object' },
        max_tokens: 800,
        temperature: 0.5
      }),
      new Promise((_, reject) =>
        setTimeout(() => reject(new Error('API timeout')), 15000)
      )
    ]);

    const result = JSON.parse(response.choices[0].message.content);

    return {
      score: Math.round(result.overallScore || 75),
      breakdown: result.breakdown || {},
      prediction: result.viralPrediction || 'MEDIUM',
      recommendedPlatform: result.recommendedPlatform || 'tiktok',
      tips: result.improvementTips || [],
      bestTime: result.bestPostingTime || 'evening'
    };
  } catch (error) {
    console.log('Enhanced virality scoring failed, using basic score:', error.message);
    // Fallback to basic scoring
    return {
      score: clipData.viralityScore || 75,
      breakdown: {},
      prediction: 'MEDIUM',
      recommendedPlatform: 'tiktok',
      tips: [],
      bestTime: 'evening'
    };
  }
}

/**
 * Batch process virality scores for multiple clips
 * More efficient than scoring one at a time
 */
async function batchCalculateViralityScores(clips, videoContext) {
  // For cost efficiency, only do enhanced scoring on top candidates
  // Sort by initial score and enhance top 6
  const sortedClips = [...clips].sort((a, b) => (b.score || 0) - (a.score || 0));
  const topClips = sortedClips.slice(0, 6);

  const enhancedClips = await Promise.all(
    topClips.map(async (clip) => {
      const enhanced = await calculateEnhancedViralityScore(clip, videoContext);
      return {
        ...clip,
        score: enhanced.score,
        viralityBreakdown: enhanced.breakdown,
        viralPrediction: enhanced.prediction,
        recommendedPlatform: enhanced.recommendedPlatform,
        improvementTips: enhanced.tips,
        bestPostingTime: enhanced.bestTime
      };
    })
  );

  // Keep remaining clips with original scores
  const remainingClips = sortedClips.slice(6).map(clip => ({
    ...clip,
    viralPrediction: clip.score >= 80 ? 'MEDIUM' : 'LOW'
  }));

  // Re-sort all clips by final score (descending) since enhancement may have changed scores
  const allClips = [...enhancedClips, ...remainingClips];
  return allClips.sort((a, b) => (b.score || 0) - (a.score || 0));
}

async function getVideoMetadata(videoId) {
  try {
    const response = await youtube.videos.list({
      part: ['snippet', 'statistics', 'contentDetails'],
      id: [videoId]
    });

    if (!response.data.items || response.data.items.length === 0) {
      throw new functions.https.HttpsError('not-found', 'Video not found. Please check the URL and try again.');
    }

    const video = response.data.items[0];
    const snippet = video.snippet || {};
    const statistics = video.statistics || {};
    const contentDetails = video.contentDetails || {};

    // Parse duration to human-readable format
    const rawDuration = contentDetails.duration || 'PT0S';
    const durationMatch = rawDuration.match(/PT(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?/);
    let duration = 'Unknown';
    if (durationMatch) {
      const hours = parseInt(durationMatch[1] || 0);
      const minutes = parseInt(durationMatch[2] || 0);
      const seconds = parseInt(durationMatch[3] || 0);
      if (hours > 0) {
        duration = `${hours}:${minutes.toString().padStart(2, '0')}:${seconds.toString().padStart(2, '0')}`;
      } else {
        duration = `${minutes}:${seconds.toString().padStart(2, '0')}`;
      }
    }

    // Return object with field names matching what optimizeVideo expects
    // All fields have default values to prevent Firestore undefined errors
    return {
      videoId: videoId || '',
      title: snippet.title || 'Untitled',
      description: snippet.description || '',
      channelTitle: snippet.channelTitle || 'Unknown Channel',
      channelId: snippet.channelId || '',
      publishedAt: snippet.publishedAt || null,
      thumbnail: (snippet.thumbnails?.high?.url || snippet.thumbnails?.default?.url) || '',
      tags: snippet.tags || [],
      categoryId: snippet.categoryId || '',
      viewCount: parseInt(statistics.viewCount) || 0,
      likeCount: parseInt(statistics.likeCount) || 0,
      commentCount: parseInt(statistics.commentCount) || 0,
      duration: duration,
      rawDuration: rawDuration,
      defaultLanguage: snippet.defaultLanguage || 'en'
    };
  } catch (error) {
    // Check for specific YouTube API errors
    if (error instanceof functions.https.HttpsError) {
      throw error;
    }

    const errorMessage = error.message || '';
    const errorCode = error.code || error.response?.status;

    // YouTube API not enabled
    if (errorMessage.includes('API') && errorMessage.includes('not been used') ||
        errorMessage.includes('accessNotConfigured') ||
        errorMessage.includes('YouTube Data API v3 has not been enabled')) {
      throw new functions.https.HttpsError(
        'failed-precondition',
        'YouTube API not enabled. Please enable YouTube Data API v3 in Google Cloud Console.'
      );
    }

    // API key issues
    if (errorMessage.includes('API key') || errorMessage.includes('invalid key') || errorCode === 400) {
      throw new functions.https.HttpsError(
        'failed-precondition',
        'YouTube API key is invalid or missing. Please check the API configuration.'
      );
    }

    // Quota exceeded
    if (errorMessage.includes('quota') || errorCode === 403) {
      throw new functions.https.HttpsError(
        'resource-exhausted',
        'YouTube API quota exceeded. Please try again later or upgrade the API quota.'
      );
    }

    // Network or other errors
    throw new functions.https.HttpsError(
      'internal',
      'Failed to fetch video data: ' + errorMessage
    );
  }
}

function parseDuration(duration) {
  const match = duration.match(/PT(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?/);
  if (!match) return 0;
  const hours = parseInt(match[1] || 0);
  const minutes = parseInt(match[2] || 0);
  const seconds = parseInt(match[3] || 0);
  return hours * 3600 + minutes * 60 + seconds;
}

function formatTimestamp(seconds) {
  const hours = Math.floor(seconds / 3600);
  const minutes = Math.floor((seconds % 3600) / 60);
  const secs = Math.floor(seconds % 60);
  if (hours > 0) {
    return `${hours}:${minutes.toString().padStart(2, '0')}:${secs.toString().padStart(2, '0')}`;
  }
  return `${minutes}:${secs.toString().padStart(2, '0')}`;
}

// ==============================================
// CONTENT TYPE DETECTION
// ==============================================

// YouTube Category IDs: 10 = Music, 20 = Gaming, 22 = People & Blogs, 24 = Entertainment,
// 25 = News & Politics, 26 = Howto & Style, 27 = Education, 28 = Science & Technology
// Complete YouTube category mapping - Official YouTube Data API v3 categories
const YOUTUBE_CATEGORIES = {
  '1': 'film_animation',
  '2': 'autos_vehicles',
  '10': 'music',
  '15': 'pets_animals',
  '17': 'sports',           // CRITICAL: Sports category (WTT, Olympics, etc.)
  '18': 'short_movies',
  '19': 'travel_events',
  '20': 'gaming',
  '21': 'videoblogging',
  '22': 'vlog',
  '23': 'comedy',
  '24': 'entertainment',
  '25': 'news',
  '26': 'howto',
  '27': 'education',
  '28': 'tech',
  '29': 'nonprofits_activism',
  '30': 'movies',
  '31': 'anime_animation',
  '32': 'action_adventure',
  '33': 'classics',
  '34': 'comedy_film',
  '35': 'documentary',
  '36': 'drama',
  '37': 'family',
  '38': 'foreign',
  '39': 'horror',
  '40': 'scifi_fantasy',
  '41': 'thriller',
  '42': 'shorts',
  '43': 'shows',
  '44': 'trailers'
};

// Non-music categories - used to prevent false music detection
const NON_MUSIC_CATEGORIES = new Set([
  '1', '2', '15', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28',
  '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44'
]);

function detectContentType(metadata) {
  const title = (metadata.title || '').toLowerCase();
  const channelTitle = (metadata.channelTitle || '').toLowerCase();
  const description = (metadata.description || '').toLowerCase();
  const tags = (metadata.tags || []).map(t => t.toLowerCase());
  const categoryId = metadata.categoryId || '';

  // ============================================================
  // PRIORITY 1: Definitive high-confidence detection sources
  // These should NEVER be overridden by keyword guessing
  // ============================================================

  // 1a. Auto-generated YouTube music channels ("Artist - Topic") - 100% music
  if (channelTitle.endsWith(' - topic') || channelTitle.includes(' - topic')) {
    return { type: 'music', subtype: 'song', confidence: 'high', source: 'topic_channel' };
  }

  // 1b. YouTube's official categoryId - HIGH CONFIDENCE
  // This takes precedence over ALL keyword-based detection
  if (categoryId && YOUTUBE_CATEGORIES[categoryId]) {
    const categoryType = YOUTUBE_CATEGORIES[categoryId];

    // If it's music category (10), determine subtype
    if (categoryId === '10') {
      const subtype = detectMusicSubtype(title, description, tags);
      return { type: 'music', subtype, confidence: 'high', source: 'category' };
    }

    // For all other categories, return the mapped type
    // This prevents sports videos, gaming videos, etc. from being misclassified as music
    const subtype = detectSubtypeForCategory(categoryType, title, description, tags);
    return { type: categoryType, subtype, confidence: 'high', source: 'category' };
  }

  // ============================================================
  // PRIORITY 2: Keyword-based detection (only when no categoryId)
  // Used as fallback when YouTube doesn't provide category data
  // ============================================================

  // 2a. Strong music indicators (requires multiple signals for confidence)
  const musicResult = detectMusicByKeywords(title, description, tags, channelTitle);
  if (musicResult) {
    return musicResult;
  }

  // 2b. Other content type keywords
  const keywordResult = detectContentByKeywords(title, description, tags);
  if (keywordResult) {
    return keywordResult;
  }

  // ============================================================
  // PRIORITY 3: Default fallback
  // ============================================================
  return { type: 'general', subtype: 'unknown', confidence: 'low', source: 'default' };
}

/**
 * Strict music detection by keywords - requires multiple indicators to avoid false positives
 * Only used when categoryId is not available
 */
function detectMusicByKeywords(title, description, tags, channelTitle) {
  // TIER 1: Very strong music indicators (single match = high confidence)
  // These are ONLY used in actual music content
  const tier1Keywords = [
    'official music video',
    'official lyric video',
    'official audio',
    'official visualizer',
    'lyric video',
    '(lyrics)',
    '[lyrics]',
    'full album stream',
    'album stream'
  ];

  if (tier1Keywords.some(kw => title.includes(kw))) {
    const subtype = detectMusicSubtype(title, description, tags);
    return { type: 'music', subtype, confidence: 'high', source: 'keywords_tier1' };
  }

  // TIER 2: Strong music indicators in TITLE only (not description - too broad)
  const tier2TitleKeywords = [
    'music video',
    'prod. by',
    'prod by',
    'produced by',
    '(prod.',
    '[prod.',
    'feat.',
    'ft.',
    '(remix)',
    '[remix]',
    'official remix',
    '(acoustic)',
    '[acoustic]',
    'acoustic version',
    'unplugged'
  ];

  // Count tier 2 matches in title
  const tier2TitleMatches = tier2TitleKeywords.filter(kw => title.includes(kw)).length;

  // TIER 3: Music genre tags (exact match only)
  const musicGenreTags = [
    'hip hop', 'hiphop', 'rap', 'trap', 'drill',
    'rock', 'metal', 'punk', 'alternative', 'grunge',
    'pop music', 'pop song',
    'jazz', 'blues', 'soul', 'r&b', 'rnb', 'funk', 'gospel',
    'classical music', 'orchestra', 'symphony',
    'electronic music', 'edm', 'house music', 'techno', 'trance', 'dubstep', 'dnb', 'drum and bass',
    'country music', 'folk music', 'bluegrass',
    'reggae', 'dancehall', 'ska',
    'latin music', 'salsa', 'bachata', 'reggaeton',
    'k-pop', 'kpop', 'j-pop', 'jpop',
    'indie music', 'indie rock', 'indie pop'
  ];

  const genreTagMatches = musicGenreTags.filter(genre => tags.includes(genre)).length;

  // TIER 4: Music-related channel indicators
  const musicChannelIndicators = [
    'records', 'music', 'official', 'vevo', 'entertainment'
  ];
  const channelMusicScore = musicChannelIndicators.filter(ind => channelTitle.includes(ind)).length;

  // TIER 5: Additional title patterns that suggest music
  const musicTitlePatterns = [
    /\s-\s.*\(official\)/i,           // "Artist - Song (Official)"
    /\s-\s.*\[official\]/i,           // "Artist - Song [Official]"
    /\sft\.\s/i,                       // " ft. " with spaces
    /\sfeat\.\s/i,                     // " feat. " with spaces
    /\(feat\./i,                       // "(feat."
    /\[feat\./i,                       // "[feat."
    /^\s*[\w\s]+\s-\s[\w\s]+$/,       // Simple "Artist - Song" pattern
  ];

  const patternMatches = musicTitlePatterns.filter(pattern => pattern.test(title)).length;

  // Calculate confidence score
  // Need multiple indicators to classify as music without categoryId
  const totalScore = (tier2TitleMatches * 2) + (genreTagMatches * 2) + channelMusicScore + patternMatches;

  // Require score >= 3 for medium confidence music detection
  // This prevents single generic keyword matches from triggering music classification
  if (totalScore >= 3) {
    const subtype = detectMusicSubtype(title, description, tags);
    return { type: 'music', subtype, confidence: 'medium', source: 'keywords_combined' };
  }

  // Even with lower score, if there are tier 2 title matches AND genre tags, it's likely music
  if (tier2TitleMatches >= 1 && genreTagMatches >= 1) {
    const subtype = detectMusicSubtype(title, description, tags);
    return { type: 'music', subtype, confidence: 'medium', source: 'keywords_combined' };
  }

  return null; // Not confidently music
}

/**
 * Detect other content types by keywords (non-music)
 */
function detectContentByKeywords(title, description, tags) {
  // Sports detection
  const sportsKeywords = ['match', 'game highlights', 'championship', 'tournament', 'vs', 'versus',
    'final', 'semifinal', 'quarterfinal', 'world cup', 'olympics', 'league', 'season', 'playoff',
    'behind the scenes', 'training', 'practice', 'warmup'];
  const sportsTerms = ['football', 'soccer', 'basketball', 'tennis', 'table tennis', 'cricket',
    'baseball', 'hockey', 'golf', 'boxing', 'mma', 'ufc', 'wrestling', 'volleyball', 'rugby',
    'f1', 'formula 1', 'nascar', 'athletics', 'swimming', 'gymnastics', 'skating', 'skiing'];

  if (sportsKeywords.some(kw => title.includes(kw)) &&
      sportsTerms.some(term => title.includes(term) || tags.some(t => t.includes(term)))) {
    return { type: 'sports', subtype: 'event', confidence: 'medium', source: 'keywords' };
  }

  // Tutorial/How-to detection
  if (title.includes('tutorial') || title.includes('how to ') || title.includes('guide to') ||
      title.includes('step by step') || title.includes('learn to') || title.includes('beginner')) {
    return { type: 'tutorial', subtype: 'educational', confidence: 'medium', source: 'keywords' };
  }

  // Review/Unboxing detection
  if (title.includes('review') || title.includes('unboxing') || title.includes('hands on') ||
      title.includes('first look') || title.includes('comparison')) {
    return { type: 'review', subtype: 'product', confidence: 'medium', source: 'keywords' };
  }

  // Gaming detection
  if (title.includes('gameplay') || title.includes('playthrough') || title.includes('let\'s play') ||
      title.includes('walkthrough') || title.includes('speedrun') || title.includes('gaming')) {
    return { type: 'gaming', subtype: 'gameplay', confidence: 'medium', source: 'keywords' };
  }

  // Vlog detection
  if (title.includes('vlog') || title.includes('day in my life') || title.includes('grwm') ||
      title.includes('get ready with me') || title.includes('daily vlog') || title.includes('weekly vlog')) {
    return { type: 'vlog', subtype: 'lifestyle', confidence: 'medium', source: 'keywords' };
  }

  // Podcast/Interview detection
  if (title.includes('podcast') || title.includes('interview with') || title.includes('conversation with') ||
      title.includes('episode') && (title.includes('ep.') || title.includes('ep ') || /ep\s*\d+/i.test(title))) {
    return { type: 'podcast', subtype: 'talk', confidence: 'medium', source: 'keywords' };
  }

  // News detection
  if (title.includes('breaking') || title.includes('news') || title.includes('update') ||
      title.includes('announcement') || title.includes('press conference')) {
    return { type: 'news', subtype: 'current_events', confidence: 'medium', source: 'keywords' };
  }

  // Documentary detection
  if (title.includes('documentary') || title.includes('the story of') || title.includes('history of') ||
      title.includes('investigation') || title.includes('explained')) {
    return { type: 'documentary', subtype: 'informational', confidence: 'medium', source: 'keywords' };
  }

  return null;
}

/**
 * Determine subtype for non-music categories based on title/description
 */
function detectSubtypeForCategory(categoryType, title, description, tags) {
  switch (categoryType) {
    case 'sports':
      if (title.includes('highlight')) return 'highlights';
      if (title.includes('behind the scenes') || title.includes('bts')) return 'behind_the_scenes';
      if (title.includes('interview')) return 'interview';
      if (title.includes('training') || title.includes('practice')) return 'training';
      if (title.includes('final') || title.includes('championship')) return 'championship';
      return 'event';

    case 'gaming':
      if (title.includes('review')) return 'review';
      if (title.includes('walkthrough') || title.includes('playthrough')) return 'walkthrough';
      if (title.includes('speedrun')) return 'speedrun';
      if (title.includes('let\'s play')) return 'lets_play';
      if (title.includes('stream') || title.includes('live')) return 'stream';
      return 'gameplay';

    case 'news':
      if (title.includes('breaking')) return 'breaking';
      if (title.includes('analysis')) return 'analysis';
      if (title.includes('opinion')) return 'opinion';
      return 'report';

    case 'education':
    case 'howto':
      if (title.includes('course') || title.includes('class')) return 'course';
      if (title.includes('tutorial')) return 'tutorial';
      if (title.includes('explained')) return 'explainer';
      return 'educational';

    case 'entertainment':
      if (title.includes('trailer')) return 'trailer';
      if (title.includes('clip')) return 'clip';
      if (title.includes('scene')) return 'scene';
      return 'general';

    case 'travel_events':
      if (title.includes('tour')) return 'tour';
      if (title.includes('travel')) return 'travel';
      if (title.includes('event')) return 'event';
      return 'general';

    default:
      return 'general';
  }
}

function detectMusicSubtype(title, description, tags) {
  const titleLower = title.toLowerCase();

  if (titleLower.includes('full album') || titleLower.includes('album')) return 'album';
  if (titleLower.includes('playlist') || titleLower.includes('mix')) return 'playlist';
  if (titleLower.includes('remix')) return 'remix';
  if (titleLower.includes('cover')) return 'cover';
  if (titleLower.includes('live') || titleLower.includes('concert')) return 'live';
  if (titleLower.includes('lyric') || titleLower.includes('lyrics')) return 'lyric_video';
  if (titleLower.includes('music video') || titleLower.includes('official video')) return 'music_video';
  if (titleLower.includes('visualizer')) return 'visualizer';

  return 'song'; // Default music subtype
}

function getContentTypeContext(contentType) {
  const { type, subtype } = contentType;

  // ============================================================
  // MUSIC CONTENT
  // ============================================================
  if (type === 'music') {
    return {
      titleInstructions: `
MUSIC CONTENT DETECTED - This is a ${subtype === 'song' ? 'song/track' : subtype}.
DO NOT create motivational or educational titles. Create MUSIC-appropriate titles:
- Include artist name and track title
- Use music-related terms: "Official Audio", "Lyrics", "Full Track", etc.
- Highlight genre, mood, or musical elements
- For remixes: mention original artist and remixer
- Keep it authentic to music industry standards`,

      descriptionInstructions: `
MUSIC CONTENT DETECTED - This is a ${subtype === 'song' ? 'song/track' : subtype}.
DO NOT create educational or motivational descriptions. Create a MUSIC description:

Include:
1. ðŸŽµ Song/Track info (artist, title, album if applicable)
2. ðŸŽ§ Genre and musical style
3. ðŸ“€ Release info (if available)
4. ðŸŽ¤ Credits (producers, features, writers if known)
5. â±ï¸ Simple timestamp if multiple sections exist
6. ðŸ”— Links section for: Spotify, Apple Music, streaming platforms
7. Music-related hashtags (#NewMusic #[Genre] #[ArtistName])

DO NOT include:
- Motivational hooks or life advice
- Educational key points or bullet lists
- Call-to-actions about "achieving goals"
- Non-music related content`,

      tagsInstructions: `
MUSIC CONTENT DETECTED - Generate MUSIC-specific tags:

1. Primary (5-8): Artist name, song title, genre, album name
2. Secondary (8-12): Related artists, music style, mood tags, record label
3. Long-tail (10-15): "[Artist] new song 2024", "[Genre] music", "best [genre] songs"
4. Trending (5-10): Current music trends, viral sounds, playlist names

DO NOT include tags about motivation, self-help, productivity, or educational content.
Focus on: artist discovery, genre, mood, similar artists, music platform names.`
    };
  }

  // ============================================================
  // SPORTS CONTENT
  // ============================================================
  if (type === 'sports') {
    const sportSubtypeContext = {
      highlights: 'match/game highlights',
      behind_the_scenes: 'behind-the-scenes content',
      interview: 'athlete/coach interview',
      training: 'training/practice session',
      championship: 'championship/final match',
      event: 'sports event'
    };
    const contextDesc = sportSubtypeContext[subtype] || 'sports content';

    return {
      titleInstructions: `
SPORTS CONTENT DETECTED - This is ${contextDesc}.
Create SPORTS-appropriate titles:
- Include the sport name, event/competition name, teams/athletes involved
- Use sports terminology: "Highlights", "Match", "Championship", "Final", "vs", etc.
- Highlight key moments: "Amazing Rally", "Championship Point", "Gold Medal", etc.
- Include dates/seasons if relevant (e.g., "2024 Finals")
- DO NOT use music or entertainment terminology`,

      descriptionInstructions: `
SPORTS CONTENT DETECTED - This is ${contextDesc}.
Create a SPORTS description:

Include:
1. ðŸ† Event/Competition info (tournament name, round, date)
2. ðŸ‘¥ Teams/Athletes involved with relevant stats or rankings
3. ðŸ“ Venue/Location information
4. â±ï¸ Timestamps for key moments (goals, points, highlights)
5. ðŸ“Š Match/game results or scores (if applicable)
6. ðŸ”— Links to official sports channels, league websites
7. Sports-related hashtags (#[Sport] #[Tournament] #[Team/Athlete])

DO NOT include:
- Music-related content (genres, artists, streaming platforms)
- Motivational self-help content
- Unrelated entertainment content`,

      tagsInstructions: `
SPORTS CONTENT DETECTED - Generate SPORTS-specific tags:

1. Primary (5-8): Sport name, event/tournament name, athlete/team names
2. Secondary (8-12): League name, season/year, venue, competition round
3. Long-tail (10-15): "[Athlete] highlights 2024", "[Tournament] finals", "[Sport] best moments"
4. Trending (5-10): Current tournament names, trending athlete names, sports events

DO NOT include music, entertainment, or motivational tags.
Focus on: sport discovery, event coverage, athlete/team names, competition names.`
    };
  }

  // ============================================================
  // GAMING CONTENT
  // ============================================================
  if (type === 'gaming') {
    return {
      titleInstructions: `
GAMING CONTENT DETECTED - This is ${subtype} content.
Create GAMING-appropriate titles:
- Include the game name prominently
- Specify content type: Gameplay, Walkthrough, Review, Let's Play, etc.
- Use gaming hooks: "Epic Win", "Insane Play", "World Record", "Boss Fight", etc.
- Include relevant details: difficulty level, character/class, game mode`,

      descriptionInstructions: `
GAMING CONTENT DETECTED - Create a GAMING description:

Include:
1. ðŸŽ® Game info (name, platform, genre)
2. ðŸ“‹ Content type (gameplay, walkthrough, review, etc.)
3. â±ï¸ Timestamps for key moments, boss fights, achievements
4. ðŸ’» PC specs or console info if relevant
5. ðŸ”— Links to game store, streamer socials, Discord
6. Gaming hashtags (#[GameName] #Gaming #[Platform])

DO NOT include music or unrelated content.`,

      tagsInstructions: `
GAMING CONTENT - Generate GAMING tags:

1. Primary (5-8): Game name, platform, game genre, content type
2. Secondary (8-12): Game modes, characters, related games
3. Long-tail (10-15): "[Game] gameplay 2024", "[Game] walkthrough", "[Game] review"
4. Trending (5-10): Current gaming trends, popular games, esports terms`
    };
  }

  // ============================================================
  // EDUCATIONAL/TUTORIAL CONTENT
  // ============================================================
  if (type === 'tutorial' || type === 'howto' || type === 'education') {
    return {
      titleInstructions: `
EDUCATIONAL/TUTORIAL CONTENT DETECTED.
Create EDUCATIONAL titles:
- Focus on the problem being solved or skill being taught
- Use "How to", step counts, or clear outcome promises
- Include skill level if relevant (Beginner, Advanced, etc.)
- Be specific about what viewers will learn`,

      descriptionInstructions: `
EDUCATIONAL CONTENT - Create an EDUCATIONAL description:

Include:
1. ðŸ“š Clear problem statement or learning objective
2. ðŸ“ Numbered steps or chapter timestamps
3. ðŸ› ï¸ Tools, resources, or materials needed
4. ðŸ’¡ Key takeaways and practical tips
5. ðŸ”— Links to resources, downloads, related tutorials
6. Educational hashtags (#Tutorial #HowTo #[Topic])`,

      tagsInstructions: `
EDUCATIONAL CONTENT - Generate EDUCATIONAL tags:

1. Primary (5-8): Topic name, skill type, "tutorial", "how to"
2. Secondary (8-12): Related topics, tools mentioned, skill level
3. Long-tail (10-15): "[Topic] tutorial for beginners", "how to [action]", "learn [skill]"
4. Trending (5-10): Current trends in the topic area`
    };
  }

  // ============================================================
  // NEWS CONTENT
  // ============================================================
  if (type === 'news') {
    return {
      titleInstructions: `
NEWS CONTENT DETECTED.
Create NEWS-appropriate titles:
- Be factual and informative
- Include key facts: who, what, when, where
- Use news terminology: "Breaking", "Update", "Report", etc.
- Avoid sensationalism while maintaining engagement`,

      descriptionInstructions: `
NEWS CONTENT - Create a NEWS description:

Include:
1. ðŸ“° Summary of the news story
2. ðŸ“… Date and relevant timeline
3. ðŸ‘¥ Key people/organizations involved
4. ðŸ”— Links to sources and related coverage
5. News-related hashtags (#News #Breaking #[Topic])`,

      tagsInstructions: `
NEWS CONTENT - Generate NEWS tags:

1. Primary (5-8): Topic, key figures, location
2. Secondary (8-12): Related events, organizations
3. Long-tail (10-15): "[Topic] news 2024", "[Event] update"
4. Trending (5-10): Current news trends, breaking topics`
    };
  }

  // ============================================================
  // VLOG/LIFESTYLE CONTENT
  // ============================================================
  if (type === 'vlog' || type === 'videoblogging') {
    return {
      titleInstructions: `
VLOG/LIFESTYLE CONTENT DETECTED.
Create VLOG-appropriate titles:
- Be personal and relatable
- Include context: location, activity, occasion
- Use vlog hooks: "Day in My Life", "GRWM", "Come With Me", etc.
- Keep it authentic to the creator's style`,

      descriptionInstructions: `
VLOG CONTENT - Create a VLOG description:

Include:
1. ðŸ“ Location and context
2. ðŸ“‹ Brief summary of what happens
3. â±ï¸ Timestamps for different segments
4. ðŸ”— Links to creator's socials, products mentioned
5. Vlog hashtags (#Vlog #DayInMyLife #[Location])`,

      tagsInstructions: `
VLOG CONTENT - Generate VLOG tags:

1. Primary (5-8): Content type, location, activity
2. Secondary (8-12): Lifestyle topics, brands mentioned
3. Long-tail (10-15): "[Activity] vlog", "day in my life [location]"
4. Trending (5-10): Current lifestyle trends`
    };
  }

  // ============================================================
  // ENTERTAINMENT CONTENT
  // ============================================================
  if (type === 'entertainment' || type === 'comedy') {
    return {
      titleInstructions: `
ENTERTAINMENT CONTENT DETECTED.
Create ENTERTAINMENT-appropriate titles:
- Be engaging and attention-grabbing
- Match the tone of the content (funny, dramatic, etc.)
- Include key performers or show names if relevant`,

      descriptionInstructions: `
ENTERTAINMENT CONTENT - Create an ENTERTAINMENT description:

Include:
1. ðŸŽ¬ Content summary
2. ðŸ‘¥ Performers/creators involved
3. â±ï¸ Timestamps for key moments
4. ðŸ”— Links to related content, creator socials
5. Entertainment hashtags (#Entertainment #Comedy #[Show])`,

      tagsInstructions: `
ENTERTAINMENT CONTENT - Generate ENTERTAINMENT tags:

1. Primary (5-8): Content type, performers, show name
2. Secondary (8-12): Genre, related content
3. Long-tail (10-15): "[Performer] funny moments", "[Show] clips"
4. Trending (5-10): Current entertainment trends`
    };
  }

  // ============================================================
  // TRAVEL/EVENTS CONTENT
  // ============================================================
  if (type === 'travel_events') {
    return {
      titleInstructions: `
TRAVEL/EVENTS CONTENT DETECTED.
Create TRAVEL-appropriate titles:
- Include destination/event name prominently
- Use travel hooks: "Travel Guide", "Hidden Gems", "Must Visit", etc.
- Be specific about location and experience`,

      descriptionInstructions: `
TRAVEL CONTENT - Create a TRAVEL description:

Include:
1. ðŸ“ Destination/event details
2. ðŸ—“ï¸ Travel dates, event schedule
3. ðŸ’° Budget tips, costs mentioned
4. â±ï¸ Timestamps for different locations/activities
5. ðŸ”— Links to booking sites, travel resources
6. Travel hashtags (#Travel #[Destination] #[Event])`,

      tagsInstructions: `
TRAVEL CONTENT - Generate TRAVEL tags:

1. Primary (5-8): Destination, event name, travel type
2. Secondary (8-12): Activities, attractions, local terms
3. Long-tail (10-15): "[Destination] travel guide", "[Event] vlog"
4. Trending (5-10): Current travel trends, popular destinations`
    };
  }

  // ============================================================
  // DOCUMENTARY CONTENT
  // ============================================================
  if (type === 'documentary' || type === 'film_animation') {
    return {
      titleInstructions: `
DOCUMENTARY/FILM CONTENT DETECTED.
Create appropriate titles:
- Be informative and intriguing
- Include the subject matter clearly
- Use documentary terms if relevant: "The Story of", "Inside", "Exploring", etc.`,

      descriptionInstructions: `
DOCUMENTARY CONTENT - Create a description:

Include:
1. ðŸ“½ï¸ Subject/topic overview
2. ðŸŽ¬ Production details if relevant
3. â±ï¸ Chapter timestamps
4. ðŸ”— Links to related documentaries, sources
5. Relevant hashtags (#Documentary #[Topic])`,

      tagsInstructions: `
DOCUMENTARY CONTENT - Generate tags:

1. Primary (5-8): Subject matter, documentary type
2. Secondary (8-12): Related topics, people featured
3. Long-tail (10-15): "[Subject] documentary", "the story of [topic]"
4. Trending (5-10): Current documentary trends`
    };
  }

  // ============================================================
  // REVIEW/PRODUCT CONTENT
  // ============================================================
  if (type === 'review') {
    return {
      titleInstructions: `
REVIEW CONTENT DETECTED.
Create REVIEW-appropriate titles:
- Include product/item name clearly
- Use review hooks: "Honest Review", "Worth It?", "Unboxing", etc.
- Be specific about what's being reviewed`,

      descriptionInstructions: `
REVIEW CONTENT - Create a REVIEW description:

Include:
1. ðŸ“¦ Product/item details (name, specs, price)
2. âœ… Pros and cons summary
3. â­ Rating or verdict
4. â±ï¸ Timestamps for different aspects
5. ðŸ”— Links to purchase, affiliate links (disclosed)
6. Review hashtags (#Review #Unboxing #[Product])`,

      tagsInstructions: `
REVIEW CONTENT - Generate REVIEW tags:

1. Primary (5-8): Product name, brand, category
2. Secondary (8-12): Specs, features, alternatives
3. Long-tail (10-15): "[Product] review 2024", "[Brand] unboxing"
4. Trending (5-10): Current product trends`
    };
  }

  // ============================================================
  // PODCAST/INTERVIEW CONTENT
  // ============================================================
  if (type === 'podcast') {
    return {
      titleInstructions: `
PODCAST/INTERVIEW CONTENT DETECTED.
Create PODCAST-appropriate titles:
- Include podcast name and episode info
- Feature guest name prominently if applicable
- Highlight the main topic or most interesting point`,

      descriptionInstructions: `
PODCAST CONTENT - Create a PODCAST description:

Include:
1. ðŸŽ™ï¸ Podcast name and episode number
2. ðŸ‘¤ Guest introduction and credentials
3. ðŸ“‹ Topics discussed
4. â±ï¸ Timestamps for different topics
5. ðŸ”— Links to podcast platforms, guest socials
6. Podcast hashtags (#Podcast #[PodcastName] #[Topic])`,

      tagsInstructions: `
PODCAST CONTENT - Generate PODCAST tags:

1. Primary (5-8): Podcast name, guest name, main topic
2. Secondary (8-12): Topics discussed, related podcasts
3. Long-tail (10-15): "[Guest] interview", "[Topic] podcast"
4. Trending (5-10): Current podcast trends, trending topics`
    };
  }

  // ============================================================
  // DEFAULT - GENERAL CONTENT
  // ============================================================
  return {
    titleInstructions: `
Analyze the video content carefully and create titles appropriate for its actual subject matter.
- Match the tone and style to the content
- Be specific about what the video contains
- Avoid generic or mismatched terminology`,

    descriptionInstructions: `
Create a description that accurately represents the video content:
- Summarize the main content/topic
- Include relevant timestamps
- Add appropriate links and hashtags
- Match the description style to the content type`,

    tagsInstructions: `
Create tags relevant to the actual video content:
- Focus on the specific topic/subject
- Include relevant terminology for that niche
- Add trending tags related to the content
- Avoid generic or mismatched tags`
  };
}

async function generateTitlesInternal(metadata, transcript) {
  const transcriptText = transcript.fullText || '';

  // Detect content type using metadata
  const contentType = detectContentType(metadata);
  const context = getContentTypeContext(contentType);

  const titlePrompt = `Generate 3 YouTube titles for this video.

=== VIDEO METADATA ===
Title: ${metadata.title}
Channel: ${metadata.channelTitle}
Category ID: ${metadata.categoryId || 'Unknown'}
Tags: ${(metadata.tags || []).slice(0, 10).join(', ')}
Description: ${metadata.description?.substring(0, 500) || ''}
Transcript: ${transcriptText.substring(0, 1500)}

=== DETECTED CONTENT TYPE ===
Type: ${contentType.type} (${contentType.subtype})
Confidence: ${contentType.confidence}
Detection Source: ${contentType.source}

=== CRITICAL INSTRUCTIONS ===
${context.titleInstructions}

Create 3 titles (60-70 chars each) appropriate for ${contentType.type.toUpperCase()} content:
1. ATTENTION-GRABBING: Eye-catching but relevant to ${contentType.type}
2. SEO-OPTIMIZED: Keyword-rich for ${contentType.type} discovery
3. DESCRIPTIVE: Clear about what the content actually is

Return ONLY valid JSON:
{
  "clickbait": "title",
  "seo": "title",
  "question": "title",
  "detectedType": "${contentType.type}"
}`;

  const systemPrompt = contentType.type === 'music'
    ? 'You are a music industry expert. Create titles appropriate for music content. Never create motivational or self-help titles for songs. Return only valid JSON.'
    : 'Create engaging YouTube titles appropriate for the detected content type. Return only valid JSON.';

  const completion = await openai.chat.completions.create({
    model: 'gpt-4o',
    messages: [
      { role: 'system', content: systemPrompt },
      { role: 'user', content: titlePrompt }
    ],
    temperature: 0.8,
    max_tokens: 400
  });

  try {
    const responseText = completion.choices[0].message.content.trim();
    const cleanJson = responseText.replace(/```json\n?/g, '').replace(/```\n?/g, '').trim();
    const result = JSON.parse(cleanJson);
    result.detectedType = contentType.type; // Ensure we use our detected type
    return result;
  } catch (error) {
    return {
      clickbait: metadata.title,
      seo: metadata.title,
      question: metadata.title,
      detectedType: contentType.type
    };
  }
}

async function generateDescriptionInternal(metadata, transcript) {
  const transcriptText = transcript.fullText || '';
  const durationSeconds = parseDuration(metadata.duration);

  // Detect content type using metadata
  const contentType = detectContentType(metadata);
  const context = getContentTypeContext(contentType);

  const descriptionPrompt = `Create a YouTube description for this video.

=== VIDEO METADATA ===
Title: ${metadata.title}
Channel: ${metadata.channelTitle}
Category ID: ${metadata.categoryId || 'Unknown'}
Duration: ${formatTimestamp(durationSeconds)}
Tags: ${(metadata.tags || []).slice(0, 10).join(', ')}
Transcript: ${transcriptText.substring(0, 2500)}

=== DETECTED CONTENT TYPE ===
Type: ${contentType.type} (${contentType.subtype})
Confidence: ${contentType.confidence}
Detection Source: ${contentType.source}

=== CRITICAL INSTRUCTIONS ===
${context.descriptionInstructions}

Create a description that is SPECIFICALLY appropriate for ${contentType.type.toUpperCase()} content.`;

  const systemPrompt = contentType.type === 'music'
    ? `You are a music industry professional writing descriptions for music releases.
NEVER write motivational or self-help content for songs.
Focus on: artist info, track details, genre, streaming links, credits.
Write in the style of official music channel descriptions.`
    : `Create engaging YouTube descriptions appropriate for ${contentType.type} content.`;

  const completion = await openai.chat.completions.create({
    model: 'gpt-4o',
    messages: [
      { role: 'system', content: systemPrompt },
      { role: 'user', content: descriptionPrompt }
    ],
    temperature: 0.7,
    max_tokens: 1000
  });

  return completion.choices[0].message.content.trim();
}

async function generateTagsInternal(metadata, transcript) {
  const transcriptText = transcript.fullText || '';

  // Detect content type using metadata
  const contentType = detectContentType(metadata);
  const context = getContentTypeContext(contentType);

  const tagsPrompt = `Generate YouTube tags for this video.

=== VIDEO METADATA ===
Title: ${metadata.title}
Channel: ${metadata.channelTitle}
Category ID: ${metadata.categoryId || 'Unknown'}
Existing Tags: ${(metadata.tags || []).join(', ')}
Transcript: ${transcriptText.substring(0, 1500)}

=== DETECTED CONTENT TYPE ===
Type: ${contentType.type} (${contentType.subtype})
Confidence: ${contentType.confidence}
Detection Source: ${contentType.source}

=== CRITICAL INSTRUCTIONS ===
${context.tagsInstructions}

Generate 30-50 tags SPECIFICALLY for ${contentType.type.toUpperCase()} content in these categories:
1. Primary (5-8): Core ${contentType.type} tags
2. Secondary (8-12): Related ${contentType.type} tags
3. Long-tail (10-15): Specific search phrases for ${contentType.type}
4. Trending (5-10): Current trends in ${contentType.type}

Return ONLY valid JSON:
{
  "primary": ["tag1"],
  "secondary": ["tag2"],
  "longTail": ["phrase"],
  "trending": ["trend"]
}`;

  const systemPrompt = contentType.type === 'music'
    ? `You are a music SEO expert. Generate tags for music discovery.
Focus on: artist names, song titles, genres, moods, similar artists, music platforms.
NEVER include motivational, self-help, or productivity tags for music content.
Return only valid JSON.`
    : `Generate YouTube tags appropriate for ${contentType.type} content. Return only valid JSON.`;

  const completion = await openai.chat.completions.create({
    model: 'gpt-4o',
    messages: [
      { role: 'system', content: systemPrompt },
      { role: 'user', content: tagsPrompt }
    ],
    temperature: 0.7,
    max_tokens: 800
  });

  try {
    const responseText = completion.choices[0].message.content.trim();
    const cleanJson = responseText.replace(/```json\n?/g, '').replace(/```\n?/g, '').trim();
    return JSON.parse(cleanJson);
  } catch (error) {
    return { primary: [], secondary: [], longTail: [], trending: [] };
  }
}

// ==============================================
// VIDEO OPTIMIZER (WITH AUTH & LIMITS)
// ==============================================

exports.optimizeVideo = functions.https.onCall(async (data, context) => {
  try {
    const uid = await verifyAuth(context);

    // SECURITY: Burst rate limiting (max 5 optimization requests per minute)
    checkRateLimit(uid, 'optimizeVideo', 5);

    const usageCheck = await checkUsageLimit(uid, 'warpOptimizer');

    const { videoUrl } = data;
    if (!videoUrl) throw new functions.https.HttpsError('invalid-argument', 'Video URL required');

    const startTime = Date.now();
    const videoId = extractVideoId(videoUrl);
    const metadata = await getVideoMetadata(videoId);
    const transcript = await getVideoTranscript(videoId);

    const [titlesResult, description, tagsResult] = await Promise.all([
      generateTitlesInternal(metadata, transcript),
      generateDescriptionInternal(metadata, transcript),
      generateTagsInternal(metadata, transcript)
    ]);

    // Convert titles object to array for frontend
    const titlesArray = [
      titlesResult.clickbait || metadata.title,
      titlesResult.seo || metadata.title,
      titlesResult.question || metadata.title
    ].filter(Boolean);

    // Flatten tags object to array for frontend
    const tagsArray = [
      ...(tagsResult.primary || []),
      ...(tagsResult.secondary || []),
      ...(tagsResult.longTail || []),
      ...(tagsResult.trending || [])
    ];

    const processingTime = Math.round((Date.now() - startTime) / 1000);

    // Calculate SEO score using the arrays
    const seoScore = Math.min(100, Math.round(
      (titlesArray.length * 10) +
      (description && description.length > 200 ? 20 : 10) +
      (Math.min(tagsArray.length, 30) * 1.5) +
      (metadata.viewCount > 10000 ? 15 : 5)
    ));

    const seoRecommendations = [];
    if (titlesArray.length < 3) seoRecommendations.push('Consider adding more title variations');
    if (!description || description.length < 200) seoRecommendations.push('Description could be more detailed');
    if (tagsArray.length < 15) seoRecommendations.push('Add more relevant tags for better discoverability');
    if (tagsArray.length > 0 && tagsArray.length < 30) seoRecommendations.push('Try to use 30-50 tags for maximum reach');

    const seoAnalysis = {
      score: seoScore,
      recommendations: seoRecommendations
    };

    // Prepare data for Firestore (ensure no undefined values)
    const videoInfo = {
      title: metadata.title || '',
      channelTitle: metadata.channelTitle || '',
      viewCount: metadata.viewCount || 0,
      duration: metadata.duration || '',
      thumbnail: metadata.thumbnail || ''
    };

    // Save to optimizations collection (for history)
    const optimizationRef = await db.collection('optimizations').add({
      userId: uid,
      videoUrl: videoUrl || '',
      videoInfo,
      titles: titlesArray,
      description: description || '',
      tags: tagsArray,
      seoAnalysis,
      timestamp: Date.now(),
      createdAt: admin.firestore.FieldValue.serverTimestamp(),
      updatedAt: admin.firestore.FieldValue.serverTimestamp()
    });

    await incrementUsage(uid, 'warpOptimizer');
    await logUsage(uid, 'warp_optimizer_used', { videoId, processingTime });

    return {
      success: true,
      optimizationId: optimizationRef.id,
      videoInfo,
      titles: titlesArray,
      description: description || '',
      tags: tagsArray,
      seoAnalysis,
      usageRemaining: usageCheck.remaining
    };
  } catch (error) {
    if (context.auth) {
      await logUsage(context.auth.uid, 'warp_optimizer_failed', { error: error.message });
    }
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Video optimization failed. Please try again.'));
  }
});

// Title generator - uses warpOptimizer quota (included in Warp Optimizer tool)
exports.generateTitles = functions.https.onCall(async (data, context) => {
  try {
    const uid = await verifyAuth(context);
    checkRateLimit(uid, 'generateTitles', 10);
    // Uses warpOptimizer quota since this is part of the optimization suite
    const usageCheck = await checkUsageLimit(uid, 'warpOptimizer');
    const { videoUrl } = data;
    if (!videoUrl) throw new functions.https.HttpsError('invalid-argument', 'Video URL required');

    const videoId = extractVideoId(videoUrl);
    const metadata = await getVideoMetadata(videoId);
    const transcript = await getVideoTranscript(videoId);
    const titles = await generateTitlesInternal(metadata, transcript);

    await incrementUsage(uid, 'warpOptimizer');
    await logUsage(uid, 'title_generator_used', { videoId });

    return { success: true, videoData: metadata, titles, usageRemaining: usageCheck.remaining };
  } catch (error) {
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Title generation failed. Please try again.'));
  }
});

// Description generator - uses warpOptimizer quota (included in Warp Optimizer tool)
exports.generateDescription = functions.https.onCall(async (data, context) => {
  try {
    const uid = await verifyAuth(context);
    checkRateLimit(uid, 'generateDescription', 10);
    // Uses warpOptimizer quota since this is part of the optimization suite
    const usageCheck = await checkUsageLimit(uid, 'warpOptimizer');
    const { videoUrl } = data;
    if (!videoUrl) throw new functions.https.HttpsError('invalid-argument', 'Video URL required');

    const videoId = extractVideoId(videoUrl);
    const metadata = await getVideoMetadata(videoId);
    const transcript = await getVideoTranscript(videoId);
    const description = await generateDescriptionInternal(metadata, transcript);

    await incrementUsage(uid, 'warpOptimizer');
    await logUsage(uid, 'description_generator_used', { videoId });

    return { success: true, videoData: metadata, description, usageRemaining: usageCheck.remaining };
  } catch (error) {
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Description generation failed. Please try again.'));
  }
});

// Tag generator - uses warpOptimizer quota (included in Warp Optimizer tool)
exports.generateTags = functions.https.onCall(async (data, context) => {
  try {
    const uid = await verifyAuth(context);
    checkRateLimit(uid, 'generateTags', 10);
    // Uses warpOptimizer quota since this is part of the optimization suite
    const usageCheck = await checkUsageLimit(uid, 'warpOptimizer');
    const { videoUrl } = data;
    if (!videoUrl) throw new functions.https.HttpsError('invalid-argument', 'Video URL required');

    const videoId = extractVideoId(videoUrl);
    const metadata = await getVideoMetadata(videoId);
    const transcript = await getVideoTranscript(videoId);
    const tags = await generateTagsInternal(metadata, transcript);

    await incrementUsage(uid, 'warpOptimizer');
    await logUsage(uid, 'tag_generator_used', { videoId });

    return { success: true, videoData: metadata, tags, usageRemaining: usageCheck.remaining };
  } catch (error) {
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Tag generation failed. Please try again.'));
  }
});

// ==============================================
// USER DASHBOARD
// ==============================================

exports.getUserProfile = functions.https.onCall(async (data, context) => {
  if (!context.auth) {
    throw new functions.https.HttpsError('unauthenticated', 'User must be logged in');
  }

  const uid = context.auth.uid;

  try {
    const userRef = db.collection('users').doc(uid);
    const userSnap = await userRef.get();

    // Get custom reset time from settings (default 1440 minutes = 24 hours)
    let resetMinutes = 1440;
    try {
      const settingsDoc = await db.collection('settings').doc('quotaSettings').get();
      if (settingsDoc.exists && settingsDoc.data().resetTimeMinutes) {
        resetMinutes = settingsDoc.data().resetTimeMinutes;
      }
    } catch (e) {
      console.log('Using default reset time');
    }
    const resetIntervalMs = resetMinutes * 60 * 1000;

    let userData;

    if (!userSnap.exists) {
      // Create new user with default free plan
      userData = {
        uid: uid,
        email: context.auth.token?.email || '',
        createdAt: new Date().toISOString(),
        lastLoginAt: new Date().toISOString(),
        isActive: true,
        isAdmin: false,
        subscription: { plan: 'free', status: 'active' },
        usage: {
          warpOptimizer: { usedToday: 0, limit: 3, lastResetAt: admin.firestore.FieldValue.serverTimestamp() },
          competitorAnalysis: { usedToday: 0, limit: 3, lastResetAt: admin.firestore.FieldValue.serverTimestamp() },
          trendPredictor: { usedToday: 0, limit: 3, lastResetAt: admin.firestore.FieldValue.serverTimestamp() },
          thumbnailGenerator: { usedToday: 0, limit: 3, lastResetAt: admin.firestore.FieldValue.serverTimestamp() },
          channelAudit: { usedToday: 0, limit: 3, lastResetAt: admin.firestore.FieldValue.serverTimestamp() }
        }
      };
      await userRef.set(userData);
      // Re-fetch to get the server timestamps
      const newSnap = await userRef.get();
      userData = newSnap.data();
    } else {
      userData = userSnap.data();
    }

    // Build quotaInfo with bonus uses included
    const tools = ['warpOptimizer', 'competitorAnalysis', 'trendPredictor', 'thumbnailGenerator', 'channelAudit'];
    const quotaInfo = {};
    const now = Date.now();

    // Ensure userData.usage has all tool keys (for existing users with old structure)
    if (!userData.usage) {
      userData.usage = {};
    }

    // Track if any updates are needed
    const updates = {};

    for (const tool of tools) {
      // Add default usage data for missing tools
      if (!userData.usage[tool]) {
        userData.usage[tool] = { usedToday: 0, limit: 2 };
        updates[`usage.${tool}.usedToday`] = 0;
        updates[`usage.${tool}.limit`] = 2;
        updates[`usage.${tool}.lastResetAt`] = admin.firestore.FieldValue.serverTimestamp();
      }

      const usage = userData.usage[tool];
      const bonusUses = userData.bonusUses?.[tool] || 0;
      const baseLimit = usage.limit || 2;
      const totalLimit = baseLimit + bonusUses;

      // Calculate last reset time in milliseconds
      let lastResetTime = 0;
      if (usage.lastResetAt) {
        if (usage.lastResetAt.toMillis) {
          lastResetTime = usage.lastResetAt.toMillis();
        } else if (typeof usage.lastResetAt === 'string') {
          lastResetTime = new Date(usage.lastResetAt).getTime();
        } else if (typeof usage.lastResetAt === 'object') {
          lastResetTime = (usage.lastResetAt.seconds || usage.lastResetAt._seconds || 0) * 1000;
        }
      }

      // Check if quota should be reset
      const nextResetMs = lastResetTime + resetIntervalMs;
      let usedToday = usage.usedToday || 0;

      if (lastResetTime > 0 && now >= nextResetMs) {
        // Quota should be reset - update in database
        updates[`usage.${tool}.usedToday`] = 0;
        updates[`usage.${tool}.lastResetAt`] = admin.firestore.FieldValue.serverTimestamp();
        usedToday = 0;
        // New next reset will be from now
        quotaInfo[tool] = {
          baseLimit: baseLimit,
          bonusUses: bonusUses,
          totalLimit: totalLimit,
          usedToday: 0,
          remaining: totalLimit,
          nextResetMs: now + resetIntervalMs
        };
      } else {
        quotaInfo[tool] = {
          baseLimit: baseLimit,
          bonusUses: bonusUses,
          totalLimit: totalLimit,
          usedToday: usedToday,
          remaining: Math.max(0, totalLimit - usedToday),
          nextResetMs: lastResetTime > 0 ? nextResetMs : now + resetIntervalMs
        };
      }
    }

    // Apply any pending updates
    if (Object.keys(updates).length > 0) {
      await userRef.update(updates);
    }

    // Convert Firestore Timestamps to ISO strings for serialization
    if (userData.createdAt?.toDate) userData.createdAt = userData.createdAt.toDate().toISOString();
    if (userData.lastLoginAt?.toDate) userData.lastLoginAt = userData.lastLoginAt.toDate().toISOString();
    if (userData.subscription?.startDate?.toDate) userData.subscription.startDate = userData.subscription.startDate.toDate().toISOString();

    // Convert usage timestamps
    tools.forEach(tool => {
      if (userData.usage?.[tool]?.lastResetAt?.toDate) {
        userData.usage[tool].lastResetAt = userData.usage[tool].lastResetAt.toDate().toISOString();
      }
    });

    return {
      success: true,
      profile: userData,
      quotaInfo: quotaInfo,
      resetTimeMinutes: resetMinutes
    };

  } catch (error) {
    console.error('getUserProfile error:', error.message);
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to load profile. Please try again.'));
  }
});

exports.getHistory = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  checkRateLimit(uid, 'getHistory', 20);

  const { limit = 20, offset = 0 } = data || {};

  // SECURITY: Bound limit and offset to prevent resource exhaustion
  const safeLimit = Math.min(Math.max(1, parseInt(limit) || 20), 100);
  const safeOffset = Math.max(0, parseInt(offset) || 0);

  const snapshot = await db.collection('optimizations')
    .where('userId', '==', uid)
    .orderBy('createdAt', 'desc')
    .limit(safeLimit)
    .offset(safeOffset)
    .get();

  const history = [];
  snapshot.forEach(doc => {
    history.push({
      id: doc.id,
      ...doc.data(),
      createdAt: doc.data().createdAt?.toDate().toISOString()
    });
  });

  return { success: true, history, count: history.length };
});

exports.deleteOptimization = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const { optimizationId } = data;
  
  const doc = await db.collection('optimizations').doc(optimizationId).get();
  if (!doc.exists || doc.data().userId !== uid) {
    throw new functions.https.HttpsError('permission-denied', 'Not authorized');
  }
  
  await db.collection('optimizations').doc(optimizationId).delete();
  return { success: true };
});

// ==============================================
// ADMIN DASHBOARD
// ==============================================

exports.adminGetUsers = functions.https.onCall(async (data, context) => {
  try {
    await requireAdmin(context);

    // Handle case where data might be null/undefined
    const safeData = data || {};
    const limitCount = safeData.limit || 100;
    const planFilter = safeData.plan || null;
    const searchQuery = safeData.search || null;
    const verifiedFilter = safeData.verifiedOnly || false;

    let query = db.collection('users').orderBy('createdAt', 'desc').limit(limitCount);
    if (planFilter && planFilter !== 'all') {
      query = query.where('subscription.plan', '==', planFilter);
    }

    const snapshot = await query.get();
    let users = [];
    const userIds = [];

    snapshot.forEach(doc => {
      userIds.push(doc.id);
      const userData = doc.data();

      // Calculate subscription status
      let subscriptionStatus = 'free';
      const plan = userData.subscription?.plan || 'free';
      const endDate = userData.subscription?.endDate;

      if (plan !== 'free') {
        if (!endDate) {
          subscriptionStatus = 'lifetime';
        } else {
          const endDateMs = endDate.toDate ? endDate.toDate().getTime() : endDate;
          const now = Date.now();
          const daysLeft = Math.ceil((endDateMs - now) / (1000 * 60 * 60 * 24));

          if (daysLeft < 0) {
            subscriptionStatus = 'expired';
          } else if (daysLeft <= 7) {
            subscriptionStatus = 'expiring';
          } else {
            subscriptionStatus = 'active';
          }
        }
      }

      users.push({
        uid: doc.id,
        email: userData.email || '',
        displayName: userData.displayName || '',
        clientAlias: userData.clientAlias || '',
        isFiverrVerified: userData.isFiverrVerified || false,
        adminNotes: userData.adminNotes || '',
        tags: userData.tags || [],
        subscription: {
          ...(userData.subscription || { plan: 'free' }),
          duration: userData.subscription?.duration || null,
          endDate: userData.subscription?.endDate?.toDate?.()?.toISOString() || null,
          startDate: userData.subscription?.startDate?.toDate?.()?.toISOString() || null
        },
        subscriptionStatus,
        usage: userData.usage || {},
        bonusUses: userData.bonusUses || {},
        isAdmin: userData.isAdmin || false,
        createdAt: userData.createdAt?.toDate?.()?.toISOString() || null,
        lastLoginAt: userData.lastLoginAt?.toDate?.()?.toISOString() || null,
        tokens: null // Will be populated below
      });
    });

    // Fetch token balances from creativeTokens collection for all users
    if (userIds.length > 0) {
      // Batch fetch in chunks of 10 (Firestore limit for 'in' queries)
      const tokenMap = {};
      for (let i = 0; i < userIds.length; i += 10) {
        const chunk = userIds.slice(i, i + 10);
        const tokenDocs = await Promise.all(
          chunk.map(uid => db.collection('creativeTokens').doc(uid).get())
        );
        tokenDocs.forEach((doc, index) => {
          if (doc.exists) {
            const tokenData = doc.data();
            tokenMap[chunk[index]] = {
              balance: tokenData.balance || 0,
              rollover: tokenData.rollover || 0,
              plan: tokenData.plan || 'free',
              monthlyAllocation: tokenData.monthlyAllocation || 0,
              lastRefresh: tokenData.lastRefresh?.toDate?.()?.toISOString() || null
            };
          }
        });
      }

      // Attach token data to users
      users = users.map(user => ({
        ...user,
        tokens: tokenMap[user.uid] || { balance: 0, rollover: 0, plan: 'free' }
      }));
    }

    // Apply client-side filters (search and verified)
    if (searchQuery && searchQuery.trim()) {
      const search = searchQuery.toLowerCase().trim();
      users = users.filter(u =>
        (u.email && u.email.toLowerCase().includes(search)) ||
        (u.clientAlias && u.clientAlias.toLowerCase().includes(search)) ||
        (u.uid && u.uid.toLowerCase().includes(search))
      );
    }

    if (verifiedFilter) {
      users = users.filter(u => u.isFiverrVerified);
    }

    return { success: true, users, count: users.length };
  } catch (error) {
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to fetch users. Please try again.'));
  }
});

exports.adminUpdateUserPlan = functions.https.onCall(async (data, context) => {
  try {
    await requireAdmin(context);
    const { userId, plan, newPlan } = data || {};
    const targetPlan = plan || newPlan; // Accept both 'plan' and 'newPlan'

    if (!userId) throw new functions.https.HttpsError('invalid-argument', 'User ID required');
    if (!targetPlan) throw new functions.https.HttpsError('invalid-argument', 'Plan required');

    const planDoc = await db.collection('subscriptionPlans').doc(targetPlan).get();
    if (!planDoc.exists) throw new functions.https.HttpsError('invalid-argument', 'Invalid plan: ' + targetPlan);

    const planLimits = planDoc.data()?.limits || {};
    const defaultToolLimit = 2;

    // Create complete usage structures for all tools (ensures tools exist even if missing)
    await db.collection('users').doc(userId).update({
      'subscription.plan': targetPlan,
      'subscription.startDate': admin.firestore.FieldValue.serverTimestamp(),
      'usage.warpOptimizer': {
        usedToday: 0,
        limit: planLimits.warpOptimizer?.dailyLimit || defaultToolLimit,
        lastResetAt: admin.firestore.FieldValue.serverTimestamp(),
        cooldownUntil: null
      },
      'usage.competitorAnalysis': {
        usedToday: 0,
        limit: planLimits.competitorAnalysis?.dailyLimit || defaultToolLimit,
        lastResetAt: admin.firestore.FieldValue.serverTimestamp(),
        cooldownUntil: null
      },
      'usage.trendPredictor': {
        usedToday: 0,
        limit: planLimits.trendPredictor?.dailyLimit || defaultToolLimit,
        lastResetAt: admin.firestore.FieldValue.serverTimestamp(),
        cooldownUntil: null
      },
      'usage.thumbnailGenerator': {
        usedToday: 0,
        limit: planLimits.thumbnailGenerator?.dailyLimit || defaultToolLimit,
        lastResetAt: admin.firestore.FieldValue.serverTimestamp(),
        cooldownUntil: null
      }
    });

    await logUsage(userId, 'plan_changed_by_admin', { plan: targetPlan, changedBy: context.auth.uid });

    // Log activity
    await logUserActivity(userId, 'subscription_change', { action: 'set_plan', plan: targetPlan }, context.auth.uid);

    return { success: true, message: 'User plan updated to ' + targetPlan };
  } catch (error) {
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to update user plan. Please try again.'));
  }
});

exports.adminSetCustomLimits = functions.https.onCall(async (data, context) => {
  const adminUid = await requireAdmin(context);

  const { userId, tool, limit, cooldownHours } = data || {};

  // SECURITY FIX: Validate all inputs
  if (!userId || typeof userId !== 'string') {
    throw new functions.https.HttpsError('invalid-argument', 'Valid user ID is required');
  }

  if (!tool || typeof tool !== 'string') {
    throw new functions.https.HttpsError('invalid-argument', 'Tool name is required');
  }

  // Validate tool is one of the allowed values to prevent field injection
  const validTools = ['warpOptimizer', 'competitorAnalysis', 'trendPredictor', 'thumbnailGenerator'];
  if (!validTools.includes(tool)) {
    throw new functions.https.HttpsError(
      'invalid-argument',
      `Invalid tool: ${tool}. Must be one of: ${validTools.join(', ')}`
    );
  }

  // Validate limit is a positive number
  const safeLimit = parseInt(limit);
  if (isNaN(safeLimit) || safeLimit < 0 || safeLimit > 10000) {
    throw new functions.https.HttpsError('invalid-argument', 'Limit must be a number between 0 and 10000');
  }

  // Validate cooldownHours is a non-negative number
  const safeCooldown = parseInt(cooldownHours) || 0;
  if (safeCooldown < 0 || safeCooldown > 720) {
    throw new functions.https.HttpsError('invalid-argument', 'Cooldown hours must be between 0 and 720');
  }

  // Verify user exists
  const userDoc = await db.collection('users').doc(userId).get();
  if (!userDoc.exists) {
    throw new functions.https.HttpsError('not-found', 'User not found');
  }

  await db.collection('users').doc(userId).update({
    [`usage.${tool}.limit`]: safeLimit,
    [`customLimits.${tool}`]: { limit: safeLimit, cooldownHours: safeCooldown }
  });

  await logUsage(userId, 'custom_limits_set', {
    tool,
    limit: safeLimit,
    cooldownHours: safeCooldown,
    setBy: adminUid
  });

  return { success: true, message: `Custom limits set for ${tool}` };
});

// ==============================================
// ADMIN: Client Management Functions
// ==============================================

// Update user subscription with duration (calculates endDate)
exports.adminUpdateUserSubscription = functions.https.onCall(async (data, context) => {
  const adminUid = await requireAdmin(context);

  const { userId, plan, duration } = data || {};

  if (!userId) throw new functions.https.HttpsError('invalid-argument', 'User ID required');
  if (!plan) throw new functions.https.HttpsError('invalid-argument', 'Plan required');

  // Validate plan
  const validPlans = ['free', 'lite', 'pro', 'enterprise'];
  if (!validPlans.includes(plan)) {
    throw new functions.https.HttpsError('invalid-argument', 'Invalid plan');
  }

  // Validate duration
  const validDurations = ['week', 'month', '3months', 'year', 'lifetime', null];
  if (duration && !validDurations.includes(duration)) {
    throw new functions.https.HttpsError('invalid-argument', 'Invalid duration');
  }

  // Verify user exists
  const userDoc = await db.collection('users').doc(userId).get();
  if (!userDoc.exists) {
    throw new functions.https.HttpsError('not-found', 'User not found');
  }

  // Get plan limits
  const planDoc = await db.collection('subscriptionPlans').doc(plan).get();
  const planLimits = planDoc.exists ? planDoc.data()?.limits || {} : {};
  const defaultToolLimit = 2;

  // Calculate end date based on duration
  let endDate = null;
  const now = new Date();

  if (plan !== 'free' && duration && duration !== 'lifetime') {
    switch (duration) {
      case 'week':
        endDate = new Date(now.getTime() + 7 * 24 * 60 * 60 * 1000);
        break;
      case 'month':
        endDate = new Date(now.getTime() + 30 * 24 * 60 * 60 * 1000);
        break;
      case '3months':
        endDate = new Date(now.getTime() + 90 * 24 * 60 * 60 * 1000);
        break;
      case 'year':
        endDate = new Date(now.getTime() + 365 * 24 * 60 * 60 * 1000);
        break;
    }
  }

  // Build update object
  const updateData = {
    'subscription.plan': plan,
    'subscription.status': plan === 'free' ? 'free' : 'active',
    'subscription.startDate': admin.firestore.FieldValue.serverTimestamp(),
    'subscription.endDate': endDate ? admin.firestore.Timestamp.fromDate(endDate) : null,
    'subscription.duration': duration || null,
    'usage.warpOptimizer': {
      usedToday: 0,
      limit: planLimits.warpOptimizer?.dailyLimit || defaultToolLimit,
      lastResetAt: admin.firestore.FieldValue.serverTimestamp(),
      cooldownUntil: null
    },
    'usage.competitorAnalysis': {
      usedToday: 0,
      limit: planLimits.competitorAnalysis?.dailyLimit || defaultToolLimit,
      lastResetAt: admin.firestore.FieldValue.serverTimestamp(),
      cooldownUntil: null
    },
    'usage.trendPredictor': {
      usedToday: 0,
      limit: planLimits.trendPredictor?.dailyLimit || defaultToolLimit,
      lastResetAt: admin.firestore.FieldValue.serverTimestamp(),
      cooldownUntil: null
    },
    'usage.thumbnailGenerator': {
      usedToday: 0,
      limit: planLimits.thumbnailGenerator?.dailyLimit || defaultToolLimit,
      lastResetAt: admin.firestore.FieldValue.serverTimestamp(),
      cooldownUntil: null
    }
  };

  await db.collection('users').doc(userId).update(updateData);

  await logUsage(userId, 'subscription_updated_by_admin', {
    plan,
    duration,
    endDate: endDate ? endDate.toISOString() : null,
    changedBy: adminUid
  });

  return {
    success: true,
    message: `Subscription updated to ${plan}` + (duration ? ` for ${duration}` : ''),
    endDate: endDate ? endDate.toISOString() : null
  };
});

// Set client alias (Fiverr username)
exports.adminSetClientAlias = functions.https.onCall(async (data, context) => {
  await requireAdmin(context);

  const { userId, alias } = data || {};

  if (!userId) throw new functions.https.HttpsError('invalid-argument', 'User ID required');

  // Verify user exists
  const userDoc = await db.collection('users').doc(userId).get();
  if (!userDoc.exists) {
    throw new functions.https.HttpsError('not-found', 'User not found');
  }

  // Sanitize alias (alphanumeric, underscores, max 50 chars)
  const sanitizedAlias = (alias || '').trim().substring(0, 50);

  await db.collection('users').doc(userId).update({
    clientAlias: sanitizedAlias
  });

  // Log activity
  await logUserActivity(userId, 'profile_update', { field: 'alias', value: sanitizedAlias }, context.auth.uid);

  return { success: true, message: 'Client alias updated' };
});

// Toggle Fiverr verified status
exports.adminSetFiverrVerified = functions.https.onCall(async (data, context) => {
  await requireAdmin(context);

  const { userId, verified } = data || {};

  if (!userId) throw new functions.https.HttpsError('invalid-argument', 'User ID required');
  if (typeof verified !== 'boolean') throw new functions.https.HttpsError('invalid-argument', 'Verified status must be boolean');

  // Verify user exists
  const userDoc = await db.collection('users').doc(userId).get();
  if (!userDoc.exists) {
    throw new functions.https.HttpsError('not-found', 'User not found');
  }

  await db.collection('users').doc(userId).update({
    isFiverrVerified: verified
  });

  // Log activity
  await logUserActivity(userId, 'profile_update', { field: 'fiverr_verified', value: verified }, context.auth.uid);

  return { success: true, message: verified ? 'User marked as Fiverr verified' : 'Fiverr verification removed' };
});

// Update admin notes for a user
exports.adminUpdateUserNotes = functions.https.onCall(async (data, context) => {
  await requireAdmin(context);

  const { userId, notes } = data || {};

  if (!userId) throw new functions.https.HttpsError('invalid-argument', 'User ID required');

  // Verify user exists
  const userDoc = await db.collection('users').doc(userId).get();
  if (!userDoc.exists) {
    throw new functions.https.HttpsError('not-found', 'User not found');
  }

  // Sanitize notes (max 2000 chars)
  const sanitizedNotes = (notes || '').substring(0, 2000);

  await db.collection('users').doc(userId).update({
    adminNotes: sanitizedNotes
  });

  return { success: true, message: 'Notes updated' };
});

// Extend subscription by duration (quick action)
exports.adminExtendSubscription = functions.https.onCall(async (data, context) => {
  const adminUid = await requireAdmin(context);

  const { userId, extensionDays } = data || {};

  if (!userId) throw new functions.https.HttpsError('invalid-argument', 'User ID required');
  if (!extensionDays || extensionDays < 1 || extensionDays > 365) {
    throw new functions.https.HttpsError('invalid-argument', 'Extension days must be between 1 and 365');
  }

  // Verify user exists
  const userDoc = await db.collection('users').doc(userId).get();
  if (!userDoc.exists) {
    throw new functions.https.HttpsError('not-found', 'User not found');
  }

  const userData = userDoc.data();
  const currentPlan = userData.subscription?.plan || 'free';

  if (currentPlan === 'free') {
    throw new functions.https.HttpsError('failed-precondition', 'Cannot extend free plan. Set a paid plan first.');
  }

  // Calculate new end date
  let baseDate = new Date();
  if (userData.subscription?.endDate) {
    const existingEnd = userData.subscription.endDate.toDate();
    // If existing end date is in the future, extend from there
    if (existingEnd > baseDate) {
      baseDate = existingEnd;
    }
  }

  const newEndDate = new Date(baseDate.getTime() + extensionDays * 24 * 60 * 60 * 1000);

  await db.collection('users').doc(userId).update({
    'subscription.endDate': admin.firestore.Timestamp.fromDate(newEndDate),
    'subscription.status': 'active'
  });

  await logUsage(userId, 'subscription_extended', {
    extensionDays,
    newEndDate: newEndDate.toISOString(),
    extendedBy: adminUid
  });

  // Log activity
  await logUserActivity(userId, 'subscription_change', { action: 'extend', days: extensionDays, newEndDate: newEndDate.toISOString() }, adminUid);

  return {
    success: true,
    message: `Subscription extended by ${extensionDays} days`,
    newEndDate: newEndDate.toISOString()
  };
});

// Scheduled function: Check expired subscriptions daily and revert to free
// Runs every day at midnight UTC
exports.checkExpiredSubscriptions = functions.pubsub
  .schedule('0 0 * * *')
  .timeZone('UTC')
  .onRun(async (context) => {
    console.log('Running subscription expiry check...');

    const now = admin.firestore.Timestamp.now();
    let expiredCount = 0;
    let expiringCount = 0;

    try {
      // Find users with expired subscriptions (endDate < now and plan != free)
      const expiredSnapshot = await db.collection('users')
        .where('subscription.endDate', '<', now)
        .where('subscription.plan', '!=', 'free')
        .get();

      const batch = db.batch();

      for (const doc of expiredSnapshot.docs) {
        const userData = doc.data();
        const plan = userData.subscription?.plan;

        // Skip if already free
        if (plan === 'free') continue;

        console.log(`Expiring subscription for user: ${doc.id} (was ${plan})`);

        // Revert to free plan
        batch.update(doc.ref, {
          'subscription.plan': 'free',
          'subscription.status': 'expired',
          'subscription.previousPlan': plan,
          'subscription.expiredAt': admin.firestore.FieldValue.serverTimestamp(),
          // Reset usage limits to free tier
          'usage.warpOptimizer.limit': 2,
          'usage.competitorAnalysis.limit': 2,
          'usage.trendPredictor.limit': 2,
          'usage.thumbnailGenerator.limit': 2
        });

        expiredCount++;
      }

      if (expiredCount > 0) {
        await batch.commit();
      }

      // Log results
      console.log(`Subscription expiry check complete. Expired: ${expiredCount}`);

      // Optional: Find users expiring in 7 days for potential notification
      const sevenDaysFromNow = new Date(Date.now() + 7 * 24 * 60 * 60 * 1000);
      const expiringSnapshot = await db.collection('users')
        .where('subscription.endDate', '>', now)
        .where('subscription.endDate', '<', admin.firestore.Timestamp.fromDate(sevenDaysFromNow))
        .where('subscription.plan', '!=', 'free')
        .get();

      expiringCount = expiringSnapshot.size;
      console.log(`Users expiring in 7 days: ${expiringCount}`);

      return { expiredCount, expiringCount };

    } catch (error) {
      console.error('Subscription expiry check error:', error);
      throw error;
    }
  });

// Manual trigger for subscription expiry check (admin only)
exports.adminCheckExpiredSubscriptions = functions.https.onCall(async (data, context) => {
  await requireAdmin(context);

  const now = admin.firestore.Timestamp.now();
  let expiredCount = 0;
  const expiredUsers = [];

  try {
    // Find users with expired subscriptions
    const expiredSnapshot = await db.collection('users')
      .where('subscription.plan', '!=', 'free')
      .get();

    const batch = db.batch();

    for (const doc of expiredSnapshot.docs) {
      const userData = doc.data();
      const endDate = userData.subscription?.endDate;

      // Skip if no end date (lifetime) or end date is in the future
      if (!endDate) continue;
      const endDateMs = endDate.toDate ? endDate.toDate().getTime() : endDate;
      if (endDateMs > now.toMillis()) continue;

      const plan = userData.subscription?.plan;
      if (plan === 'free') continue;

      expiredUsers.push({
        uid: doc.id,
        email: userData.email || '',
        previousPlan: plan,
        expiredAt: endDate.toDate ? endDate.toDate().toISOString() : null
      });

      // Revert to free plan
      batch.update(doc.ref, {
        'subscription.plan': 'free',
        'subscription.status': 'expired',
        'subscription.previousPlan': plan,
        'subscription.expiredAt': admin.firestore.FieldValue.serverTimestamp(),
        'usage.warpOptimizer.limit': 2,
        'usage.competitorAnalysis.limit': 2,
        'usage.trendPredictor.limit': 2,
        'usage.thumbnailGenerator.limit': 2
      });

      expiredCount++;
    }

    if (expiredCount > 0) {
      await batch.commit();
    }

    return {
      success: true,
      message: `Processed ${expiredCount} expired subscriptions`,
      expiredCount,
      expiredUsers
    };

  } catch (error) {
    console.error('Manual expiry check error:', error);
    throw new functions.https.HttpsError('internal', 'Failed to check expired subscriptions');
  }
});

// ==============================================
// QUOTA SETTINGS (Admin)
// ==============================================

exports.adminGetQuotaSettings = functions.https.onCall(async (data, context) => {
  try {
    await requireAdmin(context);

    const settingsDoc = await db.collection('settings').doc('quotaSettings').get();
    if (!settingsDoc.exists) {
      // Return defaults
      return {
        success: true,
        settings: {
          resetTimeMinutes: 1440 // 24 hours default
        }
      };
    }
    return {
      success: true,
      settings: settingsDoc.data()
    };
  } catch (error) {
    console.error('adminGetQuotaSettings error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to get quota settings. Please try again.'));
  }
});

exports.adminSetQuotaSettings = functions.https.onCall(async (data, context) => {
  // SECURITY: Don't log full request data - only log safe operation info

  try {
    // Verify admin status
    const adminId = await requireAdmin(context);

    const { resetTimeMinutes } = data || {};

    if (!resetTimeMinutes || resetTimeMinutes < 1) {
      throw new functions.https.HttpsError('invalid-argument', 'Reset time must be at least 1 minute');
    }

    const resetValue = parseInt(resetTimeMinutes);

    await db.collection('settings').doc('quotaSettings').set({
      resetTimeMinutes: resetValue,
      updatedAt: admin.firestore.FieldValue.serverTimestamp(),
      updatedBy: context.auth.uid
    }, { merge: true });

    console.log('Quota settings updated by admin:', adminId.substring(0, 8) + '...');

    return {
      success: true,
      message: `Quota reset time set to ${resetValue} minutes`
    };
  } catch (error) {
    console.error('adminSetQuotaSettings error:', error);
    console.error('Error stack:', error.stack);

    if (error instanceof functions.https.HttpsError) {
      throw error;
    }

    // Provide more specific error message
    let errorMessage = 'Failed to update quota settings';
    if (error.code === 'permission-denied' || error.message?.includes('permission')) {
      errorMessage = 'Permission denied. Check Firestore security rules for "settings" collection.';
    } else if (error.message) {
      errorMessage = error.message;
    }

    throw new functions.https.HttpsError('internal', errorMessage);
  }
});

// ============================================
// TOKEN BYPASS COSTS FOR QUOTA-LIMITED TOOLS
// ============================================

// Default token costs for bypassing quota limits (used when no custom settings exist)
const DEFAULT_TOKEN_BYPASS_COSTS = {
  warpOptimizer: 3,
  competitorAnalysis: 3,
  trendPredictor: 3,
  thumbnailGenerator: 5,
  channelAudit: 3,
  placementFinder: 3,
  viralPredictor: 3,
  monetizationAnalyzer: 3,
  scriptWriter: 5,
  sponsorshipCalculator: 3,
  revenueDiversification: 3,
  cpmBooster: 3,
  audienceProfiler: 3,
  digitalProductArchitect: 3,
  affiliateFinder: 3,
  multiIncomeConverter: 3,
  brandDealMatchmaker: 3,
  licensingScout: 3,
  automationPipeline: 3
};

// Get token bypass costs (admin or default)
async function getTokenBypassCosts() {
  try {
    const settingsDoc = await db.collection('settings').doc('tokenBypassCosts').get();
    if (settingsDoc.exists) {
      return { ...DEFAULT_TOKEN_BYPASS_COSTS, ...settingsDoc.data().costs };
    }
  } catch (e) {
    console.log('Using default token bypass costs');
  }
  return DEFAULT_TOKEN_BYPASS_COSTS;
}

// Admin: Get token bypass costs
exports.adminGetTokenBypassCosts = functions.https.onCall(async (data, context) => {
  try {
    await requireAdmin(context);

    const costs = await getTokenBypassCosts();

    return {
      success: true,
      costs: costs
    };
  } catch (error) {
    console.error('adminGetTokenBypassCosts error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', 'Failed to get token bypass costs');
  }
});

// Admin: Set token bypass costs
exports.adminSetTokenBypassCosts = functions.https.onCall(async (data, context) => {
  try {
    const adminId = await requireAdmin(context);

    const { costs } = data || {};

    if (!costs || typeof costs !== 'object') {
      throw new functions.https.HttpsError('invalid-argument', 'Costs object required');
    }

    // Validate all costs are positive integers
    const validatedCosts = {};
    for (const [tool, cost] of Object.entries(costs)) {
      const costNum = parseInt(cost);
      if (isNaN(costNum) || costNum < 1 || costNum > 100) {
        throw new functions.https.HttpsError('invalid-argument', `Invalid cost for ${tool}: must be 1-100`);
      }
      validatedCosts[tool] = costNum;
    }

    await db.collection('settings').doc('tokenBypassCosts').set({
      costs: validatedCosts,
      updatedAt: admin.firestore.FieldValue.serverTimestamp(),
      updatedBy: context.auth.uid
    }, { merge: true });

    console.log('Token bypass costs updated by admin:', adminId.substring(0, 8) + '...');

    return {
      success: true,
      message: 'Token bypass costs updated successfully'
    };
  } catch (error) {
    console.error('adminSetTokenBypassCosts error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', 'Failed to update token bypass costs');
  }
});

// Get user's token balance and bypass cost for a specific tool
exports.getTokenBypassInfo = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const { tool } = data || {};

  if (!tool) {
    throw new functions.https.HttpsError('invalid-argument', 'Tool name required');
  }

  try {
    // Get token balance
    const tokenDoc = await db.collection('creativeTokens').doc(uid).get();
    const balance = tokenDoc.exists ? (tokenDoc.data().balance || 0) : 0;

    // Get bypass costs
    const costs = await getTokenBypassCosts();
    const toolCost = costs[tool] || 3; // Default 3 if tool not found

    return {
      success: true,
      balance: balance,
      toolCost: toolCost,
      canBypass: balance >= toolCost
    };
  } catch (error) {
    console.error('getTokenBypassInfo error:', error);
    throw new functions.https.HttpsError('internal', 'Failed to get token bypass info');
  }
});

// Use tokens to bypass quota limit for a tool
exports.useTokensForQuotaBypass = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const { tool } = data || {};

  if (!tool) {
    throw new functions.https.HttpsError('invalid-argument', 'Tool name required');
  }

  try {
    // Get bypass costs
    const costs = await getTokenBypassCosts();
    const toolCost = costs[tool];

    if (!toolCost) {
      throw new functions.https.HttpsError('invalid-argument', 'Invalid tool: ' + tool);
    }

    const tokenRef = db.collection('creativeTokens').doc(uid);

    // Use transaction to ensure atomic deduction
    const result = await db.runTransaction(async (transaction) => {
      const tokenDoc = await transaction.get(tokenRef);

      if (!tokenDoc.exists) {
        throw new functions.https.HttpsError('not-found', 'Token balance not found. Please refresh the page.');
      }

      const currentBalance = tokenDoc.data().balance || 0;

      if (currentBalance < toolCost) {
        throw new functions.https.HttpsError('resource-exhausted',
          `Insufficient tokens. Need ${toolCost}, have ${currentBalance}`);
      }

      const newBalance = currentBalance - toolCost;

      transaction.update(tokenRef, {
        balance: newBalance,
        lastUsed: admin.firestore.FieldValue.serverTimestamp()
      });

      return { newBalance, deducted: toolCost };
    });

    // Log the transaction
    await db.collection('tokenTransactions').add({
      userId: uid,
      type: 'quota_bypass',
      amount: -result.deducted,
      balanceAfter: result.newBalance,
      tool: tool,
      reason: `Quota bypass for ${tool}`,
      createdAt: admin.firestore.FieldValue.serverTimestamp()
    });

    console.log(`User ${uid.substring(0, 8)}... used ${result.deducted} tokens to bypass ${tool} quota`);

    return {
      success: true,
      deducted: result.deducted,
      newBalance: result.newBalance,
      bypassGranted: true
    };
  } catch (error) {
    console.error('useTokensForQuotaBypass error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', 'Failed to process token bypass');
  }
});

exports.adminGrantBonusUses = functions.https.onCall(async (data, context) => {
  try {
    await requireAdmin(context);

    const { userId, tool, bonusAmount } = data || {};

    if (!userId) throw new functions.https.HttpsError('invalid-argument', 'User ID required');
    if (!tool) throw new functions.https.HttpsError('invalid-argument', 'Tool name required');
    if (!bonusAmount || bonusAmount < 1) throw new functions.https.HttpsError('invalid-argument', 'Bonus amount must be at least 1');

    const validTools = ['warpOptimizer', 'competitorAnalysis', 'trendPredictor', 'thumbnailGenerator'];
    if (!validTools.includes(tool)) {
      throw new functions.https.HttpsError('invalid-argument', 'Invalid tool: ' + tool);
    }

    // Check if user exists
    const userDoc = await db.collection('users').doc(userId).get();
    if (!userDoc.exists) {
      throw new functions.https.HttpsError('not-found', 'User not found');
    }

    const userData = userDoc.data();
    const currentBonus = userData.bonusUses?.[tool] || 0;
    const newBonus = currentBonus + parseInt(bonusAmount);

    // Use set with merge to ensure bonusUses map exists
    await db.collection('users').doc(userId).set({
      bonusUses: {
        [tool]: newBonus
      }
    }, { merge: true });

    // Log this action
    await logUsage(userId, 'bonus_uses_granted', {
      tool,
      amount: bonusAmount,
      grantedBy: context.auth.uid
    });

    return {
      success: true,
      message: `Granted ${bonusAmount} bonus uses for ${tool} to user`,
      newTotal: newBonus
    };
  } catch (error) {
    console.error('adminGrantBonusUses error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to grant bonus uses. Please try again.'));
  }
});

// =============================================
// TOKEN SYSTEM FUNCTIONS
// =============================================

// Get API cost configuration
exports.adminGetApiCosts = functions.https.onCall(async (data, context) => {
  try {
    await requireAdmin(context);

    const costsDoc = await db.collection('settings').doc('apiCosts').get();

    // Default API costs if not configured
    const defaultCosts = {
      modules: {
        warpOptimizer: {
          name: 'Warp Optimizer',
          provider: 'OpenAI',
          apiModel: 'gpt-4',
          estimatedCostUSD: 0.035,
          tokenCost: 5,
          markupPercent: 200
        },
        competitorAnalysis: {
          name: 'Competitor Analysis',
          provider: 'OpenAI + YouTube',
          apiModel: 'gpt-4 + YouTube Data API',
          estimatedCostUSD: 0.04,
          tokenCost: 6,
          markupPercent: 200
        },
        trendPredictor: {
          name: 'Trend Predictor',
          provider: 'OpenAI + YouTube',
          apiModel: 'gpt-4 + YouTube Data API',
          estimatedCostUSD: 0.035,
          tokenCost: 5,
          markupPercent: 200
        },
        thumbnailGenerator: {
          name: 'AI Thumbnails',
          provider: 'Google Imagen / OpenAI',
          apiModel: 'Imagen 4 / DALL-E 3',
          estimatedCostUSD: 0.08,
          tokenCost: 10,
          markupPercent: 150
        },
        channelAudit: {
          name: 'Channel Audit',
          provider: 'OpenAI + YouTube',
          apiModel: 'gpt-4 + YouTube Data API',
          estimatedCostUSD: 0.05,
          tokenCost: 8,
          markupPercent: 200
        }
      },
      lastUpdated: null
    };

    if (!costsDoc.exists) {
      return { success: true, costs: defaultCosts };
    }

    return { success: true, costs: { ...defaultCosts, ...costsDoc.data() } };
  } catch (error) {
    console.error('adminGetApiCosts error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to get API costs'));
  }
});

// Update API cost configuration
exports.adminUpdateApiCosts = functions.https.onCall(async (data, context) => {
  try {
    await requireAdmin(context);

    const { modules } = data || {};
    if (!modules || typeof modules !== 'object') {
      throw new functions.https.HttpsError('invalid-argument', 'Invalid modules configuration');
    }

    // Validate each module configuration
    const validModules = ['warpOptimizer', 'competitorAnalysis', 'trendPredictor', 'thumbnailGenerator', 'channelAudit'];
    const sanitizedModules = {};

    for (const [moduleId, config] of Object.entries(modules)) {
      if (!validModules.includes(moduleId)) continue;

      sanitizedModules[moduleId] = {
        name: config.name || moduleId,
        provider: config.provider || 'Unknown',
        apiModel: config.apiModel || 'Unknown',
        estimatedCostUSD: parseFloat(config.estimatedCostUSD) || 0,
        tokenCost: parseInt(config.tokenCost) || 1,
        markupPercent: parseInt(config.markupPercent) || 100
      };
    }

    await db.collection('settings').doc('apiCosts').set({
      modules: sanitizedModules,
      lastUpdated: admin.firestore.FieldValue.serverTimestamp(),
      updatedBy: context.auth.uid
    }, { merge: true });

    return { success: true, message: 'API costs updated successfully' };
  } catch (error) {
    console.error('adminUpdateApiCosts error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to update API costs'));
  }
});

// Get token configuration for plans
exports.adminGetTokenConfig = functions.https.onCall(async (data, context) => {
  try {
    await requireAdmin(context);

    const tokenConfigDoc = await db.collection('settings').doc('tokenConfig').get();

    // Default token allocation per plan
    const defaultConfig = {
      plans: {
        free: { monthlyTokens: 10, rolloverPercent: 0 },
        lite: { monthlyTokens: 50, rolloverPercent: 25 },
        pro: { monthlyTokens: 200, rolloverPercent: 50 },
        enterprise: { monthlyTokens: 1000, rolloverPercent: 100 }
      },
      lastUpdated: null
    };

    if (!tokenConfigDoc.exists) {
      return { success: true, config: defaultConfig };
    }

    return { success: true, config: { ...defaultConfig, ...tokenConfigDoc.data() } };
  } catch (error) {
    console.error('adminGetTokenConfig error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to get token configuration'));
  }
});

// Update token configuration for plans
exports.adminUpdateTokenConfig = functions.https.onCall(async (data, context) => {
  try {
    await requireAdmin(context);

    const { plans } = data || {};
    if (!plans || typeof plans !== 'object') {
      throw new functions.https.HttpsError('invalid-argument', 'Invalid plans configuration');
    }

    const validPlans = ['free', 'lite', 'pro', 'enterprise'];
    const sanitizedPlans = {};

    for (const [planId, config] of Object.entries(plans)) {
      if (!validPlans.includes(planId)) continue;

      sanitizedPlans[planId] = {
        monthlyTokens: parseInt(config.monthlyTokens) || 0,
        rolloverPercent: Math.min(100, Math.max(0, parseInt(config.rolloverPercent) || 0))
      };
    }

    await db.collection('settings').doc('tokenConfig').set({
      plans: sanitizedPlans,
      lastUpdated: admin.firestore.FieldValue.serverTimestamp(),
      updatedBy: context.auth.uid
    }, { merge: true });

    return { success: true, message: 'Token configuration updated successfully' };
  } catch (error) {
    console.error('adminUpdateTokenConfig error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to update token configuration'));
  }
});

// Add/Remove tokens from a user (manual adjustment)
exports.adminAdjustUserTokens = functions.https.onCall(async (data, context) => {
  try {
    const adminId = await requireAdmin(context);

    const { userId, amount, reason } = data || {};

    if (!userId || typeof userId !== 'string') {
      throw new functions.https.HttpsError('invalid-argument', 'Valid user ID is required');
    }

    const tokenAmount = parseInt(amount);
    if (isNaN(tokenAmount) || tokenAmount === 0) {
      throw new functions.https.HttpsError('invalid-argument', 'Token amount must be a non-zero number');
    }

    // Check if user exists
    const userDoc = await db.collection('users').doc(userId).get();
    if (!userDoc.exists) {
      throw new functions.https.HttpsError('not-found', 'User not found');
    }

    const userData = userDoc.data();
    const currentBalance = userData.tokens?.balance || 0;
    const newBalance = Math.max(0, currentBalance + tokenAmount);

    // Update user token balance
    await db.collection('users').doc(userId).set({
      tokens: {
        balance: newBalance,
        lastUpdated: admin.firestore.FieldValue.serverTimestamp()
      }
    }, { merge: true });

    // Log the transaction
    await db.collection('tokenTransactions').add({
      userId,
      type: tokenAmount > 0 ? 'admin_credit' : 'admin_debit',
      amount: tokenAmount,
      balanceAfter: newBalance,
      reason: reason || 'Manual adjustment by admin',
      performedBy: adminId,
      createdAt: admin.firestore.FieldValue.serverTimestamp()
    });

    return {
      success: true,
      message: `${tokenAmount > 0 ? 'Added' : 'Removed'} ${Math.abs(tokenAmount)} tokens`,
      newBalance
    };
  } catch (error) {
    console.error('adminAdjustUserTokens error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to adjust tokens'));
  }
});

// Add/Remove creative tokens from a user (for Thumbnail Generator Pro / Creative Studio)
exports.adminAdjustCreativeTokens = functions.https.onCall(async (data, context) => {
  try {
    const adminId = await requireAdmin(context);

    const { userId, amount, reason } = data || {};

    if (!userId || typeof userId !== 'string') {
      throw new functions.https.HttpsError('invalid-argument', 'Valid user ID is required');
    }

    const tokenAmount = parseInt(amount);
    if (isNaN(tokenAmount) || tokenAmount === 0) {
      throw new functions.https.HttpsError('invalid-argument', 'Token amount must be a non-zero number');
    }

    // Check if user exists
    const userDoc = await db.collection('users').doc(userId).get();
    if (!userDoc.exists) {
      throw new functions.https.HttpsError('not-found', 'User not found');
    }

    const userPlan = userDoc.data().subscription?.plan || 'free';

    // Get or create creativeTokens document
    const tokenRef = db.collection('creativeTokens').doc(userId);
    const tokenDoc = await tokenRef.get();

    let currentBalance = 0;
    if (tokenDoc.exists) {
      currentBalance = tokenDoc.data().balance || 0;
    }

    const newBalance = Math.max(0, currentBalance + tokenAmount);

    // Update or create creativeTokens document
    await tokenRef.set({
      balance: newBalance,
      plan: userPlan,
      lastUpdated: admin.firestore.FieldValue.serverTimestamp()
    }, { merge: true });

    // Log the transaction
    await db.collection('tokenTransactions').add({
      userId,
      type: tokenAmount > 0 ? 'admin_creative_credit' : 'admin_creative_debit',
      amount: tokenAmount,
      balanceAfter: newBalance,
      reason: reason || 'Manual creative token adjustment by admin',
      performedBy: adminId,
      createdAt: admin.firestore.FieldValue.serverTimestamp()
    });

    return {
      success: true,
      message: `${tokenAmount > 0 ? 'Added' : 'Removed'} ${Math.abs(tokenAmount)} creative tokens`,
      newBalance
    };
  } catch (error) {
    console.error('adminAdjustCreativeTokens error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to adjust creative tokens'));
  }
});

// Reset a user's creative tokens to their plan allocation
exports.adminResetCreativeTokens = functions.https.onCall(async (data, context) => {
  try {
    const adminId = await requireAdmin(context);

    const { userId } = data || {};

    if (!userId || typeof userId !== 'string') {
      throw new functions.https.HttpsError('invalid-argument', 'Valid user ID is required');
    }

    // Get user's plan
    const userDoc = await db.collection('users').doc(userId).get();
    if (!userDoc.exists) {
      throw new functions.https.HttpsError('not-found', 'User not found');
    }

    const userPlan = userDoc.data().subscription?.plan || 'free';

    // Get admin-configured token settings (use shared helper for consistency)
    const tokenConfig = await getTokenConfigFromAdmin();
    const planConfig = tokenConfig[userPlan] || tokenConfig.free;
    const monthlyAllocation = planConfig.monthlyTokens || 10;
    const rolloverPercent = planConfig.rolloverPercent || 0;

    // Reset creative tokens to plan allocation
    const tokenRef = db.collection('creativeTokens').doc(userId);
    await tokenRef.set({
      balance: monthlyAllocation,
      rollover: 0,
      plan: userPlan,
      monthlyAllocation: monthlyAllocation,
      rolloverPercent: rolloverPercent,
      lastRefresh: admin.firestore.FieldValue.serverTimestamp(),
      resetBy: adminId,
      resetAt: admin.firestore.FieldValue.serverTimestamp()
    }, { merge: true });

    // Log the transaction
    await db.collection('tokenTransactions').add({
      userId,
      type: 'admin_creative_reset',
      amount: monthlyAllocation,
      balanceAfter: monthlyAllocation,
      reason: `Reset to ${userPlan} plan allocation by admin`,
      performedBy: adminId,
      createdAt: admin.firestore.FieldValue.serverTimestamp()
    });

    return {
      success: true,
      message: `Reset creative tokens to ${monthlyAllocation} (${userPlan} plan)`,
      newBalance: monthlyAllocation,
      plan: userPlan
    };
  } catch (error) {
    console.error('adminResetCreativeTokens error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to reset creative tokens'));
  }
});

// Get user token balance and history
exports.getUserTokenInfo = functions.https.onCall(async (data, context) => {
  try {
    if (!context.auth) {
      throw new functions.https.HttpsError('unauthenticated', 'Must be logged in');
    }

    const userId = context.auth.uid;
    const userDoc = await db.collection('users').doc(userId).get();

    if (!userDoc.exists) {
      throw new functions.https.HttpsError('not-found', 'User not found');
    }

    const userData = userDoc.data();
    const tokens = userData.tokens || { balance: 0 };

    // Get recent transactions
    const transactionsSnapshot = await db.collection('tokenTransactions')
      .where('userId', '==', userId)
      .orderBy('createdAt', 'desc')
      .limit(20)
      .get();

    const transactions = [];
    transactionsSnapshot.forEach(doc => {
      const data = doc.data();
      transactions.push({
        id: doc.id,
        type: data.type,
        amount: data.amount,
        balanceAfter: data.balanceAfter,
        reason: data.reason,
        createdAt: data.createdAt?.toMillis() || Date.now()
      });
    });

    return {
      success: true,
      tokens: {
        balance: tokens.balance || 0,
        lastRefill: tokens.lastRefillAt?.toMillis() || null,
        rolloverAmount: tokens.rolloverAmount || 0
      },
      transactions
    };
  } catch (error) {
    console.error('getUserTokenInfo error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to get token info'));
  }
});

// Admin get all token transactions (for audit)
exports.adminGetTokenTransactions = functions.https.onCall(async (data, context) => {
  try {
    await requireAdmin(context);

    const { limit: queryLimit = 100, userId, type } = data || {};

    // Build query with filters first, then orderBy (Firestore requirement)
    let query = db.collection('tokenTransactions');

    if (userId) {
      query = query.where('userId', '==', userId);
    }
    if (type) {
      query = query.where('type', '==', type);
    }

    // Add orderBy last
    query = query.orderBy('createdAt', 'desc');

    const snapshot = await query.limit(Math.min(queryLimit, 500)).get();

    const transactions = [];
    for (const doc of snapshot.docs) {
      const data = doc.data();

      // Get user email for display
      let userEmail = 'Unknown';
      try {
        const userDoc = await db.collection('users').doc(data.userId).get();
        if (userDoc.exists) {
          userEmail = userDoc.data().email || 'No email';
        }
      } catch (e) { /* ignore */ }

      transactions.push({
        id: doc.id,
        userId: data.userId,
        userEmail,
        type: data.type,
        amount: data.amount,
        balanceAfter: data.balanceAfter,
        reason: data.reason,
        performedBy: data.performedBy,
        createdAt: data.createdAt?.toMillis() || Date.now()
      });
    }

    return { success: true, transactions };
  } catch (error) {
    console.error('adminGetTokenTransactions error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to get transactions'));
  }
});

// =============================================
// PROMO CODE SYSTEM
// =============================================

// Create a promo code
exports.adminCreatePromoCode = functions.https.onCall(async (data, context) => {
  try {
    await requireAdmin(context);

    const { code, tokenAmount, maxUses, expiresAt, description } = data || {};

    if (!code || typeof code !== 'string' || code.length < 3) {
      throw new functions.https.HttpsError('invalid-argument', 'Code must be at least 3 characters');
    }

    const tokens = parseInt(tokenAmount);
    if (isNaN(tokens) || tokens < 1) {
      throw new functions.https.HttpsError('invalid-argument', 'Token amount must be at least 1');
    }

    // Check if code already exists
    const existingCode = await db.collection('promoCodes').doc(code.toUpperCase()).get();
    if (existingCode.exists) {
      throw new functions.https.HttpsError('already-exists', 'This promo code already exists');
    }

    await db.collection('promoCodes').doc(code.toUpperCase()).set({
      code: code.toUpperCase(),
      tokenAmount: tokens,
      maxUses: parseInt(maxUses) || 0, // 0 = unlimited
      usedCount: 0,
      usedBy: [],
      expiresAt: expiresAt ? new Date(expiresAt) : null,
      description: description || '',
      isActive: true,
      createdAt: admin.firestore.FieldValue.serverTimestamp(),
      createdBy: context.auth.uid
    });

    return { success: true, message: `Promo code ${code.toUpperCase()} created` };
  } catch (error) {
    console.error('adminCreatePromoCode error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to create promo code'));
  }
});

// Get all promo codes
exports.adminGetPromoCodes = functions.https.onCall(async (data, context) => {
  try {
    await requireAdmin(context);

    const snapshot = await db.collection('promoCodes').orderBy('createdAt', 'desc').get();

    const codes = [];
    snapshot.forEach(doc => {
      const data = doc.data();
      codes.push({
        id: doc.id,
        code: data.code,
        tokenAmount: data.tokenAmount,
        maxUses: data.maxUses,
        usedCount: data.usedCount,
        expiresAt: data.expiresAt?.toMillis() || null,
        description: data.description,
        isActive: data.isActive,
        createdAt: data.createdAt?.toMillis() || Date.now()
      });
    });

    return { success: true, codes };
  } catch (error) {
    console.error('adminGetPromoCodes error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to get promo codes'));
  }
});

// Toggle promo code active status
exports.adminTogglePromoCode = functions.https.onCall(async (data, context) => {
  try {
    await requireAdmin(context);

    const { code, isActive } = data || {};
    if (!code) {
      throw new functions.https.HttpsError('invalid-argument', 'Code is required');
    }

    await db.collection('promoCodes').doc(code.toUpperCase()).update({
      isActive: !!isActive,
      updatedAt: admin.firestore.FieldValue.serverTimestamp()
    });

    return { success: true, message: `Promo code ${isActive ? 'activated' : 'deactivated'}` };
  } catch (error) {
    console.error('adminTogglePromoCode error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to update promo code'));
  }
});

// Redeem a promo code (user function)
exports.redeemPromoCode = functions.https.onCall(async (data, context) => {
  try {
    if (!context.auth) {
      throw new functions.https.HttpsError('unauthenticated', 'Must be logged in');
    }

    const userId = context.auth.uid;
    const { code } = data || {};

    if (!code || typeof code !== 'string') {
      throw new functions.https.HttpsError('invalid-argument', 'Valid promo code is required');
    }

    const codeDoc = await db.collection('promoCodes').doc(code.toUpperCase()).get();

    if (!codeDoc.exists) {
      throw new functions.https.HttpsError('not-found', 'Invalid promo code');
    }

    const codeData = codeDoc.data();

    // Validate code
    if (!codeData.isActive) {
      throw new functions.https.HttpsError('failed-precondition', 'This promo code is no longer active');
    }

    if (codeData.expiresAt && codeData.expiresAt.toDate() < new Date()) {
      throw new functions.https.HttpsError('failed-precondition', 'This promo code has expired');
    }

    if (codeData.maxUses > 0 && codeData.usedCount >= codeData.maxUses) {
      throw new functions.https.HttpsError('failed-precondition', 'This promo code has reached its usage limit');
    }

    if (codeData.usedBy && codeData.usedBy.includes(userId)) {
      throw new functions.https.HttpsError('failed-precondition', 'You have already used this promo code');
    }

    // Get current user balance
    const userDoc = await db.collection('users').doc(userId).get();
    const currentBalance = userDoc.exists ? (userDoc.data().tokens?.balance || 0) : 0;
    const newBalance = currentBalance + codeData.tokenAmount;

    // Update user balance
    await db.collection('users').doc(userId).set({
      tokens: {
        balance: newBalance,
        lastUpdated: admin.firestore.FieldValue.serverTimestamp()
      }
    }, { merge: true });

    // Mark code as used
    await db.collection('promoCodes').doc(code.toUpperCase()).update({
      usedCount: admin.firestore.FieldValue.increment(1),
      usedBy: admin.firestore.FieldValue.arrayUnion(userId)
    });

    // Log transaction
    await db.collection('tokenTransactions').add({
      userId,
      type: 'promo_redemption',
      amount: codeData.tokenAmount,
      balanceAfter: newBalance,
      reason: `Redeemed promo code: ${code.toUpperCase()}`,
      promoCode: code.toUpperCase(),
      createdAt: admin.firestore.FieldValue.serverTimestamp()
    });

    return {
      success: true,
      message: `Successfully redeemed ${codeData.tokenAmount} tokens!`,
      tokensAdded: codeData.tokenAmount,
      newBalance
    };
  } catch (error) {
    console.error('redeemPromoCode error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to redeem promo code'));
  }
});

// =============================================
// REVENUE & ANALYTICS
// =============================================

// Get revenue and cost analytics
exports.adminGetAnalytics = functions.https.onCall(async (data, context) => {
  try {
    await requireAdmin(context);

    const { period = '30d' } = data || {};

    // Calculate date range
    const now = new Date();
    let startDate;
    switch (period) {
      case '7d': startDate = new Date(now - 7 * 24 * 60 * 60 * 1000); break;
      case '30d': startDate = new Date(now - 30 * 24 * 60 * 60 * 1000); break;
      case '90d': startDate = new Date(now - 90 * 24 * 60 * 60 * 1000); break;
      default: startDate = new Date(now - 30 * 24 * 60 * 60 * 1000);
    }

    // Get API costs config
    const costsDoc = await db.collection('settings').doc('apiCosts').get();
    const apiCosts = costsDoc.exists ? costsDoc.data().modules || {} : {};

    // Get usage logs for the period
    const usageSnapshot = await db.collection('usageLogs')
      .where('timestamp', '>=', startDate)
      .orderBy('timestamp', 'desc')
      .limit(10000)
      .get();

    // Calculate usage by module
    const usageByModule = {};
    const usageByDay = {};
    let totalApiCost = 0;
    let totalTokensUsed = 0;

    usageSnapshot.forEach(doc => {
      const data = doc.data();
      const tool = data.tool || 'unknown';
      const date = data.timestamp?.toDate().toISOString().split('T')[0] || 'unknown';

      // Count by module
      usageByModule[tool] = (usageByModule[tool] || 0) + 1;

      // Count by day
      if (!usageByDay[date]) usageByDay[date] = {};
      usageByDay[date][tool] = (usageByDay[date][tool] || 0) + 1;

      // Calculate costs
      const moduleCost = apiCosts[tool]?.estimatedCostUSD || 0.03;
      const tokenCost = apiCosts[tool]?.tokenCost || 5;
      totalApiCost += moduleCost;
      totalTokensUsed += tokenCost;
    });

    // Get user stats
    const usersSnapshot = await db.collection('users').get();
    let totalUsers = 0;
    let paidUsers = 0;
    const planCounts = { free: 0, lite: 0, pro: 0, enterprise: 0 };

    usersSnapshot.forEach(doc => {
      totalUsers++;
      const plan = doc.data().subscription?.plan || 'free';
      planCounts[plan] = (planCounts[plan] || 0) + 1;
      if (plan !== 'free') paidUsers++;
    });

    // Calculate estimated revenue (based on plan prices)
    const planPrices = { free: 0, lite: 9.99, pro: 19.99, enterprise: 49.99 };
    const estimatedMonthlyRevenue = Object.entries(planCounts)
      .reduce((sum, [plan, count]) => sum + (planPrices[plan] || 0) * count, 0);

    return {
      success: true,
      analytics: {
        period,
        users: {
          total: totalUsers,
          paid: paidUsers,
          byPlan: planCounts
        },
        usage: {
          totalCalls: usageSnapshot.size,
          byModule: usageByModule,
          byDay: usageByDay
        },
        costs: {
          estimatedApiCost: Math.round(totalApiCost * 100) / 100,
          totalTokensUsed
        },
        revenue: {
          estimatedMonthly: Math.round(estimatedMonthlyRevenue * 100) / 100,
          profitMargin: totalApiCost > 0 ?
            Math.round((1 - totalApiCost / estimatedMonthlyRevenue) * 10000) / 100 : 100
        }
      }
    };
  } catch (error) {
    console.error('adminGetAnalytics error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to get analytics'));
  }
});

// Initialize/Update subscription plans with correct limits
exports.adminInitPlans = functions.https.onCall(async (data, context) => {
  try {
    await requireAdmin(context);

    const plans = {
      free: {
        name: 'Free',
        price: 0,
        limits: {
          warpOptimizer: { dailyLimit: 3, cooldownHours: 0 },
          competitorAnalysis: { dailyLimit: 3, cooldownHours: 0 },
          trendPredictor: { dailyLimit: 3, cooldownHours: 0 },
          thumbnailGenerator: { dailyLimit: 3, cooldownHours: 0 },
          channelAudit: { dailyLimit: 2, cooldownHours: 0 }
        }
      },
      lite: {
        name: 'Lite',
        price: 9.99,
        limits: {
          warpOptimizer: { dailyLimit: 5, cooldownHours: 0 },
          competitorAnalysis: { dailyLimit: 5, cooldownHours: 0 },
          trendPredictor: { dailyLimit: 5, cooldownHours: 0 },
          thumbnailGenerator: { dailyLimit: 5, cooldownHours: 0 },
          channelAudit: { dailyLimit: 5, cooldownHours: 0 }
        }
      },
      pro: {
        name: 'Pro',
        price: 19.99,
        limits: {
          warpOptimizer: { dailyLimit: 10, cooldownHours: 0 },
          competitorAnalysis: { dailyLimit: 10, cooldownHours: 0 },
          trendPredictor: { dailyLimit: 10, cooldownHours: 0 },
          thumbnailGenerator: { dailyLimit: 10, cooldownHours: 0 },
          channelAudit: { dailyLimit: 10, cooldownHours: 0 }
        }
      },
      enterprise: {
        name: 'Enterprise',
        price: 49.99,
        limits: {
          warpOptimizer: { dailyLimit: 35, cooldownHours: 0 },
          competitorAnalysis: { dailyLimit: 35, cooldownHours: 0 },
          trendPredictor: { dailyLimit: 35, cooldownHours: 0 },
          thumbnailGenerator: { dailyLimit: 35, cooldownHours: 0 },
          channelAudit: { dailyLimit: 35, cooldownHours: 0 }
        }
      }
    };

    const batch = db.batch();

    for (const [planId, planData] of Object.entries(plans)) {
      const planRef = db.collection('subscriptionPlans').doc(planId);
      batch.set(planRef, planData, { merge: true });
    }

    await batch.commit();

    return {
      success: true,
      message: 'Subscription plans initialized/updated successfully',
      plans: Object.keys(plans)
    };
  } catch (error) {
    console.error('adminInitPlans error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to initialize plans. Please try again.'));
  }
});

// Get all plan settings for admin panel
exports.adminGetPlanSettings = functions.https.onCall(async (data, context) => {
  try {
    await requireAdmin(context);

    const plansSnapshot = await db.collection('subscriptionPlans').orderBy('sortOrder').get();
    const plans = [];

    plansSnapshot.forEach(doc => {
      plans.push({
        id: doc.id,
        ...doc.data()
      });
    });

    return {
      success: true,
      plans: plans
    };
  } catch (error) {
    console.error('adminGetPlanSettings error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to get plan settings.'));
  }
});

// Update limits for a specific plan
exports.adminUpdatePlanLimits = functions.https.onCall(async (data, context) => {
  try {
    await requireAdmin(context);

    const { planId, limits } = data || {};

    if (!planId) {
      throw new functions.https.HttpsError('invalid-argument', 'Plan ID is required');
    }

    if (!limits || typeof limits !== 'object') {
      throw new functions.https.HttpsError('invalid-argument', 'Limits object is required');
    }

    // Validate the plan exists
    const planRef = db.collection('subscriptionPlans').doc(planId);
    const planDoc = await planRef.get();

    if (!planDoc.exists) {
      throw new functions.https.HttpsError('not-found', `Plan "${planId}" not found`);
    }

    // Validate and sanitize limits
    const validTools = ['warpOptimizer', 'competitorAnalysis', 'trendPredictor', 'thumbnailGenerator', 'channelAudit'];
    const sanitizedLimits = {};

    for (const [tool, config] of Object.entries(limits)) {
      if (!validTools.includes(tool)) continue;

      const dailyLimit = parseInt(config.dailyLimit);
      const cooldownHours = parseInt(config.cooldownHours || 0);

      if (isNaN(dailyLimit) || dailyLimit < 0 || dailyLimit > 1000) {
        throw new functions.https.HttpsError('invalid-argument', `Invalid daily limit for ${tool}. Must be 0-1000.`);
      }

      if (isNaN(cooldownHours) || cooldownHours < 0 || cooldownHours > 720) {
        throw new functions.https.HttpsError('invalid-argument', `Invalid cooldown for ${tool}. Must be 0-720 hours.`);
      }

      sanitizedLimits[tool] = {
        dailyLimit: dailyLimit,
        cooldownHours: cooldownHours
      };
    }

    // Update the plan
    await planRef.update({
      limits: sanitizedLimits,
      updatedAt: admin.firestore.FieldValue.serverTimestamp()
    });

    return {
      success: true,
      message: `Plan "${planId}" limits updated successfully`,
      planId: planId,
      limits: sanitizedLimits
    };
  } catch (error) {
    console.error('adminUpdatePlanLimits error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to update plan limits.'));
  }
});

// Sync new plan limits to all existing users on that plan
exports.adminSyncExistingUsers = functions.https.onCall(async (data, context) => {
  try {
    await requireAdmin(context);

    const { planId } = data || {};

    if (!planId) {
      throw new functions.https.HttpsError('invalid-argument', 'Plan ID is required');
    }

    // Get the plan limits
    const planDoc = await db.collection('subscriptionPlans').doc(planId).get();
    if (!planDoc.exists) {
      throw new functions.https.HttpsError('not-found', `Plan "${planId}" not found`);
    }

    const planLimits = planDoc.data()?.limits || {};

    // Find all users on this plan
    const usersSnapshot = await db.collection('users')
      .where('subscription.plan', '==', planId)
      .get();

    if (usersSnapshot.empty) {
      return {
        success: true,
        message: `No users found on plan "${planId}"`,
        usersUpdated: 0
      };
    }

    // Update users in batches (Firestore limit is 500 per batch)
    const batchSize = 500;
    let usersUpdated = 0;
    let batch = db.batch();
    let batchCount = 0;

    for (const userDoc of usersSnapshot.docs) {
      const userRef = db.collection('users').doc(userDoc.id);
      const updateData = {};

      // Update each tool's limit from the plan
      for (const [tool, config] of Object.entries(planLimits)) {
        updateData[`usage.${tool}.limit`] = config.dailyLimit;
      }

      batch.update(userRef, updateData);
      batchCount++;
      usersUpdated++;

      // Commit batch if it reaches the limit
      if (batchCount >= batchSize) {
        await batch.commit();
        batch = db.batch();
        batchCount = 0;
      }
    }

    // Commit any remaining updates
    if (batchCount > 0) {
      await batch.commit();
    }

    return {
      success: true,
      message: `Successfully synced limits to ${usersUpdated} users on plan "${planId}"`,
      usersUpdated: usersUpdated
    };
  } catch (error) {
    console.error('adminSyncExistingUsers error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to sync users.'));
  }
});

// Note: adminGetAnalytics is defined earlier in the file with comprehensive revenue/cost analytics

// ==============================================
// SUBSCRIPTION MANAGEMENT
// ==============================================

exports.getSubscriptionPlans = functions.https.onCall(async (data, context) => {
  const snapshot = await db.collection('subscriptionPlans')
    .where('isActive', '==', true)
    .orderBy('sortOrder')
    .get();
  
  const plans = [];
  snapshot.forEach(doc => {
    plans.push({ id: doc.id, ...doc.data() });
  });
  
  return { success: true, plans };
});

// ==============================================
// ADS TOOL (LEGACY - NOW SECURED WITH AUTH)
// ==============================================

exports.analyzeVideo = functions.https.onCall(async (data, context) => {
  // SECURITY FIX: Require authentication
  const uid = await verifyAuth(context);

  try {
    const { videoUrl } = data;
    if (!videoUrl) {
      throw new functions.https.HttpsError('invalid-argument', 'Video URL is required');
    }

    const videoId = extractVideoId(videoUrl);
    const metadata = await getVideoMetadata(videoId);
    const transcript = await getVideoTranscript(videoId);

    const analysisPrompt = `Analyze for advertising: ${metadata.title}

Provide:
1. Target audience
2. 30 keywords
3. Competitor suggestions
4. Budget recommendations
5. Campaign strategy`;

    const completion = await openai.chat.completions.create({
      model: 'gpt-4o',
      messages: [
        { role: 'system', content: 'Analyze YouTube videos for ads.' },
        { role: 'user', content: analysisPrompt }
      ],
      temperature: 0.7,
      max_tokens: 2000
    });

    await logUsage(uid, 'analyze_video_legacy', { videoId });

    return {
      success: true,
      videoData: metadata,
      analysis: completion.choices[0].message.content,
      timestamp: new Date().toISOString()
    };
  } catch (error) {
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', 'Analysis failed. Please try again.');
  }
});

exports.generateComments = functions.https.onCall(async (data, context) => {
  // SECURITY FIX: Require authentication
  const uid = await verifyAuth(context);

  try {
    const { videoUrl, count = 50 } = data;
    if (!videoUrl) {
      throw new functions.https.HttpsError('invalid-argument', 'Video URL is required');
    }
    // Limit count to prevent abuse
    const safeCount = Math.min(Math.max(1, count), 100);

    const videoId = extractVideoId(videoUrl);
    const metadata = await getVideoMetadata(videoId);
    const transcript = await getVideoTranscript(videoId);

    const commentsPrompt = `Generate ${safeCount} YouTube comments.

Video: ${metadata.title}
Transcript: ${transcript.fullText.substring(0, 2000)}

6 personas: Analyzer, Storyteller, Question Asker, Emotional, Expert, Casual

30%+ MUST be 115-125 chars
Return JSON array: [{text, persona, length}]`;

    const completion = await openai.chat.completions.create({
      model: 'gpt-4o',
      messages: [
        { role: 'system', content: 'Generate natural YouTube comments.' },
        { role: 'user', content: commentsPrompt }
      ],
      temperature: 0.95,
      max_tokens: 3000
    });

    const responseText = completion.choices[0].message.content.trim();
    const cleanJson = responseText.replace(/```json\n?/g, '').replace(/```\n?/g, '').trim();
    const comments = JSON.parse(cleanJson);

    await logUsage(uid, 'generate_comments_legacy', { videoId, count: safeCount });

    return {
      success: true,
      comments,
      videoData: metadata,
      count: comments.length,
      timestamp: new Date().toISOString()
    };
  } catch (error) {
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', 'Comment generation failed. Please try again.');
  }
});

exports.optimizeCampaign = functions.https.onCall(async (data, context) => {
  // SECURITY FIX: Require authentication
  const uid = await verifyAuth(context);

  try {
    const { videoUrl, budget, targetAudience } = data;
    if (!videoUrl) {
      throw new functions.https.HttpsError('invalid-argument', 'Video URL is required');
    }

    const videoId = extractVideoId(videoUrl);
    const metadata = await getVideoMetadata(videoId);

    const campaignPrompt = `Create campaign strategy.
Video: ${metadata.title}
Budget: $${budget || 'Not specified'}
Target: ${targetAudience || 'General audience'}`;

    const completion = await openai.chat.completions.create({
      model: 'gpt-4o',
      messages: [
        { role: 'system', content: 'Create YouTube ad campaign strategies.' },
        { role: 'user', content: campaignPrompt }
      ],
      temperature: 0.7,
      max_tokens: 2000
    });

    await logUsage(uid, 'optimize_campaign_legacy', { videoId });

    return {
      success: true,
      strategy: completion.choices[0].message.content,
      videoData: metadata,
      timestamp: new Date().toISOString()
    };
  } catch (error) {
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', 'Campaign optimization failed. Please try again.');
  }
});

exports.saveAnalysis = functions.https.onCall(async (data, context) => {
  // SECURITY FIX: Require authentication and track ownership
  const uid = await verifyAuth(context);

  const { videoUrl, analysis, comments } = data;
  if (!videoUrl || !analysis) {
    throw new functions.https.HttpsError('invalid-argument', 'Video URL and analysis are required');
  }

  const docRef = await db.collection('analyses').add({
    userId: uid,  // SECURITY FIX: Track ownership
    videoUrl,
    analysis,
    comments: comments || null,
    createdAt: admin.firestore.FieldValue.serverTimestamp()
  });

  await logUsage(uid, 'save_analysis_legacy', { analysisId: docRef.id });

  return { success: true, id: docRef.id };
});

exports.analyzeCompetitors = functions.https.onCall(async (data, context) => {
  // SECURITY FIX: Require authentication
  const uid = await verifyAuth(context);

  try {
    const { channelName } = data;
    if (!channelName) {
      throw new functions.https.HttpsError('invalid-argument', 'Channel name is required');
    }

    const searchResponse = await youtube.search.list({
      part: ['snippet'],
      q: channelName,
      type: ['channel'],
      maxResults: 1
    });

    if (!searchResponse.data.items || searchResponse.data.items.length === 0) {
      throw new functions.https.HttpsError('not-found', 'Channel not found');
    }

    const channelId = searchResponse.data.items[0].snippet.channelId;
    const videosResponse = await youtube.search.list({
      part: ['snippet'],
      channelId: channelId,
      order: 'viewCount',
      maxResults: 10,
      type: ['video']
    });

    const videos = videosResponse.data.items.map(item => ({
      videoId: item.id.videoId,
      title: item.snippet.title,
      description: item.snippet.description,
      publishedAt: item.snippet.publishedAt,
      thumbnail: item.snippet.thumbnails.medium.url
    }));

    await logUsage(uid, 'analyze_competitors_legacy', { channelName, channelId });

    return { success: true, channelId, videos, count: videos.length };
  } catch (error) {
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', 'Competitor analysis failed. Please try again.');
  }
});

exports.searchHistory = functions.https.onCall(async (data, context) => {
  // SECURITY FIX: Require authentication and only return user's own data
  const uid = await verifyAuth(context);

  const { limit = 10 } = data || {};
  // Limit the maximum to prevent abuse
  const safeLimit = Math.min(Math.max(1, limit), 50);

  const snapshot = await db.collection('analyses')
    .where('userId', '==', uid)  // SECURITY FIX: Only return user's own analyses
    .orderBy('createdAt', 'desc')
    .limit(safeLimit)
    .get();

  const results = [];
  snapshot.forEach(doc => {
    results.push({
      id: doc.id,
      ...doc.data(),
      createdAt: doc.data().createdAt?.toDate().toISOString()
    });
  });

  return { success: true, results, count: results.length };
});

exports.deleteAnalysis = functions.https.onCall(async (data, context) => {
  // SECURITY FIX: Require authentication and verify ownership
  const uid = await verifyAuth(context);

  const { id } = data || {};
  if (!id) {
    throw new functions.https.HttpsError('invalid-argument', 'Analysis ID is required');
  }

  // SECURITY FIX: Check ownership before deleting
  const doc = await db.collection('analyses').doc(id).get();

  if (!doc.exists) {
    throw new functions.https.HttpsError('not-found', 'Analysis not found');
  }

  const docData = doc.data();

  // Allow deletion if user owns the record OR if it's a legacy record without userId (admin can delete)
  // For legacy records without userId, check if user is admin
  if (docData.userId && docData.userId !== uid) {
    // Has userId but doesn't match - check if admin
    const isUserAdmin = await isAdmin(uid);
    if (!isUserAdmin) {
      throw new functions.https.HttpsError('permission-denied', 'You can only delete your own analyses');
    }
  } else if (!docData.userId) {
    // Legacy record without userId - only admins can delete
    const isUserAdmin = await isAdmin(uid);
    if (!isUserAdmin) {
      throw new functions.https.HttpsError('permission-denied', 'Legacy analyses can only be deleted by administrators');
    }
  }

  await db.collection('analyses').doc(id).delete();
  await logUsage(uid, 'delete_analysis_legacy', { analysisId: id });

  return { success: true };
});

// ==============================================
// OPTIMIZATION HISTORY
// ==============================================

exports.getOptimizationHistory = functions.https.onCall(async (data, context) => {
  // Verify authentication
  if (!context.auth) {
    throw new functions.https.HttpsError(
      'unauthenticated',
      'User must be authenticated to view history'
    );
  }

  const userId = context.auth.uid;

  // Helper function to sanitize any value to plain JSON
  const sanitize = (obj) => {
    if (obj === null || obj === undefined) return null;
    try {
      return JSON.parse(JSON.stringify(obj));
    } catch (e) {
      return null;
    }
  };

  // Helper to safely get timestamp as number
  const getTimestamp = (field) => {
    if (!field) return Date.now();
    if (typeof field === 'number') return field;
    if (typeof field.toMillis === 'function') return field.toMillis();
    if (field._seconds) return field._seconds * 1000;
    if (field instanceof Date) return field.getTime();
    return Date.now();
  };

  try {
    // Simple query without orderBy to avoid index issues
    const snapshot = await db.collection('optimizations')
      .where('userId', '==', userId)
      .limit(50)
      .get();

    const history = [];

    snapshot.forEach(doc => {
      try {
        const docData = doc.data();

        // Extract and sanitize each field individually
        const item = {
          id: String(doc.id),
          videoUrl: String(docData.videoUrl || ''),
          videoInfo: sanitize(docData.videoInfo),
          titles: Array.isArray(docData.titles) ? docData.titles.map(t => String(t)) : [],
          description: String(docData.description || ''),
          tags: Array.isArray(docData.tags) ? docData.tags.map(t => String(t)) : [],
          seoAnalysis: sanitize(docData.seoAnalysis),
          timestamp: getTimestamp(docData.createdAt)
        };

        history.push(item);
      } catch (docError) {
        console.error('Error processing doc:', doc.id, docError);
      }
    });

    // Sort by timestamp descending
    history.sort((a, b) => b.timestamp - a.timestamp);

    return {
      success: true,
      history: history,
      count: history.length
    };

  } catch (error) {
    console.error('Error fetching optimization history:', error);
    return {
      success: true,
      history: [],
      count: 0
    };
  }
});

// ==============================================
// BONUS HISTORY
// ==============================================

exports.getBonusHistory = functions.https.onCall(async (data, context) => {
  // Verify authentication
  if (!context.auth) {
    throw new functions.https.HttpsError(
      'unauthenticated',
      'User must be authenticated to view bonus history'
    );
  }

  const userId = context.auth.uid;

  // Helper to safely get timestamp as number
  const getTimestamp = (field) => {
    if (!field) return Date.now();
    if (typeof field === 'number') return field;
    if (typeof field.toMillis === 'function') return field.toMillis();
    if (field._seconds) return field._seconds * 1000;
    if (field instanceof Date) return field.getTime();
    return Date.now();
  };

  try {
    // Query usageLogs for bonus_uses_granted actions for this user
    const snapshot = await db.collection('usageLogs')
      .where('userId', '==', userId)
      .where('action', '==', 'bonus_uses_granted')
      .limit(50)
      .get();

    const history = [];

    snapshot.forEach(doc => {
      try {
        const docData = doc.data();
        const metadata = docData.metadata || {};

        history.push({
          id: String(doc.id),
          tool: String(metadata.tool || 'unknown'),
          amount: parseInt(metadata.amount) || 0,
          grantedBy: String(metadata.grantedBy || 'admin'),
          timestamp: getTimestamp(docData.timestamp)
        });
      } catch (docError) {
        console.error('Error processing bonus log:', doc.id, docError);
      }
    });

    // Sort by timestamp descending (most recent first)
    history.sort((a, b) => b.timestamp - a.timestamp);

    return {
      success: true,
      history: history,
      count: history.length
    };

  } catch (error) {
    console.error('Error fetching bonus history:', error);
    return {
      success: true,
      history: [],
      count: 0
    };
  }
});

// ==============================================
// SETUP ADMIN USER (One-time setup - ONLY when no admins exist)
// ==============================================

exports.setupAdmin = functions.https.onCall(async (data, context) => {
  // Must be authenticated
  if (!context.auth) {
    throw new functions.https.HttpsError(
      'unauthenticated',
      'You must be logged in to set up admin access'
    );
  }

  const userId = context.auth.uid;
  const userEmail = context.auth.token.email;

  try {
    // Check if user is already admin
    const adminDoc = await db.collection('adminUsers')
      .doc(userId)
      .get();

    if (adminDoc.exists) {
      return {
        success: true,
        message: 'You are already an admin!',
        email: userEmail
      };
    }

    // SECURITY FIX: Check if ANY admins exist in the system
    // If admins exist, this endpoint cannot be used for self-promotion
    const existingAdmins = await db.collection('adminUsers').limit(1).get();

    if (!existingAdmins.empty) {
      // Admins already exist - reject self-promotion attempt
      console.warn(`Security: User ${userEmail} (${userId}) attempted unauthorized admin setup`);
      throw new functions.https.HttpsError(
        'permission-denied',
        'Admin access can only be granted by an existing administrator. Please contact your system administrator.'
      );
    }

    // No admins exist - this is first-time setup, allow it
    console.log(`First-time admin setup by ${userEmail}`);

    await db.collection('adminUsers')
      .doc(userId)
      .set({
        uid: userId,
        email: userEmail,
        isAdmin: true,
        createdAt: admin.firestore.FieldValue.serverTimestamp(),
        createdBy: 'first-time-setup'
      });

    // Also update user profile
    await db.collection('users')
      .doc(userId)
      .update({
        isAdmin: true
      });

    await logUsage(userId, 'first_admin_setup', { email: userEmail });

    return {
      success: true,
      message: 'You are now the first admin! Additional admins must be added through the admin panel.',
      email: userEmail,
      userId: userId
    };

  } catch (error) {
    if (error instanceof functions.https.HttpsError) {
      throw error;
    }
    console.error('Error setting up admin:', error);
    throw new functions.https.HttpsError(
      'internal',
      'Failed to set up admin access'
    );
  }
});

// ==============================================
// FIX USER PROFILE (Diagnostic Tool)
// ==============================================

exports.fixUserProfile = functions.https.onCall(async (data, context) => {
  if (!context.auth) {
    throw new functions.https.HttpsError('unauthenticated', 'Must be logged in');
  }

  const userId = context.auth.uid;
  const userEmail = context.auth.token.email;

  try {
    const userDoc = await db.collection('users').doc(userId).get();

    if (!userDoc.exists) {
      console.log(`Creating user profile for ${userEmail}`);
      
      const settingsDoc = await db.collection('adminSettings').doc('config').get();
      const defaultPlan = settingsDoc.exists ? settingsDoc.data()?.defaultPlan || 'free' : 'free';
      
      const planDoc = await db.collection('subscriptionPlans').doc(defaultPlan).get();
      const planLimits = planDoc.exists ? (planDoc.data()?.limits || {}) : {};
      const defaultToolLimit = 2;

      await db.collection('users').doc(userId).set({
        uid: userId,
        email: userEmail,
        displayName: context.auth.token.name || '',
        photoURL: context.auth.token.picture || '',
        createdAt: admin.firestore.FieldValue.serverTimestamp(),
        lastLoginAt: admin.firestore.FieldValue.serverTimestamp(),
        isActive: true,
        isAdmin: false,
        subscription: {
          plan: defaultPlan,
          status: 'active',
          startDate: admin.firestore.FieldValue.serverTimestamp(),
          endDate: null,
          autoRenew: false
        },
        usage: {
          warpOptimizer: {
            usedToday: 0,
            usedTotal: 0,
            limit: planLimits.warpOptimizer?.dailyLimit || defaultToolLimit,
            lastResetAt: admin.firestore.FieldValue.serverTimestamp(),
            cooldownUntil: null
          },
          competitorAnalysis: {
            usedToday: 0,
            usedTotal: 0,
            limit: planLimits.competitorAnalysis?.dailyLimit || defaultToolLimit,
            lastResetAt: admin.firestore.FieldValue.serverTimestamp(),
            cooldownUntil: null
          },
          trendPredictor: {
            usedToday: 0,
            usedTotal: 0,
            limit: planLimits.trendPredictor?.dailyLimit || defaultToolLimit,
            lastResetAt: admin.firestore.FieldValue.serverTimestamp(),
            cooldownUntil: null
          },
          thumbnailGenerator: {
            usedToday: 0,
            usedTotal: 0,
            limit: planLimits.thumbnailGenerator?.dailyLimit || defaultToolLimit,
            lastResetAt: admin.firestore.FieldValue.serverTimestamp(),
            cooldownUntil: null
          }
        },
        notes: '',
        customLimits: {}
      });

      return {
        success: true,
        action: 'created',
        message: 'User profile created successfully!',
        userId: userId,
        email: userEmail
      };
    }

    const userData = userDoc.data();
    let needsUpdate = false;
    const updates = {};

    if (!userData.usage || !userData.usage.warpOptimizer) {
      needsUpdate = true;

      const planDoc = await db.collection('subscriptionPlans').doc(userData.subscription?.plan || 'free').get();
      const planLimits = planDoc.exists ? (planDoc.data()?.limits || {}) : {};
      const defaultToolLimit = 2;

      updates.usage = {
        warpOptimizer: {
          usedToday: 0,
          usedTotal: 0,
          limit: planLimits.warpOptimizer?.dailyLimit || defaultToolLimit,
          lastResetAt: admin.firestore.FieldValue.serverTimestamp(),
          cooldownUntil: null
        },
        competitorAnalysis: {
          usedToday: 0,
          usedTotal: 0,
          limit: planLimits.competitorAnalysis?.dailyLimit || defaultToolLimit,
          lastResetAt: admin.firestore.FieldValue.serverTimestamp(),
          cooldownUntil: null
        },
        trendPredictor: {
          usedToday: 0,
          usedTotal: 0,
          limit: planLimits.trendPredictor?.dailyLimit || defaultToolLimit,
          lastResetAt: admin.firestore.FieldValue.serverTimestamp(),
          cooldownUntil: null
        },
        thumbnailGenerator: {
          usedToday: 0,
          usedTotal: 0,
          limit: planLimits.thumbnailGenerator?.dailyLimit || defaultToolLimit,
          lastResetAt: admin.firestore.FieldValue.serverTimestamp(),
          cooldownUntil: null
        }
      };
    }

    if (needsUpdate) {
      await db.collection('users').doc(userId).update(updates);
      return {
        success: true,
        action: 'updated',
        message: 'User profile updated with usage structure!',
        userId: userId,
        email: userEmail
      };
    }

    return {
      success: true,
      action: 'verified',
      message: 'User profile is correct!',
      userId: userId,
      email: userEmail,
      usage: userData.usage
    };

  } catch (error) {
    console.error('Error fixing user profile:', error);
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to fix user profile. Please try again.'));
  }
});

// ==============================================
// NEW FEATURE: COMPETITOR ANALYSIS
// ==============================================

exports.analyzeCompetitor = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  checkRateLimit(uid, 'analyzeCompetitor', 10);
  await checkUsageLimit(uid, 'competitorAnalysis');

  const { videoUrl } = data;
  if (!videoUrl) {
    throw new functions.https.HttpsError('invalid-argument', 'Video URL is required');
  }

  try {
    const videoId = extractVideoId(videoUrl);

    // Get competitor video data
    const videoResponse = await youtube.videos.list({
      part: 'snippet,statistics,contentDetails',
      id: videoId
    });

    if (!videoResponse.data.items || videoResponse.data.items.length === 0) {
      throw new functions.https.HttpsError('not-found', 'Competitor video not found');
    }

    const video = videoResponse.data.items[0];
    const snippet = video.snippet;
    const stats = video.statistics;

    // Get channel data
    const channelResponse = await youtube.channels.list({
      part: 'snippet,statistics',
      id: snippet.channelId
    });

    const channel = channelResponse.data.items?.[0];

    // Analyze with AI
    const analysisPrompt = `You are a YouTube SEO expert. Analyze this competitor's video and provide actionable insights to BEAT their performance.

COMPETITOR VIDEO DATA:
- Title: ${snippet.title}
- Description: ${snippet.description?.substring(0, 500) || 'No description'}
- Tags: ${snippet.tags?.join(', ') || 'No visible tags'}
- Views: ${parseInt(stats.viewCount || 0).toLocaleString()}
- Likes: ${parseInt(stats.likeCount || 0).toLocaleString()}
- Comments: ${parseInt(stats.commentCount || 0).toLocaleString()}
- Channel: ${snippet.channelTitle}
- Channel Subscribers: ${channel?.statistics?.subscriberCount ? parseInt(channel.statistics.subscriberCount).toLocaleString() : 'Hidden'}
- Published: ${snippet.publishedAt}

Provide your analysis in this EXACT JSON format:
{
  "seoScore": <number 0-100>,
  "strengths": ["strength1", "strength2", "strength3"],
  "weaknesses": ["weakness1", "weakness2", "weakness3"],
  "opportunities": ["opportunity1", "opportunity2", "opportunity3"],
  "betterTitles": ["title1", "title2", "title3"],
  "betterTags": ["tag1", "tag2", "tag3", "tag4", "tag5"],
  "contentGaps": ["gap1", "gap2"],
  "engagementTips": ["tip1", "tip2", "tip3"],
  "estimatedDifficulty": "<easy|medium|hard>",
  "summary": "2-3 sentence summary of how to beat this competitor"
}`;

    const aiResponse = await openai.chat.completions.create({
      model: 'gpt-4',
      messages: [{ role: 'user', content: analysisPrompt }],
      temperature: 0.7,
      max_tokens: 1500
    });

    let analysis;
    try {
      const responseText = aiResponse.choices[0].message.content.trim();
      const jsonMatch = responseText.match(/\{[\s\S]*\}/);
      analysis = JSON.parse(jsonMatch ? jsonMatch[0] : responseText);
    } catch (e) {
      analysis = {
        seoScore: 70,
        strengths: ['Unable to parse full analysis'],
        weaknesses: [],
        opportunities: [],
        betterTitles: [],
        betterTags: [],
        contentGaps: [],
        engagementTips: [],
        estimatedDifficulty: 'medium',
        summary: aiResponse.choices[0].message.content
      };
    }

    await incrementUsage(uid, 'competitorAnalysis');
    await logUsage(uid, 'competitor_analysis', { videoId, competitorChannel: snippet.channelTitle });

    const competitorData = {
      videoId,
      title: snippet.title,
      channelTitle: snippet.channelTitle,
      channelId: snippet.channelId,
      thumbnail: snippet.thumbnails?.high?.url || snippet.thumbnails?.default?.url,
      viewCount: parseInt(stats.viewCount || 0),
      likeCount: parseInt(stats.likeCount || 0),
      commentCount: parseInt(stats.commentCount || 0),
      publishedAt: snippet.publishedAt,
      tags: snippet.tags || [],
      channelSubscribers: channel?.statistics?.subscriberCount ? parseInt(channel.statistics.subscriberCount) : null
    };

    // Save to history
    const historyRef = await db.collection('competitorHistory').add({
      userId: uid,
      videoUrl,
      competitor: competitorData,
      analysis,
      createdAt: admin.firestore.FieldValue.serverTimestamp()
    });

    return {
      success: true,
      historyId: historyRef.id,
      competitor: competitorData,
      analysis
    };

  } catch (error) {
    console.error('Competitor analysis error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Competitor analysis failed. Please try again.'));
  }
});

// ==============================================
// NEW FEATURE: TREND PREDICTOR
// ==============================================

exports.predictTrends = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  checkRateLimit(uid, 'predictTrends', 10);
  await checkUsageLimit(uid, 'trendPredictor');

  const { niche, country = 'US' } = data;
  if (!niche) {
    throw new functions.https.HttpsError('invalid-argument', 'Niche/topic is required');
  }

  try {
    // Get trending videos in the niche
    const searchResponse = await youtube.search.list({
      part: 'snippet',
      q: niche,
      type: 'video',
      order: 'viewCount',
      publishedAfter: new Date(Date.now() - 7 * 24 * 60 * 60 * 1000).toISOString(), // Last 7 days
      maxResults: 15,
      regionCode: country
    });

    const trendingVideos = searchResponse.data.items || [];

    // Get video statistics
    const videoIds = trendingVideos.map(v => v.id.videoId).filter(Boolean);
    let videoStats = [];

    if (videoIds.length > 0) {
      const statsResponse = await youtube.videos.list({
        part: 'statistics,snippet',
        id: videoIds.join(',')
      });
      videoStats = statsResponse.data.items || [];
    }

    // Prepare data for AI analysis
    const trendData = videoStats.map(v => ({
      title: v.snippet.title,
      views: parseInt(v.statistics.viewCount || 0),
      likes: parseInt(v.statistics.likeCount || 0),
      channel: v.snippet.channelTitle,
      published: v.snippet.publishedAt
    }));

    const trendPrompt = `You are a YouTube trend analyst and viral content predictor. Based on this recent trending data in the "${niche}" niche, predict upcoming trends.

RECENT TRENDING VIDEOS (last 7 days):
${trendData.map((v, i) => `${i+1}. "${v.title}" - ${v.views.toLocaleString()} views by ${v.channel}`).join('\n')}

Analyze patterns and predict what will trend next. Provide in this EXACT JSON format:
{
  "currentTrends": [
    {"topic": "topic1", "description": "why it's trending", "growthRate": "rising|stable|declining"},
    {"topic": "topic2", "description": "why it's trending", "growthRate": "rising|stable|declining"},
    {"topic": "topic3", "description": "why it's trending", "growthRate": "rising|stable|declining"}
  ],
  "predictedTrends": [
    {"topic": "predicted1", "reasoning": "why this will trend", "confidence": "high|medium|low", "timeframe": "1-2 weeks|2-4 weeks|1-2 months"},
    {"topic": "predicted2", "reasoning": "why this will trend", "confidence": "high|medium|low", "timeframe": "1-2 weeks|2-4 weeks|1-2 months"},
    {"topic": "predicted3", "reasoning": "why this will trend", "confidence": "high|medium|low", "timeframe": "1-2 weeks|2-4 weeks|1-2 months"}
  ],
  "videoIdeas": [
    {"title": "Suggested video title 1", "description": "Brief description of content", "estimatedViews": "10K-50K|50K-100K|100K-500K|500K+"},
    {"title": "Suggested video title 2", "description": "Brief description of content", "estimatedViews": "10K-50K|50K-100K|100K-500K|500K+"},
    {"title": "Suggested video title 3", "description": "Brief description of content", "estimatedViews": "10K-50K|50K-100K|100K-500K|500K+"},
    {"title": "Suggested video title 4", "description": "Brief description of content", "estimatedViews": "10K-50K|50K-100K|100K-500K|500K+"},
    {"title": "Suggested video title 5", "description": "Brief description of content", "estimatedViews": "10K-50K|50K-100K|100K-500K|500K+"}
  ],
  "bestUploadTimes": ["Day time1", "Day time2", "Day time3"],
  "hashtagsToUse": ["#hashtag1", "#hashtag2", "#hashtag3", "#hashtag4", "#hashtag5"],
  "summary": "2-3 sentence summary of the trend landscape in this niche"
}`;

    const aiResponse = await openai.chat.completions.create({
      model: 'gpt-4',
      messages: [{ role: 'user', content: trendPrompt }],
      temperature: 0.8,
      max_tokens: 2000
    });

    let predictions;
    try {
      const responseText = aiResponse.choices[0].message.content.trim();
      const jsonMatch = responseText.match(/\{[\s\S]*\}/);
      predictions = JSON.parse(jsonMatch ? jsonMatch[0] : responseText);
    } catch (e) {
      predictions = {
        currentTrends: [],
        predictedTrends: [],
        videoIdeas: [],
        bestUploadTimes: [],
        hashtagsToUse: [],
        summary: aiResponse.choices[0].message.content
      };
    }

    await incrementUsage(uid, 'trendPredictor');
    await logUsage(uid, 'trend_prediction', { niche, country });

    // Save to history
    const historyRef = await db.collection('trendHistory').add({
      userId: uid,
      niche,
      country,
      analyzedVideos: trendData.length,
      topPerformers: trendData.slice(0, 5),
      predictions,
      createdAt: admin.firestore.FieldValue.serverTimestamp()
    });

    return {
      success: true,
      historyId: historyRef.id,
      niche,
      country,
      analyzedVideos: trendData.length,
      topPerformers: trendData.slice(0, 5),
      predictions
    };

  } catch (error) {
    console.error('Trend prediction error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Trend prediction failed. Please try again.'));
  }
});

// ==============================================
// NEW FEATURE: AI THUMBNAIL GENERATOR (RunPod)
// ==============================================

exports.generateThumbnail = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  checkRateLimit(uid, 'generateThumbnail', 3); // Lower limit for expensive AI operation
  await checkUsageLimit(uid, 'thumbnailGenerator');

  const { title, style = 'youtube_thumbnail', customPrompt } = data;
  if (!title) {
    throw new functions.https.HttpsError('invalid-argument', 'Video title is required');
  }

  const runpodKey = functions.config().runpod?.key;
  if (!runpodKey) {
    throw new functions.https.HttpsError('failed-precondition', 'RunPod API key not configured. Please set it with: firebase functions:config:set runpod.key="YOUR_KEY"');
  }

  try {
    // Generate optimized prompt for thumbnail using OpenAI
    let imagePrompt;
    try {
      const promptGeneratorResponse = await openai.chat.completions.create({
        model: 'gpt-4',
        messages: [{
          role: 'user',
          content: `Create a detailed image generation prompt for a YouTube thumbnail. The video title is: "${title}"

Style guidelines:
- Eye-catching and click-worthy
- Bold colors and high contrast
- Professional YouTube thumbnail aesthetic
- Should include relevant visual elements
- Text overlay areas should be considered

${customPrompt ? `Additional requirements: ${customPrompt}` : ''}

Provide ONLY the image generation prompt, no explanations. Make it detailed and specific for best results.`
        }],
        temperature: 0.7,
        max_tokens: 300
      });

      imagePrompt = promptGeneratorResponse?.choices?.[0]?.message?.content?.trim();
    } catch (openaiError) {
      console.error('OpenAI prompt generation failed:', openaiError);
      // Fallback: create a direct prompt from title and style
      imagePrompt = `Professional YouTube thumbnail for video titled "${title}". ${customPrompt || 'Eye-catching, high contrast, vibrant colors, professional quality, 4K resolution, dramatic lighting.'}`;
    }

    // Validate imagePrompt is not empty
    if (!imagePrompt || imagePrompt.length < 10) {
      console.log('Generated empty or too short prompt, using fallback');
      imagePrompt = `Professional YouTube thumbnail for video titled "${title}". Eye-catching design with bold colors, high contrast, dramatic lighting, clean composition, suitable for YouTube, 4K quality.`;
    }

    console.log('Generated image prompt:', imagePrompt.substring(0, 100) + '...');
    const negativePrompt = "blurry, low quality, ugly, distorted, watermark, nsfw, text overlay";
    const seed = Math.floor(Math.random() * 999999999999);

    // Generate a signed URL for Firebase Storage upload
    // Use the configured default bucket (ytseo-6d1b0.firebasestorage.app)
    const fileName = `thumbnails/${uid}/${Date.now()}_${seed}.png`;
    const bucket = admin.storage().bucket();
    let uploadUrl;

    console.log('Using storage bucket:', bucket.name);

    try {
      const file = bucket.file(fileName);
      const [signedUrl] = await file.getSignedUrl({
        version: 'v4',
        action: 'write',
        expires: Date.now() + 30 * 60 * 1000, // 30 minutes
        contentType: 'application/octet-stream',
      });
      uploadUrl = signedUrl;
      console.log('Successfully generated signed URL for bucket:', bucket.name);
    } catch (signError) {
      console.error(`Failed to generate signed URL:`, signError.message);
      if (signError.message.includes('iam.serviceAccounts.signBlob') ||
          signError.message.includes('Permission') ||
          signError.message.includes('denied')) {
        throw new functions.https.HttpsError(
          'failed-precondition',
          'Firebase Storage permission not configured. Please grant "Service Account Token Creator" role to your Cloud Functions service account in Google Cloud Console > IAM.'
        );
      }
      throw new functions.https.HttpsError('internal', 'Failed to prepare storage: ' + signError.message);
    }

    const file = bucket.file(fileName);

    // Call RunPod API - HiDream text-to-image
    const runpodEndpoint = 'https://api.runpod.ai/v2/rgq0go2nkcfx4h/run';

    // Build input object with all required parameters
    const runpodInput = {
      positive_prompt: imagePrompt,
      negative_prompt: negativePrompt,
      width: 1280,
      height: 720,
      batch_size: 1,
      shift: 3.0,
      seed: seed,
      steps: 35,
      cfg: 5,
      sampler_name: "euler",
      scheduler: "simple",
      denoise: 1,
      image_upload_url: uploadUrl
    };

    // SECURITY: Log sanitized request info only (no sensitive data)
    console.log('RunPod request:', {
      width: runpodInput.width,
      height: runpodInput.height,
      steps: runpodInput.steps,
      promptLength: runpodInput.positive_prompt?.length || 0
    });

    // Send request to RunPod
    let runpodResponse;
    try {
      runpodResponse = await axios.post(runpodEndpoint, {
        input: runpodInput
      }, {
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${runpodKey}`
        },
        timeout: 30000
      });
    } catch (runpodError) {
      // SECURITY: Log error internally but return sanitized message
      console.error('RunPod API call failed:', runpodError.message);
      throw new functions.https.HttpsError(
        'internal',
        'Image generation service unavailable. Please try again later.'
      );
    }

    const jobId = runpodResponse.data.id;
    const status = runpodResponse.data.status;
    console.log('RunPod job started:', { jobId, status });

    // Generate public URL for the uploaded image using Firebase Storage download URL format
    // The .firebasestorage.app bucket format requires this specific URL structure
    const encodedFileName = encodeURIComponent(fileName);
    const publicUrl = `https://firebasestorage.googleapis.com/v0/b/${bucket.name}/o/${encodedFileName}?alt=media`;

    await incrementUsage(uid, 'thumbnailGenerator');
    await logUsage(uid, 'thumbnail_generation', { title, jobId, fileName });

    // Save to history
    const historyRef = await db.collection('thumbnailHistory').add({
      userId: uid,
      title,
      style: style || 'youtube_thumbnail',
      customPrompt: customPrompt || null,
      prompt: imagePrompt,
      jobId,
      status,
      imageUrl: publicUrl,
      fileName,
      createdAt: admin.firestore.FieldValue.serverTimestamp()
    });

    return {
      success: true,
      historyId: historyRef.id,
      jobId,
      status,
      prompt: imagePrompt,
      imageUrl: publicUrl,
      fileName: fileName,
      message: 'Thumbnail generation started. Image will be available at imageUrl when complete.',
      checkEndpoint: `https://api.runpod.ai/v2/rgq0go2nkcfx4h/status/${jobId}`
    };

  } catch (error) {
    console.error('Thumbnail generation error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Thumbnail generation failed. Please try again.'));
  }
});

// Check thumbnail generation status
exports.checkThumbnailStatus = functions.https.onCall(async (data, context) => {
  await verifyAuth(context);

  const { jobId } = data;
  if (!jobId) {
    throw new functions.https.HttpsError('invalid-argument', 'Job ID is required');
  }

  const runpodKey = functions.config().runpod?.key;
  if (!runpodKey) {
    throw new functions.https.HttpsError('failed-precondition', 'RunPod API key not configured');
  }

  try {
    const statusResponse = await axios.get(
      `https://api.runpod.ai/v2/rgq0go2nkcfx4h/status/${jobId}`,
      {
        headers: {
          'Authorization': `Bearer ${runpodKey}`
        },
        timeout: 10000
      }
    );

    const result = statusResponse.data;

    return {
      success: true,
      jobId,
      status: result.status,
      output: result.output || null,
      error: result.error || null
    };

  } catch (error) {
    console.error('Check thumbnail status error:', error);
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to check thumbnail status. Please try again.'));
  }
});

// ==============================================
// FETCH YOUTUBE VIDEO DATA - For Thumbnail Upgrade Feature
// Fetches video metadata and thumbnail URL from YouTube
// ==============================================

exports.fetchYoutubeVideoData = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  checkRateLimit(uid, 'fetchYoutubeData', 10); // 10 per minute

  const { videoId, videoUrl } = data;

  // Extract video ID from URL if provided
  let extractedId = videoId;
  if (!extractedId && videoUrl) {
    const patterns = [
      /(?:youtube\.com\/watch\?v=|youtu\.be\/|youtube\.com\/embed\/|youtube\.com\/v\/)([^&\s?#]+)/,
      /youtube\.com\/shorts\/([^&\s?#]+)/
    ];
    for (const pattern of patterns) {
      const match = videoUrl.match(pattern);
      if (match) {
        extractedId = match[1];
        break;
      }
    }
  }

  if (!extractedId) {
    throw new functions.https.HttpsError('invalid-argument', 'Valid YouTube video ID or URL is required');
  }

  try {
    // Try YouTube Data API if key is configured
    const youtubeApiKey = functions.config().youtube?.key;

    if (youtubeApiKey) {
      const response = await axios.get(
        `https://www.googleapis.com/youtube/v3/videos?part=snippet&id=${extractedId}&key=${youtubeApiKey}`,
        { timeout: 10000 }
      );

      const video = response.data.items?.[0];
      if (!video) {
        throw new functions.https.HttpsError('not-found', 'Video not found');
      }

      const snippet = video.snippet;
      const thumbnails = snippet.thumbnails;

      // Get highest quality thumbnail available
      const thumbnailUrl = thumbnails.maxres?.url ||
                          thumbnails.standard?.url ||
                          thumbnails.high?.url ||
                          thumbnails.medium?.url ||
                          thumbnails.default?.url;

      return {
        success: true,
        videoId: extractedId,
        title: snippet.title,
        description: snippet.description?.substring(0, 500) || '',
        channelName: snippet.channelTitle,
        thumbnailUrl: thumbnailUrl,
        publishedAt: snippet.publishedAt,
        tags: snippet.tags?.slice(0, 10) || []
      };
    }

    // Fallback: Construct thumbnail URL directly (works without API key)
    // YouTube thumbnails follow predictable patterns
    const maxresThumbnail = `https://img.youtube.com/vi/${extractedId}/maxresdefault.jpg`;
    const hqThumbnail = `https://img.youtube.com/vi/${extractedId}/hqdefault.jpg`;

    // Verify thumbnail exists by checking maxres first
    try {
      await axios.head(maxresThumbnail, { timeout: 5000 });
      return {
        success: true,
        videoId: extractedId,
        title: null,
        description: null,
        channelName: null,
        thumbnailUrl: maxresThumbnail,
        fallbackMode: true,
        message: 'Video thumbnail found. Title/description not available without YouTube API key.'
      };
    } catch {
      // Fallback to HQ thumbnail if maxres doesn't exist
      return {
        success: true,
        videoId: extractedId,
        title: null,
        description: null,
        channelName: null,
        thumbnailUrl: hqThumbnail,
        fallbackMode: true,
        message: 'Video thumbnail found. Title/description not available without YouTube API key.'
      };
    }

  } catch (error) {
    console.error('fetchYoutubeVideoData error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', 'Failed to fetch video data. Please check the URL and try again.');
  }
});

// ==============================================
// FETCH YOUTUBE PLAYLIST - For Bulk Thumbnail Upgrade Feature
// Fetches all videos from a YouTube playlist
// ==============================================

exports.fetchYoutubePlaylist = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  checkRateLimit(uid, 'fetchYoutubePlaylist', 5); // 5 per minute

  const { playlistUrl } = data;

  if (!playlistUrl) {
    throw new functions.https.HttpsError('invalid-argument', 'Playlist URL is required');
  }

  // Extract playlist ID from various URL formats
  const extractPlaylistId = (url) => {
    const patterns = [
      /[?&]list=([a-zA-Z0-9_-]+)/,
      /\/playlist\/([a-zA-Z0-9_-]+)/
    ];
    for (const pattern of patterns) {
      const match = url.match(pattern);
      if (match) return match[1];
    }
    return null;
  };

  const playlistId = extractPlaylistId(playlistUrl);
  if (!playlistId) {
    throw new functions.https.HttpsError('invalid-argument', 'Invalid playlist URL. Please use a valid YouTube playlist link.');
  }

  const youtubeApiKey = functions.config().youtube?.key;
  if (!youtubeApiKey) {
    throw new functions.https.HttpsError('failed-precondition', 'YouTube API key not configured. Contact administrator.');
  }

  try {
    const videos = [];
    let nextPageToken = null;
    const maxVideos = 100; // Cap at 100 videos

    // Fetch all pages (YouTube returns max 50 per page)
    do {
      const params = new URLSearchParams({
        part: 'snippet',
        playlistId: playlistId,
        maxResults: '50',
        key: youtubeApiKey
      });
      if (nextPageToken) params.append('pageToken', nextPageToken);

      const response = await axios.get(
        `https://www.googleapis.com/youtube/v3/playlistItems?${params}`,
        { timeout: 15000 }
      );

      if (response.data.error) {
        throw new Error(response.data.error.message);
      }

      for (const item of response.data.items || []) {
        // Skip deleted/private videos
        const title = item.snippet?.title;
        if (title === 'Deleted video' || title === 'Private video') {
          continue;
        }

        const videoId = item.snippet?.resourceId?.videoId;
        if (!videoId) continue;

        const thumbnails = item.snippet?.thumbnails || {};
        videos.push({
          videoId: videoId,
          title: title,
          description: (item.snippet?.description || '').substring(0, 300),
          channelName: item.snippet?.channelTitle || '',
          thumbnailUrl: thumbnails.maxres?.url ||
                        thumbnails.high?.url ||
                        thumbnails.medium?.url ||
                        `https://img.youtube.com/vi/${videoId}/maxresdefault.jpg`,
          position: item.snippet?.position || 0
        });
      }

      nextPageToken = response.data.nextPageToken;
    } while (nextPageToken && videos.length < maxVideos);

    // Get playlist metadata
    const playlistResponse = await axios.get(
      `https://www.googleapis.com/youtube/v3/playlists?part=snippet&id=${playlistId}&key=${youtubeApiKey}`,
      { timeout: 10000 }
    );
    const playlistInfo = playlistResponse.data.items?.[0]?.snippet || {};

    return {
      success: true,
      playlistId: playlistId,
      playlistTitle: playlistInfo.title || 'Unknown Playlist',
      channelName: playlistInfo.channelTitle || 'Unknown Channel',
      videoCount: videos.length,
      videos: videos.slice(0, maxVideos) // Ensure max limit
    };

  } catch (error) {
    console.error('fetchYoutubePlaylist error:', error);
    if (error.response?.data?.error?.message) {
      throw new functions.https.HttpsError('internal', error.response.data.error.message);
    }
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', 'Failed to fetch playlist. Please check the URL and try again.');
  }
});

// ==============================================
// SMART CAPTION SYSTEM v3 - Intelligent Part Selection
// Detects attribution vs content, extracts key phrases
// ==============================================

/**
 * Generate optimal caption by cleaning title and intelligently selecting best part
 * @param {string} title - The full video title
 * @param {number} maxChars - Maximum characters allowed (default 35)
 * @returns {Promise<string>} - Cleaned caption in uppercase
 */
async function generateOptimalCaption(title, maxChars = 35) {
  // Step 1: Try to extract quoted content first (highest priority)
  const quotedContent = extractQuotedContent(title);
  if (quotedContent && quotedContent.length <= maxChars && quotedContent.length >= 3) {
    console.log(`Smart Caption v3: "${title}" â†’ "${quotedContent.toUpperCase()}" (quoted content)`);
    return quotedContent.toUpperCase();
  }

  // Step 2: Clean the title (remove YouTube junk)
  let caption = cleanYouTubeTitle(title);

  // Step 3: If already fits, return it
  if (caption.length <= maxChars) {
    console.log(`Smart Caption v3: "${title}" â†’ "${caption}" (${caption.length} chars) [clean]`);
    return caption;
  }

  // Step 4: Smart truncation with intelligent part selection
  caption = smartTruncateV3(caption, title, maxChars);

  // Step 5: If still too long, use AI to shorten intelligently
  if (caption.length > maxChars) {
    try {
      caption = await aiShortenCaption(title, caption, maxChars);
    } catch (error) {
      console.error('AI caption shortening failed:', error.message);
      // Final fallback: hard truncate at word boundary
      caption = truncateAtWordBoundary(caption, maxChars);
    }
  }

  console.log(`Smart Caption v3: "${title}" â†’ "${caption}" (${caption.length} chars)`);
  return caption;
}

/**
 * Extract content in quotes (single or double) - often the song/key phrase
 * Examples: 'Your Truth', "Night Rainbows", 'L'infinito'
 */
function extractQuotedContent(title) {
  // Try single quotes first (more common in titles)
  const singleQuoteMatch = title.match(/'([^']+)'/);
  if (singleQuoteMatch && singleQuoteMatch[1].length >= 3) {
    return singleQuoteMatch[1];
  }

  // Try double quotes
  const doubleQuoteMatch = title.match(/"([^"]+)"/);
  if (doubleQuoteMatch && doubleQuoteMatch[1].length >= 3) {
    return doubleQuoteMatch[1];
  }

  // Try fancy quotes
  const fancyQuoteMatch = title.match(/[''"]([^''"]+)[''""]/);
  if (fancyQuoteMatch && fancyQuoteMatch[1].length >= 3) {
    return fancyQuoteMatch[1];
  }

  return null;
}

/**
 * Detect if text is "attribution" - generic phrases that should be deprioritized
 * Examples: "Lyrics by KÄ“rd DaiKur", "A New Release by Artist", "Music by X"
 */
function isAttribution(text) {
  const upperText = text.toUpperCase();

  // Patterns that indicate attribution (not the main content)
  const attributionPatterns = [
    /\bBY\s+[A-Z]/i,                    // "by [Name]" - strong indicator
    /\bLYRICS\s+BY\b/i,                 // "Lyrics by"
    /\bMUSIC\s+BY\b/i,                  // "Music by"
    /\bPRODUCED\s+BY\b/i,               // "Produced by"
    /\bDIRECTED\s+BY\b/i,               // "Directed by"
    /\bA\s+NEW\s+(RELEASE|SINGLE|TRACK|SONG)\b/i,  // "A New Release/Single"
    /\bNEW\s+(RELEASE|SINGLE|TRACK|SONG)\s+BY\b/i, // "New Release by"
    /\bFEAT(URING)?\.?\s/i,             // "feat." or "featuring"
    /\bFT\.?\s/i,                       // "ft."
    /\bPRESENTS?\b/i,                   // "presents"
    /\bFROM\s+THE\s+ALBUM\b/i,          // "from the album"
    /\bOUT\s+NOW\b/i,                   // "out now"
  ];

  for (const pattern of attributionPatterns) {
    if (pattern.test(upperText)) {
      return true;
    }
  }

  return false;
}

/**
 * Detect if text looks like a song/content title (short, punchy, no attribution)
 */
function isSongTitle(text) {
  // Short text without attribution patterns is likely a song title
  if (text.length <= 25 && !isAttribution(text)) {
    return true;
  }
  return false;
}

/**
 * Clean YouTube title by removing common junk
 * Preserves the meaningful structure (Artist â€“ Song, How to X, etc.)
 */
function cleanYouTubeTitle(title) {
  let cleaned = title;

  // ROBUST: Remove any parentheses containing common YouTube junk words
  const junkWordsInParens = /\s*\([^)]*\b(official|video|lyric|lyrics|audio|music|hd|4k|1080p|720p|full|visualizer|clip|mv|remaster|remastered|live|acoustic|remix|cover|version|premiere|explicit|clean)\b[^)]*\)/gi;
  cleaned = cleaned.replace(junkWordsInParens, '');

  // Remove remaining parentheses with just years or short codes
  cleaned = cleaned.replace(/\s*\(\s*\d{4}\s*\)/g, ''); // (2024)
  cleaned = cleaned.replace(/\s*\(\s*(feat|ft|prod)\.?[^)]*\)/gi, ''); // (feat. X), (ft. X), (prod. X)

  // ROBUST: Remove any brackets containing common junk words
  const junkWordsInBrackets = /\s*\[[^\]]*\b(official|video|lyric|lyrics|audio|music|hd|4k|full|new|premiere)\b[^\]]*\]/gi;
  cleaned = cleaned.replace(junkWordsInBrackets, '');
  cleaned = cleaned.replace(/\s*\[\s*\d{4}\s*\]/g, ''); // [2024]

  // Remove everything after | (channel name, topic, etc.)
  cleaned = cleaned.replace(/\s*\|.*$/g, '');

  // Remove trailing indicators (but be careful not to remove song parts)
  cleaned = cleaned.replace(/\s*[-â€“â€”]\s*(official\s*)?(video|audio|lyric|lyrics|hd|4k|full)(\s+video)?$/gi, '');

  // Remove "Official" at the start
  cleaned = cleaned.replace(/^official\s*[-â€“â€”:]\s*/gi, '');

  // Remove hashtags
  cleaned = cleaned.replace(/\s*#\w+/g, '');

  // Clean up multiple spaces and trim
  cleaned = cleaned.replace(/\s+/g, ' ').trim();

  // Remove trailing punctuation except ? and !
  cleaned = cleaned.replace(/[,;:\-â€“â€”]+$/, '').trim();

  // Convert to uppercase for thumbnail
  return cleaned.toUpperCase();
}

/**
 * Smart truncation v3 - Intelligently selects best part
 * Key improvement: Detects attribution vs content and prioritizes accordingly
 */
function smartTruncateV3(caption, originalTitle, maxChars) {
  if (caption.length <= maxChars) return caption;

  // Try to cut at natural separators: â€“ - â€” :
  const separators = [' â€“ ', ' â€” ', ' - ', ': '];

  for (const sep of separators) {
    const sepIndex = caption.indexOf(sep);
    if (sepIndex > 0) {
      const parts = caption.split(sep);
      if (parts.length >= 2) {
        const part1 = parts[0].trim();
        const part2 = parts.slice(1).join(sep).trim();

        // Analyze both parts
        const part1IsAttribution = isAttribution(part1);
        const part2IsAttribution = isAttribution(part2);
        const part2IsSong = isSongTitle(part2);

        console.log(`Smart Caption v3 Analysis: part1="${part1}" (attr:${part1IsAttribution}), part2="${part2}" (attr:${part2IsAttribution}, song:${part2IsSong})`);

        // Decision logic: prioritize the content part, not the attribution
        let primaryPart, secondaryPart;

        if (part2IsAttribution && !part1IsAttribution) {
          // Part 2 is attribution (e.g., "Lyrics by X"), use Part 1
          primaryPart = part1;
          secondaryPart = part2;
        } else if (part1IsAttribution && !part2IsAttribution) {
          // Part 1 is attribution, use Part 2
          primaryPart = part2;
          secondaryPart = part1;
        } else if (part2IsSong && part2.length <= part1.length) {
          // Part 2 looks like a song title and is shorter - classic "Artist â€“ Song" format
          primaryPart = part2;
          secondaryPart = part1;
        } else {
          // Default: prefer the first part (usually the hook/title)
          primaryPart = part1;
          secondaryPart = part2;
        }

        // Try 1: Full combination if fits
        const fullCombo = part1 + ' â€“ ' + part2;
        if (fullCombo.length <= maxChars) {
          return fullCombo;
        }

        // Try 2: Primary part alone
        if (primaryPart.length <= maxChars) {
          return primaryPart;
        }

        // Try 3: Truncated primary part
        const truncatedPrimary = truncateAtWordBoundary(primaryPart, maxChars);
        if (truncatedPrimary.length >= 5) {
          return truncatedPrimary;
        }

        // Try 4: Secondary part if primary is too short
        if (secondaryPart.length <= maxChars) {
          return secondaryPart;
        }

        // Try 5: Truncated secondary part
        const truncatedSecondary = truncateAtWordBoundary(secondaryPart, maxChars);
        if (truncatedSecondary.length >= 5) {
          return truncatedSecondary;
        }
      }
    }
  }

  // No good break point found, truncate at word boundary
  return truncateAtWordBoundary(caption, maxChars);
}

/**
 * Truncate at word boundary
 */
function truncateAtWordBoundary(text, maxChars) {
  if (text.length <= maxChars) return text;

  const words = text.split(' ');
  let result = '';

  for (const word of words) {
    const potential = result ? result + ' ' + word : word;
    if (potential.length <= maxChars) {
      result = potential;
    } else {
      break;
    }
  }

  return result || text.substring(0, maxChars);
}

/**
 * Use AI to intelligently shorten a caption while preserving meaning
 */
async function aiShortenCaption(originalTitle, cleanedCaption, maxChars) {
  const response = await openai.chat.completions.create({
    model: 'gpt-4',
    messages: [{
      role: 'system',
      content: `You shorten YouTube thumbnail captions while preserving meaning. Rules:
- Maximum ${maxChars} characters (STRICT)
- Keep the essential meaning and structure
- For "Artist â€“ Song" format: keep both if possible, or prioritize song name
- For "How to X" format: keep the full phrase
- For questions: keep the question intact
- Output in ALL CAPS
- NO extra punctuation
- Output ONLY the shortened caption`
    }, {
      role: 'user',
      content: `Original: "${originalTitle}"
Cleaned: "${cleanedCaption}"

Shorten to max ${maxChars} characters while keeping the meaning:`
    }],
    temperature: 0.3,
    max_tokens: 50
  });

  let result = response.choices?.[0]?.message?.content?.trim().toUpperCase() || '';
  result = result.replace(/^["']|["']$/g, ''); // Remove quotes

  // Validate result
  if (result && result.length <= maxChars && result.length >= 3) {
    return result;
  }

  // AI failed to meet requirements, fall back
  return truncateAtWordBoundary(cleanedCaption, maxChars);
}

// ==============================================
// THUMBNAIL PRO - Multi-Model AI Thumbnail Generator
// Supports: Imagen 4, Gemini (Nano Banana Pro), DALL-E 3
// Features: Reference images, multiple variations, content categories
// ==============================================

exports.generateThumbnailPro = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  checkRateLimit(uid, 'generateThumbnailPro', 5); // 5 per minute for pro generation

  const {
    title,
    style = 'professional',
    customPrompt = '',
    mode = 'quick', // quick | reference | upgrade | faceHero | styleClone | productPro
    category = 'general', // general | gaming | tutorial | vlog | review | news | entertainment
    variations = 1, // 1-4
    referenceImage = null, // { base64, mimeType }
    // NEW Phase 1 & 6 parameters
    referenceType = 'auto', // auto | face | product | style | background | upgrade
    compositionTemplate = 'auto', // auto | face-right | face-center | split-screen | product-hero | action-shot
    faceStrength = 0.85, // 0.5-1.0 - how much to preserve face
    styleStrength = 0.7, // 0.3-1.0 - how much to match style
    expressionModifier = 'keep', // keep | excited | serious | surprised
    backgroundStyle = 'auto', // auto | studio | blur | gradient | custom
    // Thumbnail Upgrade specific parameters
    originalThumbnailUrl = null, // URL to fetch thumbnail from (YouTube)
    youtubeContext = null, // { videoId, title, description, channelName }
    // Face Lock feature - preserve face across batch generation
    faceReferenceImage = null // { base64, mimeType } - face to preserve in all thumbnails
  } = data;

  if (!title || title.trim().length < 3) {
    throw new functions.https.HttpsError('invalid-argument', 'Video title is required (min 3 characters)');
  }

  // Validate variations
  const imageCount = Math.min(Math.max(parseInt(variations) || 1, 1), 4);

  // ==========================================
  // PHASE 1: ENHANCED MODE CONFIGURATION
  // ==========================================
  const modeConfig = {
    quick: { model: 'imagen-4', tokenCost: 2, supportsReference: false },
    reference: { model: 'nano-banana-pro', tokenCost: 4, supportsReference: true },
    upgrade: { model: 'nano-banana-pro', tokenCost: 4, supportsReference: true, isUpgrade: true },
    // Specialized modes
    faceHero: { model: 'nano-banana-pro', tokenCost: 5, supportsReference: true, specialization: 'face' },
    styleClone: { model: 'nano-banana-pro', tokenCost: 4, supportsReference: true, specialization: 'style' },
    productPro: { model: 'nano-banana-pro', tokenCost: 6, supportsReference: true, specialization: 'product' }
  };

  const config = modeConfig[mode] || modeConfig.quick;
  const totalCost = config.tokenCost * imageCount;

  // Validate reference image for reference-supporting modes
  const needsReference = ['reference', 'upgrade', 'faceHero', 'styleClone'].includes(mode);
  const hasReferenceImage = referenceImage?.base64 || originalThumbnailUrl;
  if (needsReference && !hasReferenceImage) {
    throw new functions.https.HttpsError('invalid-argument', `${mode} mode requires a reference image or thumbnail URL`);
  }

  // Fetch thumbnail from URL if provided (for upgrade mode with YouTube)
  let effectiveReferenceImage = referenceImage;
  if (mode === 'upgrade' && originalThumbnailUrl && !referenceImage?.base64) {
    try {
      console.log('Fetching thumbnail from URL:', originalThumbnailUrl);
      const thumbnailResponse = await axios.get(originalThumbnailUrl, {
        responseType: 'arraybuffer',
        timeout: 15000,
        headers: {
          'User-Agent': 'Mozilla/5.0 (compatible; ThumbnailFetcher/1.0)'
        }
      });
      effectiveReferenceImage = {
        base64: Buffer.from(thumbnailResponse.data).toString('base64'),
        mimeType: thumbnailResponse.headers['content-type'] || 'image/jpeg'
      };
      console.log('Successfully fetched thumbnail, size:', effectiveReferenceImage.base64.length);
    } catch (fetchError) {
      console.error('Failed to fetch thumbnail from URL:', fetchError.message);
      throw new functions.https.HttpsError('invalid-argument', 'Failed to fetch thumbnail from URL. Please try uploading the image directly.');
    }
  }

  // ==========================================
  // SMART CAPTION PRE-GENERATION
  // Generate optimal caption ONCE before any prompts
  // Cleans title, preserves structure, max 35 chars
  // ==========================================
  const optimizedCaption = await generateOptimalCaption(title, 35);

  // ==========================================
  // PHASE 1: REVOLUTIONARY PROMPT ENGINEERING
  // ==========================================

  // Reference Type Specialized Prompts (THE KEY FIX)
  const referenceTypePrompts = {
    face: `CRITICAL FACE PRESERVATION REQUIREMENTS:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FACE IDENTITY - MUST PRESERVE EXACTLY:                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Facial structure: exact bone structure, face shape        â”‚
â”‚ â€¢ Eyes: precise shape, color, spacing, brow arch           â”‚
â”‚ â€¢ Nose: exact shape, size, bridge profile                  â”‚
â”‚ â€¢ Mouth: lip shape, size, natural expression               â”‚
â”‚ â€¢ Skin: tone, texture, any distinctive features            â”‚
â”‚ â€¢ Hair: color, style, texture, length exactly as shown     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

COMPOSITION FOR FACE THUMBNAILS:
â€¢ Face should occupy 35-45% of the thumbnail
â€¢ Position face on RIGHT THIRD of frame (golden ratio)
â€¢ Eyes should be in upper third of frame
â€¢ Leave LEFT 40% clear for text overlay space
â€¢ Face should "pop" from background with rim lighting

LIGHTING FOR FACES:
â€¢ Soft key light at 45Â° angle (beauty lighting)
â€¢ Subtle fill light to reduce harsh shadows
â€¢ Rim/hair light for separation from background
â€¢ Catch lights in eyes (essential for life-like look)

QUALITY REQUIREMENTS:
â€¢ 4K photorealistic quality
â€¢ Magazine cover / professional headshot quality
â€¢ Sharp focus on face, subtle background blur (f/2.8 equivalent)
â€¢ Color grade: cinematic with skin tone preservation`,

    product: `CRITICAL PRODUCT SHOWCASE REQUIREMENTS:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PRODUCT IDENTITY - MUST PRESERVE EXACTLY:                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Product shape and proportions: exact dimensions          â”‚
â”‚ â€¢ Brand colors: match precisely                            â”‚
â”‚ â€¢ Logos/text on product: if visible, keep accurate         â”‚
â”‚ â€¢ Material/texture: show quality and finish                â”‚
â”‚ â€¢ Key features: highlight what makes it special            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

COMPOSITION FOR PRODUCT THUMBNAILS:
â€¢ Product as HERO - center or golden ratio position
â€¢ 45Â° hero angle (most flattering for products)
â€¢ Clean background: gradient, solid, or contextual lifestyle
â€¢ Subtle reflection/shadow for grounding and depth
â€¢ Leave space for text overlay (top or side)

LIGHTING FOR PRODUCTS:
â€¢ 3-point professional product photography lighting
â€¢ Soft key light to show form
â€¢ Fill to reveal details in shadows
â€¢ Accent light for highlights and rim

QUALITY REQUIREMENTS:
â€¢ Commercial product photography quality
â€¢ Sharp focus throughout (deep depth of field)
â€¢ Clean, distraction-free presentation
â€¢ Premium, aspirational feel`,

    style: `STYLE TRANSFER REQUIREMENTS:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STYLE ELEMENTS TO EXTRACT AND APPLY:                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Color palette: exact hues, saturation, contrast levels   â”‚
â”‚ â€¢ Lighting style: direction, quality, mood                 â”‚
â”‚ â€¢ Composition approach: framing, balance, focal points     â”‚
â”‚ â€¢ Texture/finish: glossy, matte, gritty, smooth           â”‚
â”‚ â€¢ Overall mood: energetic, calm, dramatic, playful        â”‚
â”‚ â€¢ Visual effects: any gradients, overlays, treatments     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Apply these extracted style elements to create a NEW thumbnail for the given topic.
The result should feel like it belongs in the same "series" as the reference.`,

    background: `BACKGROUND REFERENCE REQUIREMENTS:
Use the reference image as the BACKGROUND or ENVIRONMENT.
Place new subjects/elements INTO this background setting.
Maintain the lighting direction and color temperature of the background.
Ensure new elements are properly composited and lit to match.`,

    upgrade: `THUMBNAIL COMPLETE TRANSFORMATION - CRITICAL INSTRUCTIONS:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ âš ï¸ WARNING: DO NOT JUST ADD TEXT TO THE EXISTING IMAGE!     â”‚
â”‚ You must COMPLETELY TRANSFORM the thumbnail quality.        â”‚
â”‚ The output should look like it was made by a different      â”‚
â”‚ (much better) artist/photographer.                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

TRANSFORMATION REQUIREMENTS (NOT OPTIONAL):

1. DRAMATIC QUALITY UPGRADE:
   â€¢ If original looks "AI-generated" â†’ Make it look PROFESSIONALLY MADE
   â€¢ If original is anime/cartoon â†’ Create STUNNING hyper-detailed version
   â€¢ If original is low-quality â†’ Generate CRYSTAL-CLEAR 4K imagery
   â€¢ The difference should be IMMEDIATELY OBVIOUS

2. PROFESSIONAL PRODUCTION VALUE:
   â€¢ Add CINEMATIC lighting (3-point: key, fill, rim)
   â€¢ Add ATMOSPHERIC effects (particles, haze, volumetric light)
   â€¢ Add MICRO-DETAILS (textures, reflections, environmental elements)
   â€¢ Apply HOLLYWOOD-GRADE color grading

3. COMPOSITION OVERHAUL:
   â€¢ Apply golden ratio / rule of thirds
   â€¢ Create dramatic depth (foreground, subject, background layers)
   â€¢ Add visual storytelling elements
   â€¢ Design for MAXIMUM thumbnail impact

4. WHAT TO KEEP vs CHANGE:
   â€¢ KEEP: The core subject/topic of the video
   â€¢ CHANGE: Everything else - quality, style, composition, lighting

OUTPUT REQUIREMENTS:
â€¢ 16:9 aspect ratio, broadcast/print quality
â€¢ Must look like it cost $500+ to produce
â€¢ Photorealistic OR stunning illustration (your choice based on topic)
â€¢ MUST include bold, professional text caption`
  };

  // ==========================================
  // PHASE 4: ADVANCED PROMPT TEMPLATES
  // ==========================================

  // YouTube-Optimized Thumbnail Formulas
  const thumbnailFormulas = {
    curiosityGap: {
      prompt: 'Subject showing shocked/surprised expression, one hand raised pointing at something mysterious off-screen or partially visible. Big expressive eyes, slightly open mouth conveying amazement. Mystery element blurred or partially hidden to create intrigue.',
      composition: 'Subject on right, mystery element on left, dramatic lighting'
    },
    transformation: {
      prompt: 'Split-screen style showing dramatic before/after transformation. Clear visual divide (diagonal or vertical). High contrast between the two states. Arrow or visual indicator showing progression.',
      composition: 'Even split, clear contrast, transformation arrow'
    },
    faceContext: {
      prompt: 'Large expressive face taking up right portion of frame, relevant context/props/background filling the left side. Face shows appropriate emotion for the content. Context elements support the video topic visually.',
      composition: 'Face 40% on right, context 60% on left'
    },
    productHero: {
      prompt: 'Product showcased as the hero with dramatic studio lighting. Clean gradient or contextual background. Product at compelling 45-degree angle. Subtle reflection below for premium feel. Space for title text.',
      composition: 'Product centered, clean background, text space top/bottom'
    },
    reaction: {
      prompt: 'Close-up face showing intense, exaggerated emotion filling most of the frame. Expression is unmistakable and attention-grabbing. Bold, vibrant colors. High energy feel with subtle effect elements (sparkles, glow, etc).',
      composition: 'Face 70% of frame, minimal background, maximum impact'
    },
    educational: {
      prompt: 'Clean, professional layout with clear visual hierarchy. Subject matter visualized clearly. Trust-building elements. Step numbers or progression indicators if applicable. Expert/authority positioning.',
      composition: 'Organized layout, clear focal point, professional feel'
    },
    gaming: {
      prompt: 'Dynamic, action-packed composition with vibrant neon colors. Game-style effects (particles, glow, energy). High contrast and saturation. Dramatic pose or moment captured. Esports/streaming aesthetic.',
      composition: 'Dynamic angles, effect overlays, gaming aesthetic'
    }
  };

  // ==========================================
  // PHASE 6: COMPOSITION TEMPLATES
  // ==========================================

  const compositionTemplates = {
    'auto': {
      name: 'Auto (AI decides)',
      prompt: 'Compose for maximum YouTube thumbnail impact. Position key elements using rule of thirds. Leave appropriate space for text overlay.',
      textSpace: 'adaptive'
    },
    'face-right': {
      name: 'Face Right (Most Effective)',
      prompt: 'Position the main subject/face on the RIGHT THIRD of the frame, looking slightly toward center-left. Face should occupy 35-45% of frame height. LEFT 40% of frame should be relatively clear or have non-competing elements for text overlay. Background should complement but not distract.',
      textSpace: 'left 40%'
    },
    'face-center': {
      name: 'Face Center Impact',
      prompt: 'Position the main subject/face CENTERED in frame, large and impactful (50-60% of frame). Dramatic lighting from above or side. Minimal, dark, or blurred background. This composition relies on facial expression alone - make it powerful.',
      textSpace: 'top and bottom edges'
    },
    'split-screen': {
      name: 'Before/After Split',
      prompt: 'Divide the image vertically or diagonally into two distinct halves. Left side shows "before" state, right side shows "after" state. Clear visual contrast between the two. Consider adding a subtle dividing line or gradient transition.',
      textSpace: 'top banner area'
    },
    'product-hero': {
      name: 'Product Spotlight',
      prompt: 'Position the product/object as the CENTRAL HERO of the image. Clean background (gradient or solid, not busy). Product lit dramatically with rim lighting. Subtle shadow/reflection below for grounding. Premium, aspirational feel.',
      textSpace: 'top third or bottom third'
    },
    'action-shot': {
      name: 'Dynamic Action',
      prompt: 'Capture a dynamic moment with sense of movement and energy. Subject positioned off-center (rule of thirds). Motion blur on background or secondary elements. Bright accent colors and high contrast. Convey excitement and energy.',
      textSpace: 'varies - work around action'
    },
    'collage': {
      name: 'Multi-Element',
      prompt: 'Arrange multiple elements/images in a cohesive collage style. Main element largest and most prominent. Supporting elements smaller and positioned around edges. Unified color treatment ties everything together.',
      textSpace: 'center or strategic gaps'
    }
  };

  // ==========================================
  // ENHANCED CATEGORY PROMPTS
  // ==========================================

  const categoryPrompts = {
    general: 'Professional YouTube thumbnail with eye-catching design, bold visual hierarchy, and click-worthy appeal. Universal style that works across topics.',
    gaming: 'HIGH ENERGY gaming thumbnail with: vibrant neon colors (cyan, magenta, electric blue), dramatic RGB-style lighting, action-packed dynamic composition, game UI elements or effects, esports/streaming aesthetic, particle effects and glow, dark background with color pops.',
    tutorial: 'EDUCATIONAL thumbnail with: clean organized layout, professional and trustworthy appearance, clear visual hierarchy showing the topic, step indicators or numbered elements, expert positioning, before/after if applicable, tools or materials visible if relevant.',
    vlog: 'AUTHENTIC vlog thumbnail with: warm personal aesthetic, lifestyle photography feel, genuine relatable expression, natural lighting (golden hour ideal), candid moment captured, personal branding consistency, emotional connection focus.',
    review: 'PRODUCT REVIEW thumbnail with: professional product showcase as hero, comparison layout if vs video, trust signals (checkmarks, ratings visual), clean background letting product shine, verdict/conclusion visual hint, expert reviewer positioning.',
    news: 'NEWS/COMMENTARY thumbnail with: bold impactful headline-style design, serious professional tone, current events aesthetic, authority positioning, dramatic or concerned expression if person featured, bold typography-friendly layout.',
    entertainment: 'ENTERTAINMENT thumbnail with: maximum energy and drama, bold saturated colors, exaggerated expressions, movie-poster quality production, dynamic composition, celebrity/influencer styling, peak emotional moment captured.'
  };

  // ==========================================
  // ENHANCED STYLE PROMPTS
  // ==========================================

  const stylePrompts = {
    professional: 'PROFESSIONAL STYLE: Clean and polished look, sharp focus throughout, studio-quality lighting (soft key, fill, rim), high contrast with controlled highlights, corporate-appropriate color palette, premium finish, trustworthy and competent feel.',
    dramatic: 'DRAMATIC STYLE: Cinematic movie-poster quality, intense chiaroscuro lighting, bold shadows and highlights, rich saturated colors, emotional intensity, epic scale feeling, film color grade (teal/orange or similar), theatrical composition.',
    minimal: 'MINIMAL STYLE: Clean simplicity, generous negative space, limited color palette (2-3 colors max), elegant typography-friendly, soft muted tones, breathing room in composition, sophisticated restraint, Scandinavian design influence.',
    bold: 'BOLD STYLE: Maximum visual impact, vibrant fully-saturated colors, high energy composition, dynamic angles, attention-demanding contrast, graphic design influence, pattern/texture use, unapologetic brightness.'
  };

  // ==========================================
  // EXPRESSION MODIFIER PROMPTS
  // ==========================================

  const expressionModifiers = {
    keep: '', // Don't modify expression
    excited: 'Expression should convey excitement and enthusiasm - bright eyes, genuine smile, energetic and engaging.',
    serious: 'Expression should convey seriousness and authority - confident gaze, composed demeanor, professional gravitas.',
    surprised: 'Expression should convey surprise and amazement - widened eyes, raised eyebrows, open mouth showing genuine shock.',
    curious: 'Expression should convey curiosity and intrigue - slightly raised eyebrow, thoughtful look, engaged and interested.',
    confident: 'Expression should convey confidence and expertise - direct eye contact feel, slight knowing smile, authoritative presence.'
  };

  // ==========================================
  // BACKGROUND STYLE PROMPTS
  // ==========================================

  const backgroundStyles = {
    auto: 'Background should complement the subject and content appropriately.',
    studio: 'Clean professional studio background - seamless gradient (dark to light or vice versa), perfect for subject isolation, corporate and polished feel.',
    blur: 'Softly blurred background (bokeh effect, f/1.8 equivalent) keeping subject sharp. Creates depth and focuses attention on subject.',
    gradient: 'Smooth color gradient background that complements the subject. Can be radial (spotlight effect) or linear (modern feel).',
    contextual: 'Relevant contextual background that supports the video topic. Should add meaning but not distract from the main subject.',
    dark: 'Dark/black background for dramatic effect and maximum subject pop. Good for gaming, dramatic, or premium feel.',
    vibrant: 'Vibrant colorful background with energy. Gradients, patterns, or abstract elements that add visual interest.'
  };

  try {
    // Check and deduct tokens
    const tokenRef = db.collection('creativeTokens').doc(uid);
    let tokenDoc = await tokenRef.get();
    let balance = 0;

    if (!tokenDoc.exists) {
      // Initialize new user with free tier tokens
      const initialTokens = {
        balance: 50,
        rollover: 0,
        plan: 'free',
        monthlyAllocation: 50,
        lastRefresh: admin.firestore.FieldValue.serverTimestamp(),
        createdAt: admin.firestore.FieldValue.serverTimestamp()
      };
      await tokenRef.set(initialTokens);
      balance = 50;
    } else {
      balance = tokenDoc.data().balance || 0;
    }

    if (balance < totalCost) {
      throw new functions.https.HttpsError('resource-exhausted',
        `Insufficient tokens. Need ${totalCost}, have ${balance}. Please upgrade your plan.`);
    }

    // ==========================================
    // PHASE 2: SMART REFERENCE ANALYSIS
    // ==========================================

    // Determine effective reference type (auto-detect or use specified)
    let effectiveReferenceType = referenceType;
    let referenceAnalysis = null;

    if (effectiveReferenceImage && effectiveReferenceImage.base64 && (referenceType === 'auto' || referenceType === 'upgrade')) {
      // Auto-detect reference type using Gemini Vision (also used for upgrade mode analysis)
      try {
        const geminiApiKey = functions.config().gemini?.key;
        if (geminiApiKey) {
          const aiAnalysis = new GoogleGenAI({ apiKey: geminiApiKey });
          const analysisResult = await aiAnalysis.models.generateContent({
            model: 'gemini-2.0-flash',
            contents: [{
              role: 'user',
              parts: [
                { inlineData: { mimeType: effectiveReferenceImage.mimeType || 'image/png', data: effectiveReferenceImage.base64 } },
                { text: `Analyze this image for YouTube thumbnail generation. Respond in JSON format only:
{
  "primarySubject": "face|product|scene|style",
  "hasFace": true/false,
  "faceDetails": { "position": "left|center|right", "expression": "description", "prominentFeatures": ["feature1", "feature2"] },
  "hasProduct": true/false,
  "productDetails": { "type": "description", "colors": ["color1", "color2"], "brandVisible": true/false },
  "dominantColors": ["#hex1", "#hex2", "#hex3"],
  "lightingStyle": "studio|natural|dramatic|soft|harsh",
  "mood": "energetic|calm|professional|playful|serious",
  "compositionStyle": "portrait|product-shot|scene|abstract",
  "recommendedUse": "face-preservation|style-transfer|product-showcase|background"
}` }
              ]
            }]
          });

          const analysisText = analysisResult.candidates?.[0]?.content?.parts?.[0]?.text || '';
          const jsonMatch = analysisText.match(/\{[\s\S]*\}/);
          if (jsonMatch) {
            referenceAnalysis = JSON.parse(jsonMatch[0]);
            // Auto-determine reference type based on analysis
            if (referenceAnalysis.hasFace && referenceAnalysis.primarySubject === 'face') {
              effectiveReferenceType = 'face';
            } else if (referenceAnalysis.hasProduct || referenceAnalysis.primarySubject === 'product') {
              effectiveReferenceType = 'product';
            } else if (referenceAnalysis.primarySubject === 'scene') {
              effectiveReferenceType = 'background';
            } else {
              effectiveReferenceType = 'style';
            }
            console.log(`Reference analysis: detected ${effectiveReferenceType} type`, referenceAnalysis);
          }
        }
      } catch (analysisError) {
        console.log('Reference analysis skipped:', analysisError.message);
        // Default to face if analysis fails and mode suggests face
        effectiveReferenceType = (mode === 'faceHero') ? 'face' : 'style';
      }
    }

    // Override reference type for specialized modes
    if (mode === 'faceHero') effectiveReferenceType = 'face';
    if (mode === 'styleClone') effectiveReferenceType = 'style';
    if (mode === 'productPro') effectiveReferenceType = 'product';
    if (mode === 'upgrade') effectiveReferenceType = 'upgrade'; // Keep upgrade type

    // Build the enhanced prompt
    const categoryEnhancement = categoryPrompts[category] || categoryPrompts.general;
    const styleEnhancement = stylePrompts[style] || stylePrompts.professional;
    const compositionGuide = compositionTemplates[compositionTemplate] || compositionTemplates.auto;
    const expressionGuide = expressionModifiers[expressionModifier] || '';
    const backgroundGuide = backgroundStyles[backgroundStyle] || backgroundStyles.auto;

    let imagePrompt;
    try {
      // ==========================================
      // ENHANCED GPT-4 PROMPT GENERATION
      // ==========================================

      // Build context for reference-based generation
      let referenceContext = '';
      if (effectiveReferenceImage && effectiveReferenceType) {
        // Add YouTube context for upgrade mode
        const youtubeContextStr = (mode === 'upgrade' && youtubeContext) ? `
VIDEO CONTEXT (for better relevance):
- Title: "${youtubeContext.title || 'Unknown'}"
- Channel: "${youtubeContext.channelName || 'Unknown'}"
- Description: "${(youtubeContext.description || '').substring(0, 200)}"
` : '';

        referenceContext = `
REFERENCE IMAGE PROVIDED - Type: ${effectiveReferenceType.toUpperCase()}
${referenceAnalysis ? `Analysis: ${JSON.stringify(referenceAnalysis)}` : ''}
${youtubeContextStr}

${referenceTypePrompts[effectiveReferenceType] || ''}
`;
      }

      // Use pre-generated optimal caption (no inline generation needed)
      const promptGeneratorResponse = await openai.chat.completions.create({
        model: 'gpt-4',
        messages: [{
          role: 'system',
          content: `You are an expert YouTube thumbnail designer and AI image prompt engineer. Your prompts consistently produce viral, high-CTR thumbnails. You understand composition, color psychology, typography, and what makes viewers click. You ALWAYS include bold text captions in your thumbnail designs because professional YouTube thumbnails need eye-catching text.`
        }, {
          role: 'user',
          content: `Create a DETAILED image generation prompt for an AMAZING YouTube thumbnail.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
VIDEO TOPIC
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Category: ${category}
Visual Style: ${style}
${customPrompt ? `Creator's Notes: ${customPrompt}` : ''}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
STYLE & CATEGORY REQUIREMENTS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
${categoryEnhancement}

${styleEnhancement}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
COMPOSITION TEMPLATE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
${compositionGuide.prompt}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
BACKGROUND STYLE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
${backgroundGuide}

${expressionGuide ? `â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
EXPRESSION GUIDANCE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
${expressionGuide}` : ''}

${referenceContext ? `â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
REFERENCE IMAGE REQUIREMENTS (CRITICAL)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
${referenceContext}` : ''}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âš ï¸ MANDATORY TEXT CAPTION - USE EXACT TEXT âš ï¸
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
The thumbnail MUST include this EXACT text: "${optimizedCaption}"
âš ï¸ DO NOT MODIFY, ADD TO, OR CHANGE THIS TEXT IN ANY WAY
âš ï¸ Use these EXACT ${optimizedCaption.length} characters, no more, no less
- Text style: Thick, bold sans-serif (Impact/Bebas Neue style)
- Text color: High contrast - white with black stroke, or vibrant color
- Text size: Large enough to read at small thumbnail sizes
- Text position: Prominent placement that doesn't cover faces

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
OUTPUT REQUIREMENTS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
- Format: 16:9 aspect ratio (1280x720), YouTube thumbnail
- Quality: 4K photorealistic, professional photography quality
- Must be INSTANTLY eye-catching at small sizes (search results)
- Colors should pop and contrast well
- Main subject must be immediately clear
- Text must be EXACTLY as specified above - no additions or changes

Generate a comprehensive, detailed prompt that will produce a STUNNING thumbnail WITH the exact text specified.
Output ONLY the prompt, no explanations or preamble.`
        }],
        temperature: 0.7,
        max_tokens: 600
      });

      imagePrompt = promptGeneratorResponse?.choices?.[0]?.message?.content?.trim();
    } catch (openaiError) {
      console.error('OpenAI prompt generation failed:', openaiError.message);
      // Fallback prompt using pre-generated optimal caption
      imagePrompt = `${categoryEnhancement}. ${styleEnhancement}. ${compositionGuide.prompt}. ${customPrompt || ''} ${backgroundGuide}. Add EXACT text "${optimizedCaption}" in thick bold sans-serif font with high contrast (white with black outline). DO NOT change or add to this text. High quality, 4K resolution, professional YouTube thumbnail, 16:9 aspect ratio.`;
    }

    // Ensure prompt is valid
    if (!imagePrompt || imagePrompt.length < 20) {
      imagePrompt = `Professional YouTube thumbnail. ${categoryEnhancement}. ${styleEnhancement}. ${compositionGuide.prompt}. Add EXACT bold text "${optimizedCaption}" in thick sans-serif font - DO NOT modify this text. Eye-catching design, bold colors, high contrast, 4K quality.`;
    }

    // Enhanced negative prompt (note: we allow text for captions, but avoid illegible/excessive text)
    const negativePrompt = "blurry, low quality, ugly, distorted faces, watermark, nsfw, cluttered, amateur, bad anatomy, disfigured, poorly drawn face, mutation, mutated, extra limbs, ugly, poorly drawn hands, missing limbs, floating limbs, disconnected limbs, malformed hands, blur, out of focus, long neck, long body, disgusting, bad proportions, gross proportions, error, missing fingers, cropped, worst quality, jpeg artifacts, signature, illegible text, garbled text, misspelled words";
    const storage = admin.storage().bucket();
    const timestamp = Date.now();
    const generatedImages = [];
    let usedModel = config.model;

    // ==========================================
    // MODEL-SPECIFIC GENERATION
    // ==========================================

    if (config.model === 'nano-banana-pro') {
      // Gemini Image Generation with Reference Support
      const geminiApiKey = functions.config().gemini?.key;
      if (!geminiApiKey) {
        throw new functions.https.HttpsError('failed-precondition', 'Gemini API key not configured');
      }

      const ai = new GoogleGenAI({ apiKey: geminiApiKey });
      // Use gemini-3-pro-image-preview - SAME as Creative Studio where face preservation works!
      const geminiModelId = 'gemini-3-pro-image-preview';

      console.log(`Generating ${imageCount} thumbnail(s) with Gemini: ${geminiModelId}`);

      for (let imgIdx = 0; imgIdx < imageCount; imgIdx++) {
        try {
          // Build content parts - REFERENCE IMAGE FIRST (like Creative Studio)
          const contentParts = [];

          // Check if Face Lock is enabled (face reference provided separately)
          const hasFaceLock = faceReferenceImage && faceReferenceImage.base64;

          if (hasFaceLock) {
            // FACE LOCK MODE: Add face reference FIRST (for identity preservation)
            contentParts.push({
              inlineData: {
                mimeType: faceReferenceImage.mimeType || 'image/jpeg',
                data: faceReferenceImage.base64
              }
            });
            console.log('Added FACE LOCK reference image as input (identity preservation)');

            // Then add the original thumbnail SECOND (for content/style reference)
            if (effectiveReferenceImage && effectiveReferenceImage.base64) {
              contentParts.push({
                inlineData: {
                  mimeType: effectiveReferenceImage.mimeType || 'image/png',
                  data: effectiveReferenceImage.base64
                }
              });
              console.log('Added original thumbnail as SECOND input (content reference)');
            }
          } else if (effectiveReferenceImage && effectiveReferenceImage.base64) {
            // Standard mode - add reference image FIRST (this is how Creative Studio does it)
            contentParts.push({
              inlineData: {
                mimeType: effectiveReferenceImage.mimeType || 'image/png',
                data: effectiveReferenceImage.base64
              }
            });
            console.log('Added reference image as input (face/character reference)');
          }

          // Build prompt - USE SAME SIMPLE FORMAT AS CREATIVE STUDIO
          // Creative Studio's working format: "Using the provided image as a character/face reference to maintain consistency, generate a new image: ${prompt}"
          let finalPrompt;

          // Check if Face Lock mode with upgrade
          if (hasFaceLock && (mode === 'upgrade' || effectiveReferenceType === 'upgrade')) {
            // ============================================================
            // FACE LOCK + UPGRADE MODE - Preserve face while upgrading content
            // ============================================================
            let youtubeCtx = '';
            if (youtubeContext) {
              const tags = youtubeContext.tags?.slice(0, 5).join(', ') || '';
              const descPreview = (youtubeContext.description || '').substring(0, 200);
              youtubeCtx = `
Video Title: "${youtubeContext.title || 'Unknown'}"
Channel: ${youtubeContext.channelName || 'Unknown'}
${descPreview ? `Topic: ${descPreview}...` : ''}`;
            }

            // Use pre-generated optimal caption
            finalPrompt = `You are creating a PROFESSIONAL YouTube thumbnail with MANDATORY FACE PRESERVATION.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âš ï¸ TWO IMAGES PROVIDED - CRITICAL INSTRUCTIONS âš ï¸
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
IMAGE 1 (FACE REFERENCE): This shows the EXACT FACE you MUST use.
- The person in your output MUST look EXACTLY like this person
- Same facial structure, same eyes, same nose, same mouth
- Same skin tone, same hair style/color
- This is NON-NEGOTIABLE - the face must be RECOGNIZABLE as the same person

IMAGE 2 (CONTENT REFERENCE): This shows the CONTENT/STYLE to upgrade.
- Use this for the scene concept, composition, and theme
- DRAMATICALLY improve the quality to professional level
- But REPLACE any face with the EXACT face from Image 1
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
${youtubeCtx}

REQUIREMENTS:
1. FACE IDENTITY (CRITICAL): Output face MUST match Image 1 exactly
   - Same person, instantly recognizable
   - Viewers should say "That's the same person!"

2. CONTENT UPGRADE: Transform Image 2's concept to studio quality
   - Cinematic lighting, 4K clarity, professional composition
   - Magazine/movie poster level quality
   - Use the theme/concept from Image 2

3. COMPOSITION:
   - Position the face on right side (golden ratio)
   - Leave space on left for text overlay

âš ï¸ MANDATORY TEXT - USE EXACT TEXT âš ï¸
Add EXACTLY this text: "${optimizedCaption}" - thick sans-serif font, high contrast, with shadow/glow.
DO NOT modify, add to, or change this text in ANY way.

OUTPUT: 16:9 YouTube thumbnail with the EXACT face from Image 1 in an upgraded version of Image 2's scene.`;

          } else if (effectiveReferenceImage && effectiveReferenceImage.base64) {
            // ============================================================
            // MATCH CREATIVE STUDIO'S SIMPLE, WORKING FORMAT
            // ============================================================
            if (mode === 'upgrade' || effectiveReferenceType === 'upgrade') {
              // THUMBNAIL UPGRADE MODE - Create SEO-optimized improved version
              let youtubeCtx = '';
              if (youtubeContext) {
                const tags = youtubeContext.tags?.slice(0, 5).join(', ') || '';
                const descPreview = (youtubeContext.description || '').substring(0, 200);
                youtubeCtx = `
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
VIDEO CONTEXT (USE THIS FOR SEO-OPTIMIZED THUMBNAIL):
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Title: "${youtubeContext.title || 'Unknown'}"
Channel: ${youtubeContext.channelName || 'Unknown'}
${descPreview ? `Description: ${descPreview}...` : ''}
${tags ? `Keywords/Tags: ${tags}` : ''}
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•`;
              }

              // Use pre-generated optimal caption
              finalPrompt = `You are a world-class thumbnail designer creating a COMPLETE VISUAL TRANSFORMATION.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âš ï¸ CRITICAL INSTRUCTION - READ CAREFULLY âš ï¸
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
The provided image is a LOW-QUALITY original that needs DRAMATIC improvement.
DO NOT just add text to the existing image.
DO NOT preserve the original style if it looks amateur/AI-generated.
You MUST CREATE A COMPLETELY NEW, PROFESSIONAL-GRADE thumbnail.

TRANSFORM the concept into STUDIO-QUALITY, BROADCAST-READY artwork.
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
${youtubeCtx}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
VISUAL TRANSFORMATION REQUIREMENTS (MANDATORY):
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. QUALITY LEAP: Transform amateur/AI-looking images into PHOTOREALISTIC, CINEMATIC quality
   - If input is anime/cartoon style â†’ Create stunning, hyper-detailed illustration OR photorealistic version
   - If input is low-res/blurry â†’ Generate crystal-clear, 4K-quality imagery
   - If input looks "AI-generated" â†’ Make it indistinguishable from professional photography/art

2. LIGHTING REVOLUTION:
   - Add dramatic, professional 3-point lighting (key, fill, rim)
   - Create depth with volumetric light, god rays, or atmospheric haze
   - Use cinematic color grading (teal/orange, moody blues, warm golds)

3. COMPOSITION MASTERY:
   - Apply golden ratio / rule of thirds
   - Create clear visual hierarchy with dominant focal point
   - Add depth layers (foreground interest, subject, background)

4. DETAIL ENHANCEMENT:
   - Add micro-details: textures, reflections, particles, atmosphere
   - Include environmental storytelling elements
   - Create a sense of scale and drama

5. PROFESSIONAL POLISH:
   - Magazine cover / movie poster quality
   - No amateur artifacts, no "AI look"
   - Hollywood production value

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âš ï¸ MANDATORY TEXT - USE EXACT TEXT âš ï¸
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Add EXACTLY this text: "${optimizedCaption}"
âš ï¸ DO NOT MODIFY, ADD TO, OR CHANGE THIS TEXT IN ANY WAY
âš ï¸ These EXACT ${optimizedCaption.length} characters, no more, no less

TEXT STYLE:
- Font: Thick, bold sans-serif (Impact/Bebas Neue style)
- Size: LARGE - readable at thumbnail size
- Color: High contrast with 3D effect (white + black stroke + glow/shadow)
- Position: Prominent placement, never covering faces
- Effect: Professional drop shadow or outer glow for pop

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
OUTPUT MUST BE:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âœ“ DRAMATICALLY better than the original (night and day difference)
âœ“ Professional enough for a major YouTube channel with millions of subs
âœ“ Eye-catching at small sizes in YouTube feed
âœ“ 16:9 aspect ratio, 1280x720, broadcast quality
âœ“ Includes the EXACT text caption specified above (no modifications)

The viewer should think "WOW, this looks professional!" not "oh, they just added text."`;

            } else if (effectiveReferenceType === 'face' || mode === 'faceHero') {
              // Face preservation - use Creative Studio's exact working pattern
              finalPrompt = `Using the provided image as a character/face reference to maintain consistency, generate a YouTube thumbnail: ${imagePrompt}

The person in the thumbnail must look exactly like the person in the reference image - same face, same hair, same features. Position them on the right side of the frame.

âš ï¸ MANDATORY TEXT - USE EXACT TEXT: Add EXACTLY this text: "${optimizedCaption}" - thick sans-serif font, high contrast (white with black outline). Position on left side. DO NOT modify this text.

16:9 aspect ratio, professional YouTube thumbnail quality.`;

            } else if (effectiveReferenceType === 'product') {
              // Product reference
              finalPrompt = `Using the provided image as a product reference, generate a YouTube thumbnail showcasing this exact product: ${imagePrompt}

Keep the product's appearance accurate. Professional product photography, clean background.

âš ï¸ MANDATORY TEXT - USE EXACT TEXT: Add EXACTLY this text: "${optimizedCaption}" prominently. Thick sans-serif font, high contrast. DO NOT modify this text.

16:9 YouTube thumbnail format.`;

            } else if (effectiveReferenceType === 'style') {
              // Style transfer - use Creative Studio's style reference pattern
              finalPrompt = `Using the provided image as a style reference, generate a new YouTube thumbnail with the following description: ${imagePrompt}

Match the color palette, lighting style, and overall aesthetic of the reference.

âš ï¸ MANDATORY TEXT - USE EXACT TEXT: Add EXACTLY this text: "${optimizedCaption}" in thick sans-serif font matching the aesthetic. High contrast. DO NOT modify this text.

16:9 YouTube thumbnail format.`;

            } else {
              // Background/general reference
              finalPrompt = `Using the provided image as reference, generate a YouTube thumbnail: ${imagePrompt}

âš ï¸ MANDATORY TEXT - USE EXACT TEXT: Add EXACTLY this text: "${optimizedCaption}" prominently. Thick sans-serif font, high contrast (white with black outline). DO NOT modify this text.

16:9 aspect ratio, professional quality, eye-catching design.`;
            }

          } else {
            // No reference image - use full enhanced prompt with mandatory text
            finalPrompt = `${imagePrompt}

COMPOSITION: ${compositionGuide.prompt}

âš ï¸ MANDATORY TEXT - USE EXACT TEXT âš ï¸
Add EXACTLY this text: "${optimizedCaption}"
DO NOT MODIFY, ADD TO, OR CHANGE THIS TEXT IN ANY WAY.
- Font: Thick, bold, sans-serif (Impact/Bebas Neue style)
- Color: High contrast - white with black outline, or bright color with shadow
- Size: LARGE - readable at small thumbnail sizes
- Position: Top-left, bottom, or where it complements the composition

FORMAT: 16:9 YouTube thumbnail (1280x720), professional photography quality, vibrant colors.

AVOID: ${negativePrompt}`;
          }

          // Add text prompt AFTER the reference image (Creative Studio order)
          contentParts.push({ text: finalPrompt });

          const result = await ai.models.generateContent({
            model: geminiModelId,
            contents: [{ role: 'user', parts: contentParts }],
            config: {
              responseModalities: ['image', 'text']
            }
          });

          // Extract image from response
          const candidates = result.candidates || (result.response && result.response.candidates);
          if (candidates && candidates.length > 0) {
            const parts = candidates[0].content?.parts || [];
            for (const part of parts) {
              const inlineData = part.inlineData || part.inline_data;
              if (inlineData && (inlineData.data || inlineData.bytesBase64Encoded)) {
                const imageBytes = inlineData.data || inlineData.bytesBase64Encoded;
                const mimeType = inlineData.mimeType || 'image/png';
                const extension = mimeType.includes('jpeg') ? 'jpg' : 'png';

                const fileName = `thumbnails-pro/${uid}/${timestamp}-gemini-${imgIdx + 1}.${extension}`;
                const file = storage.file(fileName);

                const buffer = Buffer.from(imageBytes, 'base64');
                await file.save(buffer, {
                  metadata: {
                    contentType: mimeType,
                    metadata: {
                      prompt: imagePrompt.substring(0, 500),
                      model: geminiModelId,
                      category,
                      style
                    }
                  }
                });

                await file.makePublic();
                const publicUrl = `https://storage.googleapis.com/${storage.name}/${fileName}`;

                generatedImages.push({
                  url: publicUrl,
                  fileName,
                  seed: Math.floor(Math.random() * 1000000),
                  model: 'nano-banana-pro'
                });

                console.log(`Gemini thumbnail ${imgIdx + 1} saved: ${fileName}`);
                break;
              }
            }
          }
        } catch (genError) {
          console.error(`Gemini generation error for image ${imgIdx + 1}:`, genError.message);
        }
      }

      usedModel = geminiModelId;

    } else if (config.model === 'dall-e-3') {
      // DALL-E 3 Premium Generation (ENHANCED Phase 3)
      console.log(`Generating ${imageCount} thumbnail(s) with DALL-E 3`);

      // Build enhanced DALL-E prompt with all improvements
      const dalleEnhancedPrompt = `${imagePrompt}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
COMPOSITION REQUIREMENTS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
${compositionGuide.prompt}
Leave space for text overlay: ${compositionGuide.textSpace}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
TECHNICAL SPECIFICATIONS (CRITICAL)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â€¢ Format: YouTube thumbnail, 16:9 aspect ratio
â€¢ Quality: 4K photorealistic, professional photography
â€¢ Lighting: Professional studio or cinematic lighting
â€¢ Colors: Vibrant, high-contrast, YouTube-optimized color palette
â€¢ Focus: Crystal sharp on main subject

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
AVOID
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
${negativePrompt}`;

      for (let imgIdx = 0; imgIdx < imageCount; imgIdx++) {
        try {
          const dalleResponse = await openai.images.generate({
            model: 'dall-e-3',
            prompt: dalleEnhancedPrompt,
            n: 1,
            size: '1792x1024', // Closest to 16:9 for DALL-E 3
            quality: 'hd',
            style: style === 'dramatic' || style === 'bold' ? 'vivid' : 'natural',
            response_format: 'b64_json'
          });

          if (dalleResponse.data && dalleResponse.data[0]) {
            const imageData = dalleResponse.data[0];
            const imageBytes = imageData.b64_json;

            const fileName = `thumbnails-pro/${uid}/${timestamp}-dalle-${imgIdx + 1}.png`;
            const file = storage.file(fileName);

            const buffer = Buffer.from(imageBytes, 'base64');
            await file.save(buffer, {
              metadata: {
                contentType: 'image/png',
                metadata: {
                  prompt: imagePrompt.substring(0, 500),
                  model: 'dall-e-3',
                  category,
                  style,
                  revisedPrompt: imageData.revised_prompt || ''
                }
              }
            });

            await file.makePublic();
            const publicUrl = `https://storage.googleapis.com/${storage.name}/${fileName}`;

            generatedImages.push({
              url: publicUrl,
              fileName,
              seed: Math.floor(Math.random() * 1000000),
              model: 'dall-e-3',
              revisedPrompt: imageData.revised_prompt
            });

            console.log(`DALL-E thumbnail ${imgIdx + 1} saved: ${fileName}`);
          }
        } catch (dalleError) {
          console.error(`DALL-E generation error for image ${imgIdx + 1}:`, dalleError.message);
        }
      }

      usedModel = 'dall-e-3';

    } else {
      // Imagen 4 Quick Generation (Default) - ENHANCED Phase 3
      const geminiApiKey = functions.config().gemini?.key;
      if (!geminiApiKey) {
        throw new functions.https.HttpsError('failed-precondition', 'Image generation service not configured');
      }

      const ai = new GoogleGenAI({ apiKey: geminiApiKey });
      const imagenModelId = 'imagen-4.0-generate-001';

      console.log(`Generating ${imageCount} thumbnail(s) with Imagen 4`);

      // Build enhanced Imagen prompt with composition and quality guidance
      // Note: Imagen 4 doesn't support negativePrompt parameter, so we include it in the prompt text
      const imagenEnhancedPrompt = `${imagePrompt}

COMPOSITION: ${compositionGuide.prompt}
STYLE: ${styleEnhancement}
FORMAT: YouTube thumbnail, 16:9 aspect ratio, 4K quality, professional photography, high contrast, vibrant colors optimized for small preview sizes.

AVOID: ${negativePrompt}`;

      try {
        const result = await ai.models.generateImages({
          model: imagenModelId,
          prompt: imagenEnhancedPrompt,
          config: {
            numberOfImages: imageCount,
            aspectRatio: '16:9',
            personGeneration: 'allow_adult'
          }
        });

        if (result.generatedImages && result.generatedImages.length > 0) {
          for (let imgIdx = 0; imgIdx < result.generatedImages.length; imgIdx++) {
            const genImage = result.generatedImages[imgIdx];

            // Check if image was filtered by safety
            if (genImage.raiFilteredReason) {
              console.warn(`Thumbnail ${imgIdx + 1} filtered: ${genImage.raiFilteredReason}`);
              continue;
            }

            const imageBytes = genImage.image?.imageBytes;

            if (imageBytes) {
              const fileName = `thumbnails-pro/${uid}/${timestamp}-imagen-${imgIdx + 1}.png`;
              const file = storage.file(fileName);

              const buffer = Buffer.from(imageBytes, 'base64');
              await file.save(buffer, {
                metadata: {
                  contentType: 'image/png',
                  metadata: {
                    prompt: imagePrompt.substring(0, 500),
                    model: imagenModelId,
                    category,
                    style
                  }
                }
              });

              await file.makePublic();
              const publicUrl = `https://storage.googleapis.com/${storage.name}/${fileName}`;

              generatedImages.push({
                url: publicUrl,
                fileName,
                seed: Math.floor(Math.random() * 1000000),
                model: 'imagen-4'
              });

              console.log(`Imagen thumbnail ${imgIdx + 1} saved: ${fileName}`);
            }
          }
        }
      } catch (imagenError) {
        console.error('Imagen generation error:', imagenError.message, imagenError.stack);
        // Provide more detailed error message for debugging
        const errMsg = imagenError.message?.toLowerCase() || '';
        let userMessage = 'Image generation failed: ';
        if (errMsg.includes('quota') || errMsg.includes('rate')) {
          userMessage += 'API rate limit reached. Please wait a moment and try again.';
        } else if (errMsg.includes('safety') || errMsg.includes('blocked') || errMsg.includes('policy')) {
          userMessage += 'Content was blocked by safety filters. Try a different prompt.';
        } else if (errMsg.includes('billing') || errMsg.includes('payment')) {
          userMessage += 'Billing issue with the API. Please contact support.';
        } else if (errMsg.includes('permission') || errMsg.includes('403') || errMsg.includes('denied')) {
          userMessage += 'API permission denied. Please contact support.';
        } else if (errMsg.includes('not found') || errMsg.includes('404')) {
          userMessage += 'Imagen model not available. Please contact support.';
        } else {
          userMessage += imagenError.message || 'Unknown error. Please try again.';
        }
        throw new functions.https.HttpsError('internal', userMessage);
      }

      usedModel = imagenModelId;
    }

    // Check if any images were generated
    if (generatedImages.length === 0) {
      throw new functions.https.HttpsError('internal', 'No images were generated. Please try again with different settings.');
    }

    // Deduct tokens
    const actualCost = config.tokenCost * generatedImages.length;
    await tokenRef.update({
      balance: admin.firestore.FieldValue.increment(-actualCost),
      lastUsed: admin.firestore.FieldValue.serverTimestamp()
    });

    // ==========================================
    // PHASE 7: QUALITY ENHANCEMENT PIPELINE
    // ==========================================

    // Build generation metadata for quality tracking and improvement
    const generationMetadata = {
      // Reference analysis data (if applicable)
      referenceAnalysis: referenceAnalysis || null,
      effectiveReferenceType: referenceImage ? effectiveReferenceType : null,

      // Settings used
      settings: {
        style,
        category,
        mode,
        compositionTemplate,
        faceStrength: effectiveReferenceType === 'face' ? faceStrength : null,
        styleStrength: effectiveReferenceType === 'style' ? styleStrength : null,
        expressionModifier,
        backgroundStyle
      },

      // Quality hints for user feedback
      qualityHints: {
        // Composition feedback
        composition: compositionGuide.name || 'Auto',
        textOverlaySpace: compositionGuide.textSpace || 'adaptive',

        // Suggestions for improvement
        suggestions: []
      },

      // A/B testing data
      abTestData: {
        promptVersion: 'v2.0-enhanced',
        modelVersion: usedModel,
        generationTimestamp: Date.now(),
        promptHash: imagePrompt.length > 100 ? imagePrompt.substring(0, 100) : imagePrompt
      }
    };

    // Add contextual suggestions based on settings
    if (mode === 'quick' && !referenceImage) {
      generationMetadata.qualityHints.suggestions.push(
        'Try "Reference Mode" with your photo for personalized thumbnails',
        'Upload a reference image to match your channel style'
      );
    }
    if (effectiveReferenceType === 'face' && faceStrength < 0.8) {
      generationMetadata.qualityHints.suggestions.push(
        'Increase "Face Strength" for better facial accuracy'
      );
    }
    if (category === 'general') {
      generationMetadata.qualityHints.suggestions.push(
        'Select a specific category for more optimized results'
      );
    }

    // Save to history with enhanced metadata
    const historyRef = await db.collection('thumbnailHistory').add({
      userId: uid,
      title,
      style,
      category,
      mode,
      customPrompt: customPrompt || null,
      prompt: imagePrompt,
      images: generatedImages,
      imageUrl: generatedImages[0]?.url, // Primary image for backward compatibility
      model: usedModel,
      tokenCost: actualCost,
      hasReference: !!referenceImage,
      // Phase 7: Enhanced metadata
      metadata: generationMetadata,
      // User feedback placeholders for quality improvement
      userFeedback: {
        rating: null,
        selectedImage: null,
        usedInVideo: null,
        improvementNotes: null
      },
      createdAt: admin.firestore.FieldValue.serverTimestamp()
    });

    // Log usage
    await logUsage(uid, 'thumbnail_pro_generation', {
      title,
      mode,
      category,
      model: usedModel,
      imageCount: generatedImages.length,
      tokenCost: actualCost
    });

    return {
      success: true,
      historyId: historyRef.id,
      images: generatedImages,
      imageUrl: generatedImages[0]?.url,
      prompt: imagePrompt,
      model: usedModel,
      tokenCost: actualCost,
      remainingBalance: balance - actualCost,
      message: `Generated ${generatedImages.length} thumbnail(s) successfully`,
      // Phase 7: Enhanced response data
      metadata: {
        referenceType: referenceImage ? effectiveReferenceType : null,
        composition: compositionGuide.name || 'Auto',
        textOverlaySpace: compositionGuide.textSpace || 'adaptive',
        suggestions: generationMetadata.qualityHints.suggestions
      }
    };

  } catch (error) {
    console.error('Thumbnail Pro generation error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Thumbnail generation failed. Please try again.'));
  }
});

// ==============================================
// HD UPSCALE - Upscale thumbnails to 1080p using fal.ai AuraSR
// FREE upscaling with AuraSR model
// ==============================================

/**
 * upscaleThumbnail - Upscale a single thumbnail to HD (1920x1080)
 * Uses fal.ai AuraSR (FREE) for 4x upscaling
 * Cost: 1 token (configurable)
 *
 * @param {string} imageUrl - URL of the image to upscale
 * @returns {object} { success, hdUrl, originalUrl, dimensions }
 */
exports.upscaleThumbnail = functions
  .runWith({ timeoutSeconds: 120, memory: '1GB' })
  .https.onCall(async (data, context) => {
    const uid = await verifyAuth(context);
    checkRateLimit(uid, 'upscaleThumbnail', 10);

    const { imageUrl } = data;
    const TOKEN_COST = 1; // Low cost since AuraSR is free

    if (!imageUrl) {
      throw new functions.https.HttpsError('invalid-argument', 'Image URL is required');
    }

    try {
      // Check token balance
      const tokenDoc = await db.collection('creativeTokens').doc(uid).get();
      if (!tokenDoc.exists) {
        throw new functions.https.HttpsError('failed-precondition', 'Token balance not found');
      }

      const balance = tokenDoc.data().balance || 0;
      if (balance < TOKEN_COST) {
        throw new functions.https.HttpsError('resource-exhausted',
          `Insufficient tokens. Need ${TOKEN_COST}, have ${balance}`);
      }

      // Configure fal.ai client
      fal.config({
        credentials: process.env.FAL_KEY || functions.config().fal?.key
      });

      console.log(`Starting HD upscale for user ${uid}: ${imageUrl}`);

      // Call AuraSR for 4x upscale (FREE!)
      const result = await fal.subscribe('fal-ai/aura-sr', {
        input: {
          image_url: imageUrl,
          upscaling_factor: 4,
          overlapping_tiles: true // Removes seams for better quality
        }
      });

      if (!result.data?.image?.url) {
        throw new Error('Upscale failed - no image returned');
      }

      const upscaledUrl = result.data.image.url;
      console.log(`AuraSR upscale complete: ${upscaledUrl}`);

      // Download upscaled image and resize to exactly 1920x1080
      const upscaledResponse = await axios.get(upscaledUrl, {
        responseType: 'arraybuffer',
        timeout: 30000
      });

      const resizedBuffer = await sharp(upscaledResponse.data)
        .resize(1920, 1080, {
          fit: 'cover',
          position: 'center'
        })
        .png({ quality: 95, compressionLevel: 6 })
        .toBuffer();

      // Upload to Firebase Storage
      const storage = admin.storage().bucket();
      const timestamp = Date.now();
      const hdPath = `thumbnails-hd/${uid}/${timestamp}_hd.png`;
      const file = storage.file(hdPath);

      await file.save(resizedBuffer, {
        metadata: {
          contentType: 'image/png',
          metadata: {
            originalUrl: imageUrl,
            upscaleModel: 'aura-sr',
            dimensions: '1920x1080'
          }
        }
      });

      await file.makePublic();
      const hdUrl = `https://storage.googleapis.com/${storage.name}/${hdPath}`;

      // Deduct token
      await db.collection('creativeTokens').doc(uid).update({
        balance: admin.firestore.FieldValue.increment(-TOKEN_COST),
        lastUsed: admin.firestore.FieldValue.serverTimestamp()
      });

      // Log usage
      await logUsage(uid, 'hd_upscale', {
        originalUrl: imageUrl,
        hdUrl: hdUrl,
        tokenCost: TOKEN_COST
      });

      console.log(`HD upscale complete for user ${uid}: ${hdUrl}`);

      return {
        success: true,
        hdUrl: hdUrl,
        originalUrl: imageUrl,
        dimensions: { width: 1920, height: 1080 },
        tokensUsed: TOKEN_COST,
        remainingBalance: balance - TOKEN_COST
      };

    } catch (error) {
      console.error('HD Upscale error:', error);
      if (error instanceof functions.https.HttpsError) throw error;
      throw new functions.https.HttpsError('internal',
        sanitizeErrorMessage(error, 'HD upscale failed. Please try again.'));
    }
  });

// =============================================
// SCENE IMAGE UPSCALE - Upscale to HD or 4K
// Uses fal.ai AuraSR for high-quality upscaling
// =============================================

/**
 * upscaleSceneImage - Upscale a scene/storyboard image to HD or 4K
 * Uses fal.ai AuraSR (FREE) for 4x upscaling
 * Enforces exact dimensions based on aspect ratio
 *
 * @param {string} imageUrl - URL of the image to upscale
 * @param {string} aspectRatio - Aspect ratio (e.g., '16:9', '9:16')
 * @param {string} quality - Target quality: 'hd' (1920x1080) or '4k' (3840x2160)
 * @param {string} sceneId - Optional scene ID for tracking
 * @returns {object} { success, upscaledUrl, width, height, quality }
 */
exports.upscaleSceneImage = functions
  .runWith({ timeoutSeconds: 180, memory: '2GB' })
  .https.onCall(async (data, context) => {
    const uid = await verifyAuth(context);
    checkRateLimit(uid, 'upscaleSceneImage', 10);

    const { imageUrl, aspectRatio = '16:9', quality = 'hd', sceneId } = data;
    const TOKEN_COST = quality === '4k' ? 3 : 1; // 4K costs more

    if (!imageUrl) {
      throw new functions.https.HttpsError('invalid-argument', 'Image URL is required');
    }

    try {
      // Check token balance
      const tokenDoc = await db.collection('creativeTokens').doc(uid).get();
      if (!tokenDoc.exists) {
        throw new functions.https.HttpsError('failed-precondition', 'Token balance not found');
      }

      const balance = tokenDoc.data().balance || 0;
      if (balance < TOKEN_COST) {
        throw new functions.https.HttpsError('resource-exhausted',
          `Insufficient tokens. Need ${TOKEN_COST}, have ${balance}`);
      }

      // Get target dimensions based on aspect ratio and quality
      const targetDimensions = getTargetDimensions(aspectRatio, quality);
      const { width: targetWidth, height: targetHeight } = targetDimensions;

      console.log(`Starting ${quality.toUpperCase()} upscale for scene image`);
      console.log(`Target dimensions: ${targetWidth}x${targetHeight} (${aspectRatio})`);

      // Configure fal.ai client
      fal.config({
        credentials: process.env.FAL_KEY || functions.config().fal?.key
      });

      // Call AuraSR for 4x upscale (FREE!)
      const result = await fal.subscribe('fal-ai/aura-sr', {
        input: {
          image_url: imageUrl,
          upscaling_factor: 4,
          overlapping_tiles: true // Removes seams for better quality
        }
      });

      if (!result.data?.image?.url) {
        throw new Error('Upscale failed - no image returned');
      }

      const upscaledUrl = result.data.image.url;
      console.log(`AuraSR upscale complete: ${upscaledUrl}`);

      // Download upscaled image and resize to exact target dimensions
      const upscaledResponse = await axios.get(upscaledUrl, {
        responseType: 'arraybuffer',
        timeout: 60000 // Longer timeout for 4K images
      });

      const resizedBuffer = await sharp(upscaledResponse.data)
        .resize(targetWidth, targetHeight, {
          fit: 'cover',
          position: 'center'
        })
        .png({ quality: 95, compressionLevel: 6 })
        .toBuffer();

      // Upload to Firebase Storage
      const storage = admin.storage().bucket();
      const timestamp = Date.now();
      const qualitySuffix = quality === '4k' ? '4k' : 'hd';
      const storagePath = `scene-upscales/${uid}/${timestamp}_${qualitySuffix}.png`;
      const file = storage.file(storagePath);

      await file.save(resizedBuffer, {
        metadata: {
          contentType: 'image/png',
          metadata: {
            originalUrl: imageUrl,
            upscaleModel: 'aura-sr',
            quality: quality,
            aspectRatio: aspectRatio,
            width: String(targetWidth),
            height: String(targetHeight),
            sceneId: sceneId || 'unknown'
          }
        }
      });

      await file.makePublic();
      const finalUrl = `https://storage.googleapis.com/${storage.name}/${storagePath}`;

      // Deduct tokens
      await db.collection('creativeTokens').doc(uid).update({
        balance: admin.firestore.FieldValue.increment(-TOKEN_COST),
        lastUsed: admin.firestore.FieldValue.serverTimestamp()
      });

      // Log usage
      await logUsage(uid, 'scene_upscale', {
        originalUrl: imageUrl,
        upscaledUrl: finalUrl,
        quality: quality,
        aspectRatio: aspectRatio,
        dimensions: `${targetWidth}x${targetHeight}`,
        sceneId: sceneId,
        tokenCost: TOKEN_COST
      });

      const newBalance = balance - TOKEN_COST;
      console.log(`Scene upscale complete: ${finalUrl} (${targetWidth}x${targetHeight})`);

      return {
        success: true,
        upscaledUrl: finalUrl,
        originalUrl: imageUrl,
        width: targetWidth,
        height: targetHeight,
        quality: quality,
        aspectRatio: aspectRatio,
        tokenCost: TOKEN_COST,
        remainingBalance: newBalance
      };

    } catch (error) {
      console.error('Scene upscale error:', error);
      throw new functions.https.HttpsError('internal',
        sanitizeErrorMessage(error, 'Scene upscale failed. Please try again.'));
    }
  });

/**
 * upscaleImageInternal - Internal helper for upscaling images (Fix 3)
 * Used by shot generation functions - no auth/token checks
 * Uses fal.ai Recraft Crisp for face-preserving enhancement ($0.004/image)
 * Fallback to AuraSR (FREE) if Recraft fails
 *
 * @param {string} imageUrl - URL of image to upscale
 * @param {string} aspectRatio - Target aspect ratio (default '16:9')
 * @param {string} quality - 'hd' (1920x1080) or '4k' (3840x2160)
 * @param {boolean} preserveFaces - Use Recraft Crisp for face preservation (default true)
 * @returns {Promise<object>} { success, url, width, height }
 */
async function upscaleImageInternal(imageUrl, aspectRatio = '16:9', quality = 'hd', preserveFaces = true) {
  if (!imageUrl) {
    console.warn('[upscaleImageInternal] No image URL provided, skipping upscale');
    return { success: false, url: imageUrl, skipped: true };
  }

  try {
    // Configure fal.ai client
    const falKey = process.env.FAL_KEY || functions.config().fal?.key;
    if (!falKey) {
      console.warn('[upscaleImageInternal] FAL API key not configured, skipping upscale');
      return { success: false, url: imageUrl, skipped: true, reason: 'No FAL API key' };
    }

    fal.config({ credentials: falKey });

    let upscaledUrl = null;
    let modelUsed = null;

    // PRIMARY: Use Recraft Crisp for face-preserving enhancement ($0.004/image)
    if (preserveFaces) {
      try {
        console.log(`[upscaleImageInternal] Using Recraft Crisp (face-preserving): ${imageUrl.substring(0, 60)}...`);

        const recraftResult = await fal.subscribe('fal-ai/recraft/upscale/crisp', {
          input: {
            image_url: imageUrl
          }
        });

        if (recraftResult.data?.image?.url) {
          upscaledUrl = recraftResult.data.image.url;
          modelUsed = 'recraft-crisp';
          console.log('[upscaleImageInternal] Recraft Crisp enhancement complete');
        } else {
          console.warn('[upscaleImageInternal] Recraft Crisp returned no image, falling back to AuraSR');
        }
      } catch (recraftError) {
        console.warn('[upscaleImageInternal] Recraft Crisp failed, falling back to AuraSR:', recraftError.message);
      }
    }

    // FALLBACK: Use AuraSR (FREE) if Recraft failed or not requested
    if (!upscaledUrl) {
      console.log(`[upscaleImageInternal] Using AuraSR (free fallback): ${imageUrl.substring(0, 60)}...`);

      const auraResult = await fal.subscribe('fal-ai/aura-sr', {
        input: {
          image_url: imageUrl,
          upscaling_factor: 4,
          overlapping_tiles: true
        }
      });

      if (!auraResult.data?.image?.url) {
        console.error('[upscaleImageInternal] AuraSR also failed - no image returned');
        return { success: false, url: imageUrl, error: 'Both upscalers failed' };
      }

      upscaledUrl = auraResult.data.image.url;
      modelUsed = 'aura-sr';
      console.log('[upscaleImageInternal] AuraSR upscale complete');
    }

    // Get target dimensions
    const targetDimensions = getTargetDimensions(aspectRatio, quality);
    const { width: targetWidth, height: targetHeight } = targetDimensions;

    // Download and resize to exact target dimensions
    const upscaledResponse = await axios.get(upscaledUrl, {
      responseType: 'arraybuffer',
      timeout: 60000
    });

    const resizedBuffer = await sharp(upscaledResponse.data)
      .resize(targetWidth, targetHeight, {
        fit: 'cover',
        position: 'center'
      })
      .png({ quality: 95, compressionLevel: 6 })
      .toBuffer();

    // Upload to Firebase Storage
    const storage = admin.storage().bucket();
    const timestamp = Date.now();
    const storagePath = `shot-upscales/${timestamp}_${modelUsed}_${quality}.png`;
    const file = storage.file(storagePath);

    await file.save(resizedBuffer, {
      metadata: {
        contentType: 'image/png',
        metadata: {
          originalUrl: imageUrl,
          upscaleModel: modelUsed,
          quality: quality,
          dimensions: `${targetWidth}x${targetHeight}`
        }
      }
    });

    await file.makePublic();
    const finalUrl = `https://storage.googleapis.com/${storage.name}/${storagePath}`;

    console.log(`[upscaleImageInternal] Upscale complete (${modelUsed}): ${targetWidth}x${targetHeight}`);

    return {
      success: true,
      url: finalUrl,
      originalUrl: imageUrl,
      width: targetWidth,
      height: targetHeight,
      quality: quality,
      model: modelUsed
    };

  } catch (error) {
    console.error('[upscaleImageInternal] Error:', error.message);
    // Return original URL on failure - don't break the flow
    return { success: false, url: imageUrl, error: error.message };
  }
}

// =============================================
// THUMBNAIL EDITING - AI Inpainting/Generative Fill
// Uses Gemini for targeted modifications with mask
// =============================================

/**
 * editThumbnailWithAI - Edit a thumbnail using AI inpainting
 * Allows users to paint/select areas and provide edit prompts
 * Uses Gemini image editing with fal.ai fallback
 *
 * @param {string} imageUrl - URL of the original thumbnail
 * @param {string} maskBase64 - Base64 PNG of the mask (white = edit area)
 * @param {string} editPrompt - What to change in the masked area
 * @param {number} editStrength - 0.5-1.0 (how much to modify)
 * @returns {object} { success, editedUrl, originalUrl, tokenCost, remainingBalance }
 */
exports.editThumbnailWithAI = functions
  .runWith({ timeoutSeconds: 120, memory: '1GB' })
  .https.onCall(async (data, context) => {
    const uid = await verifyAuth(context);
    checkRateLimit(uid, 'editThumbnail', 10); // 10 per minute

    const { imageUrl, maskBase64, editPrompt, editStrength = 0.85 } = data;
    const TOKEN_COST = 2; // Edit cost - cheaper than regeneration
    const hasMask = maskBase64 && maskBase64.length > 100; // Check if mask has actual content

    // Validation
    if (!imageUrl) {
      throw new functions.https.HttpsError('invalid-argument', 'Image URL is required');
    }
    // Mask is now OPTIONAL - if not provided, AI will edit entire image based on prompt
    if (!editPrompt || editPrompt.trim().length < 3) {
      throw new functions.https.HttpsError('invalid-argument', 'Edit prompt is required (min 3 characters)');
    }

    // Check token balance
    const tokenDoc = await db.collection('creativeTokens').doc(uid).get();
    if (!tokenDoc.exists) {
      throw new functions.https.HttpsError('failed-precondition', 'Token balance not found. Please contact support.');
    }

    const balance = tokenDoc.data().balance || 0;
    if (balance < TOKEN_COST) {
      throw new functions.https.HttpsError('resource-exhausted',
        `Insufficient tokens. Need ${TOKEN_COST}, have ${balance}`);
    }

    try {
      // Fetch original image
      console.log(`[EditThumbnail] Starting edit for user ${uid}: ${imageUrl}`);
      console.log(`[EditThumbnail] Edit prompt: "${editPrompt}"`);

      const imageResponse = await axios.get(imageUrl, {
        responseType: 'arraybuffer',
        timeout: 30000,
        headers: { 'User-Agent': 'ThumbnailEditor/1.0' }
      });
      const imageBase64 = Buffer.from(imageResponse.data).toString('base64');
      const imageMimeType = imageResponse.headers['content-type'] || 'image/png';

      let editedImageUrl;
      let usedModel;

      // Use Gemini for image editing
      const geminiApiKey = functions.config().gemini?.key;
      if (geminiApiKey) {
        try {
          const ai = new GoogleGenAI({ apiKey: geminiApiKey });

          // Build prompt based on whether mask is provided
          let fullPrompt;
          let contentParts;

          if (hasMask) {
            // WITH MASK - targeted editing
            fullPrompt = `You are performing a PRECISE IMAGE EDIT on a YouTube thumbnail.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
EDITING INSTRUCTIONS - MASKED AREA EDIT
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

TWO IMAGES PROVIDED:
1. ORIGINAL IMAGE: The YouTube thumbnail to edit
2. MASK IMAGE: White areas show WHERE to make changes

USER'S EDIT REQUEST: "${editPrompt.trim()}"

CRITICAL RULES:
1. ONLY modify the WHITE/MASKED areas
2. Keep ALL BLACK/UNMASKED areas PIXEL-PERFECT IDENTICAL
3. The edit must blend SEAMLESSLY with surrounding areas
4. Match lighting, shadows, and style perfectly
5. Maintain the professional YouTube thumbnail quality

OUTPUT REQUIREMENTS:
- Same dimensions as input (1280x720 or similar)
- High quality, no artifacts at edit boundaries
- The edit should look like it was always part of the original

Generate the edited thumbnail now.`;

            contentParts = [
              { inlineData: { mimeType: imageMimeType, data: imageBase64 } },
              { inlineData: { mimeType: 'image/png', data: maskBase64 } },
              { text: fullPrompt }
            ];
            console.log(`[EditThumbnail] Calling Gemini WITH mask`);
          } else {
            // WITHOUT MASK - general editing based on prompt
            fullPrompt = `You are editing a YouTube thumbnail based on the user's instructions.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
EDITING INSTRUCTIONS - GENERAL EDIT
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

USER'S EDIT REQUEST: "${editPrompt.trim()}"

Apply the requested changes to the thumbnail. Use your judgment to:
- Identify what needs to be changed based on the prompt
- Make the changes while maintaining the overall style and quality
- Ensure text is crisp and readable if text changes are requested
- Preserve elements that should not be changed

OUTPUT REQUIREMENTS:
- Same dimensions as input (1280x720 or similar)
- High quality YouTube thumbnail
- Professional appearance

Generate the edited thumbnail now.`;

            contentParts = [
              { inlineData: { mimeType: imageMimeType, data: imageBase64 } },
              { text: fullPrompt }
            ];
            console.log(`[EditThumbnail] Calling Gemini WITHOUT mask (prompt-only edit)`);
          }

          const result = await ai.models.generateContent({
            model: 'gemini-3-pro-image-preview',
            contents: [{
              role: 'user',
              parts: contentParts
            }],
            config: {
              responseModalities: ['image', 'text']
            }
          });

          // Extract image from response
          const candidates = result.candidates || (result.response && result.response.candidates);
          if (candidates && candidates.length > 0) {
            const parts = candidates[0].content?.parts || candidates[0].parts || [];
            for (const part of parts) {
              const inlineData = part.inlineData || part.inline_data;
              if (inlineData && (inlineData.data || inlineData.bytesBase64Encoded)) {
                const imageBytes = inlineData.data || inlineData.bytesBase64Encoded;
                const mimeType = inlineData.mimeType || inlineData.mime_type || 'image/png';
                const extension = mimeType.includes('jpeg') ? 'jpg' : 'png';

                // Save to Storage
                const storage = admin.storage().bucket();
                const timestamp = Date.now();
                const fileName = `thumbnails-edited/${uid}/${timestamp}-edited.${extension}`;
                const file = storage.file(fileName);

                const buffer = Buffer.from(imageBytes, 'base64');
                await file.save(buffer, {
                  metadata: {
                    contentType: mimeType,
                    metadata: {
                      originalUrl: imageUrl,
                      editPrompt: editPrompt.substring(0, 200),
                      model: 'gemini-3-pro-image-preview',
                      editedAt: new Date().toISOString()
                    }
                  }
                });

                await file.makePublic();
                editedImageUrl = `https://storage.googleapis.com/${storage.name}/${fileName}`;
                usedModel = 'gemini-3-pro';
                console.log(`[EditThumbnail] Gemini edit successful: ${editedImageUrl}`);
                break;
              }
            }
          }

          if (!editedImageUrl) {
            console.log('[EditThumbnail] Gemini returned no image, trying fallback');
          }
        } catch (geminiError) {
          console.error('[EditThumbnail] Gemini edit failed:', geminiError.message);
          // Will try fallback below
        }
      }

      // Fallback to fal.ai if Gemini didn't work
      if (!editedImageUrl) {
        console.log('[EditThumbnail] Using fal.ai inpainting fallback');

        const falKey = process.env.FAL_KEY || functions.config().fal?.key;
        if (!falKey) {
          throw new Error('Image editing service not available. Please try again later.');
        }

        // Upload mask to temporary storage for fal.ai
        const storage = admin.storage().bucket();
        const maskFileName = `temp-masks/${uid}/${Date.now()}-mask.png`;
        const maskFile = storage.file(maskFileName);
        await maskFile.save(Buffer.from(maskBase64, 'base64'), {
          metadata: { contentType: 'image/png' }
        });
        await maskFile.makePublic();
        const maskUrl = `https://storage.googleapis.com/${storage.name}/${maskFileName}`;

        // Configure fal.ai
        fal.config({ credentials: falKey });

        // Call fal.ai inpainting
        const falResult = await fal.subscribe('fal-ai/flux/inpaint', {
          input: {
            image_url: imageUrl,
            mask_url: maskUrl,
            prompt: `YouTube thumbnail edit: ${editPrompt}. Professional quality, seamless blend.`,
            strength: editStrength,
            num_inference_steps: 50
          }
        });

        if (falResult.data?.images?.[0]?.url) {
          // Download and save the result
          const editedResponse = await axios.get(falResult.data.images[0].url, {
            responseType: 'arraybuffer',
            timeout: 30000
          });

          const timestamp = Date.now();
          const fileName = `thumbnails-edited/${uid}/${timestamp}-edited.png`;
          const file = storage.file(fileName);

          await file.save(editedResponse.data, {
            metadata: {
              contentType: 'image/png',
              metadata: {
                originalUrl: imageUrl,
                editPrompt: editPrompt.substring(0, 200),
                model: 'fal-flux-inpaint',
                editedAt: new Date().toISOString()
              }
            }
          });

          await file.makePublic();
          editedImageUrl = `https://storage.googleapis.com/${storage.name}/${fileName}`;
          usedModel = 'fal-flux-inpaint';
          console.log(`[EditThumbnail] fal.ai edit successful: ${editedImageUrl}`);
        }

        // Clean up temp mask
        await maskFile.delete().catch(err => console.log('Mask cleanup error:', err.message));
      }

      if (!editedImageUrl) {
        throw new Error('Failed to generate edited image. Please try again.');
      }

      // Deduct tokens
      await db.collection('creativeTokens').doc(uid).update({
        balance: admin.firestore.FieldValue.increment(-TOKEN_COST),
        lastUsed: admin.firestore.FieldValue.serverTimestamp()
      });

      const newBalance = balance - TOKEN_COST;

      // Save to edit history
      await db.collection('thumbnailEditHistory').add({
        userId: uid,
        originalUrl: imageUrl,
        editedUrl: editedImageUrl,
        editPrompt: editPrompt,
        model: usedModel,
        tokenCost: TOKEN_COST,
        createdAt: admin.firestore.FieldValue.serverTimestamp()
      });

      // Log usage
      await logUsage(uid, 'thumbnail_edit', {
        originalUrl: imageUrl,
        editedUrl: editedImageUrl,
        editPrompt: editPrompt.substring(0, 100),
        model: usedModel,
        tokenCost: TOKEN_COST
      });

      console.log(`[EditThumbnail] Complete for ${uid}. Model: ${usedModel}, Cost: ${TOKEN_COST} tokens`);

      return {
        success: true,
        editedUrl: editedImageUrl,
        originalUrl: imageUrl,
        model: usedModel,
        tokenCost: TOKEN_COST,
        remainingBalance: newBalance
      };

    } catch (error) {
      console.error('[EditThumbnail] Error:', error);
      if (error instanceof functions.https.HttpsError) throw error;
      throw new functions.https.HttpsError('internal',
        error.message || 'Failed to edit thumbnail. Please try again.');
    }
  });

/**
 * upscaleBatch - Upscale multiple thumbnails to HD
 * Processes in parallel for efficiency
 * Cost: 1 token per image
 *
 * @param {array} images - Array of { url, id } objects
 * @returns {object} { success, results, totalTokensUsed, successCount }
 */
exports.upscaleBatch = functions
  .runWith({ timeoutSeconds: 540, memory: '2GB' })
  .https.onCall(async (data, context) => {
    const uid = await verifyAuth(context);
    checkRateLimit(uid, 'upscaleBatch', 3);

    const { images } = data;
    const TOKEN_COST_PER_IMAGE = 1;

    if (!images || !Array.isArray(images) || images.length === 0) {
      throw new functions.https.HttpsError('invalid-argument', 'Images array is required');
    }

    if (images.length > 50) {
      throw new functions.https.HttpsError('invalid-argument', 'Maximum 50 images per batch');
    }

    const totalCost = images.length * TOKEN_COST_PER_IMAGE;

    try {
      // Check token balance
      const tokenDoc = await db.collection('creativeTokens').doc(uid).get();
      if (!tokenDoc.exists) {
        throw new functions.https.HttpsError('failed-precondition', 'Token balance not found');
      }

      const balance = tokenDoc.data().balance || 0;
      if (balance < totalCost) {
        throw new functions.https.HttpsError('resource-exhausted',
          `Insufficient tokens. Need ${totalCost}, have ${balance}`);
      }

      // Configure fal.ai client
      fal.config({
        credentials: process.env.FAL_KEY || functions.config().fal?.key
      });

      const storage = admin.storage().bucket();
      const results = [];
      let successCount = 0;

      console.log(`Starting batch upscale for user ${uid}: ${images.length} images`);

      // Process in chunks of 3 for parallel processing
      const chunkSize = 3;
      for (let i = 0; i < images.length; i += chunkSize) {
        const chunk = images.slice(i, i + chunkSize);

        const chunkResults = await Promise.all(
          chunk.map(async (img) => {
            try {
              // Call AuraSR
              const result = await fal.subscribe('fal-ai/aura-sr', {
                input: {
                  image_url: img.url,
                  upscaling_factor: 4,
                  overlapping_tiles: true
                }
              });

              if (!result.data?.image?.url) {
                throw new Error('No image returned');
              }

              // Download and resize
              const upscaledResponse = await axios.get(result.data.image.url, {
                responseType: 'arraybuffer',
                timeout: 30000
              });

              const resizedBuffer = await sharp(upscaledResponse.data)
                .resize(1920, 1080, { fit: 'cover', position: 'center' })
                .png({ quality: 95, compressionLevel: 6 })
                .toBuffer();

              // Upload to Firebase Storage
              const timestamp = Date.now();
              const hdPath = `thumbnails-hd/${uid}/${timestamp}_${img.id}_hd.png`;
              const file = storage.file(hdPath);

              await file.save(resizedBuffer, {
                metadata: {
                  contentType: 'image/png',
                  metadata: { originalUrl: img.url, upscaleModel: 'aura-sr' }
                }
              });

              await file.makePublic();
              const hdUrl = `https://storage.googleapis.com/${storage.name}/${hdPath}`;

              successCount++;
              return { id: img.id, hdUrl, status: 'success' };

            } catch (error) {
              console.error(`Batch upscale error for image ${img.id}:`, error.message);
              return { id: img.id, error: error.message, status: 'error' };
            }
          })
        );

        results.push(...chunkResults);
      }

      // Deduct tokens only for successful upscales
      const tokensUsed = successCount * TOKEN_COST_PER_IMAGE;
      if (tokensUsed > 0) {
        await db.collection('creativeTokens').doc(uid).update({
          balance: admin.firestore.FieldValue.increment(-tokensUsed),
          lastUsed: admin.firestore.FieldValue.serverTimestamp()
        });
      }

      // Log usage
      await logUsage(uid, 'hd_upscale_batch', {
        totalImages: images.length,
        successCount,
        tokensUsed
      });

      console.log(`Batch upscale complete for user ${uid}: ${successCount}/${images.length} successful`);

      return {
        success: true,
        results,
        totalTokensUsed: tokensUsed,
        successCount,
        failedCount: images.length - successCount,
        remainingBalance: balance - tokensUsed
      };

    } catch (error) {
      console.error('Batch upscale error:', error);
      if (error instanceof functions.https.HttpsError) throw error;
      throw new functions.https.HttpsError('internal',
        sanitizeErrorMessage(error, 'Batch upscale failed. Please try again.'));
    }
  });

// Get user's creative token balance (for Thumbnail Pro)
// Syncs with admin token configuration and user subscription plan
exports.getThumbnailTokenBalance = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);

  try {
    // Get user's subscription plan
    const userDoc = await db.collection('users').doc(uid).get();
    const userPlan = userDoc.exists ? (userDoc.data().subscription?.plan || 'free') : 'free';

    // Get admin-configured token settings (use shared helper for consistency)
    const tokenConfig = await getTokenConfigFromAdmin();

    // Get plan-specific allocation
    const planConfig = tokenConfig[userPlan] || tokenConfig.free;
    const monthlyAllocation = planConfig.monthlyTokens || 10;
    const rolloverPercent = planConfig.rolloverPercent || 0;

    // Get or create user's token balance
    const tokenDoc = await db.collection('creativeTokens').doc(uid).get();

    if (!tokenDoc.exists) {
      // Initialize new user with plan-appropriate tokens
      const initialTokens = {
        balance: monthlyAllocation,
        rollover: 0,
        plan: userPlan,
        monthlyAllocation: monthlyAllocation,
        rolloverPercent: rolloverPercent,
        lastRefresh: admin.firestore.FieldValue.serverTimestamp(),
        createdAt: admin.firestore.FieldValue.serverTimestamp()
      };
      await db.collection('creativeTokens').doc(uid).set(initialTokens);
      return {
        success: true,
        balance: monthlyAllocation,
        plan: userPlan,
        monthlyAllocation: monthlyAllocation
      };
    }

    const tokenData = tokenDoc.data();

    // Check if plan has changed - sync if needed
    if (tokenData.plan !== userPlan) {
      const updatedTokens = {
        plan: userPlan,
        monthlyAllocation: monthlyAllocation,
        rolloverPercent: rolloverPercent
      };
      await db.collection('creativeTokens').doc(uid).update(updatedTokens);
      tokenData.plan = userPlan;
      tokenData.monthlyAllocation = monthlyAllocation;
    }

    // Check if monthly refresh is needed
    const now = new Date();
    const lastRefresh = tokenData.lastRefresh?.toDate() || new Date(0);
    const monthsSinceRefresh = (now.getFullYear() - lastRefresh.getFullYear()) * 12 +
                               (now.getMonth() - lastRefresh.getMonth());

    if (monthsSinceRefresh >= 1) {
      // Calculate rollover based on plan's rollover percent
      const maxRollover = Math.floor(tokenData.balance * (rolloverPercent / 100));
      const newBalance = monthlyAllocation + maxRollover;

      const refreshedTokens = {
        balance: newBalance,
        rollover: maxRollover,
        plan: userPlan,
        monthlyAllocation: monthlyAllocation,
        rolloverPercent: rolloverPercent,
        lastRefresh: admin.firestore.FieldValue.serverTimestamp()
      };

      await db.collection('creativeTokens').doc(uid).update(refreshedTokens);

      return {
        success: true,
        balance: newBalance,
        plan: userPlan,
        monthlyAllocation: monthlyAllocation,
        rollover: maxRollover
      };
    }

    return {
      success: true,
      balance: tokenData.balance || 0,
      plan: userPlan,
      monthlyAllocation: monthlyAllocation
    };
  } catch (error) {
    console.error('Get token balance error:', error);
    throw new functions.https.HttpsError('internal', 'Failed to get token balance');
  }
});

// ==============================================
// VIDEO WIZARD TOKEN FUNCTIONS
// ==============================================

/**
 * Default token costs for Video Wizard operations
 * Can be overridden in admin settings: settings/wizardTokenCosts
 */
const DEFAULT_WIZARD_TOKEN_COSTS = {
  analyzeVideo: 5,      // Analyze a YouTube video
  showMoreClips: 3,     // Generate additional clips
  generateSEO: 2,       // Generate SEO for a clip
  generateBRoll: 4,     // Generate B-Roll suggestions
  detectSpeakers: 3,    // Detect speakers in video
  exportClip: 0         // Export is free (already paid for analysis)
};

/**
 * Helper: Get wizard token costs from admin config or defaults
 */
async function getWizardTokenCosts() {
  try {
    const costsDoc = await db.collection('settings').doc('wizardTokenCosts').get();
    if (costsDoc.exists) {
      return { ...DEFAULT_WIZARD_TOKEN_COSTS, ...costsDoc.data() };
    }
  } catch (error) {
    console.log('[getWizardTokenCosts] Using defaults:', error.message);
  }
  return DEFAULT_WIZARD_TOKEN_COSTS;
}

/**
 * Helper: Deduct tokens for wizard operations
 * Uses creativeTokens collection (same as Thumbnail Generator)
 * @returns {Object} { success, newBalance, error }
 */
async function deductWizardTokens(uid, amount, operation, metadata = {}) {
  if (amount <= 0) {
    return { success: true, newBalance: null, deducted: 0 };
  }

  try {
    // Get current balance
    const tokenDoc = await db.collection('creativeTokens').doc(uid).get();

    if (!tokenDoc.exists) {
      // Initialize with free plan defaults
      const tokenConfig = await getTokenConfigFromAdmin();
      const planConfig = tokenConfig.free || { monthlyTokens: 10 };

      await db.collection('creativeTokens').doc(uid).set({
        balance: planConfig.monthlyTokens,
        rollover: 0,
        plan: 'free',
        monthlyAllocation: planConfig.monthlyTokens,
        rolloverPercent: 0,
        lastRefresh: admin.firestore.FieldValue.serverTimestamp(),
        createdAt: admin.firestore.FieldValue.serverTimestamp()
      });

      // Re-fetch
      const newDoc = await db.collection('creativeTokens').doc(uid).get();
      if (!newDoc.exists) {
        return { success: false, error: 'Failed to initialize tokens' };
      }
    }

    const tokenData = (await db.collection('creativeTokens').doc(uid).get()).data();
    const currentBalance = tokenData.balance || 0;

    if (currentBalance < amount) {
      return {
        success: false,
        error: 'Insufficient tokens',
        required: amount,
        available: currentBalance
      };
    }

    const newBalance = currentBalance - amount;

    // Update balance
    await db.collection('creativeTokens').doc(uid).update({
      balance: newBalance,
      lastUpdated: admin.firestore.FieldValue.serverTimestamp()
    });

    // Log transaction
    await db.collection('tokenTransactions').add({
      userId: uid,
      type: 'wizard_' + operation,
      amount: -amount,
      balanceAfter: newBalance,
      operation: operation,
      metadata: metadata,
      createdAt: admin.firestore.FieldValue.serverTimestamp()
    });

    console.log(`[deductWizardTokens] User ${uid}: ${operation} cost ${amount} tokens, new balance: ${newBalance}`);

    return { success: true, newBalance, deducted: amount };
  } catch (error) {
    console.error('[deductWizardTokens] Error:', error);
    return { success: false, error: error.message };
  }
}

/**
 * Get user's token balance for Video Wizard
 * Uses creativeTokens collection (shared with Thumbnail Generator)
 */
exports.getWizardTokenBalance = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);

  try {
    // Get user's subscription plan
    const userDoc = await db.collection('users').doc(uid).get();
    const userPlan = userDoc.exists ? (userDoc.data().subscription?.plan || 'free') : 'free';

    // Get admin-configured token settings
    const tokenConfig = await getTokenConfigFromAdmin();
    const planConfig = tokenConfig[userPlan] || tokenConfig.free;
    const monthlyAllocation = planConfig.monthlyTokens || 10;
    const rolloverPercent = planConfig.rolloverPercent || 0;

    // Get or create user's token balance
    const tokenDoc = await db.collection('creativeTokens').doc(uid).get();

    if (!tokenDoc.exists) {
      // Initialize new user with plan-appropriate tokens
      const initialTokens = {
        balance: monthlyAllocation,
        rollover: 0,
        plan: userPlan,
        monthlyAllocation: monthlyAllocation,
        rolloverPercent: rolloverPercent,
        lastRefresh: admin.firestore.FieldValue.serverTimestamp(),
        createdAt: admin.firestore.FieldValue.serverTimestamp()
      };
      await db.collection('creativeTokens').doc(uid).set(initialTokens);

      // Get token costs
      const costs = await getWizardTokenCosts();

      return {
        success: true,
        balance: monthlyAllocation,
        plan: userPlan,
        monthlyAllocation: monthlyAllocation,
        rollover: 0,
        costs: costs
      };
    }

    const tokenData = tokenDoc.data();

    // Check if plan has changed - sync if needed
    if (tokenData.plan !== userPlan) {
      await db.collection('creativeTokens').doc(uid).update({
        plan: userPlan,
        monthlyAllocation: monthlyAllocation,
        rolloverPercent: rolloverPercent
      });
    }

    // Check if monthly refresh is needed
    const now = new Date();
    const lastRefresh = tokenData.lastRefresh?.toDate() || new Date(0);
    const monthsSinceRefresh = (now.getFullYear() - lastRefresh.getFullYear()) * 12 +
                               (now.getMonth() - lastRefresh.getMonth());

    let balance = tokenData.balance || 0;
    let rollover = tokenData.rollover || 0;

    if (monthsSinceRefresh >= 1) {
      // Calculate rollover based on plan's rollover percent
      const maxRollover = Math.floor(balance * (rolloverPercent / 100));
      balance = monthlyAllocation + maxRollover;
      rollover = maxRollover;

      await db.collection('creativeTokens').doc(uid).update({
        balance: balance,
        rollover: rollover,
        plan: userPlan,
        monthlyAllocation: monthlyAllocation,
        rolloverPercent: rolloverPercent,
        lastRefresh: admin.firestore.FieldValue.serverTimestamp()
      });
    }

    // Get token costs
    const costs = await getWizardTokenCosts();

    return {
      success: true,
      balance: balance,
      plan: userPlan,
      monthlyAllocation: monthlyAllocation,
      rollover: rollover,
      costs: costs
    };
  } catch (error) {
    console.error('[getWizardTokenBalance] Error:', error);
    throw new functions.https.HttpsError('internal', 'Failed to get token balance');
  }
});

/**
 * Admin function: Set Video Wizard token costs
 */
exports.adminSetWizardTokenCosts = functions.https.onCall(async (data, context) => {
  await requireAdmin(context);

  const { analyzeVideo, showMoreClips, generateSEO, generateBRoll, detectSpeakers, exportClip } = data;

  try {
    const costs = {};

    if (analyzeVideo !== undefined) costs.analyzeVideo = Math.max(0, parseInt(analyzeVideo));
    if (showMoreClips !== undefined) costs.showMoreClips = Math.max(0, parseInt(showMoreClips));
    if (generateSEO !== undefined) costs.generateSEO = Math.max(0, parseInt(generateSEO));
    if (generateBRoll !== undefined) costs.generateBRoll = Math.max(0, parseInt(generateBRoll));
    if (detectSpeakers !== undefined) costs.detectSpeakers = Math.max(0, parseInt(detectSpeakers));
    if (exportClip !== undefined) costs.exportClip = Math.max(0, parseInt(exportClip));

    costs.updatedAt = admin.firestore.FieldValue.serverTimestamp();
    costs.updatedBy = context.auth.uid;

    await db.collection('settings').doc('wizardTokenCosts').set(costs, { merge: true });

    return { success: true, costs };
  } catch (error) {
    console.error('[adminSetWizardTokenCosts] Error:', error);
    throw new functions.https.HttpsError('internal', error.message);
  }
});

/**
 * Admin function: Get Video Wizard token costs
 */
exports.adminGetWizardTokenCosts = functions.https.onCall(async (data, context) => {
  await requireAdmin(context);

  try {
    const costs = await getWizardTokenCosts();
    return { success: true, costs };
  } catch (error) {
    console.error('[adminGetWizardTokenCosts] Error:', error);
    throw new functions.https.HttpsError('internal', error.message);
  }
});

/**
 * Deduct tokens for Video Wizard operations
 * Called by frontend for operations like showMoreClips
 */
exports.wizardDeductTokens = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);

  const { amount, operation, metadata = {} } = data;

  if (!amount || amount <= 0) {
    throw new functions.https.HttpsError('invalid-argument', 'Invalid token amount');
  }

  if (!operation) {
    throw new functions.https.HttpsError('invalid-argument', 'Operation is required');
  }

  try {
    // Verify the cost matches the configured amount
    const costs = await getWizardTokenCosts();
    const expectedCost = costs[operation];

    if (expectedCost !== undefined && amount !== expectedCost) {
      console.warn(`[wizardDeductTokens] Client requested ${amount} tokens but ${operation} costs ${expectedCost}`);
      // Use the server-configured cost, not the client-provided one
    }

    const actualCost = expectedCost !== undefined ? expectedCost : amount;

    // Deduct tokens using the helper function
    const result = await deductWizardTokens(uid, actualCost, operation, metadata);

    return {
      success: true,
      newBalance: result.newBalance,
      tokensDeducted: actualCost
    };
  } catch (error) {
    console.error('[wizardDeductTokens] Error:', error);
    if (error.code === 'resource-exhausted') {
      throw error;
    }
    throw new functions.https.HttpsError('internal', error.message);
  }
});

// ==============================================
// HISTORY RETRIEVAL & MANAGEMENT FUNCTIONS
// ==============================================

// Get Competitor Analysis History
exports.getCompetitorHistory = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  checkRateLimit(uid, 'getCompetitorHistory', 20);

  const { limit = 20, offset = 0 } = data || {};
  const safeLimit = Math.min(Math.max(1, parseInt(limit) || 20), 50);
  const safeOffset = Math.max(0, parseInt(offset) || 0);

  // Safe timestamp handler
  const getTs = (field) => {
    if (!field) return Date.now();
    if (typeof field === 'number') return field;
    if (typeof field.toMillis === 'function') return field.toMillis();
    if (field._seconds) return field._seconds * 1000;
    if (field instanceof Date) return field.getTime();
    return Date.now();
  };

  try {
    const snapshot = await db.collection('competitorHistory')
      .where('userId', '==', uid)
      .orderBy('createdAt', 'desc')
      .limit(safeLimit)
      .offset(safeOffset)
      .get();

    const history = [];
    snapshot.forEach(doc => {
      try {
        const docData = doc.data();
        const timestamp = getTs(docData.createdAt);
        const { createdAt, ...rest } = docData; // Exclude raw createdAt
        history.push({
          id: doc.id,
          ...rest,
          timestamp,
          createdAt: new Date(timestamp).toISOString()
        });
      } catch (e) {
        console.error('Error processing competitor doc:', doc.id, e);
      }
    });

    return { success: true, history, count: history.length };
  } catch (error) {
    console.error('Get competitor history error:', error);
    throw new functions.https.HttpsError('internal', 'Failed to load history.');
  }
});

// Get Trend Predictor History
exports.getTrendHistory = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  checkRateLimit(uid, 'getTrendHistory', 20);

  const { limit = 20, offset = 0 } = data || {};
  const safeLimit = Math.min(Math.max(1, parseInt(limit) || 20), 50);
  const safeOffset = Math.max(0, parseInt(offset) || 0);

  // Safe timestamp handler
  const getTs = (field) => {
    if (!field) return Date.now();
    if (typeof field === 'number') return field;
    if (typeof field.toMillis === 'function') return field.toMillis();
    if (field._seconds) return field._seconds * 1000;
    if (field instanceof Date) return field.getTime();
    return Date.now();
  };

  try {
    const snapshot = await db.collection('trendHistory')
      .where('userId', '==', uid)
      .orderBy('createdAt', 'desc')
      .limit(safeLimit)
      .offset(safeOffset)
      .get();

    const history = [];
    snapshot.forEach(doc => {
      try {
        const docData = doc.data();
        const timestamp = getTs(docData.createdAt);
        const { createdAt, ...rest } = docData; // Exclude raw createdAt
        history.push({
          id: doc.id,
          ...rest,
          timestamp,
          createdAt: new Date(timestamp).toISOString()
        });
      } catch (e) {
        console.error('Error processing trend doc:', doc.id, e);
      }
    });

    return { success: true, history, count: history.length };
  } catch (error) {
    console.error('Get trend history error:', error);
    throw new functions.https.HttpsError('internal', 'Failed to load history.');
  }
});

// Get Thumbnail History
exports.getThumbnailHistory = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  checkRateLimit(uid, 'getThumbnailHistory', 20);

  const { limit = 20, offset = 0 } = data || {};
  const safeLimit = Math.min(Math.max(1, parseInt(limit) || 20), 50);
  const safeOffset = Math.max(0, parseInt(offset) || 0);

  // Safe timestamp handler
  const getTs = (field) => {
    if (!field) return Date.now();
    if (typeof field === 'number') return field;
    if (typeof field.toMillis === 'function') return field.toMillis();
    if (field._seconds) return field._seconds * 1000;
    if (field instanceof Date) return field.getTime();
    return Date.now();
  };

  try {
    const snapshot = await db.collection('thumbnailHistory')
      .where('userId', '==', uid)
      .orderBy('createdAt', 'desc')
      .limit(safeLimit)
      .offset(safeOffset)
      .get();

    const history = [];
    snapshot.forEach(doc => {
      try {
        const docData = doc.data();
        const timestamp = getTs(docData.createdAt);
        const { createdAt, ...rest } = docData; // Exclude raw createdAt
        history.push({
          id: doc.id,
          ...rest,
          timestamp,
          createdAt: new Date(timestamp).toISOString()
        });
      } catch (e) {
        console.error('Error processing thumbnail doc:', doc.id, e);
      }
    });

    return { success: true, history, count: history.length };
  } catch (error) {
    console.error('Get thumbnail history error:', error);
    throw new functions.https.HttpsError('internal', 'Failed to load history.');
  }
});

// Delete Competitor Analysis
exports.deleteCompetitorAnalysis = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);

  const { id } = data || {};
  if (!id) {
    throw new functions.https.HttpsError('invalid-argument', 'History ID is required');
  }

  try {
    const doc = await db.collection('competitorHistory').doc(id).get();
    if (!doc.exists || doc.data().userId !== uid) {
      throw new functions.https.HttpsError('permission-denied', 'Not authorized to delete this item');
    }

    await db.collection('competitorHistory').doc(id).delete();
    return { success: true, message: 'Analysis deleted successfully' };
  } catch (error) {
    if (error instanceof functions.https.HttpsError) throw error;
    console.error('Delete competitor analysis error:', error);
    throw new functions.https.HttpsError('internal', 'Failed to delete analysis.');
  }
});

// Delete Trend Prediction
exports.deleteTrendPrediction = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);

  const { id } = data || {};
  if (!id) {
    throw new functions.https.HttpsError('invalid-argument', 'History ID is required');
  }

  try {
    const doc = await db.collection('trendHistory').doc(id).get();
    if (!doc.exists || doc.data().userId !== uid) {
      throw new functions.https.HttpsError('permission-denied', 'Not authorized to delete this item');
    }

    await db.collection('trendHistory').doc(id).delete();
    return { success: true, message: 'Prediction deleted successfully' };
  } catch (error) {
    if (error instanceof functions.https.HttpsError) throw error;
    console.error('Delete trend prediction error:', error);
    throw new functions.https.HttpsError('internal', 'Failed to delete prediction.');
  }
});

// Delete Thumbnail (also deletes from Storage)
exports.deleteThumbnail = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);

  const { id } = data || {};
  if (!id) {
    throw new functions.https.HttpsError('invalid-argument', 'History ID is required');
  }

  try {
    const doc = await db.collection('thumbnailHistory').doc(id).get();
    if (!doc.exists || doc.data().userId !== uid) {
      throw new functions.https.HttpsError('permission-denied', 'Not authorized to delete this item');
    }

    const thumbnailData = doc.data();

    // Delete from Firebase Storage if file exists
    if (thumbnailData.fileName) {
      try {
        const bucket = admin.storage().bucket();
        await bucket.file(thumbnailData.fileName).delete();
      } catch (storageError) {
        console.log('Storage file may not exist or already deleted:', storageError.message);
      }
    }

    await db.collection('thumbnailHistory').doc(id).delete();
    return { success: true, message: 'Thumbnail deleted successfully' };
  } catch (error) {
    if (error instanceof functions.https.HttpsError) throw error;
    console.error('Delete thumbnail error:', error);
    throw new functions.https.HttpsError('internal', 'Failed to delete thumbnail.');
  }
});

// Get All History (Unified View)
exports.getAllHistory = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  checkRateLimit(uid, 'getAllHistory', 10);

  const { limit = 10 } = data || {};
  const safeLimit = Math.min(Math.max(1, parseInt(limit) || 10), 20);

  // Safe query helper - returns empty array if collection/index doesn't exist
  const safeQuery = async (collectionName) => {
    try {
      return await db.collection(collectionName)
        .where('userId', '==', uid)
        .orderBy('createdAt', 'desc')
        .limit(safeLimit)
        .get();
    } catch (e) {
      console.warn(`Query failed for ${collectionName}:`, e.message);
      return { forEach: () => {}, size: 0 }; // Return empty mock snapshot
    }
  };

  try {
    // Fetch from all history collections in parallel (including enterprise tools)
    const [
      optimizationsSnap, competitorSnap, trendSnap, thumbnailSnap,
      placementSnap, channelAuditSnap, viralSnap, monetizationSnap, scriptSnap,
      sponsorshipSnap, diversificationSnap, cpmBoosterSnap, audienceProfileSnap,
      digitalProductSnap, affiliateSnap, multiIncomeSnap,
      brandDealSnap, licensingSnap, automationSnap
    ] = await Promise.all([
      safeQuery('optimizations'),
      safeQuery('competitorHistory'),
      safeQuery('trendHistory'),
      safeQuery('thumbnailHistory'),
      safeQuery('placementFinderHistory'),
      safeQuery('channelAuditHistory'),
      // Enterprise tools
      safeQuery('viralPredictorHistory'),
      safeQuery('monetizationHistory'),
      safeQuery('scriptWriterHistory'),
      // Enterprise monetization tools - Phase 1
      safeQuery('sponsorshipHistory'),
      safeQuery('diversificationHistory'),
      safeQuery('cpmBoosterHistory'),
      safeQuery('audienceProfileHistory'),
      // Enterprise monetization tools - Phase 2
      safeQuery('digitalProductHistory'),
      safeQuery('affiliateHistory'),
      safeQuery('multiIncomeHistory'),
      // Enterprise monetization tools - Phase 3
      safeQuery('brandDealHistory'),
      safeQuery('licensingHistory'),
      safeQuery('automationHistory')
    ]);

    // Safe timestamp handler - handles various Firestore timestamp formats
    const getTimestamp = (field) => {
      if (!field) return Date.now();
      if (typeof field === 'number') return field;
      if (typeof field.toMillis === 'function') return field.toMillis();
      if (field._seconds) return field._seconds * 1000;
      if (field instanceof Date) return field.getTime();
      return Date.now();
    };

    // Safe serialization - removes non-serializable Firestore objects
    const sanitize = (obj) => {
      if (obj === null || obj === undefined) return null;
      try {
        return JSON.parse(JSON.stringify(obj));
      } catch (e) {
        return null;
      }
    };

    const formatHistory = (snap, type) => {
      const items = [];
      snap.forEach(doc => {
        try {
          const data = doc.data();
          const timestamp = getTimestamp(data.createdAt);

          // Create clean item without raw createdAt (non-serializable)
          const item = {
            id: doc.id,
            type,
            timestamp,
            createdAt: new Date(timestamp).toISOString()
          };

          // Safely copy other fields, excluding raw createdAt
          Object.keys(data).forEach(key => {
            if (key !== 'createdAt') {
              item[key] = sanitize(data[key]) ?? data[key];
            }
          });

          items.push(item);
        } catch (docError) {
          console.error('Error processing history doc:', doc.id, docError);
        }
      });
      return items;
    };

    const allHistory = [
      ...formatHistory(optimizationsSnap, 'optimization'),
      ...formatHistory(competitorSnap, 'competitor'),
      ...formatHistory(trendSnap, 'trend'),
      ...formatHistory(thumbnailSnap, 'thumbnail'),
      ...formatHistory(placementSnap, 'placement'),
      ...formatHistory(channelAuditSnap, 'channelAudit'),
      // Enterprise tools
      ...formatHistory(viralSnap, 'viral'),
      ...formatHistory(monetizationSnap, 'monetization'),
      ...formatHistory(scriptSnap, 'script'),
      // Enterprise monetization tools - Phase 1
      ...formatHistory(sponsorshipSnap, 'sponsorship'),
      ...formatHistory(diversificationSnap, 'diversification'),
      ...formatHistory(cpmBoosterSnap, 'cpmbooster'),
      ...formatHistory(audienceProfileSnap, 'audienceprofile'),
      // Enterprise monetization tools - Phase 2
      ...formatHistory(digitalProductSnap, 'digitalproduct'),
      ...formatHistory(affiliateSnap, 'affiliate'),
      ...formatHistory(multiIncomeSnap, 'multiincome'),
      // Enterprise monetization tools - Phase 3
      ...formatHistory(brandDealSnap, 'branddeal'),
      ...formatHistory(licensingSnap, 'licensing'),
      ...formatHistory(automationSnap, 'automation')
    ];

    // Sort by timestamp descending
    allHistory.sort((a, b) => b.timestamp - a.timestamp);

    return {
      success: true,
      history: {
        all: allHistory.slice(0, safeLimit * 3),
        optimizations: formatHistory(optimizationsSnap, 'optimization'),
        competitor: formatHistory(competitorSnap, 'competitor'),
        trends: formatHistory(trendSnap, 'trend'),
        thumbnails: formatHistory(thumbnailSnap, 'thumbnail'),
        placements: formatHistory(placementSnap, 'placement'),
        channelAudit: formatHistory(channelAuditSnap, 'channelAudit'),
        // Enterprise tools
        viral: formatHistory(viralSnap, 'viral'),
        monetization: formatHistory(monetizationSnap, 'monetization'),
        scripts: formatHistory(scriptSnap, 'script'),
        // New enterprise monetization tools
        sponsorship: formatHistory(sponsorshipSnap, 'sponsorship'),
        diversification: formatHistory(diversificationSnap, 'diversification'),
        cpmbooster: formatHistory(cpmBoosterSnap, 'cpmbooster'),
        audienceprofile: formatHistory(audienceProfileSnap, 'audienceprofile')
      },
      counts: {
        optimizations: optimizationsSnap.size,
        competitor: competitorSnap.size,
        trends: trendSnap.size,
        thumbnails: thumbnailSnap.size,
        placements: placementSnap.size,
        channelAudit: channelAuditSnap.size,
        // Enterprise tools
        viral: viralSnap.size,
        monetization: monetizationSnap.size,
        scripts: scriptSnap.size,
        // New enterprise monetization tools
        sponsorship: sponsorshipSnap.size,
        diversification: diversificationSnap.size,
        cpmbooster: cpmBoosterSnap.size,
        audienceprofile: audienceProfileSnap.size
      }
    };
  } catch (error) {
    console.error('Get all history error:', error);
    throw new functions.https.HttpsError('internal', 'Failed to load history.');
  }
});

// ==============================================
// PLACEMENT FINDER - Find YouTube Channels for Google Ads
// ==============================================

/**
 * Extract channel ID from various YouTube channel URL formats
 * Supports: /channel/UCxxx, /@handle, /c/customname, /user/username
 */
function extractChannelInfo(url) {
  const patterns = [
    // Channel ID format: youtube.com/channel/UCxxxxxx
    { regex: /youtube\.com\/channel\/([^\/\?&]+)/, type: 'id' },
    // Handle format: youtube.com/@handle
    { regex: /youtube\.com\/@([^\/\?&]+)/, type: 'handle' },
    // Custom URL: youtube.com/c/customname
    { regex: /youtube\.com\/c\/([^\/\?&]+)/, type: 'custom' },
    // User format: youtube.com/user/username
    { regex: /youtube\.com\/user\/([^\/\?&]+)/, type: 'user' }
  ];

  for (const { regex, type } of patterns) {
    const match = url.match(regex);
    if (match) return { value: match[1], type };
  }

  throw new Error('Invalid YouTube channel URL. Please use a valid channel link.');
}

/**
 * Find Placements - Main function to find YouTube channels for Google Ads
 * Analyzes user's channel and finds similar high-exposure channels
 */
exports.findPlacements = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  checkRateLimit(uid, 'findPlacements', 5);
  await checkUsageLimit(uid, 'placementFinder');

  const { channelUrl } = data;
  if (!channelUrl) {
    throw new functions.https.HttpsError('invalid-argument', 'Channel URL is required');
  }

  try {
    // Step 1: Extract channel info from URL
    const channelInfo = extractChannelInfo(channelUrl);

    // Step 2: Get channel details from YouTube API
    let channelResponse;
    if (channelInfo.type === 'id') {
      channelResponse = await youtube.channels.list({
        part: 'snippet,statistics,brandingSettings,topicDetails',
        id: channelInfo.value
      });
    } else if (channelInfo.type === 'handle') {
      channelResponse = await youtube.channels.list({
        part: 'snippet,statistics,brandingSettings,topicDetails',
        forHandle: channelInfo.value
      });
    } else {
      // For custom URLs and usernames, search first
      const searchResponse = await youtube.search.list({
        part: 'snippet',
        q: channelInfo.value,
        type: 'channel',
        maxResults: 1
      });

      if (!searchResponse.data.items?.length) {
        throw new functions.https.HttpsError('not-found', 'Channel not found');
      }

      channelResponse = await youtube.channels.list({
        part: 'snippet,statistics,brandingSettings,topicDetails',
        id: searchResponse.data.items[0].snippet.channelId
      });
    }

    if (!channelResponse.data.items?.length) {
      throw new functions.https.HttpsError('not-found', 'Channel not found');
    }

    const userChannel = channelResponse.data.items[0];
    const channelId = userChannel.id;
    const channelName = userChannel.snippet.title;
    const channelDescription = userChannel.snippet.description || '';
    const subscriberCount = parseInt(userChannel.statistics.subscriberCount) || 0;
    const channelThumbnail = userChannel.snippet.thumbnails?.medium?.url || userChannel.snippet.thumbnails?.default?.url;

    // Step 3: Get recent videos with FULL details to understand content
    const videosResponse = await youtube.search.list({
      part: 'snippet',
      channelId: channelId,
      type: 'video',
      order: 'date',
      maxResults: 10
    });

    // Get video IDs for detailed stats
    const videoIds = videosResponse.data.items?.map(v => v.id.videoId).filter(Boolean) || [];

    // Fetch detailed video statistics and content details
    let videoDetails = [];
    if (videoIds.length > 0) {
      const videoDetailsResponse = await youtube.videos.list({
        part: 'snippet,statistics,contentDetails',
        id: videoIds.join(',')
      });
      videoDetails = videoDetailsResponse.data.items || [];
    }

    // Build rich video context for AI
    const sourceVideos = videoDetails.map(v => ({
      title: v.snippet.title,
      description: (v.snippet.description || '').substring(0, 300),
      views: parseInt(v.statistics.viewCount) || 0,
      likes: parseInt(v.statistics.likeCount) || 0,
      tags: v.snippet.tags?.slice(0, 10) || [],
      category: v.snippet.categoryId,
      duration: v.contentDetails.duration
    }));

    // Sort by views to identify most popular content
    const topVideos = [...sourceVideos].sort((a, b) => b.views - a.views).slice(0, 5);
    const recentVideoTitles = sourceVideos.map(v => v.title);
    const allTags = [...new Set(sourceVideos.flatMap(v => v.tags))].slice(0, 20);
    const topicCategories = userChannel.topicDetails?.topicCategories?.map(t => t.split('/').pop()) || [];

    // Step 4: Use AI to identify PRIMARY TOPIC (audience interest) vs STYLE (presentation)
    // This is CRITICAL for ad placement - we need to find the RIGHT AUDIENCE
    const analysisPrompt = `You are a YouTube advertising expert. Your goal is to find channels with the SAME AUDIENCE for ad placement.

=== CHANNEL INFO ===
Name: ${channelName}
Description: ${channelDescription.substring(0, 500)}
Subscribers: ${subscriberCount.toLocaleString()}

=== CHANNEL'S VIDEOS ===
${sourceVideos.slice(0, 8).map((v, i) => `
VIDEO ${i + 1}: "${v.title}"
- Description: ${v.description.substring(0, 150)}
- Tags: ${v.tags.slice(0, 5).join(', ') || 'none'}
`).join('\n')}

=== ALL VIDEO TAGS ===
${allTags.join(', ') || 'No tags found'}

CRITICAL: Distinguish between PRIMARY TOPIC and STYLE:

PRIMARY TOPIC = What the content is ABOUT (determines the AUDIENCE)
Examples: Christmas, cooking, gaming, fitness, kids content, meditation, travel, tech reviews

STYLE = How the content is PRESENTED (just the format/genre)
Examples: rock music, animation, vlog style, tutorial format, comedy

For ad placement, we want to reach the SAME AUDIENCE. The audience for "Christmas rock music" is people who watch CHRISTMAS content, NOT rock music fans in general.

Respond in this EXACT JSON format:
{
  "primaryTopic": "The main subject/theme that defines the AUDIENCE (e.g., 'Christmas', 'Cooking', 'Gaming', 'Kids Entertainment')",
  "style": "How the content is presented (e.g., 'rock music', 'animation', 'tutorial')",
  "niche": "Combined description (e.g., 'Christmas Music')",
  "audienceInterest": "What the audience is interested in (e.g., 'Christmas content', 'holiday music', 'seasonal entertainment')",
  "language": "Primary language",
  "primaryTopicKeywords": ["keyword directly related to PRIMARY TOPIC", "another primary keyword", "third primary keyword"],
  "searchQueries": [
    "search query focused on PRIMARY TOPIC",
    "another PRIMARY TOPIC focused search",
    "third search for PRIMARY TOPIC content",
    "fourth PRIMARY TOPIC search",
    "fifth search query"
  ]
}

IMPORTANT EXAMPLES:
- "Christmas rock song" â†’ primaryTopic: "Christmas", style: "rock music", searchQueries should find Christmas content
- "Animated cooking tutorial" â†’ primaryTopic: "Cooking", style: "animation", searchQueries should find cooking content
- "Kids nursery rhymes" â†’ primaryTopic: "Kids Entertainment", style: "music", searchQueries should find kids content

The searchQueries MUST focus on the PRIMARY TOPIC, not the style!`;

    const aiResponse = await openai.chat.completions.create({
      model: 'gpt-4',
      messages: [{ role: 'user', content: analysisPrompt }],
      temperature: 0.5,
      max_tokens: 800
    });

    let analysis;
    try {
      const responseText = aiResponse.choices[0].message.content.trim();
      const jsonMatch = responseText.match(/\{[\s\S]*\}/);
      analysis = JSON.parse(jsonMatch ? jsonMatch[0] : responseText);
    } catch (e) {
      // Fallback: try to detect primary topic from content
      const contentText = (channelName + ' ' + recentVideoTitles.join(' ') + ' ' + allTags.join(' ')).toLowerCase();

      // Detect common primary topics
      let detectedTopic = 'General';
      const topicPatterns = {
        'Christmas': ['christmas', 'xmas', 'holiday', 'santa', 'noel', 'festive'],
        'Kids Entertainment': ['kids', 'children', 'nursery', 'cartoon', 'toddler'],
        'Gaming': ['game', 'gaming', 'gameplay', 'playthrough', 'gamer'],
        'Cooking': ['recipe', 'cooking', 'food', 'chef', 'kitchen'],
        'Fitness': ['workout', 'fitness', 'exercise', 'gym', 'training'],
        'Music': ['music', 'song', 'album', 'concert', 'band'],
        'Tech': ['tech', 'review', 'unboxing', 'gadget', 'smartphone']
      };

      for (const [topic, keywords] of Object.entries(topicPatterns)) {
        if (keywords.some(k => contentText.includes(k))) {
          detectedTopic = topic;
          break;
        }
      }

      analysis = {
        primaryTopic: detectedTopic,
        style: 'video',
        niche: detectedTopic,
        audienceInterest: detectedTopic + ' content',
        language: 'en',
        primaryTopicKeywords: allTags.slice(0, 3),
        searchQueries: [...recentVideoTitles.slice(0, 2), ...allTags.slice(0, 2), channelName].filter(Boolean).slice(0, 5)
      };
    }

    console.log('Placement Finder - Primary Topic:', analysis.primaryTopic, '| Style:', analysis.style);

    // Step 5: Search for channels with the SAME PRIMARY TOPIC
    const channelVideoMap = new Map();

    // Build search queries focused on PRIMARY TOPIC
    const primaryTopicQueries = analysis.searchQueries || [];
    const topicKeywordQueries = (analysis.primaryTopicKeywords || []).map(k => k + ' channel');

    // Also search using primary topic directly
    const directTopicQueries = [
      analysis.primaryTopic,
      analysis.primaryTopic + ' music',
      analysis.primaryTopic + ' videos',
      analysis.audienceInterest
    ].filter(q => q && q.length > 2);

    // Combine all queries, prioritizing topic-focused ones
    const allSearchQueries = [
      ...primaryTopicQueries,
      ...directTopicQueries,
      ...topicKeywordQueries
    ].filter(Boolean);

    const searchQueries = [...new Set(allSearchQueries)].slice(0, 10);

    console.log('Placement Finder search queries:', searchQueries);

    for (const query of searchQueries) {
      try {
        // Search for VIDEOS (don't restrict language - let it find all relevant content)
        const searchResponse = await youtube.search.list({
          part: 'snippet',
          q: query,
          type: 'video',
          maxResults: 25,
          order: 'relevance'
        });

        // Collect videos and their channels
        searchResponse.data.items?.forEach(item => {
          const vidChannelId = item.snippet.channelId;
          if (vidChannelId !== channelId) { // Exclude source channel
            if (!channelVideoMap.has(vidChannelId)) {
              channelVideoMap.set(vidChannelId, {
                channelId: vidChannelId,
                channelName: item.snippet.channelTitle,
                foundVideos: []
              });
            }
            channelVideoMap.get(vidChannelId).foundVideos.push({
              title: item.snippet.title,
              description: (item.snippet.description || '').substring(0, 200)
            });
          }
        });

        // If we found enough channels, we can stop early
        if (channelVideoMap.size >= 40) break;

      } catch (e) {
        console.log('Video search query failed:', query, e.message);
      }
    }

    // If video search failed, try channel search as fallback
    if (channelVideoMap.size < 5) {
      console.log('Video search found few results, trying channel search fallback');

      for (const query of searchQueries.slice(0, 4)) {
        try {
          const channelSearchResponse = await youtube.search.list({
            part: 'snippet',
            q: query,
            type: 'channel',
            maxResults: 15
          });

          channelSearchResponse.data.items?.forEach(item => {
            const chId = item.snippet.channelId;
            if (chId !== channelId && !channelVideoMap.has(chId)) {
              channelVideoMap.set(chId, {
                channelId: chId,
                channelName: item.snippet.channelTitle,
                foundVideos: []
              });
            }
          });
        } catch (e) {
          console.log('Channel search fallback failed:', query, e.message);
        }
      }
    }

    // Step 6: Get detailed channel info for found channels
    const channelIds = Array.from(channelVideoMap.keys()).slice(0, 50);

    console.log('Placement Finder found', channelIds.length, 'candidate channels');

    if (channelIds.length === 0) {
      throw new functions.https.HttpsError('not-found', 'No similar channels found. Try a different channel.');
    }

    const detailsResponse = await youtube.channels.list({
      part: 'snippet,statistics',
      id: channelIds.join(','),
      maxResults: 50
    });

    // Step 7: Get recent videos from each found channel for content analysis
    const channelsWithContent = [];
    const channelDetailsMap = new Map();

    detailsResponse.data.items?.forEach(ch => {
      channelDetailsMap.set(ch.id, ch);
    });

    // Batch fetch recent videos from top candidate channels (limit to save API quota)
    const topCandidates = channelIds.slice(0, 25);

    for (const candidateId of topCandidates) {
      try {
        const chDetails = channelDetailsMap.get(candidateId);
        const foundData = channelVideoMap.get(candidateId);

        if (!chDetails) continue;

        // Get recent videos from this channel
        const recentVidsResponse = await youtube.search.list({
          part: 'snippet',
          channelId: candidateId,
          type: 'video',
          order: 'date',
          maxResults: 8
        });

        const candidateVideoTitles = recentVidsResponse.data.items?.map(v => v.snippet.title) || [];

        channelsWithContent.push({
          channelId: candidateId,
          channelName: chDetails.snippet.title,
          channelDescription: (chDetails.snippet.description || '').substring(0, 300),
          handle: chDetails.snippet.customUrl || null,
          thumbnail: chDetails.snippet.thumbnails?.medium?.url || chDetails.snippet.thumbnails?.default?.url,
          subscribers: parseInt(chDetails.statistics.subscriberCount) || 0,
          totalViews: parseInt(chDetails.statistics.viewCount) || 0,
          videoCount: parseInt(chDetails.statistics.videoCount) || 0,
          recentVideoTitles: candidateVideoTitles,
          foundVideos: foundData?.foundVideos || []
        });
      } catch (e) {
        console.log('Failed to get videos for channel:', candidateId, e.message);
      }
    }

    if (channelsWithContent.length === 0) {
      throw new functions.https.HttpsError('not-found', 'No quality channels found. The analyzed channel may be too niche.');
    }

    // Step 8: Build PRIMARY TOPIC keywords for strict matching
    const primaryTopic = analysis.primaryTopic || 'General';
    const primaryTopicLower = primaryTopic.toLowerCase();

    // Build list of keywords that MUST be present for high scores
    const primaryTopicKeywords = [];

    // Add primary topic keywords from AI analysis
    if (analysis.primaryTopicKeywords) {
      primaryTopicKeywords.push(...analysis.primaryTopicKeywords);
    }

    // Add the primary topic itself
    primaryTopicKeywords.push(primaryTopic);

    // Detect specific topic patterns and add related keywords
    const topicPatterns = {
      'christmas': ['christmas', 'xmas', 'holiday', 'santa', 'noel', 'festive', 'carol'],
      'halloween': ['halloween', 'spooky', 'scary', 'horror', 'trick or treat'],
      'kids': ['kids', 'children', 'nursery', 'toddler', 'baby', 'educational'],
      'gaming': ['gaming', 'game', 'gameplay', 'gamer', 'playthrough', 'lets play'],
      'cooking': ['cooking', 'recipe', 'food', 'chef', 'kitchen', 'baking'],
      'fitness': ['fitness', 'workout', 'exercise', 'gym', 'training', 'health']
    };

    // Check which topic patterns match and add their keywords
    for (const [topic, keywords] of Object.entries(topicPatterns)) {
      if (primaryTopicLower.includes(topic) || keywords.some(k => primaryTopicLower.includes(k))) {
        primaryTopicKeywords.push(...keywords);
        break;
      }
    }

    const uniqueKeywords = [...new Set(primaryTopicKeywords.map(k => k.toLowerCase()))].slice(0, 10);
    console.log('PRIMARY TOPIC keywords for scoring:', uniqueKeywords);

    // Step 9: Use AI to score with STRICT focus on PRIMARY TOPIC (audience match)
    const scoringPrompt = `You are scoring YouTube channels for Google Ads placement targeting.

GOAL: Find channels with the SAME AUDIENCE as the source channel.

=== SOURCE CHANNEL ===
Name: ${channelName}
PRIMARY TOPIC (defines the audience): ${primaryTopic}
Style: ${analysis.style || 'video'}
Niche: ${analysis.niche}

Source Videos:
${topVideos.slice(0, 4).map(v => `- "${v.title}"`).join('\n')}

=== CANDIDATE CHANNELS ===
${channelsWithContent.slice(0, 20).map((ch, i) => `
[${i + 1}] ${ch.channelName}
Videos: ${ch.recentVideoTitles.slice(0, 3).join(' | ')}
`).join('\n')}

STRICT SCORING - Based on PRIMARY TOPIC match (audience match):
- 80-100: Channel has SAME PRIMARY TOPIC (e.g., both are Christmas content, both are kids content)
- 50-79: Channel is somewhat related to PRIMARY TOPIC
- 0-49: Channel does NOT match PRIMARY TOPIC (wrong audience)

CRITICAL RULE for "${primaryTopic}":
${primaryTopic === 'Christmas' || primaryTopicLower.includes('christmas') ?
  '- ONLY channels with Christmas/holiday content should score 60+\n- Regular music/rock/pop channels WITHOUT Christmas = MAX 40 points\n- Christmas Songs, Holiday Music, Carols = 80+ points' :
  primaryTopic === 'Kids Entertainment' || primaryTopicLower.includes('kids') ?
  '- ONLY channels with kids/children content should score 60+\n- Adult music or entertainment channels = MAX 40 points' :
  `- ONLY channels about "${primaryTopic}" should score 60+\n- Channels about different topics = MAX 40 points`
}

Respond with ONLY a JSON array of ${Math.min(channelsWithContent.length, 20)} scores:
[score1, score2, ...]`;

    let contentScores = [];
    try {
      const scoringResponse = await openai.chat.completions.create({
        model: 'gpt-4',
        messages: [{ role: 'user', content: scoringPrompt }],
        temperature: 0.2,
        max_tokens: 500
      });

      const scoresText = scoringResponse.choices[0].message.content.trim();
      const scoresMatch = scoresText.match(/\[[\d,\s]+\]/);
      if (scoresMatch) {
        contentScores = JSON.parse(scoresMatch[0]);
      }
    } catch (e) {
      console.log('AI scoring failed, using fallback:', e.message);
    }

    // Step 10: Build final placements with STRICT PRIMARY TOPIC matching
    const placements = channelsWithContent.slice(0, 20).map((ch, index) => {
      // Build channel text for keyword matching
      const channelText = (ch.channelName + ' ' + ch.channelDescription + ' ' + ch.recentVideoTitles.join(' ')).toLowerCase();

      // Check how many PRIMARY TOPIC keywords this channel matches
      const topicMatches = uniqueKeywords.filter(k => channelText.includes(k)).length;
      const hasPrimaryTopicMatch = topicMatches >= 1;

      // Get AI content score
      let contentScore = contentScores[index];

      // If AI didn't score, calculate based on keyword matching
      if (contentScore === undefined || contentScore === null) {
        if (hasPrimaryTopicMatch) {
          // Channel matches PRIMARY TOPIC - score based on match strength
          contentScore = 50 + (topicMatches * 15); // 65 for 1 match, 80 for 2, etc.
        } else {
          // Channel does NOT match PRIMARY TOPIC - very low score
          contentScore = 25;
        }
      } else {
        // AI scored it, but ENFORCE PRIMARY TOPIC requirement
        if (!hasPrimaryTopicMatch && uniqueKeywords.length > 0) {
          // Channel doesn't match PRIMARY TOPIC - cap at 40 regardless of AI score
          contentScore = Math.min(contentScore, 40);
        } else if (topicMatches >= 2) {
          // Strong match - boost score
          contentScore = Math.min(contentScore + 10, 100);
        }
      }

      // Very small engagement bonus (max 3 points) - content match is primary
      let engagementBonus = 0;
      if (ch.subscribers > 50000) engagementBonus += 1;
      if (ch.subscribers > 500000) engagementBonus += 2;

      const finalScore = Math.min(Math.round(contentScore + engagementBonus), 100);

      return {
        channelId: ch.channelId,
        channelName: ch.channelName,
        channelUrl: `https://www.youtube.com/channel/${ch.channelId}`,
        handle: ch.handle,
        thumbnail: ch.thumbnail,
        description: ch.channelDescription.substring(0, 150),
        subscribers: ch.subscribers,
        subscribersFormatted: formatNumber(ch.subscribers),
        totalViews: ch.totalViews,
        videoCount: ch.videoCount,
        relevanceScore: finalScore,
        matchesPrimaryTopic: hasPrimaryTopicMatch,
        sampleVideos: ch.recentVideoTitles.slice(0, 3)
      };
    })
    .filter(ch => ch.subscribers >= 500)
    .sort((a, b) => {
      // Sort by: primary topic match first, then by score
      if (a.matchesPrimaryTopic && !b.matchesPrimaryTopic) return -1;
      if (!a.matchesPrimaryTopic && b.matchesPrimaryTopic) return 1;
      return b.relevanceScore - a.relevanceScore;
    })
    .slice(0, 30);

    if (placements.length === 0) {
      throw new functions.https.HttpsError('not-found', 'No quality channels found. The analyzed channel may be too niche.');
    }

    // Step 11: Save to history
    const historyData = {
      userId: uid,
      channelUrl,
      channelInfo: {
        id: channelId,
        name: channelName,
        subscribers: subscriberCount,
        thumbnail: channelThumbnail,
        description: channelDescription.substring(0, 300),
        // Include source videos so users can see what was analyzed
        topVideos: topVideos.slice(0, 5).map(v => ({
          title: v.title,
          views: v.views
        }))
      },
      analysis: {
        primaryTopic: primaryTopic, // The main subject that defines the audience
        style: analysis.style || 'video', // How content is presented
        niche: analysis.niche,
        language: analysis.language || 'en',
        audienceInterest: analysis.audienceInterest,
        primaryTopicKeywords: uniqueKeywords, // Keywords used for matching
        targetAudience: analysis.targetAudience || analysis.audienceInterest
      },
      placements,
      totalFound: placements.length,
      searchQueries: searchQueries.slice(0, 5),
      createdAt: admin.firestore.FieldValue.serverTimestamp(),
      updatedAt: admin.firestore.FieldValue.serverTimestamp()
    };

    const historyRef = await db.collection('placementFinderHistory').add(historyData);

    // Step 12: Update usage
    await incrementUsage(uid, 'placementFinder');
    await logUsage(uid, 'placement_finder', {
      channelId,
      channelName,
      placementsFound: placements.length
    });

    return {
      success: true,
      historyId: historyRef.id,
      channelInfo: historyData.channelInfo,
      analysis: historyData.analysis,
      placements,
      totalFound: placements.length,
      maxAllowed: 50
    };

  } catch (error) {
    console.error('Placement finder error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal',
      sanitizeErrorMessage(error, 'Failed to find placements. Please try again.'));
  }
});

/**
 * Helper to format large numbers (1000 -> 1K, 1000000 -> 1M)
 */
function formatNumber(num) {
  if (num >= 1000000) return (num / 1000000).toFixed(1) + 'M';
  if (num >= 1000) return (num / 1000).toFixed(1) + 'K';
  return num.toString();
}

/**
 * Find More Placements - Add 10 more channels to existing search
 */
exports.findMorePlacements = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  checkRateLimit(uid, 'findMorePlacements', 10);

  const { historyId } = data;
  if (!historyId) {
    throw new functions.https.HttpsError('invalid-argument', 'History ID is required');
  }

  try {
    // Get existing history entry
    const historyDoc = await db.collection('placementFinderHistory').doc(historyId).get();

    if (!historyDoc.exists) {
      throw new functions.https.HttpsError('not-found', 'History entry not found');
    }

    const historyData = historyDoc.data();

    if (historyData.userId !== uid) {
      throw new functions.https.HttpsError('permission-denied', 'Not authorized');
    }

    const currentPlacements = historyData.placements || [];

    if (currentPlacements.length >= 50) {
      throw new functions.https.HttpsError('resource-exhausted', 'Maximum of 50 placements reached');
    }

    // Get IDs of already found channels to exclude
    const existingIds = new Set(currentPlacements.map(p => p.channelId));
    existingIds.add(historyData.channelInfo.id); // Also exclude user's own channel

    // Search for more channels using stored queries
    const searchQueries = historyData.searchQueries || historyData.analysis?.keywords || [];
    const allNewChannelIds = new Set();

    for (const query of searchQueries) {
      try {
        // Use pageToken or different query variations to get different results
        const searchResponse = await youtube.search.list({
          part: 'snippet',
          q: `${query} channel`,
          type: 'channel',
          maxResults: 20,
          relevanceLanguage: 'en'
        });

        searchResponse.data.items?.forEach(item => {
          if (!existingIds.has(item.snippet.channelId)) {
            allNewChannelIds.add(item.snippet.channelId);
          }
        });
      } catch (e) {
        console.log('Search query failed:', query, e.message);
      }
    }

    const newChannelIds = Array.from(allNewChannelIds).slice(0, 15);

    if (newChannelIds.length === 0) {
      return {
        success: true,
        message: 'No more channels found matching your criteria',
        placements: currentPlacements,
        totalFound: currentPlacements.length,
        maxAllowed: 50,
        added: 0
      };
    }

    // Get channel details
    const detailsResponse = await youtube.channels.list({
      part: 'snippet,statistics',
      id: newChannelIds.join(',')
    });

    const newPlacements = detailsResponse.data.items
      ?.map(ch => {
        const subs = parseInt(ch.statistics.subscriberCount) || 0;
        const views = parseInt(ch.statistics.viewCount) || 0;
        const videos = parseInt(ch.statistics.videoCount) || 0;

        let score = 50;
        if (subs > 10000) score += 10;
        if (subs > 100000) score += 10;
        if (subs > 1000000) score += 10;
        if (views > 1000000) score += 10;
        if (videos > 50) score += 5;
        if (videos > 200) score += 5;

        return {
          channelId: ch.id,
          channelName: ch.snippet.title,
          channelUrl: `https://www.youtube.com/channel/${ch.id}`,
          handle: ch.snippet.customUrl || null,
          thumbnail: ch.snippet.thumbnails?.medium?.url || ch.snippet.thumbnails?.default?.url,
          description: (ch.snippet.description || '').substring(0, 150),
          subscribers: subs,
          subscribersFormatted: formatNumber(subs),
          totalViews: views,
          videoCount: videos,
          relevanceScore: Math.min(score, 100)
        };
      })
      .filter(ch => ch.subscribers >= 1000)
      .sort((a, b) => b.relevanceScore - a.relevanceScore || b.subscribers - a.subscribers)
      .slice(0, 10);

    // Combine and limit to 50
    const combinedPlacements = [...currentPlacements, ...newPlacements].slice(0, 50);

    // Update history document
    await db.collection('placementFinderHistory').doc(historyId).update({
      placements: combinedPlacements,
      totalFound: combinedPlacements.length,
      updatedAt: admin.firestore.FieldValue.serverTimestamp()
    });

    await logUsage(uid, 'placement_finder_expand', {
      historyId,
      added: newPlacements.length,
      total: combinedPlacements.length
    });

    return {
      success: true,
      placements: combinedPlacements,
      totalFound: combinedPlacements.length,
      maxAllowed: 50,
      added: newPlacements.length
    };

  } catch (error) {
    console.error('Find more placements error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal',
      sanitizeErrorMessage(error, 'Failed to find more placements.'));
  }
});

/**
 * Get Placement Finder History
 */
exports.getPlacementFinderHistory = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  checkRateLimit(uid, 'getPlacementFinderHistory', 20);

  const { limit = 20, offset = 0 } = data || {};
  const safeLimit = Math.min(Math.max(1, parseInt(limit) || 20), 50);
  const safeOffset = Math.max(0, parseInt(offset) || 0);

  // Safe timestamp handler
  const getTs = (field) => {
    if (!field) return Date.now();
    if (typeof field === 'number') return field;
    if (typeof field.toMillis === 'function') return field.toMillis();
    if (field._seconds) return field._seconds * 1000;
    if (field instanceof Date) return field.getTime();
    return Date.now();
  };

  try {
    const snapshot = await db.collection('placementFinderHistory')
      .where('userId', '==', uid)
      .orderBy('createdAt', 'desc')
      .limit(safeLimit)
      .offset(safeOffset)
      .get();

    const history = [];
    snapshot.forEach(doc => {
      try {
        const docData = doc.data();
        const timestamp = getTs(docData.createdAt);
        const { createdAt, updatedAt, ...rest } = docData;
        history.push({
          id: doc.id,
          ...rest,
          timestamp,
          createdAt: new Date(timestamp).toISOString()
        });
      } catch (e) {
        console.error('Error processing placement doc:', doc.id, e);
      }
    });

    return { success: true, history, count: history.length };
  } catch (error) {
    console.error('Get placement finder history error:', error);
    throw new functions.https.HttpsError('internal', 'Failed to load history.');
  }
});

/**
 * Delete Placement Finder Entry
 */
exports.deletePlacementFinder = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);

  const { id } = data || {};
  if (!id) {
    throw new functions.https.HttpsError('invalid-argument', 'History ID is required');
  }

  try {
    const doc = await db.collection('placementFinderHistory').doc(id).get();
    if (!doc.exists || doc.data().userId !== uid) {
      throw new functions.https.HttpsError('permission-denied', 'Not authorized to delete this item');
    }

    await db.collection('placementFinderHistory').doc(id).delete();
    return { success: true, message: 'Placement search deleted successfully' };
  } catch (error) {
    if (error instanceof functions.https.HttpsError) throw error;
    console.error('Delete placement finder error:', error);
    throw new functions.https.HttpsError('internal', 'Failed to delete.');
  }
});

/**
 * Delete Channel Audit History Item
 */
exports.deleteChannelAudit = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);

  const { id } = data || {};
  if (!id) {
    throw new functions.https.HttpsError('invalid-argument', 'History ID is required');
  }

  try {
    const doc = await db.collection('channelAuditHistory').doc(id).get();
    if (!doc.exists || doc.data().userId !== uid) {
      throw new functions.https.HttpsError('permission-denied', 'Not authorized to delete this item');
    }

    await db.collection('channelAuditHistory').doc(id).delete();
    return { success: true, message: 'Channel audit deleted successfully' };
  } catch (error) {
    if (error instanceof functions.https.HttpsError) throw error;
    console.error('Delete channel audit error:', error);
    throw new functions.https.HttpsError('internal', 'Failed to delete.');
  }
});

// ==============================================
// CAMPAIGN REPORTS (Admin Feature)
// ==============================================

/**
 * Upload Campaign Report Images
 * Admin uploads screenshots from Google Ads campaigns
 */
exports.uploadReportImages = functions.https.onCall(async (data, context) => {
  const adminId = await requireAdmin(context);
  checkRateLimit(adminId, 'uploadReportImages', 20);

  const { images } = data || {};

  if (!images || !Array.isArray(images) || images.length === 0) {
    throw new functions.https.HttpsError('invalid-argument', 'At least one image is required');
  }

  if (images.length > 6) {
    throw new functions.https.HttpsError('invalid-argument', 'Maximum 6 images allowed');
  }

  try {
    const bucket = admin.storage().bucket();
    const reportId = db.collection('campaignReports').doc().id;
    const uploadedImages = [];

    for (let i = 0; i < images.length; i++) {
      const imageData = images[i];

      // Validate base64 image data
      if (!imageData.base64 || !imageData.mimeType) {
        throw new functions.https.HttpsError('invalid-argument', `Invalid image data at index ${i}`);
      }

      // Support common image formats
      const allowedTypes = ['image/png', 'image/jpeg', 'image/jpg', 'image/webp'];
      if (!allowedTypes.includes(imageData.mimeType)) {
        throw new functions.https.HttpsError('invalid-argument', `Unsupported image type: ${imageData.mimeType}`);
      }

      // Decode base64
      const buffer = Buffer.from(imageData.base64, 'base64');

      // Validate file size (max 10MB per image)
      if (buffer.length > 10 * 1024 * 1024) {
        throw new functions.https.HttpsError('invalid-argument', `Image ${i + 1} exceeds 10MB limit`);
      }

      // Generate filename
      const extension = imageData.mimeType.split('/')[1];
      const fileName = `campaign-reports/${reportId}/image_${i + 1}_${Date.now()}.${extension}`;

      // Upload to Firebase Storage
      const file = bucket.file(fileName);
      await file.save(buffer, {
        metadata: {
          contentType: imageData.mimeType,
          metadata: {
            uploadedBy: adminId,
            reportId: reportId
          }
        }
      });

      // Get signed URL (valid for 7 days)
      const [url] = await file.getSignedUrl({
        action: 'read',
        expires: Date.now() + 7 * 24 * 60 * 60 * 1000
      });

      uploadedImages.push({
        url: url,
        storageRef: fileName,
        uploadedAt: new Date().toISOString(),
        index: i + 1
      });
    }

    return {
      success: true,
      reportId: reportId,
      images: uploadedImages,
      message: `${uploadedImages.length} images uploaded successfully`
    };
  } catch (error) {
    if (error instanceof functions.https.HttpsError) throw error;
    console.error('Upload report images error:', error);
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to upload images'));
  }
});

/**
 * Analyze Campaign Report Images with GPT-4 Vision
 * Uses AI to extract metrics and generate recommendations
 */
exports.analyzeReportImages = functions.https.onCall(async (data, context) => {
  const adminId = await requireAdmin(context);
  checkRateLimit(adminId, 'analyzeReportImages', 10);

  const { images, campaignName, additionalContext } = data || {};

  if (!images || !Array.isArray(images) || images.length === 0) {
    throw new functions.https.HttpsError('invalid-argument', 'At least one image is required');
  }

  try {
    // Build image content for GPT-4 Vision
    const imageContent = images.map((img, index) => ({
      type: 'image_url',
      image_url: {
        url: img.base64 ? `data:${img.mimeType};base64,${img.base64}` : img.url,
        detail: 'high'
      }
    }));

    const systemPrompt = `You are an expert YouTube channel growth strategist and Google Ads analyst creating a professional client report. Your task is to provide COMPREHENSIVE VISUAL ANALYSIS of the provided campaign screenshots.

## YOUR ROLE
You are preparing a detailed report for a YouTube creator client. This report should demonstrate deep expertise by thoroughly analyzing every visual element in the screenshots - not just extracting numbers, but interpreting what the data MEANS and what the visuals SHOW.

## VISUAL ANALYSIS REQUIREMENTS
For EACH screenshot, you must:
1. **Describe what you see**: Layout, columns, charts, graphs, status indicators, color coding, warning icons
2. **Interpret visual trends**: Are graphs going up/down? What colors indicate? What do status badges mean?
3. **Note all visible text**: Campaign names, video titles, ad group names, labels, column headers
4. **Identify data patterns**: Which metrics stand out? Any anomalies? Comparisons between rows?
5. **Describe the dashboard context**: What Google Ads section is this? What time period? What filters are applied?

## RESPONSE FORMAT
Your response MUST be valid JSON with this exact structure:
{
  "screenshotAnalysis": [
    {
      "imageNumber": 1,
      "description": "Detailed 3-5 sentence description of what this screenshot shows visually - the layout, visible data, charts, colors, and notable elements",
      "keyObservations": ["Specific observation 1", "Specific observation 2", "Specific observation 3"],
      "dataExtracted": "Summary of key metrics visible in this specific image"
    }
  ],
  "campaignType": "Search|Display|Video|Shopping|Performance Max|Discovery",
  "dateRange": "extracted date range from screenshots",
  "youtubeMetrics": {
    "publicViews": "number with commas as shown (e.g., '15,443') or null",
    "impressions": "number with commas as shown (e.g., '19,824') or null",
    "videoTitle": "full video title if visible or null",
    "adType": "ad type like 'Responsive video ad' or null",
    "adGroup": "ad group name if visible or null",
    "status": "Eligible|Paused|etc or null"
  },
  "metrics": {
    "impressions": "number or null",
    "clicks": "number or null",
    "ctr": "percentage string or null",
    "avgCpc": "currency string or null",
    "cost": "currency string or null",
    "conversions": "number or null",
    "conversionRate": "percentage string or null",
    "costPerConversion": "currency string or null",
    "impressionShare": "percentage string or null"
  },
  "performance": {
    "overall": "Excellent|Good|Average|Needs Improvement|Poor",
    "trend": "Improving|Stable|Declining",
    "highlights": ["array of positive points - reference specific visual evidence from screenshots"],
    "concerns": ["array of concerns - reference specific visual evidence from screenshots"]
  },
  "recommendations": [
    {
      "priority": "High|Medium|Low",
      "category": "Thumbnails|Titles|Descriptions|Content|Posting Schedule|Engagement|SEO|Branding|Analytics",
      "title": "Short recommendation title",
      "description": "Detailed explanation that references what you observed in the screenshots. Connect your advice to specific data points you saw.",
      "expectedImpact": "Expected improvement with specific projections based on current metrics",
      "evidenceFromScreenshots": "Quote or reference the specific data from screenshots that supports this recommendation"
    }
  ],
  "narrativeSummary": "A 4-6 sentence professional narrative summary that weaves together observations from ALL screenshots. Reference specific visuals, trends, and data points. This should read like a professional analyst's assessment, not just a list of numbers.",
  "summary": "2-3 sentence executive summary",
  "nextSteps": "Prioritized immediate actions with specific targets based on current metrics",
  "fiverCTA": "A compelling call-to-action for professional YouTube optimization services"
}

## CRITICAL INSTRUCTIONS
1. **SCREENSHOT ANALYSIS IS MANDATORY**: The "screenshotAnalysis" array must have one entry per image. Describe what you LITERALLY SEE.
2. Extract "YouTube public views" metric - look for columns labeled "YouTube public views" in the screenshots.
3. Extract "Impr." (Impressions), "Video" (video title), "Ad type", and "Status" columns.
4. **Connect recommendations to visual evidence**: Every recommendation should reference specific data you observed.
5. **Be descriptive about charts/graphs**: If you see a performance graph, describe if it trends up, down, has spikes, etc.
6. For recommendations, focus on YOUTUBE CHANNEL IMPROVEMENT:
   - Thumbnail design and optimization
   - Video title strategies (CTR improvement)
   - Description and tags optimization
   - Content quality and watch time
   - Posting schedule and consistency
   - Audience engagement tactics
   - Channel branding and identity
7. Provide at least 4-6 detailed, evidence-based YouTube growth recommendations.
8. The narrativeSummary should tell a STORY about what the screenshots reveal.`;

    const userPrompt = `Analyze these ${images.length} Google Ads campaign screenshot(s)${campaignName ? ` for the "${campaignName}" campaign` : ''}.${additionalContext ? `\n\nAdditional context: ${additionalContext}` : ''}

## REQUIRED ANALYSIS

### Step 1: Visual Description (MOST IMPORTANT)
For each screenshot, describe in detail:
- What dashboard/section is shown
- What columns, metrics, and data are visible
- Any charts, graphs, or visual indicators
- Colors, status badges, icons, or warnings
- The overall layout and what it tells us

### Step 2: Data Extraction
- Find "YouTube public views" - the most important metric
- Extract impressions, video title, ad type, status
- Note any other visible performance metrics

### Step 3: Professional Analysis
- What story do these screenshots tell about the campaign?
- What patterns or trends are visible?
- What should the client understand from this data?

### Step 4: Recommendations
- Provide YouTube CHANNEL growth recommendations (thumbnails, titles, content strategy)
- Connect each recommendation to specific data you observed in the screenshots

Remember: Describe what you SEE, not just what numbers say. The client wants to understand their campaign visually.`;

    const response = await openai.chat.completions.create({
      model: 'gpt-4o',
      messages: [
        { role: 'system', content: systemPrompt },
        {
          role: 'user',
          content: [
            { type: 'text', text: userPrompt },
            ...imageContent
          ]
        }
      ],
      max_tokens: 6000,
      temperature: 0.3
    });

    const content = response.choices[0]?.message?.content || '';

    // Try to parse JSON response
    let analysis;
    try {
      // Extract JSON from response (handle markdown code blocks)
      const jsonMatch = content.match(/```json\s*([\s\S]*?)\s*```/) ||
                        content.match(/```\s*([\s\S]*?)\s*```/) ||
                        [null, content];
      analysis = JSON.parse(jsonMatch[1] || content);
    } catch (parseError) {
      console.error('Failed to parse AI response as JSON:', parseError);
      // Return raw response if parsing fails
      analysis = {
        rawResponse: content,
        parseError: true,
        summary: 'Analysis completed. Please review the raw response.',
        recommendations: [],
        metrics: {}
      };
    }

    return {
      success: true,
      analysis: analysis,
      analyzedAt: new Date().toISOString()
    };
  } catch (error) {
    if (error instanceof functions.https.HttpsError) throw error;
    console.error('Analyze report images error:', error);
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to analyze images'));
  }
});

/**
 * Create Campaign Report
 * Save a new campaign report (draft or ready)
 */
exports.createCampaignReport = functions.https.onCall(async (data, context) => {
  const adminId = await requireAdmin(context);
  checkRateLimit(adminId, 'createCampaignReport', 20);

  const { reportId, images, aiAnalysis, editedContent, campaignName, status } = data || {};

  if (!images || images.length === 0) {
    throw new functions.https.HttpsError('invalid-argument', 'At least one image is required');
  }

  try {
    const docId = reportId || db.collection('campaignReports').doc().id;

    const reportData = {
      adminId: adminId,
      clientId: null,
      status: status || 'draft',
      campaignName: campaignName || 'Campaign Report',
      images: images,
      aiAnalysis: aiAnalysis || null,
      editedContent: editedContent || {
        title: campaignName || 'Campaign Performance Report',
        summary: aiAnalysis?.summary || '',
        metrics: aiAnalysis?.metrics || {},
        youtubeMetrics: aiAnalysis?.youtubeMetrics || {},
        performance: aiAnalysis?.performance || {},
        recommendations: aiAnalysis?.recommendations || [],
        callToAction: aiAnalysis?.fiverCTA || ''
      },
      createdAt: admin.firestore.FieldValue.serverTimestamp(),
      updatedAt: admin.firestore.FieldValue.serverTimestamp(),
      sentAt: null,
      viewedAt: null,
      clientViewedCount: 0
    };

    await db.collection('campaignReports').doc(docId).set(reportData);

    return {
      success: true,
      reportId: docId,
      message: 'Report created successfully'
    };
  } catch (error) {
    if (error instanceof functions.https.HttpsError) throw error;
    console.error('Create campaign report error:', error);
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to create report'));
  }
});

/**
 * Update Campaign Report
 * Admin edits report content
 */
exports.updateCampaignReport = functions.https.onCall(async (data, context) => {
  const adminId = await requireAdmin(context);
  checkRateLimit(adminId, 'updateCampaignReport', 30);

  const { reportId, editedContent, campaignName, status } = data || {};

  if (!reportId) {
    throw new functions.https.HttpsError('invalid-argument', 'Report ID is required');
  }

  try {
    const reportRef = db.collection('campaignReports').doc(reportId);
    const reportDoc = await reportRef.get();

    if (!reportDoc.exists) {
      throw new functions.https.HttpsError('not-found', 'Report not found');
    }

    const updateData = {
      updatedAt: admin.firestore.FieldValue.serverTimestamp()
    };

    if (editedContent) {
      updateData.editedContent = editedContent;
    }
    if (campaignName) {
      updateData.campaignName = campaignName;
    }
    if (status) {
      updateData.status = status;
    }

    await reportRef.update(updateData);

    return {
      success: true,
      message: 'Report updated successfully'
    };
  } catch (error) {
    if (error instanceof functions.https.HttpsError) throw error;
    console.error('Update campaign report error:', error);
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to update report'));
  }
});

/**
 * Delete Campaign Report
 * Admin deletes a report and its images
 */
exports.deleteCampaignReport = functions.https.onCall(async (data, context) => {
  const adminId = await requireAdmin(context);
  checkRateLimit(adminId, 'deleteCampaignReport', 20);

  const { reportId } = data || {};

  if (!reportId) {
    throw new functions.https.HttpsError('invalid-argument', 'Report ID is required');
  }

  try {
    const reportRef = db.collection('campaignReports').doc(reportId);
    const reportDoc = await reportRef.get();

    if (!reportDoc.exists) {
      throw new functions.https.HttpsError('not-found', 'Report not found');
    }

    const reportData = reportDoc.data();

    // Delete images from storage
    const bucket = admin.storage().bucket();
    if (reportData.images && Array.isArray(reportData.images)) {
      for (const image of reportData.images) {
        if (image.storageRef) {
          try {
            await bucket.file(image.storageRef).delete();
          } catch (e) {
            console.log('Image already deleted or not found:', image.storageRef);
          }
        }
      }
    }

    // Delete any notifications related to this report
    const notificationsSnapshot = await db.collection('userNotifications')
      .where('reportId', '==', reportId)
      .get();

    const batch = db.batch();
    notificationsSnapshot.docs.forEach(doc => {
      batch.delete(doc.ref);
    });
    batch.delete(reportRef);
    await batch.commit();

    return {
      success: true,
      message: 'Report deleted successfully'
    };
  } catch (error) {
    if (error instanceof functions.https.HttpsError) throw error;
    console.error('Delete campaign report error:', error);
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to delete report'));
  }
});

/**
 * Send Report to Client
 * Assign report to a user and create notification
 */
exports.sendReportToClient = functions.https.onCall(async (data, context) => {
  const adminId = await requireAdmin(context);
  checkRateLimit(adminId, 'sendReportToClient', 20);

  const { reportId, clientId } = data || {};

  if (!reportId) {
    throw new functions.https.HttpsError('invalid-argument', 'Report ID is required');
  }
  if (!clientId) {
    throw new functions.https.HttpsError('invalid-argument', 'Client ID is required');
  }

  try {
    // Verify client exists
    const clientDoc = await db.collection('users').doc(clientId).get();
    if (!clientDoc.exists) {
      throw new functions.https.HttpsError('not-found', 'Client not found');
    }

    // Get report
    const reportRef = db.collection('campaignReports').doc(reportId);
    const reportDoc = await reportRef.get();

    if (!reportDoc.exists) {
      throw new functions.https.HttpsError('not-found', 'Report not found');
    }

    const reportData = reportDoc.data();
    const campaignName = reportData.campaignName || 'Campaign Report';

    // Update report with client assignment
    await reportRef.update({
      clientId: clientId,
      status: 'sent',
      sentAt: admin.firestore.FieldValue.serverTimestamp(),
      updatedAt: admin.firestore.FieldValue.serverTimestamp()
    });

    // Create notification for client
    await db.collection('userNotifications').add({
      userId: clientId,
      type: 'new_report',
      reportId: reportId,
      title: `New Campaign Report: ${campaignName}`,
      message: 'Your campaign performance report is ready to view.',
      isRead: false,
      createdAt: admin.firestore.FieldValue.serverTimestamp()
    });

    return {
      success: true,
      message: `Report sent to ${clientDoc.data().email || 'client'}`
    };
  } catch (error) {
    if (error instanceof functions.https.HttpsError) throw error;
    console.error('Send report to client error:', error);
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to send report'));
  }
});

/**
 * Get Admin Reports
 * Fetch all campaign reports for admin
 */
exports.getAdminReports = functions.https.onCall(async (data, context) => {
  const adminId = await requireAdmin(context);
  checkRateLimit(adminId, 'getAdminReports', 30);

  const { limit: queryLimit, status } = data || {};

  try {
    let query = db.collection('campaignReports')
      .orderBy('createdAt', 'desc');

    if (status) {
      query = query.where('status', '==', status);
    }

    if (queryLimit) {
      query = query.limit(queryLimit);
    } else {
      query = query.limit(50);
    }

    const snapshot = await query.get();
    const reports = [];

    for (const doc of snapshot.docs) {
      const data = doc.data();

      // Get client info if assigned
      let clientInfo = null;
      if (data.clientId) {
        const clientDoc = await db.collection('users').doc(data.clientId).get();
        if (clientDoc.exists) {
          clientInfo = {
            uid: data.clientId,
            email: clientDoc.data().email
          };
        }
      }

      // Safe timestamp serialization
      const createdAt = data.createdAt;
      const sentAt = data.sentAt;
      const viewedAt = data.viewedAt;

      reports.push({
        id: doc.id,
        ...data,
        createdAt: createdAt ? (createdAt.toDate ? createdAt.toDate().toISOString() : createdAt) : null,
        sentAt: sentAt ? (sentAt.toDate ? sentAt.toDate().toISOString() : sentAt) : null,
        viewedAt: viewedAt ? (viewedAt.toDate ? viewedAt.toDate().toISOString() : viewedAt) : null,
        updatedAt: data.updatedAt ? (data.updatedAt.toDate ? data.updatedAt.toDate().toISOString() : data.updatedAt) : null,
        clientInfo: clientInfo
      });
    }

    return {
      success: true,
      reports: reports,
      count: reports.length
    };
  } catch (error) {
    if (error instanceof functions.https.HttpsError) throw error;
    console.error('Get admin reports error:', error);
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to load reports'));
  }
});

/**
 * Get Client Reports
 * Fetch reports assigned to a specific client
 */
exports.getClientReports = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  checkRateLimit(uid, 'getClientReports', 30);

  try {
    const snapshot = await db.collection('campaignReports')
      .where('clientId', '==', uid)
      .where('status', 'in', ['sent', 'viewed'])
      .orderBy('createdAt', 'desc')
      .limit(50)
      .get();

    const reports = snapshot.docs.map(doc => {
      const data = doc.data();
      const createdAt = data.createdAt;
      const sentAt = data.sentAt;

      return {
        id: doc.id,
        campaignName: data.campaignName,
        status: data.status,
        images: data.images,
        editedContent: data.editedContent,
        createdAt: createdAt ? (createdAt.toDate ? createdAt.toDate().toISOString() : createdAt) : null,
        sentAt: sentAt ? (sentAt.toDate ? sentAt.toDate().toISOString() : sentAt) : null
      };
    });

    return {
      success: true,
      reports: reports,
      count: reports.length
    };
  } catch (error) {
    if (error instanceof functions.https.HttpsError) throw error;
    console.error('Get client reports error:', error);
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to load reports'));
  }
});

/**
 * Mark Report as Viewed
 * Track when a client views their report
 */
exports.markReportViewed = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  checkRateLimit(uid, 'markReportViewed', 30);

  const { reportId } = data || {};

  if (!reportId) {
    throw new functions.https.HttpsError('invalid-argument', 'Report ID is required');
  }

  try {
    const reportRef = db.collection('campaignReports').doc(reportId);
    const reportDoc = await reportRef.get();

    if (!reportDoc.exists) {
      throw new functions.https.HttpsError('not-found', 'Report not found');
    }

    const reportData = reportDoc.data();

    // Verify user is the assigned client
    if (reportData.clientId !== uid) {
      throw new functions.https.HttpsError('permission-denied', 'Not authorized to view this report');
    }

    // Update view count and status
    const updates = {
      clientViewedCount: admin.firestore.FieldValue.increment(1)
    };

    // Set first view time and status if not already viewed
    if (reportData.status === 'sent') {
      updates.status = 'viewed';
      updates.viewedAt = admin.firestore.FieldValue.serverTimestamp();
    }

    await reportRef.update(updates);

    return {
      success: true,
      message: 'Report marked as viewed'
    };
  } catch (error) {
    if (error instanceof functions.https.HttpsError) throw error;
    console.error('Mark report viewed error:', error);
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to update report'));
  }
});

/**
 * Get Unread Notifications
 * Get notification count and list for a user
 */
exports.getUnreadNotifications = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  checkRateLimit(uid, 'getUnreadNotifications', 60);

  try {
    const snapshot = await db.collection('userNotifications')
      .where('userId', '==', uid)
      .where('isRead', '==', false)
      .orderBy('createdAt', 'desc')
      .limit(20)
      .get();

    const notifications = snapshot.docs.map(doc => {
      const data = doc.data();
      const createdAt = data.createdAt;

      return {
        id: doc.id,
        type: data.type,
        reportId: data.reportId,
        title: data.title,
        message: data.message,
        isRead: data.isRead,
        createdAt: createdAt ? (createdAt.toDate ? createdAt.toDate().toISOString() : createdAt) : null
      };
    });

    return {
      success: true,
      notifications: notifications,
      unreadCount: notifications.length
    };
  } catch (error) {
    if (error instanceof functions.https.HttpsError) throw error;
    console.error('Get unread notifications error:', error);
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to load notifications'));
  }
});

/**
 * Mark Notification as Read
 * Clear notification for a user
 */
exports.markNotificationRead = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  checkRateLimit(uid, 'markNotificationRead', 60);

  const { notificationId, markAll } = data || {};

  try {
    if (markAll) {
      // Mark all unread notifications as read
      const snapshot = await db.collection('userNotifications')
        .where('userId', '==', uid)
        .where('isRead', '==', false)
        .get();

      const batch = db.batch();
      snapshot.docs.forEach(doc => {
        batch.update(doc.ref, { isRead: true });
      });
      await batch.commit();

      return {
        success: true,
        message: `${snapshot.docs.length} notifications marked as read`
      };
    } else if (notificationId) {
      // Mark specific notification as read
      const notificationRef = db.collection('userNotifications').doc(notificationId);
      const notificationDoc = await notificationRef.get();

      if (!notificationDoc.exists) {
        throw new functions.https.HttpsError('not-found', 'Notification not found');
      }

      if (notificationDoc.data().userId !== uid) {
        throw new functions.https.HttpsError('permission-denied', 'Not authorized');
      }

      await notificationRef.update({ isRead: true });

      return {
        success: true,
        message: 'Notification marked as read'
      };
    } else {
      throw new functions.https.HttpsError('invalid-argument', 'Notification ID or markAll flag required');
    }
  } catch (error) {
    if (error instanceof functions.https.HttpsError) throw error;
    console.error('Mark notification read error:', error);
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to update notification'));
  }
});

/**
 * Get Single Report (for viewing)
 * Get a specific report with full details
 */
exports.getCampaignReport = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  checkRateLimit(uid, 'getCampaignReport', 30);

  const { reportId } = data || {};

  if (!reportId) {
    throw new functions.https.HttpsError('invalid-argument', 'Report ID is required');
  }

  try {
    const reportDoc = await db.collection('campaignReports').doc(reportId).get();

    if (!reportDoc.exists) {
      throw new functions.https.HttpsError('not-found', 'Report not found');
    }

    const reportData = reportDoc.data();
    const isUserAdmin = await isAdmin(uid);

    // Check authorization
    if (!isUserAdmin && reportData.clientId !== uid) {
      throw new functions.https.HttpsError('permission-denied', 'Not authorized to view this report');
    }

    // Serialize timestamps
    const createdAt = reportData.createdAt;
    const sentAt = reportData.sentAt;
    const viewedAt = reportData.viewedAt;

    return {
      success: true,
      report: {
        id: reportDoc.id,
        ...reportData,
        createdAt: createdAt ? (createdAt.toDate ? createdAt.toDate().toISOString() : createdAt) : null,
        sentAt: sentAt ? (sentAt.toDate ? sentAt.toDate().toISOString() : sentAt) : null,
        viewedAt: viewedAt ? (viewedAt.toDate ? viewedAt.toDate().toISOString() : viewedAt) : null,
        updatedAt: reportData.updatedAt ? (reportData.updatedAt.toDate ? reportData.updatedAt.toDate().toISOString() : reportData.updatedAt) : null
      }
    };
  } catch (error) {
    if (error instanceof functions.https.HttpsError) throw error;
    console.error('Get campaign report error:', error);
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to load report'));
  }
});

// ==============================================
// ENTERPRISE SUITE FUNCTIONS
// ==============================================

/**
 * Channel Audit Pro - Comprehensive channel analysis with SEO health scores
 * Analyzes a YouTube channel and provides detailed growth recommendations
 */
exports.auditChannel = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  checkRateLimit(uid, 'auditChannel', 5);
  await checkUsageLimit(uid, 'channelAudit');

  const { channelUrl } = data;
  if (!channelUrl) {
    throw new functions.https.HttpsError('invalid-argument', 'Channel URL is required');
  }

  try {
    // Step 1: Extract channel info from URL
    const channelInfo = extractChannelInfo(channelUrl);

    // Step 2: Get channel details from YouTube API
    let channelResponse;
    if (channelInfo.type === 'id') {
      channelResponse = await youtube.channels.list({
        part: 'snippet,statistics,brandingSettings,topicDetails,contentDetails',
        id: channelInfo.value
      });
    } else if (channelInfo.type === 'handle') {
      channelResponse = await youtube.channels.list({
        part: 'snippet,statistics,brandingSettings,topicDetails,contentDetails',
        forHandle: channelInfo.value
      });
    } else {
      // For custom URLs and usernames, search first
      const searchResponse = await youtube.search.list({
        part: 'snippet',
        q: channelInfo.value,
        type: 'channel',
        maxResults: 1
      });

      if (!searchResponse.data.items?.length) {
        throw new functions.https.HttpsError('not-found', 'Channel not found');
      }

      channelResponse = await youtube.channels.list({
        part: 'snippet,statistics,brandingSettings,topicDetails,contentDetails',
        id: searchResponse.data.items[0].snippet.channelId
      });
    }

    if (!channelResponse.data.items?.length) {
      throw new functions.https.HttpsError('not-found', 'Channel not found');
    }

    const channel = channelResponse.data.items[0];
    const channelId = channel.id;
    const channelName = channel.snippet.title;
    const channelDescription = channel.snippet.description || '';
    const subscriberCount = parseInt(channel.statistics.subscriberCount) || 0;
    const viewCount = parseInt(channel.statistics.viewCount) || 0;
    const videoCount = parseInt(channel.statistics.videoCount) || 0;
    const channelThumbnail = channel.snippet.thumbnails?.medium?.url || channel.snippet.thumbnails?.default?.url;

    // Step 3: Get recent videos to analyze content strategy
    const videosResponse = await youtube.search.list({
      part: 'snippet',
      channelId: channelId,
      type: 'video',
      order: 'date',
      maxResults: 20
    });

    const recentVideoIds = videosResponse.data.items?.map(v => v.id.videoId).filter(Boolean) || [];

    // Get video statistics
    let videoStats = [];
    if (recentVideoIds.length > 0) {
      const videoDetailsResponse = await youtube.videos.list({
        part: 'statistics,snippet,contentDetails',
        id: recentVideoIds.join(',')
      });
      videoStats = videoDetailsResponse.data.items || [];
    }

    // Calculate basic metrics
    const avgViews = videoStats.length > 0
      ? Math.round(videoStats.reduce((sum, v) => sum + parseInt(v.statistics.viewCount || 0), 0) / videoStats.length)
      : 0;
    const avgLikes = videoStats.length > 0
      ? Math.round(videoStats.reduce((sum, v) => sum + parseInt(v.statistics.likeCount || 0), 0) / videoStats.length)
      : 0;
    const avgComments = videoStats.length > 0
      ? Math.round(videoStats.reduce((sum, v) => sum + parseInt(v.statistics.commentCount || 0), 0) / videoStats.length)
      : 0;

    // Calculate engagement rate
    const engagementRate = avgViews > 0 ? ((avgLikes + avgComments) / avgViews * 100).toFixed(2) : 0;

    // Step 4: AI Analysis for comprehensive audit
    const videoTitles = videoStats.slice(0, 10).map(v => v.snippet.title);
    const videoDescriptions = videoStats.slice(0, 5).map(v => (v.snippet.description || '').substring(0, 200));

    const auditPrompt = `You are a YouTube growth expert. Perform a comprehensive channel audit and provide actionable insights.

CHANNEL DATA:
- Name: ${channelName}
- Description: ${channelDescription.substring(0, 500)}
- Subscribers: ${subscriberCount.toLocaleString()}
- Total Views: ${viewCount.toLocaleString()}
- Video Count: ${videoCount}
- Avg Views (recent): ${avgViews.toLocaleString()}
- Avg Likes (recent): ${avgLikes.toLocaleString()}
- Engagement Rate: ${engagementRate}%
- Recent Video Titles: ${videoTitles.join(' | ')}
- Sample Descriptions: ${videoDescriptions.join(' ... ')}

Analyze this channel and respond in this EXACT JSON format:
{
  "scores": {
    "overall": <0-100>,
    "seo": <0-100>,
    "content": <0-100>,
    "engagement": <0-100>,
    "growth": <0-100>
  },
  "analysis": {
    "summary": "2-3 sentence overview of channel health",
    "strengths": ["strength1", "strength2", "strength3"],
    "weaknesses": ["weakness1", "weakness2", "weakness3"]
  },
  "recommendations": [
    {"title": "Action item 1", "description": "Detailed explanation", "priority": "high"},
    {"title": "Action item 2", "description": "Detailed explanation", "priority": "medium"},
    {"title": "Action item 3", "description": "Detailed explanation", "priority": "low"}
  ]
}

Score Guidelines:
- SEO: Title optimization, description quality, tag usage, keyword targeting
- Content: Consistency, video quality indicators, niche focus
- Engagement: Like/comment ratio, community interaction
- Growth: Subscriber trends, view-to-subscriber ratio, potential`;

    const aiResponse = await openai.chat.completions.create({
      model: 'gpt-4',
      messages: [{ role: 'user', content: auditPrompt }],
      temperature: 0.7,
      max_tokens: 1500
    });

    let auditData;
    try {
      const responseText = aiResponse.choices[0].message.content.trim();
      const jsonMatch = responseText.match(/\{[\s\S]*\}/);
      auditData = JSON.parse(jsonMatch ? jsonMatch[0] : responseText);
    } catch (e) {
      // Fallback audit data
      auditData = {
        scores: {
          overall: 65,
          seo: 60,
          content: 70,
          engagement: 65,
          growth: 65
        },
        analysis: {
          summary: `${channelName} has ${subscriberCount.toLocaleString()} subscribers with an average engagement rate of ${engagementRate}%. The channel shows potential for growth with consistent content strategy improvements.`,
          strengths: ['Active content creation', 'Established audience base', 'Consistent posting'],
          weaknesses: ['SEO optimization needed', 'Description could be improved', 'Tag strategy unclear']
        },
        recommendations: [
          { title: 'Optimize video titles for search', description: 'Include target keywords naturally in your titles', priority: 'high' },
          { title: 'Improve description SEO', description: 'Add timestamps, links, and keyword-rich descriptions', priority: 'medium' },
          { title: 'Increase community engagement', description: 'Reply to comments and create community posts', priority: 'low' }
        ]
      };
    }

    // Step 5: Save to history
    const historyData = {
      userId: uid,
      channelUrl,
      channelInfo: {
        id: channelId,
        name: channelName,
        thumbnail: channelThumbnail,
        subscribers: subscriberCount,
        videoCount: videoCount,
        totalViews: viewCount
      },
      scores: auditData.scores,
      analysis: auditData.analysis,
      recommendations: auditData.recommendations,
      metrics: {
        avgViews,
        avgLikes,
        avgComments,
        engagementRate: parseFloat(engagementRate)
      },
      createdAt: admin.firestore.FieldValue.serverTimestamp()
    };

    const historyRef = await db.collection('channelAuditHistory').add(historyData);

    // Step 6: Update usage
    await incrementUsage(uid, 'channelAudit');
    await logUsage(uid, 'channel_audit', {
      channelId,
      channelName,
      overallScore: auditData.scores.overall
    });

    return {
      success: true,
      historyId: historyRef.id,
      channelInfo: historyData.channelInfo,
      scores: auditData.scores,
      analysis: auditData.analysis,
      recommendations: auditData.recommendations,
      metrics: historyData.metrics
    };

  } catch (error) {
    console.error('Channel audit error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal',
      sanitizeErrorMessage(error, 'Failed to audit channel. Please try again.'));
  }
});

/**
 * Viral Score Predictor - Predicts viral potential of video content
 * Analyzes title, description, and tags to estimate viral potential
 */
exports.predictViralScore = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  checkRateLimit(uid, 'predictViralScore', 10);
  await checkUsageLimit(uid, 'viralPredictor');

  const { title, description, tags } = data;
  if (!title) {
    throw new functions.https.HttpsError('invalid-argument', 'Video title is required');
  }

  try {
    const viralPrompt = `You are a YouTube viral content expert. Analyze this video content and predict its viral potential.

VIDEO CONTENT:
- Title: ${title}
- Description: ${description || 'Not provided'}
- Tags: ${tags || 'Not provided'}

Analyze the viral potential and respond in this EXACT JSON format:
{
  "viralScore": <0-100>,
  "verdict": "One sentence verdict about viral potential",
  "factors": [
    {"name": "Title Appeal", "score": <0-100>},
    {"name": "Emotional Hook", "score": <0-100>},
    {"name": "Clickability", "score": <0-100>},
    {"name": "Shareability", "score": <0-100>},
    {"name": "Trend Alignment", "score": <0-100>}
  ],
  "tips": [
    {"title": "Improvement tip 1", "detail": "Detailed explanation"},
    {"title": "Improvement tip 2", "detail": "Detailed explanation"},
    {"title": "Improvement tip 3", "detail": "Detailed explanation"}
  ]
}

Scoring Guidelines:
- 80-100: High viral potential - strong emotional hook, trending topic, shareable
- 60-79: Moderate potential - good elements but room for improvement
- 40-59: Average potential - needs significant optimization
- 0-39: Low potential - major changes needed`;

    const aiResponse = await openai.chat.completions.create({
      model: 'gpt-4',
      messages: [{ role: 'user', content: viralPrompt }],
      temperature: 0.7,
      max_tokens: 1000
    });

    let viralData;
    try {
      const responseText = aiResponse.choices[0].message.content.trim();
      const jsonMatch = responseText.match(/\{[\s\S]*\}/);
      viralData = JSON.parse(jsonMatch ? jsonMatch[0] : responseText);
    } catch (e) {
      viralData = {
        viralScore: 55,
        verdict: 'Moderate viral potential. Consider optimizing the title for more emotional impact.',
        factors: [
          { name: 'Title Appeal', score: 60 },
          { name: 'Emotional Hook', score: 50 },
          { name: 'Clickability', score: 55 },
          { name: 'Shareability', score: 55 },
          { name: 'Trend Alignment', score: 55 }
        ],
        tips: [
          { title: 'Add emotional triggers', detail: 'Use words that evoke curiosity or excitement' },
          { title: 'Create urgency', detail: 'Include time-sensitive elements when relevant' },
          { title: 'Optimize for sharing', detail: 'Make the content easy to share and discuss' }
        ]
      };
    }

    // Save to history
    const historyData = {
      userId: uid,
      title,
      description: description || '',
      tags: tags || '',
      viralScore: viralData.viralScore,
      verdict: viralData.verdict,
      factors: viralData.factors,
      tips: viralData.tips,
      createdAt: admin.firestore.FieldValue.serverTimestamp()
    };

    await db.collection('viralPredictorHistory').add(historyData);

    // Update usage
    await incrementUsage(uid, 'viralPredictor');
    await logUsage(uid, 'viral_predictor', { title, viralScore: viralData.viralScore });

    return {
      success: true,
      viralScore: viralData.viralScore,
      verdict: viralData.verdict,
      factors: viralData.factors,
      tips: viralData.tips
    };

  } catch (error) {
    console.error('Viral prediction error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal',
      sanitizeErrorMessage(error, 'Failed to predict viral score. Please try again.'));
  }
});

/**
 * Monetization Analyzer - Estimates channel earnings and revenue potential
 * Analyzes a YouTube channel and provides monetization insights
 */
exports.analyzeMonetization = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  checkRateLimit(uid, 'analyzeMonetization', 5);
  await checkUsageLimit(uid, 'monetizationAnalyzer');

  const { channelUrl } = data;
  if (!channelUrl) {
    throw new functions.https.HttpsError('invalid-argument', 'Channel URL is required');
  }

  try {
    // Extract channel info
    const channelInfo = extractChannelInfo(channelUrl);

    // Get channel details
    let channelResponse;
    if (channelInfo.type === 'id') {
      channelResponse = await youtube.channels.list({
        part: 'snippet,statistics,topicDetails',
        id: channelInfo.value
      });
    } else if (channelInfo.type === 'handle') {
      channelResponse = await youtube.channels.list({
        part: 'snippet,statistics,topicDetails',
        forHandle: channelInfo.value
      });
    } else {
      const searchResponse = await youtube.search.list({
        part: 'snippet',
        q: channelInfo.value,
        type: 'channel',
        maxResults: 1
      });

      if (!searchResponse.data.items?.length) {
        throw new functions.https.HttpsError('not-found', 'Channel not found');
      }

      channelResponse = await youtube.channels.list({
        part: 'snippet,statistics,topicDetails',
        id: searchResponse.data.items[0].snippet.channelId
      });
    }

    if (!channelResponse.data.items?.length) {
      throw new functions.https.HttpsError('not-found', 'Channel not found');
    }

    const channel = channelResponse.data.items[0];
    const channelId = channel.id;
    const channelName = channel.snippet.title;
    const channelThumbnail = channel.snippet.thumbnails?.medium?.url || channel.snippet.thumbnails?.default?.url;
    const subscriberCount = parseInt(channel.statistics.subscriberCount) || 0;
    const viewCount = parseInt(channel.statistics.viewCount) || 0;
    const videoCount = parseInt(channel.statistics.videoCount) || 0;
    const topicCategories = channel.topicDetails?.topicCategories?.map(t => t.split('/').pop()) || [];

    // Get recent videos for view analysis
    const videosResponse = await youtube.search.list({
      part: 'snippet',
      channelId: channelId,
      type: 'video',
      order: 'date',
      maxResults: 30
    });

    const recentVideoIds = videosResponse.data.items?.map(v => v.id.videoId).filter(Boolean) || [];

    let monthlyViews = 0;
    if (recentVideoIds.length > 0) {
      const videoDetailsResponse = await youtube.videos.list({
        part: 'statistics',
        id: recentVideoIds.slice(0, 20).join(',')
      });

      const recentStats = videoDetailsResponse.data.items || [];
      const totalRecentViews = recentStats.reduce((sum, v) => sum + parseInt(v.statistics.viewCount || 0), 0);
      // Estimate monthly views based on recent video performance
      monthlyViews = Math.round(totalRecentViews / Math.max(recentStats.length, 1) * 4); // Assuming ~4 videos/month
    }

    // CPM estimation based on niche
    const nicheCPM = {
      'Finance': 12,
      'Technology': 8,
      'Gaming': 4,
      'Entertainment': 3,
      'Education': 6,
      'Lifestyle': 5,
      'Music': 2,
      'Sports': 4,
      'News': 5,
      'default': 4
    };

    // Determine niche CPM
    let estimatedCPM = nicheCPM.default;
    for (const topic of topicCategories) {
      for (const [niche, cpm] of Object.entries(nicheCPM)) {
        if (topic.toLowerCase().includes(niche.toLowerCase())) {
          estimatedCPM = Math.max(estimatedCPM, cpm);
        }
      }
    }

    // Calculate earnings (monetized views are typically 40-60% of total)
    const monetizedViewRate = 0.5;
    const monthlyMonetizedViews = monthlyViews * monetizedViewRate;
    const monthlyAdRevenue = (monthlyMonetizedViews / 1000) * estimatedCPM;

    // Estimate other revenue streams based on subscriber count
    const sponsorshipPotential = subscriberCount > 10000 ? subscriberCount * 0.01 : 0;
    const membershipPotential = subscriberCount > 30000 ? subscriberCount * 0.002 : 0;
    const merchandisePotential = subscriberCount > 50000 ? subscriberCount * 0.001 : 0;

    const monthlyEarnings = monthlyAdRevenue + sponsorshipPotential + membershipPotential + merchandisePotential;
    const yearlyEarnings = monthlyEarnings * 12;

    // AI-powered recommendations
    const monetizationPrompt = `You are a YouTube monetization expert. Based on this channel data, provide 5 specific revenue optimization recommendations.

CHANNEL DATA:
- Subscribers: ${subscriberCount.toLocaleString()}
- Monthly Views (estimated): ${monthlyViews.toLocaleString()}
- Video Count: ${videoCount}
- Niche/Topics: ${topicCategories.join(', ') || 'General'}
- Estimated Monthly Revenue: $${monthlyEarnings.toFixed(2)}

Respond with a JSON array of 5 short, actionable recommendations (each under 100 characters):
["recommendation 1", "recommendation 2", "recommendation 3", "recommendation 4", "recommendation 5"]`;

    const aiResponse = await openai.chat.completions.create({
      model: 'gpt-4',
      messages: [{ role: 'user', content: monetizationPrompt }],
      temperature: 0.7,
      max_tokens: 500
    });

    let recommendations;
    try {
      const responseText = aiResponse.choices[0].message.content.trim();
      const jsonMatch = responseText.match(/\[[\s\S]*\]/);
      recommendations = JSON.parse(jsonMatch ? jsonMatch[0] : responseText);
    } catch (e) {
      recommendations = [
        'Optimize upload schedule for consistent viewer engagement',
        'Add end screens and cards to increase watch time',
        'Consider channel memberships for dedicated fans',
        'Explore brand sponsorship opportunities in your niche',
        'Create merchandise for your most engaged audience'
      ];
    }

    // Save to history
    const historyData = {
      userId: uid,
      channelUrl,
      channelInfo: {
        id: channelId,
        name: channelName,
        thumbnail: channelThumbnail,
        subscribers: subscriberCount,
        videoCount
      },
      earnings: {
        monthly: monthlyEarnings,
        yearly: yearlyEarnings,
        estimatedCPM,
        breakdown: {
          adRevenue: monthlyAdRevenue,
          sponsorships: sponsorshipPotential,
          memberships: membershipPotential,
          merchandise: merchandisePotential
        }
      },
      recommendations,
      createdAt: admin.firestore.FieldValue.serverTimestamp()
    };

    await db.collection('monetizationHistory').add(historyData);

    // Update usage
    await incrementUsage(uid, 'monetizationAnalyzer');
    await logUsage(uid, 'monetization_analyzer', { channelId, monthlyEarnings });

    return {
      success: true,
      channelName,
      channelThumbnail,
      subscribers: subscriberCount,
      monthlyEarnings,
      yearlyEarnings,
      estimatedCPM,
      breakdown: {
        adRevenue: monthlyAdRevenue,
        sponsorships: sponsorshipPotential,
        memberships: membershipPotential,
        merchandise: merchandisePotential
      },
      recommendations
    };

  } catch (error) {
    console.error('Monetization analysis error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal',
      sanitizeErrorMessage(error, 'Failed to analyze monetization. Please try again.'));
  }
});

/**
 * Script Writer Pro - AI-powered video script generation
 * Creates engaging video scripts based on topic, style, and duration
 */
exports.generateScript = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  checkRateLimit(uid, 'generateScript', 5);
  await checkUsageLimit(uid, 'scriptWriter');

  const { topic, duration, style, keywords } = data;
  if (!topic) {
    throw new functions.https.HttpsError('invalid-argument', 'Video topic is required');
  }

  try {
    // Duration mapping
    const durationMap = {
      'short': { minutes: '3-5', words: '600-900' },
      'medium': { minutes: '8-12', words: '1500-2200' },
      'long': { minutes: '15-20', words: '2800-3800' }
    };

    const targetDuration = durationMap[duration] || durationMap.medium;

    // Style descriptions
    const styleDescriptions = {
      'engaging': 'High energy, attention-grabbing, with strong hooks and calls to action. Use questions and direct audience engagement.',
      'educational': 'Informative, well-structured, with clear explanations. Include examples and step-by-step guidance.',
      'storytelling': 'Narrative-driven, with a clear beginning, middle, and end. Include personal anecdotes and emotional moments.',
      'listicle': 'Organized as a numbered list. Each point should be concise but valuable. Include transitions between points.'
    };

    const styleGuide = styleDescriptions[style] || styleDescriptions.engaging;

    const scriptPrompt = `You are an expert YouTube script writer. Create an engaging video script.

REQUIREMENTS:
- Topic: ${topic}
- Style: ${style || 'engaging'} - ${styleGuide}
- Target Duration: ${targetDuration.minutes} minutes (${targetDuration.words} words)
- Keywords to include: ${keywords || 'none specified'}

SCRIPT STRUCTURE:
1. Hook (first 5-10 seconds) - Grab attention immediately
2. Introduction - Brief overview of what viewers will learn
3. Main Content - Organized sections with clear value
4. Call to Action - Subscribe, like, comment
5. Outro - Wrap up and tease future content

FORMAT YOUR RESPONSE AS JSON:
{
  "title": "Suggested video title (SEO optimized)",
  "script": "The full script with [SECTIONS] marked, including speaking directions in (parentheses)",
  "wordCount": <number>,
  "estimatedDuration": "<X-Y>"
}

IMPORTANT: Write naturally as if speaking to camera. Include:
- Pauses marked as [PAUSE]
- Emphasis marked as *word*
- Visual cues marked as [B-ROLL: description]
- Transitions between sections`;

    const aiResponse = await openai.chat.completions.create({
      model: 'gpt-4',
      messages: [{ role: 'user', content: scriptPrompt }],
      temperature: 0.8,
      max_tokens: 3500
    });

    let scriptData;
    try {
      const responseText = aiResponse.choices[0].message.content.trim();
      const jsonMatch = responseText.match(/\{[\s\S]*\}/);
      scriptData = JSON.parse(jsonMatch ? jsonMatch[0] : responseText);
    } catch (e) {
      // If JSON parsing fails, treat the response as the script itself
      const rawScript = aiResponse.choices[0].message.content.trim();
      const wordCount = rawScript.split(/\s+/).length;
      scriptData = {
        title: topic,
        script: rawScript,
        wordCount: wordCount,
        estimatedDuration: duration === 'short' ? '3-5' : duration === 'long' ? '15-20' : '8-12'
      };
    }

    // Calculate actual word count if not provided
    if (!scriptData.wordCount && scriptData.script) {
      scriptData.wordCount = scriptData.script.split(/\s+/).length;
    }

    // Save to history
    const historyData = {
      userId: uid,
      topic,
      duration: duration || 'medium',
      style: style || 'engaging',
      keywords: keywords || '',
      title: scriptData.title,
      script: scriptData.script,
      wordCount: scriptData.wordCount,
      estimatedDuration: scriptData.estimatedDuration,
      createdAt: admin.firestore.FieldValue.serverTimestamp()
    };

    await db.collection('scriptWriterHistory').add(historyData);

    // Update usage
    await incrementUsage(uid, 'scriptWriter');
    await logUsage(uid, 'script_writer', { topic, style, wordCount: scriptData.wordCount });

    return {
      success: true,
      title: scriptData.title,
      script: scriptData.script,
      wordCount: scriptData.wordCount,
      estimatedDuration: scriptData.estimatedDuration
    };

  } catch (error) {
    console.error('Script generation error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal',
      sanitizeErrorMessage(error, 'Failed to generate script. Please try again.'));
  }
});

// ==============================================
// CREATIVE STUDIO - IMAGE GENERATION (NanoBanana API)
// ==============================================

/**
 * Token costs:
 * - Basic: 1 token per image
 * - HD: 2 tokens per image
 * - Ultra: 4 tokens per image
 * - Templates: vary by template (2-3 tokens)
 * - Upscale: 2 tokens
 * - Motion: 3 tokens
 *
 * Token rollover: Unused tokens roll over to next month (max 500)
 */

// Get user's creative tokens balance (synced with admin settings)
exports.getCreativeTokens = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);

  try {
    // Get user's subscription plan
    const userDoc = await db.collection('users').doc(uid).get();
    const userPlan = userDoc.exists ? (userDoc.data().subscription?.plan || 'free') : 'free';

    // Get admin-configured token settings (use shared helper for consistency)
    const tokenConfig = await getTokenConfigFromAdmin();
    const planConfig = tokenConfig[userPlan] || tokenConfig.free;
    const monthlyAllocation = planConfig.monthlyTokens || 10;
    const rolloverPercent = planConfig.rolloverPercent || 0;

    const tokenDoc = await db.collection('creativeTokens').doc(uid).get();

    if (!tokenDoc.exists) {
      // Initialize new user with plan-appropriate tokens
      const now = new Date();
      const initialTokens = {
        balance: monthlyAllocation,
        rollover: 0,
        plan: userPlan,
        monthlyAllocation: monthlyAllocation,
        rolloverPercent: rolloverPercent,
        lastRefresh: admin.firestore.FieldValue.serverTimestamp(),
        createdAt: admin.firestore.FieldValue.serverTimestamp()
      };
      await db.collection('creativeTokens').doc(uid).set(initialTokens);
      return {
        balance: monthlyAllocation,
        rollover: 0,
        plan: userPlan,
        monthlyAllocation: monthlyAllocation,
        lastRefresh: now.toISOString(),
        createdAt: now.toISOString()
      };
    }

    const tokenData = tokenDoc.data();

    // Check if plan has changed - sync if needed
    if (tokenData.plan !== userPlan) {
      await db.collection('creativeTokens').doc(uid).update({
        plan: userPlan,
        monthlyAllocation: monthlyAllocation,
        rolloverPercent: rolloverPercent
      });
      tokenData.plan = userPlan;
      tokenData.monthlyAllocation = monthlyAllocation;
    }

    // Check if monthly refresh is needed
    const now = new Date();
    const lastRefresh = tokenData.lastRefresh?.toDate() || new Date(0);
    const monthsSinceRefresh = (now.getFullYear() - lastRefresh.getFullYear()) * 12 +
                               (now.getMonth() - lastRefresh.getMonth());

    if (monthsSinceRefresh >= 1) {
      // Calculate rollover based on plan's rollover percent
      const maxRollover = Math.floor(tokenData.balance * (rolloverPercent / 100));
      const newBalance = monthlyAllocation + maxRollover;

      const updatedTokens = {
        balance: newBalance,
        rollover: maxRollover,
        plan: userPlan,
        monthlyAllocation: monthlyAllocation,
        rolloverPercent: rolloverPercent,
        lastRefresh: admin.firestore.FieldValue.serverTimestamp()
      };

      await db.collection('creativeTokens').doc(uid).update(updatedTokens);

      return {
        ...tokenData,
        ...updatedTokens,
        balance: newBalance
      };
    }

    return tokenData;

  } catch (error) {
    console.error('Get creative tokens error:', error);
    throw new functions.https.HttpsError('internal', 'Failed to get token balance');
  }
});

// Deduct creative tokens
exports.deductCreativeTokens = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const { amount, reason } = data;

  if (!amount || amount <= 0) {
    throw new functions.https.HttpsError('invalid-argument', 'Invalid token amount');
  }

  try {
    const tokenRef = db.collection('creativeTokens').doc(uid);

    return await db.runTransaction(async (transaction) => {
      const tokenDoc = await transaction.get(tokenRef);

      if (!tokenDoc.exists) {
        throw new functions.https.HttpsError('not-found', 'Token balance not found');
      }

      const currentBalance = tokenDoc.data().balance || 0;

      if (currentBalance < amount) {
        throw new functions.https.HttpsError('resource-exhausted', 'Insufficient tokens');
      }

      const newBalance = currentBalance - amount;
      transaction.update(tokenRef, {
        balance: newBalance,
        lastUsed: admin.firestore.FieldValue.serverTimestamp()
      });

      // Log token usage
      transaction.set(db.collection('creativeTokenUsage').doc(), {
        userId: uid,
        amount: amount,
        reason: reason || 'generation',
        timestamp: admin.firestore.FieldValue.serverTimestamp()
      });

      return { success: true, newBalance };
    });

  } catch (error) {
    console.error('Deduct tokens error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', 'Failed to deduct tokens');
  }
});

// =====================================================
// DIAGNOSTIC: Test Imagen API Configuration
// This helps debug API key and model availability issues
// =====================================================
exports.testImagenApi = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);

  // Check if user is admin for full diagnostics
  const userDoc = await db.collection('adminUsers').doc(uid).get();
  const isAdmin = userDoc.exists && userDoc.data().isAdmin === true;

  const results = {
    timestamp: new Date().toISOString(),
    apiKeyConfigured: false,
    apiKeyPrefix: null,
    modelTest: null,
    simpleGenerationTest: null,
    errors: []
  };

  try {
    // Check if API key is configured
    const geminiApiKey = functions.config().gemini?.key;
    results.apiKeyConfigured = !!geminiApiKey;

    if (!geminiApiKey) {
      results.errors.push('Gemini API key is not configured in Firebase. Run: firebase functions:config:set gemini.key="YOUR_API_KEY"');
      return results;
    }

    // Show API key prefix for debugging (safe - only first 8 chars)
    results.apiKeyPrefix = geminiApiKey.substring(0, 8) + '...';

    // Initialize the SDK
    const ai = new GoogleGenAI({ apiKey: geminiApiKey });

    // Test 1: Try to list models (if available)
    try {
      // The SDK might not have listModels, so we'll catch any error
      if (ai.models && typeof ai.models.list === 'function') {
        const modelsList = await ai.models.list();
        results.availableModels = modelsList.models?.map(m => m.name) || [];
      } else {
        results.modelTest = 'listModels not available in this SDK version';
      }
    } catch (listError) {
      results.modelTest = `listModels failed: ${listError.message}`;
    }

    // Test 2: Try a simple image generation with minimal settings
    try {
      console.log('Testing Imagen API with simple generation...');
      const testResponse = await ai.models.generateImages({
        model: 'imagen-4.0-generate-001',
        prompt: 'A simple red circle on white background',
        config: {
          numberOfImages: 1,
          aspectRatio: '1:1'
        }
      });

      if (testResponse.generatedImages && testResponse.generatedImages.length > 0) {
        results.simpleGenerationTest = 'SUCCESS - Imagen API is working!';
        results.imageGenerated = true;
      } else if (testResponse.generatedImages?.length === 0) {
        results.simpleGenerationTest = 'No images returned - might be safety filtered';
        results.imageGenerated = false;
      } else {
        results.simpleGenerationTest = 'Unexpected response format';
        results.rawResponse = JSON.stringify(testResponse).substring(0, 500);
      }
    } catch (genError) {
      results.simpleGenerationTest = 'FAILED';
      results.generationError = {
        message: genError.message,
        code: genError.code,
        status: genError.status,
        details: genError.details
      };

      // Parse specific error types
      const errMsg = genError.message?.toLowerCase() || '';
      if (errMsg.includes('api key')) {
        results.errors.push('API Key Error: Your API key is invalid or not authorized for Imagen. Get a key from https://aistudio.google.com/apikey');
      } else if (errMsg.includes('not found') || errMsg.includes('404')) {
        results.errors.push('Model Not Found: The imagen-4.0-generate-001 model is not accessible. This could mean: (1) Your API key does not have Imagen access, (2) Imagen is not available in your region, or (3) You need to accept terms at https://aistudio.google.com');
      } else if (errMsg.includes('permission') || errMsg.includes('403') || errMsg.includes('denied')) {
        results.errors.push('Permission Denied: Your API key does not have permission to use Imagen. Make sure you created the key at https://aistudio.google.com/apikey and that billing is enabled.');
      } else if (errMsg.includes('billing')) {
        results.errors.push('Billing Required: Imagen requires billing to be enabled. Go to https://aistudio.google.com and set up billing.');
      } else if (errMsg.includes('quota') || errMsg.includes('rate')) {
        results.errors.push('Rate Limited: You have hit the API rate limit. Wait a moment and try again.');
      } else {
        results.errors.push(`Unknown Error: ${genError.message}`);
      }
    }

    // Summary
    if (results.imageGenerated) {
      results.summary = 'All tests passed! Imagen API is working correctly.';
    } else {
      results.summary = 'Imagen API test failed. Check the errors array for details.';
    }

    return results;

  } catch (error) {
    console.error('Diagnostic error:', error);
    results.errors.push(`Diagnostic failed: ${error.message}`);
    return results;
  }
});

// Generate creative image using Google Gemini/Imagen API (NanoBanana)
// Documentation: https://ai.google.dev/gemini-api/docs/imagen
exports.generateCreativeImage = functions
  .runWith({
    timeoutSeconds: 300, // 5 minutes - Gemini image generation can take 60-120s
    memory: '1GB' // Increased memory for image processing with sharp
  })
  .https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  checkRateLimit(uid, 'generateImage', 10);

  const { prompt, model, quantity, aspectRatio, quality, templateId, templateVariables, negativePrompt, seed, styleReference, characterReference } = data;

  if (!prompt || prompt.trim().length === 0) {
    throw new functions.https.HttpsError('invalid-argument', 'Prompt is required');
  }

  // Validate prompt length (Imagen has limits)
  if (prompt.length > 2000) {
    throw new functions.https.HttpsError('invalid-argument', 'Prompt too long. Maximum 2000 characters.');
  }

  // Determine the final prompt to use
  let finalPrompt = prompt;
  const userPromptTrimmed = prompt.trim();

  // If a template is selected, fetch the professional prompt from Firestore
  if (templateId) {
    try {
      const templateDoc = await db.collection('promptTemplates').doc(templateId).get();
      if (templateDoc.exists) {
        const templateData = templateDoc.data();
        if (templateData.professionalPrompt) {
          // Use the professional prompt as the base
          finalPrompt = templateData.professionalPrompt;

          // Replace any {{variables}} in the professional prompt with user values
          if (templateVariables && typeof templateVariables === 'object') {
            Object.keys(templateVariables).forEach(key => {
              const placeholder = new RegExp(`\\{\\{${key}\\}\\}`, 'gi');
              finalPrompt = finalPrompt.replace(placeholder, templateVariables[key] || '');
            });
          }

          // IMPORTANT: Replace ALL remaining {{variable}} placeholders with the user's prompt
          // This ensures user input is incorporated even without explicit templateVariables
          if (userPromptTrimmed) {
            // Replace common variable patterns with user's prompt content
            const remainingPlaceholders = finalPrompt.match(/\{\{[^}]+\}\}/g);
            if (remainingPlaceholders && remainingPlaceholders.length > 0) {
              // Replace the first placeholder with the user's main description
              finalPrompt = finalPrompt.replace(remainingPlaceholders[0], userPromptTrimmed);
              // Replace remaining placeholders with empty string or a generic term
              remainingPlaceholders.slice(1).forEach(placeholder => {
                finalPrompt = finalPrompt.replace(placeholder, '');
              });
            }
          }

          // Clean up any double spaces from removed placeholders
          finalPrompt = finalPrompt.replace(/\s{2,}/g, ' ').trim();

          // ALWAYS append user's custom input if they provided meaningful content
          // This ensures their specific requests are included in the generation
          if (userPromptTrimmed && userPromptTrimmed.length > 5) {
            // Don't duplicate if the prompt is already fully in finalPrompt
            if (!finalPrompt.includes(userPromptTrimmed)) {
              finalPrompt = `${finalPrompt}\n\nUser's specific request: ${userPromptTrimmed}`;
            }
          }

          console.log(`Using professional prompt for template: ${templateId}`);
          console.log(`Final prompt length: ${finalPrompt.length} chars`);
        }
      }
    } catch (templateError) {
      console.warn('Could not fetch template, using original prompt:', templateError.message);
    }
  }

  // Calculate token cost
  const qualityCosts = { basic: 1, hd: 2, ultra: 4 };
  const baseCost = qualityCosts[quality] || 2;
  const imageCount = Math.min(Math.max(quantity || 1, 1), 4); // 1-4 images
  const totalCost = baseCost * imageCount;

  // Map aspect ratios to Imagen supported values
  const aspectRatioMap = {
    '1:1': '1:1',
    '16:9': '16:9',
    '9:16': '9:16',
    '4:3': '4:3',
    '3:4': '3:4'
  };
  const validAspectRatio = aspectRatioMap[aspectRatio] || '1:1';

  // Map model selection to AI models
  // Supports: Gemini Image Models, OpenAI DALL-E, and legacy Imagen API

  // Check model type
  const dalleModels = ['dall-e-3', 'dall-e-2', 'dalle-3', 'dalle-2', 'openai'];
  const isDalleModel = dalleModels.includes(model);

  // Gemini Image models (use generateContent API with image output)
  // These models support reference images via multimodal input
  // Include aliases without hyphens for backwards compatibility
  const geminiImageModels = ['nano-banana-pro', 'nano-banana', 'nanobanana-pro', 'nanobanana'];
  const isGeminiImageModel = geminiImageModels.includes(model);

  // Gemini Image model mapping (uses generateContent with responseModalities)
  // NOTE: gemini-2.5-flash-image does NOT exist in Google AI Studio
  // Valid models for image generation: gemini-3-pro-image-preview, gemini-2.0-flash-exp
  const geminiImageModelMap = {
    'auto': 'gemini-3-pro-image-preview',
    'nano-banana-pro': 'gemini-3-pro-image-preview',
    'nano-banana': 'gemini-2.0-flash-exp',  // Was gemini-2.5-flash-image which doesn't exist!
    // Aliases without hyphens for backwards compatibility
    'nanobanana-pro': 'gemini-3-pro-image-preview',
    'nanobanana': 'gemini-2.0-flash-exp'
  };

  // Imagen model mapping (uses ai.models.generateImages)
  // Auto defaults to Imagen 4 (best working model)
  const imagenModelMap = {
    'auto': 'imagen-4.0-generate-001',
    'imagen-4': 'imagen-4.0-generate-001',
    'imagen-4-ultra': 'imagen-4.0-ultra-generate-001',
    'imagen-3': 'imagen-3.0-generate-001',
    // Legacy keys for backwards compatibility
    'banana1': 'imagen-4.0-generate-001',
    'banana2': 'imagen-4.0-ultra-generate-001'
  };

  // DALL-E model mapping
  const dalleModelMap = {
    'dall-e-3': 'dall-e-3',
    'dalle-3': 'dall-e-3',
    'dall-e-2': 'dall-e-2',
    'dalle-2': 'dall-e-2',
    'openai': 'dall-e-3'
  };

  // Get the model ID based on type [VERIFIED-FIX-2025-12-01]
  const geminiImageModelId = geminiImageModelMap[model] || geminiImageModelMap['auto'];
  const imagenModelId = imagenModelMap[model] || 'imagen-4.0-generate-001'; // Default to Imagen 4 (NOT Imagen 3!)
  const dalleModelId = dalleModelMap[model] || 'dall-e-3';

  try {
    // Verify token balance - initialize if new user
    let tokenDoc = await db.collection('creativeTokens').doc(uid).get();
    let balance = 0;

    if (!tokenDoc.exists) {
      // Initialize new user with free tier tokens
      const initialTokens = {
        balance: 50,
        rollover: 0,
        plan: 'free',
        monthlyAllocation: 50,
        lastRefresh: admin.firestore.FieldValue.serverTimestamp(),
        createdAt: admin.firestore.FieldValue.serverTimestamp()
      };
      await db.collection('creativeTokens').doc(uid).set(initialTokens);
      // Use the known initial balance directly (avoid re-fetch timing issues)
      balance = 50;
    } else {
      balance = tokenDoc.data().balance || 0;
    }

    if (balance < totalCost) {
      throw new functions.https.HttpsError('resource-exhausted',
        `Insufficient tokens. Need ${totalCost}, have ${balance}`);
    }

    // Process generated images
    const generatedImages = [];
    const storage = admin.storage().bucket();
    const timestamp = Date.now();
    let usedModel = '';

    if (isDalleModel) {
      // ==========================================
      // DALL-E Image Generation (OpenAI)
      // ==========================================
      console.log(`Generating ${imageCount} image(s) with DALL-E ${dalleModelId}, aspect: ${validAspectRatio}`);
      console.log(`Prompt length: ${finalPrompt.length} chars, template: ${templateId || 'none'}`);

      // Map aspect ratios to DALL-E sizes
      const dalleSizeMap = {
        '1:1': '1024x1024',
        '16:9': '1792x1024', // DALL-E 3 only
        '9:16': '1024x1792', // DALL-E 3 only
        '4:3': '1024x1024',  // DALL-E doesn't support 4:3, use 1:1
        '3:4': '1024x1024'   // DALL-E doesn't support 3:4, use 1:1
      };

      // DALL-E 2 only supports 1024x1024, 512x512, 256x256
      const dalleSize = dalleModelId === 'dall-e-3'
        ? (dalleSizeMap[validAspectRatio] || '1024x1024')
        : '1024x1024';

      // DALL-E 3 only supports 1 image per request, DALL-E 2 supports up to 10
      const imagesPerRequest = dalleModelId === 'dall-e-3' ? 1 : Math.min(imageCount, 4);
      const requestsNeeded = dalleModelId === 'dall-e-3' ? imageCount : 1;

      for (let req = 0; req < requestsNeeded; req++) {
        try {
          const dalleResponse = await openai.images.generate({
            model: dalleModelId,
            prompt: finalPrompt,
            n: imagesPerRequest,
            size: dalleSize,
            quality: quality === 'ultra' || quality === 'hd' ? 'hd' : 'standard',
            response_format: 'b64_json'
          });

          if (dalleResponse.data && dalleResponse.data.length > 0) {
            for (let i = 0; i < dalleResponse.data.length; i++) {
              const imageData = dalleResponse.data[i];
              const imageBytes = imageData.b64_json;

              if (!imageBytes) continue;

              // Upload base64 image to Firebase Storage
              const imageIndex = generatedImages.length + 1;
              const fileName = `creative-studio/${uid}/${timestamp}-dalle-${imageIndex}.png`;
              const file = storage.file(fileName);

              const buffer = Buffer.from(imageBytes, 'base64');
              await file.save(buffer, {
                metadata: {
                  contentType: 'image/png',
                  metadata: {
                    userId: uid,
                    prompt: prompt.substring(0, 200),
                    model: dalleModelId,
                    size: dalleSize,
                    revisedPrompt: imageData.revised_prompt || ''
                  }
                }
              });

              // Make file publicly accessible [RESTORED-FIX-2025-12-01]
              await file.makePublic();
              const publicUrl = `https://storage.googleapis.com/${storage.name}/${fileName}`;

              generatedImages.push({
                url: publicUrl,
                fileName: fileName,
                seed: Math.floor(Math.random() * 1000000),
                revisedPrompt: imageData.revised_prompt || null
              });
            }
          }
        } catch (dalleError) {
          console.error(`DALL-E generation error (request ${req + 1}):`, dalleError);
          if (requestsNeeded === 1) {
            // If single request fails, throw error
            throw new functions.https.HttpsError('internal',
              `DALL-E generation failed: ${dalleError.message || 'Unknown error'}`);
          }
          // For multiple requests, continue with remaining
        }
      }

      usedModel = dalleModelId;

    } else if (isGeminiImageModel) {
      // ==========================================
      // Gemini Image Generation (Google AI Studio)
      // Uses generateContent API with image output
      // Supports: gemini-3-pro-image-preview, gemini-2.5-flash-image
      // ==========================================
      const geminiApiKey = functions.config().gemini?.key;
      if (!geminiApiKey) {
        console.error('Gemini API key not configured');
        throw new functions.https.HttpsError('failed-precondition',
          'Image generation service not configured. Please contact support.');
      }

      const ai = new GoogleGenAI({ apiKey: geminiApiKey });

      console.log(`Generating image with Gemini model: ${geminiImageModelId}, aspect ratio: ${validAspectRatio}`);
      console.log(`Prompt length: ${finalPrompt.length} chars, template: ${templateId || 'none'}, images: ${imageCount}`);

      // Build the content parts for the request
      const contentParts = [];

      // Add reference images if provided (Gemini supports multimodal input)
      if (styleReference && styleReference.base64) {
        contentParts.push({
          inlineData: {
            mimeType: styleReference.mimeType || 'image/png',
            data: styleReference.base64
          }
        });
        console.log('Adding style reference image as input');
      }

      if (characterReference && characterReference.base64) {
        contentParts.push({
          inlineData: {
            mimeType: characterReference.mimeType || 'image/png',
            data: characterReference.base64
          }
        });
        console.log('Adding character reference image as input');
      }

      // Build the prompt with reference instructions if needed
      let imagePrompt = finalPrompt;
      if (styleReference && styleReference.base64) {
        imagePrompt = `Using the provided image as a style reference, generate a new image with the following description: ${finalPrompt}`;
      }
      if (characterReference && characterReference.base64) {
        imagePrompt = `Using the provided image as a character/face reference to maintain consistency, generate a new image: ${finalPrompt}`;
      }
      if (styleReference && characterReference) {
        imagePrompt = `Using the first image as style reference and the second image as character reference, generate: ${finalPrompt}`;
      }

      // Add negative prompt instruction if provided
      if (negativePrompt && negativePrompt.trim()) {
        imagePrompt += `\n\nIMPORTANT: Avoid the following in the image: ${negativePrompt.trim()}`;
      }

      // Add the text prompt
      contentParts.push({ text: imagePrompt });

      try {
        // Gemini Image Generation using @google/genai SDK
        // Uses ai.models.generateContent() with responseModalities for image output
        // Reference: https://ai.google.dev/gemini-api/docs/image-generation

        // Generate images (Gemini generates one at a time)
        // Retry mechanism for transient failures (up to 2 retries per image)
        const MAX_RETRIES = 2;
        const lastErrors = [];

        for (let imgIdx = 0; imgIdx < imageCount; imgIdx++) {
          let success = false;

          for (let retry = 0; retry <= MAX_RETRIES && !success; retry++) {
            try {
              if (retry > 0) {
                console.log(`[generateCreativeImage] Retry ${retry}/${MAX_RETRIES} for image ${imgIdx + 1}`);
                // Exponential backoff: 2s, 4s
                await new Promise(resolve => setTimeout(resolve, retry * 2000));
              }

              // Aspect ratio handling for Gemini
              // Note: The @google/genai SDK's imageConfig may not work with all model versions
              // So we embed aspect ratio in the prompt AND try imageConfig as fallback
              const geminiAspectRatio = validAspectRatio || '16:9';

              // Enhance the prompt with aspect ratio instruction and quality hints
              const aspectRatioInstruction = `[Generate image in ${geminiAspectRatio} aspect ratio format. High quality, detailed, sharp focus.]\n\n`;
              const enhancedParts = contentParts.map((part, idx) => {
                if (idx === 0 && part.text) {
                  return { text: aspectRatioInstruction + part.text };
                }
                return part;
              });

              // Determine output resolution - gemini-3-pro-image-preview supports 2K and 4K
              const isProModel = geminiImageModelId.includes('gemini-3-pro');
              const outputImageSize = isProModel ? '2K' : undefined;

              console.log(`[generateCreativeImage] Calling Gemini model: ${geminiImageModelId}, image ${imgIdx + 1}/${imageCount}, image_size: ${outputImageSize || 'default'}`);
              const startTime = Date.now();

              const result = await ai.models.generateContent({
                model: geminiImageModelId,
                contents: [{ role: 'user', parts: enhancedParts }],
                config: {
                  responseModalities: ['IMAGE', 'TEXT'],
                  imageConfig: {
                    aspectRatio: geminiAspectRatio,
                    ...(outputImageSize && { image_size: outputImageSize })  // HD resolution
                  }
                }
              });

              console.log(`[generateCreativeImage] Gemini response received in ${Date.now() - startTime}ms`);

              // Extract image from response - handle both SDK response structures
              const candidates = result.candidates || (result.response && result.response.candidates);
              if (candidates && candidates.length > 0) {
                const candidate = candidates[0];
                const parts = candidate.content?.parts || candidate.parts || [];
                for (const part of parts) {
                  const inlineData = part.inlineData || part.inline_data;
                  if (inlineData && (inlineData.data || inlineData.bytesBase64Encoded)) {
                    // Found an image
                    const imageBytes = inlineData.data || inlineData.bytesBase64Encoded;
                    const rawBuffer = Buffer.from(imageBytes, 'base64');

                    // Log original image size for debugging quality issues
                    console.log(`[generateCreativeImage] Raw image buffer size: ${rawBuffer.length} bytes`);

                    // Enforce exact dimensions based on aspect ratio
                    // This ensures consistent 1920x1080 for 16:9, etc.
                    const { buffer: processedBuffer, width: targetWidth, height: targetHeight, mimeType: processedMimeType } =
                      await enforceImageDimensions(rawBuffer, geminiAspectRatio, 'hd');

                    console.log(`[generateCreativeImage] Processed image size: ${processedBuffer.length} bytes, dimensions: ${targetWidth}x${targetHeight}`);

                    const fileName = `creative-studio/${uid}/${timestamp}-gemini-${imgIdx + 1}.png`;
                    const file = storage.file(fileName);

                    await file.save(processedBuffer, {
                      metadata: {
                        contentType: processedMimeType,
                        metadata: {
                          prompt: finalPrompt.substring(0, 500),
                          model: geminiImageModelId,
                          aspectRatio: geminiAspectRatio,
                          width: String(targetWidth),
                          height: String(targetHeight),
                          generatedAt: new Date().toISOString()
                        }
                      }
                    });

                    // Make file publicly accessible [RESTORED-FIX-2025-12-01]
                    await file.makePublic();
                    const publicUrl = `https://storage.googleapis.com/${storage.name}/${fileName}`;

                    generatedImages.push({
                      url: publicUrl,
                      fileName: fileName,
                      seed: Math.floor(Math.random() * 1000000),
                      aspectRatio: geminiAspectRatio,
                      width: targetWidth,
                      height: targetHeight
                    });

                    console.log(`[generateCreativeImage] Image ${imgIdx + 1} saved: ${fileName} (${targetWidth}x${targetHeight}, ${geminiAspectRatio})`);
                    success = true; // Mark success to exit retry loop
                    break; // Only take first image from this response
                  }
                }
              } else {
                console.warn(`[generateCreativeImage] No candidates in Gemini response for image ${imgIdx + 1}`);
              }
            } catch (genError) {
              const errorMsg = genError.message || 'Unknown error';
              console.error(`[generateCreativeImage] Gemini error for image ${imgIdx + 1}, retry ${retry}:`, errorMsg);
              lastErrors.push(errorMsg);
              // Continue to next retry
            }
          } // End retry loop
        } // End image count loop

        if (generatedImages.length === 0) {
          const errorDetails = lastErrors.length > 0 ? ` Last error: ${lastErrors[lastErrors.length - 1]}` : '';
          throw new functions.https.HttpsError('internal',
            `Gemini did not generate any images. The content may have been filtered or the model encountered an error.${errorDetails}`);
        }

        usedModel = geminiImageModelId;

      } catch (geminiError) {
        console.error('Gemini image generation error:', geminiError);
        throw new functions.https.HttpsError('internal',
          `Gemini generation failed: ${geminiError.message || 'Unknown error'}`);
      }

    } else {
      // ==========================================
      // Legacy Imagen Image Generation (Google)
      // Uses ai.models.generateImages API
      // ==========================================
      const geminiApiKey = functions.config().gemini?.key;
      if (!geminiApiKey) {
        console.error('Gemini API key not configured');
        throw new functions.https.HttpsError('failed-precondition',
          'Image generation service not configured. Please contact support.');
      }

      const ai = new GoogleGenAI({ apiKey: geminiApiKey });

      console.log(`Generating ${imageCount} image(s) with Imagen model: ${imagenModelId}`);
      console.log(`Prompt length: ${finalPrompt.length} chars, template: ${templateId || 'none'}`);

      // Build config object with optional parameters
      const imagenConfig = {
        numberOfImages: imageCount,
        aspectRatio: validAspectRatio,
        includeRaiReason: true,
        personGeneration: 'allow_adult'
      };

      // Add negative prompt if provided (Imagen supports this)
      if (negativePrompt && negativePrompt.trim()) {
        imagenConfig.negativePrompt = negativePrompt.trim();
        console.log(`Using negative prompt: ${negativePrompt.substring(0, 50)}...`);
      }

      // Add seed if provided (for reproducible results)
      if (seed !== undefined && seed !== null && !isNaN(seed)) {
        imagenConfig.seed = parseInt(seed, 10);
        console.log(`Using seed: ${seed}`);
      }

      // Add reference images if provided (Imagen 3 only)
      // Reference images support style transfer and subject consistency
      if (imagenModelId.includes('imagen-3') && (styleReference || characterReference)) {
        const referenceImages = [];

        // Add style reference
        if (styleReference) {
          if (styleReference.base64) {
            referenceImages.push({
              referenceType: 'STYLE',
              referenceImage: {
                bytesBase64Encoded: styleReference.base64
              }
            });
            console.log('Adding style reference image');
          } else if (styleReference.url) {
            // For URL-based references, we'd need to fetch and convert to base64
            console.log('Style reference URL provided - URL-based references not yet supported');
          }
        }

        // Add character/subject reference
        if (characterReference) {
          if (characterReference.base64) {
            referenceImages.push({
              referenceType: 'SUBJECT',
              referenceImage: {
                bytesBase64Encoded: characterReference.base64
              }
            });
            console.log('Adding character/subject reference image');
          } else if (characterReference.url) {
            console.log('Character reference URL provided - URL-based references not yet supported');
          }
        }

        if (referenceImages.length > 0) {
          imagenConfig.referenceImages = referenceImages;
          console.log(`Using ${referenceImages.length} reference image(s)`);
        }
      } else if ((styleReference || characterReference) && !imagenModelId.includes('imagen-3')) {
        console.log('Reference images only supported with Imagen 3 - ignoring references');
      }

      const response = await ai.models.generateImages({
        model: imagenModelId,
        prompt: finalPrompt,
        config: imagenConfig
      });

      if (response.generatedImages && response.generatedImages.length > 0) {
        for (let i = 0; i < response.generatedImages.length; i++) {
          const genImage = response.generatedImages[i];

          if (genImage.raiFilteredReason) {
            console.warn(`Image ${i + 1} filtered: ${genImage.raiFilteredReason}`);
            continue;
          }

          const imageBytes = genImage.image?.imageBytes;
          if (!imageBytes) continue;

          const rawBuffer = Buffer.from(imageBytes, 'base64');

          // Enforce exact dimensions based on aspect ratio
          const { buffer: processedBuffer, width: targetWidth, height: targetHeight, mimeType: processedMimeType } =
            await enforceImageDimensions(rawBuffer, validAspectRatio, 'hd');

          const fileName = `creative-studio/${uid}/${timestamp}-${i + 1}.png`;
          const file = storage.file(fileName);

          await file.save(processedBuffer, {
            metadata: {
              contentType: processedMimeType,
              metadata: {
                userId: uid,
                prompt: prompt.substring(0, 200),
                model: imagenModelId,
                aspectRatio: validAspectRatio,
                width: String(targetWidth),
                height: String(targetHeight)
              }
            }
          });

          // Make file publicly accessible [RESTORED-FIX-2025-12-01]
          await file.makePublic();
          const publicUrl = `https://storage.googleapis.com/${storage.name}/${fileName}`;

          generatedImages.push({
            url: publicUrl,
            fileName: fileName,
            seed: Math.floor(Math.random() * 1000000),
            aspectRatio: validAspectRatio,
            width: targetWidth,
            height: targetHeight
          });

          console.log(`Imagen image ${i + 1} saved: ${fileName} (${targetWidth}x${targetHeight}, ${validAspectRatio})`);
        }
      }

      usedModel = imagenModelId;
    }

    // Check if any images were generated
    if (generatedImages.length === 0) {
      throw new functions.https.HttpsError('internal',
        'No images generated. The prompt may have been filtered for safety. Try a different prompt.');
    }

    // Deduct tokens (only charge for successfully generated images)
    const actualCost = baseCost * generatedImages.length;
    await db.collection('creativeTokens').doc(uid).update({
      balance: admin.firestore.FieldValue.increment(-actualCost),
      lastUsed: admin.firestore.FieldValue.serverTimestamp()
    });
    const historyData = {
      userId: uid,
      prompt: prompt,
      model: usedModel,
      quantity: generatedImages.length,
      aspectRatio: validAspectRatio,
      quality: quality || 'hd',
      templateId: templateId || null,
      images: generatedImages,
      tokenCost: actualCost,
      createdAt: admin.firestore.FieldValue.serverTimestamp()
    };

    const historyRef = await db.collection('creativeHistory').add(historyData);

    // Log usage
    await logUsage(uid, 'creative_image', {
      prompt: prompt.substring(0, 100),
      quality,
      quantity: generatedImages.length,
      model: usedModel
    });

    // AUTO-SHARE FOR FREE USERS
    // Free users' images are automatically shared to the community gallery
    // Premium users can choose to share or keep private
    let autoSharedToGallery = false;
    let galleryId = null;

    try {
      const tokenDoc = await db.collection('creativeTokens').doc(uid).get();
      const userPlan = tokenDoc.exists ? (tokenDoc.data().plan || 'free') : 'free';
      const isPremium = ['lite', 'pro', 'business', 'enterprise'].includes(userPlan);

      if (!isPremium) {
        // Free user - auto-share to community gallery
        const userDoc = await db.collection('users').doc(uid).get();
        const userData = userDoc.exists ? userDoc.data() : {};

        const galleryData = {
          userId: uid,
          userName: userData.displayName || 'Anonymous',
          userAvatar: (userData.displayName || 'A').substring(0, 2).toUpperCase(),
          historyId: historyRef.id,
          imageUrl: generatedImages[0]?.url || '',
          prompt: prompt,
          isPrivate: false, // Free users can't have private prompts
          promptPrice: 0,
          tool: 'imageCreation',
          likes: 0,
          views: 0,
          autoShared: true, // Flag indicating this was auto-shared
          createdAt: admin.firestore.FieldValue.serverTimestamp()
        };

        const galleryRef = await db.collection('creativeGallery').add(galleryData);
        galleryId = galleryRef.id;
        autoSharedToGallery = true;

        // Update history to mark as shared
        await db.collection('creativeHistory').doc(historyRef.id).update({
          sharedToGallery: true,
          galleryId: galleryRef.id,
          autoShared: true
        });

        console.log(`Auto-shared image to gallery for free user ${uid}, galleryId: ${galleryRef.id}`);
      }
    } catch (autoShareError) {
      // Don't fail the whole generation if auto-share fails
      console.error('Auto-share to gallery failed:', autoShareError);
    }

    return {
      success: true,
      historyId: historyRef.id,
      images: generatedImages,
      tokenCost: actualCost,
      remainingBalance: balance - actualCost,
      autoSharedToGallery,
      galleryId
    };

  } catch (error) {
    console.error('Generate image error:', error);
    console.error('Error details:', JSON.stringify({
      message: error.message,
      code: error.code,
      status: error.status,
      statusCode: error.statusCode,
      details: error.details,
      name: error.name,
      stack: error.stack?.substring(0, 500)
    }));

    const errorMsg = error.message || '';
    const errorStr = JSON.stringify(error).toLowerCase();

    // Handle specific Gemini API errors with clear instructions
    if (errorMsg.includes('API key') || errorMsg.includes('API_KEY') || errorMsg.includes('invalid key') || errorMsg.includes('Invalid API key')) {
      throw new functions.https.HttpsError('failed-precondition',
        'Invalid API Key. Please ensure you are using an API key from Google AI Studio (aistudio.google.com/apikey), NOT from Google Cloud Console.');
    }

    // Model not found or not available
    if (errorMsg.includes('not found') || errorMsg.includes('404') || (errorMsg.includes('model') && errorMsg.includes('available'))) {
      throw new functions.https.HttpsError('failed-precondition',
        'Imagen model not accessible. Please ensure your API key is from Google AI Studio (aistudio.google.com/apikey) and has billing enabled.');
    }

    // API not enabled or permission issues
    if (errorMsg.includes('permission') || errorMsg.includes('403') || errorMsg.includes('denied') ||
        errorMsg.includes('enable') || errorMsg.includes('PERMISSION_DENIED')) {
      throw new functions.https.HttpsError('permission-denied',
        'API access denied. Imagen requires an API key from Google AI Studio with billing enabled. Go to aistudio.google.com/apikey to create the correct key type.');
    }

    // Rate limiting or quota
    if (errorMsg.includes('quota') || errorMsg.includes('rate') || errorMsg.includes('429') || errorMsg.includes('RESOURCE_EXHAUSTED')) {
      throw new functions.https.HttpsError('resource-exhausted',
        'Service temporarily busy. Please try again in a moment.');
    }

    // Safety filtering
    if (errorMsg.includes('safety') || errorMsg.includes('blocked') || errorMsg.includes('SAFETY')) {
      throw new functions.https.HttpsError('invalid-argument',
        'Your prompt was blocked for safety reasons. Please try a different prompt.');
    }

    // Invalid request format
    if (errorMsg.includes('invalid') || errorMsg.includes('INVALID_ARGUMENT')) {
      throw new functions.https.HttpsError('invalid-argument',
        'Invalid image generation request. Please check your prompt and settings.');
    }

    // Billing not enabled
    if (errorMsg.includes('billing') || errorStr.includes('billing')) {
      throw new functions.https.HttpsError('failed-precondition',
        'Billing is not enabled for this Google Cloud project. Image generation requires an active billing account.');
    }

    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal',
      sanitizeErrorMessage(error, 'Failed to generate image. Please try again.'));
  }
});

// Get user's creative history
exports.getCreativeHistory = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const { limit: queryLimit, offset } = data;

  // Safe timestamp handler - handles various Firestore timestamp formats
  const getTimestamp = (field) => {
    if (!field) return Date.now();
    if (typeof field === 'number') return field;
    if (typeof field.toMillis === 'function') return field.toMillis();
    if (field._seconds) return field._seconds * 1000;
    if (field instanceof Date) return field.getTime();
    return Date.now();
  };

  try {
    let query = db.collection('creativeHistory')
      .where('userId', '==', uid)
      .orderBy('createdAt', 'desc')
      .limit(queryLimit || 50);

    if (offset) {
      const lastDoc = await db.collection('creativeHistory').doc(offset).get();
      if (lastDoc.exists) {
        query = query.startAfter(lastDoc);
      }
    }

    const snapshot = await query.get();
    const history = [];

    snapshot.forEach(doc => {
      const docData = doc.data();
      const timestamp = getTimestamp(docData.createdAt);
      // Create clean object without raw createdAt (non-serializable)
      const { createdAt: rawCreatedAt, ...rest } = docData;
      history.push({
        id: doc.id,
        ...rest,
        timestamp,
        createdAt: new Date(timestamp).toISOString()
      });
    });

    return { success: true, history };

  } catch (error) {
    console.error('Get history error:', error);
    throw new functions.https.HttpsError('internal', 'Failed to get history');
  }
});

// Enhance prompt using AI
exports.enhanceCreativePrompt = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  checkRateLimit(uid, 'enhancePrompt', 20); // More lenient rate limit for quick operation

  const { prompt, style } = data;

  if (!prompt || prompt.trim().length === 0) {
    throw new functions.https.HttpsError('invalid-argument', 'Prompt is required');
  }

  if (prompt.length > 500) {
    throw new functions.https.HttpsError('invalid-argument', 'Prompt too long. Maximum 500 characters for enhancement.');
  }

  try {
    const geminiApiKey = functions.config().gemini?.key;
    if (!geminiApiKey) {
      throw new functions.https.HttpsError('failed-precondition', 'AI service not configured');
    }

    const ai = new GoogleGenAI({ apiKey: geminiApiKey });

    // Style-specific enhancements
    const styleInstructions = {
      'photorealistic': 'Make it suitable for photorealistic image generation with natural lighting and realistic details.',
      'artistic': 'Make it suitable for artistic/creative image generation with expressive and stylized elements.',
      'cinematic': 'Make it suitable for cinematic image generation with dramatic lighting and movie-like composition.',
      'anime': 'Make it suitable for anime/manga style image generation with appropriate visual elements.',
      'fantasy': 'Make it suitable for fantasy art generation with magical and imaginative elements.',
      'default': 'Make it suitable for high-quality AI image generation.'
    };

    const styleGuide = styleInstructions[style] || styleInstructions['default'];

    const systemPrompt = `You are an expert AI image prompt engineer. Your task is to enhance user prompts to get better results from AI image generators like Imagen and DALL-E.

Rules:
1. Keep the core concept and intent of the original prompt
2. Add specific details about composition, lighting, colors, and style
3. Include technical photography/art terms where appropriate
4. ${styleGuide}
5. Keep the enhanced prompt under 200 words
6. Don't add controversial or inappropriate content
7. Output ONLY the enhanced prompt, no explanations or formatting

Original prompt: "${prompt}"

Enhanced prompt:`;

    // Use correct SDK method: ai.models.generateContent() with proper contents format
    const result = await ai.models.generateContent({
      model: 'gemini-2.0-flash',
      contents: [{
        role: 'user',
        parts: [{ text: systemPrompt }]
      }]
    });

    // Handle response - try multiple formats
    let enhancedPrompt = '';
    if (result.text) {
      enhancedPrompt = result.text.trim();
    } else if (result.response && typeof result.response.text === 'function') {
      enhancedPrompt = result.response.text().trim();
    } else if (result.candidates && result.candidates[0]?.content?.parts?.[0]?.text) {
      enhancedPrompt = result.candidates[0].content.parts[0].text.trim();
    } else {
      console.warn('Could not extract text from Gemini response');
      return { success: true, enhancedPrompt: prompt, wasEnhanced: false };
    }

    // Clean up the response (remove quotes if present)
    let cleanedPrompt = enhancedPrompt
      .replace(/^["']|["']$/g, '') // Remove surrounding quotes
      .replace(/^Enhanced prompt:\s*/i, '') // Remove prefix if present
      .trim();

    // Validate the response
    if (cleanedPrompt.length < 10 || cleanedPrompt.length > 2000) {
      console.warn('Enhanced prompt invalid length, returning original');
      return { success: true, enhancedPrompt: prompt, wasEnhanced: false };
    }

    console.log(`Prompt enhanced: ${prompt.substring(0, 50)}... -> ${cleanedPrompt.substring(0, 50)}...`);

    return {
      success: true,
      enhancedPrompt: cleanedPrompt,
      wasEnhanced: true,
      originalLength: prompt.length,
      enhancedLength: cleanedPrompt.length
    };

  } catch (error) {
    console.error('Enhance prompt error:', error);
    // Return original prompt on error instead of failing
    return {
      success: true,
      enhancedPrompt: prompt,
      wasEnhanced: false,
      error: error.message
    };
  }
});

// Delete image from creative history
exports.deleteCreativeHistory = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const { historyId } = data;

  if (!historyId) {
    throw new functions.https.HttpsError('invalid-argument', 'History ID is required');
  }

  try {
    // Get the history item to verify ownership and get file info
    const historyDoc = await db.collection('creativeHistory').doc(historyId).get();

    if (!historyDoc.exists) {
      throw new functions.https.HttpsError('not-found', 'Image not found');
    }

    const historyData = historyDoc.data();

    // Verify ownership
    if (historyData.userId !== uid) {
      throw new functions.https.HttpsError('permission-denied', 'Not your image');
    }

    // Delete from Firebase Storage (if images exist)
    const storage = admin.storage().bucket();
    if (historyData.images && historyData.images.length > 0) {
      for (const image of historyData.images) {
        if (image.fileName) {
          try {
            await storage.file(image.fileName).delete();
            console.log(`Deleted storage file: ${image.fileName}`);
          } catch (storageError) {
            // File might not exist, continue anyway
            console.warn(`Could not delete file ${image.fileName}:`, storageError.message);
          }
        }
      }
    }

    // If shared to gallery, also delete from gallery
    if (historyData.sharedToGallery && historyData.galleryId) {
      try {
        await db.collection('creativeGallery').doc(historyData.galleryId).delete();
        console.log(`Deleted gallery entry: ${historyData.galleryId}`);
      } catch (galleryError) {
        console.warn(`Could not delete gallery entry:`, galleryError.message);
      }
    }

    // Delete the history document
    await db.collection('creativeHistory').doc(historyId).delete();

    console.log(`Deleted creative history ${historyId} for user ${uid}`);

    return { success: true, message: 'Image deleted successfully' };

  } catch (error) {
    console.error('Delete history error:', error);
    if (error.code) {
      throw error; // Re-throw HttpsErrors
    }
    throw new functions.https.HttpsError('internal', 'Failed to delete image');
  }
});

// Share image to community gallery
exports.shareToGallery = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const { historyId, makePrivate, promptPrice } = data;

  if (!historyId) {
    throw new functions.https.HttpsError('invalid-argument', 'History ID is required');
  }

  try {
    // Get the history item
    const historyDoc = await db.collection('creativeHistory').doc(historyId).get();

    if (!historyDoc.exists) {
      throw new functions.https.HttpsError('not-found', 'Image not found');
    }

    const historyData = historyDoc.data();

    if (historyData.userId !== uid) {
      throw new functions.https.HttpsError('permission-denied', 'Not your image');
    }

    // Check if user can make prompt private (needs paid subscription)
    let canMakePrivate = false;
    if (makePrivate) {
      const tokenDoc = await db.collection('creativeTokens').doc(uid).get();
      const plan = tokenDoc.exists ? tokenDoc.data().plan : 'free';
      canMakePrivate = ['lite', 'pro', 'business'].includes(plan);

      if (!canMakePrivate) {
        throw new functions.https.HttpsError('permission-denied',
          'Only paid subscribers can make prompts private');
      }
    }

    // Get user profile for display name
    const userDoc = await db.collection('users').doc(uid).get();
    const userData = userDoc.exists ? userDoc.data() : {};

    // Create gallery entry
    const galleryData = {
      userId: uid,
      userName: userData.displayName || 'Anonymous',
      userAvatar: (userData.displayName || 'A').substring(0, 2).toUpperCase(),
      historyId: historyId,
      imageUrl: historyData.images?.[0]?.url || '',
      prompt: historyData.prompt,
      isPrivate: makePrivate && canMakePrivate,
      promptPrice: (makePrivate && canMakePrivate) ? (promptPrice || 5) : 0,
      tool: 'imageCreation',
      likes: 0,
      views: 0,
      createdAt: admin.firestore.FieldValue.serverTimestamp()
    };

    const galleryRef = await db.collection('creativeGallery').add(galleryData);

    // Update history to mark as shared
    await db.collection('creativeHistory').doc(historyId).update({
      sharedToGallery: true,
      galleryId: galleryRef.id
    });

    return {
      success: true,
      galleryId: galleryRef.id,
      isPrivate: galleryData.isPrivate
    };

  } catch (error) {
    console.error('Share to gallery error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', 'Failed to share to gallery');
  }
});

// Purchase private prompt from another user
exports.purchasePrompt = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const { galleryId } = data;

  if (!galleryId) {
    throw new functions.https.HttpsError('invalid-argument', 'Gallery ID is required');
  }

  try {
    return await db.runTransaction(async (transaction) => {
      // Get gallery item
      const galleryRef = db.collection('creativeGallery').doc(galleryId);
      const galleryDoc = await transaction.get(galleryRef);

      if (!galleryDoc.exists) {
        throw new functions.https.HttpsError('not-found', 'Gallery item not found');
      }

      const galleryData = galleryDoc.data();

      if (!galleryData.isPrivate) {
        // Prompt is free, just return it
        return { success: true, prompt: galleryData.prompt, cost: 0 };
      }

      if (galleryData.userId === uid) {
        // User owns this prompt
        return { success: true, prompt: galleryData.prompt, cost: 0 };
      }

      const price = galleryData.promptPrice || 5;

      // Check buyer's balance
      const buyerTokenRef = db.collection('creativeTokens').doc(uid);
      const buyerTokenDoc = await transaction.get(buyerTokenRef);
      const buyerBalance = buyerTokenDoc.exists ? buyerTokenDoc.data().balance : 0;

      if (buyerBalance < price) {
        throw new functions.https.HttpsError('resource-exhausted',
          `Insufficient tokens. Need ${price}, have ${buyerBalance}`);
      }

      // Check if already purchased
      const purchaseQuery = await db.collection('promptPurchases')
        .where('buyerId', '==', uid)
        .where('galleryId', '==', galleryId)
        .limit(1)
        .get();

      if (!purchaseQuery.empty) {
        // Already purchased, return prompt
        return { success: true, prompt: galleryData.prompt, cost: 0, alreadyPurchased: true };
      }

      // Deduct from buyer
      transaction.update(buyerTokenRef, {
        balance: admin.firestore.FieldValue.increment(-price)
      });

      // Add to seller (creator)
      const sellerTokenRef = db.collection('creativeTokens').doc(galleryData.userId);
      transaction.update(sellerTokenRef, {
        balance: admin.firestore.FieldValue.increment(price),
        earnings: admin.firestore.FieldValue.increment(price)
      });

      // Record purchase
      const purchaseRef = db.collection('promptPurchases').doc();
      transaction.set(purchaseRef, {
        buyerId: uid,
        sellerId: galleryData.userId,
        galleryId: galleryId,
        price: price,
        purchasedAt: admin.firestore.FieldValue.serverTimestamp()
      });

      return {
        success: true,
        prompt: galleryData.prompt,
        cost: price
      };
    });

  } catch (error) {
    console.error('Purchase prompt error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', 'Failed to purchase prompt');
  }
});

// Get community gallery
exports.getCommunityGallery = functions.https.onCall(async (data, context) => {
  // This can be called without authentication for browsing
  const { sortBy, filter, category, limit: queryLimit, offset } = data || {};

  // Safe timestamp handler - handles various Firestore timestamp formats
  const getTimestamp = (field) => {
    if (!field) return Date.now();
    if (typeof field === 'number') return field;
    if (typeof field.toMillis === 'function') return field.toMillis();
    if (field._seconds) return field._seconds * 1000;
    if (field instanceof Date) return field.getTime();
    return Date.now();
  };

  try {
    let query = db.collection('creativeGallery')
      .orderBy(sortBy === 'newest' ? 'createdAt' : 'likes', 'desc')
      .limit(queryLimit || 50);

    if (filter && filter !== 'all') {
      query = query.where('tool', '==', filter);
    }

    const snapshot = await query.get();
    const items = [];

    snapshot.forEach(doc => {
      const docData = doc.data();
      const timestamp = getTimestamp(docData.createdAt);
      items.push({
        id: doc.id,
        imageUrl: docData.imageUrl,
        prompt: docData.isPrivate ? '[Private Prompt]' : docData.prompt,
        isPrivate: docData.isPrivate,
        promptPrice: docData.promptPrice || 0,
        user: {
          name: docData.userName,
          avatar: docData.userAvatar
        },
        likes: docData.likes || 0,
        tool: docData.tool,
        createdAt: new Date(timestamp).toISOString()
      });
    });

    return { success: true, items };

  } catch (error) {
    console.error('Get gallery error:', error);
    throw new functions.https.HttpsError('internal', 'Failed to get gallery');
  }
});

// Like a gallery item
exports.likeGalleryItem = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const { galleryId } = data;

  if (!galleryId) {
    throw new functions.https.HttpsError('invalid-argument', 'Gallery ID is required');
  }

  try {
    // Check if already liked
    const likeId = `${uid}_${galleryId}`;
    const likeDoc = await db.collection('galleryLikes').doc(likeId).get();

    if (likeDoc.exists) {
      // Unlike
      await db.collection('galleryLikes').doc(likeId).delete();
      await db.collection('creativeGallery').doc(galleryId).update({
        likes: admin.firestore.FieldValue.increment(-1)
      });
      return { success: true, liked: false };
    } else {
      // Like
      await db.collection('galleryLikes').doc(likeId).set({
        userId: uid,
        galleryId: galleryId,
        createdAt: admin.firestore.FieldValue.serverTimestamp()
      });
      await db.collection('creativeGallery').doc(galleryId).update({
        likes: admin.firestore.FieldValue.increment(1)
      });
      return { success: true, liked: true };
    }

  } catch (error) {
    console.error('Like gallery error:', error);
    throw new functions.https.HttpsError('internal', 'Failed to like item');
  }
});

// ==========================================
// SEED CREATIVE PROMPTS - Admin only
// Seeds 31 professional prompts from Newimagemoduls.md
// ==========================================
exports.seedCreativePrompts = functions.https.onCall(async (data, context) => {
  await requireAdmin(context);

  // Professional prompts organized by category
  const professionalPrompts = [
    // PHOTOREALISM & AESTHETICS (6 prompts)
    {
      id: 'business-photo',
      name: 'Professional Business Photo',
      category: 'photorealism',
      description: 'LinkedIn headshots and corporate portraits',
      tokenCost: 2,
      userPrompt: 'Create a professional business portrait of {{subject}}',
      professionalPrompt: 'Create a highly realistic professional headshot portrait suitable for LinkedIn or corporate use. The subject should be photographed from the chest up, with perfect lighting that eliminates harsh shadows. Use a neutral, slightly blurred office or studio background. The subject should have a confident, approachable expression with natural skin tones and textures. Professional attire appropriate for a business setting. Shot with the equivalent of an 85mm lens at f/2.8 for pleasing bokeh. Color grading should be clean and professional with accurate skin tones.',
      isActive: true
    },
    {
      id: 'film-style',
      name: 'Vintage Film Photography',
      category: 'photorealism',
      description: 'Cinematic shots with authentic film grain and classic camera aesthetics',
      tokenCost: 2,
      userPrompt: 'Create a {{film_era}} style photograph of {{subject}}',
      professionalPrompt: 'Generate an image with authentic vintage film photography aesthetics. Include natural film grain characteristic of high ISO film stock, slightly lifted blacks, and the distinctive color rendering of classic film emulsions like Kodak Portra 400 or Fuji Pro 400H. The image should have subtle light leaks at the edges, gentle vignetting, and the organic imperfections that make film photography distinctive. Composition should follow classic photography rules with attention to golden ratio and leading lines. Lighting should appear natural and uncontrived.',
      isActive: true
    },
    {
      id: 'mirror-selfie',
      name: '2000s Mirror Selfie',
      category: 'photorealism',
      description: 'Authentic early 2000s mirror selfie with detailed styling - JSON structured prompt',
      tokenCost: 2,
      userPrompt: 'Create a 2000s-style mirror selfie of {{subject}} in {{setting}}',
      professionalPrompt: `Create a 2000s Mirror Selfie using this detailed specification:

Subject: {{subject}} taking a mirror selfie. The subject should have the following characteristics:
- Age: young adult
- Expression: confident and slightly playful
- Hair: very long, voluminous waves with soft wispy bangs
- Clothing: fitted cropped t-shirt in cream white featuring a large cute anime-style cat face graphic
- Face: preserve original features, natural glam makeup with soft pink dewy blush and glossy red pouty lips

Accessories:
- Gold geometric hoop earrings
- Silver waistchain
- Smartphone with patterned case visible in hand

Photography Style:
- Camera style: early-2000s digital camera aesthetic
- Lighting: harsh super-flash with bright blown-out highlights but subject still visible
- Angle: mirror selfie
- Shot type: tight selfie composition
- Texture: subtle grain, retro highlights, V6 realism, crisp details, soft shadows

Background Setting: {{setting}}
- Nostalgic early-2000s bedroom atmosphere
- Pastel wall tones
- Period elements: chunky wooden dresser, CD player, posters of 2000s pop icons, hanging beaded door curtain, cluttered vanity with lip glosses
- Retro lighting creating authentic 2000s nostalgic vibe`,
      isActive: true
    },
    {
      id: '90s-portrait',
      name: '90s Yearbook Portrait',
      category: 'photorealism',
      description: 'Classic 90s school photo aesthetic with laser backgrounds',
      tokenCost: 2,
      userPrompt: 'Create a 90s yearbook photo of {{subject}}',
      professionalPrompt: 'Create a nostalgic 1990s school yearbook portrait with the iconic laser background or abstract geometric patterns in teal, purple, and pink gradients. The subject should be posed in the classic yearbook style with head slightly tilted, soft studio lighting, and that distinctive early 90s look. Include the characteristic softness and color palette of 90s portrait photography. Hair and styling should reflect the era appropriately.',
      isActive: true
    },
    {
      id: 'vs-fashion',
      name: 'High Fashion Editorial',
      category: 'photorealism',
      description: 'Victoria\'s Secret / high fashion runway aesthetic',
      tokenCost: 3,
      userPrompt: 'Create a high fashion editorial photo of {{subject}} in {{style}}',
      professionalPrompt: 'Generate a stunning high fashion editorial photograph worthy of Vogue or Elle magazine. Professional studio lighting with dramatic rim lights and soft fill. The composition should be dynamic and editorial in nature. Flawless skin with natural texture visible, professional makeup artistry evident. Background should be clean studio or artistically relevant setting. Color grading should be rich and magazine-quality. Capture movement and energy in the pose.',
      isActive: true
    },
    {
      id: 'crowd-composition',
      name: 'Crowd Composite Photo',
      category: 'photorealism',
      description: 'Same person appearing multiple times in one image',
      tokenCost: 3,
      userPrompt: 'Create a crowd scene where {{subject}} appears {{count}} times in different poses',
      professionalPrompt: 'Create a seamless composite photograph showing the same person appearing multiple times within a single image, each in a different position and pose. Ensure consistent lighting across all instances, matching shadows and highlights. Each appearance should have natural variation in pose, expression, and potentially clothing. The background environment should be coherent and realistic. Pay attention to scale consistency based on position in the scene.',
      isActive: true
    },

    // CREATIVE EXPERIMENTS (8 prompts)
    {
      id: 'wheres-waldo',
      name: 'Where\'s Waldo Scene',
      category: 'creative',
      description: 'Detailed crowded scene with hidden subject to find',
      tokenCost: 3,
      userPrompt: 'Create a Where\'s Waldo style scene with {{subject}} hidden among {{setting}}',
      professionalPrompt: 'Generate a highly detailed, densely packed illustration in the iconic Where\'s Waldo (Where\'s Wally) style. Create a busy, chaotic scene filled with hundreds of tiny characters engaged in various activities. Include the target subject cleverly hidden among the crowd, wearing distinctive clothing. The scene should be colorful, whimsical, and filled with visual gags, sight jokes, and interesting details that reward careful observation. Use a bird\'s eye or slightly elevated perspective to show maximum activity.',
      isActive: true
    },
    {
      id: 'aging-effect',
      name: 'Age Progression',
      category: 'creative',
      description: 'Show how a person would look at different ages',
      tokenCost: 3,
      userPrompt: 'Show {{subject}} at age {{age}}',
      professionalPrompt: 'Create a photorealistic age progression image showing how the subject would appear at the specified age. Consider natural aging processes: changes in skin elasticity, hair color and density, facial fat distribution, and bone structure changes. Maintain the subject\'s core identifying features while applying age-appropriate modifications. For older ages, include natural wrinkles, age spots, and changes in skin texture. For younger ages, smooth features and adjust proportions appropriately.',
      isActive: true
    },
    {
      id: 'recursive-image',
      name: 'Droste Effect',
      category: 'creative',
      description: 'Recursive image within image effect',
      tokenCost: 3,
      userPrompt: 'Create a recursive image of {{subject}} holding a picture of themselves',
      professionalPrompt: 'Generate a Droste effect / recursive image where the subject holds or displays a picture of themselves holding the same picture, creating an infinite recursive loop. The recursion should be visible for at least 4-5 iterations, each getting progressively smaller while maintaining detail. The lighting and perspective should be consistent across all recursive levels. The effect should feel natural and seamlessly integrated.',
      isActive: true
    },
    {
      id: 'glitch-art',
      name: 'Digital Glitch Art',
      category: 'creative',
      description: 'Artistic digital corruption and databending effects',
      tokenCost: 2,
      userPrompt: 'Create glitch art of {{subject}} with {{intensity}} distortion',
      professionalPrompt: 'Create a striking digital glitch art image with artistic data corruption effects. Include horizontal scan line displacement, RGB channel separation, pixel sorting regions, and compression artifact aesthetics. The glitch effects should be visually interesting and intentional-looking rather than randomly destructive. Balance between recognizable subject matter and abstract corruption. Use the glitch elements to create visual rhythm and draw attention to key areas.',
      isActive: true
    },
    {
      id: 'miniature-world',
      name: 'Tilt-Shift Miniature',
      category: 'creative',
      description: 'Real scenes that look like tiny models',
      tokenCost: 2,
      userPrompt: 'Create a tilt-shift miniature effect of {{scene}}',
      professionalPrompt: 'Generate an image with convincing tilt-shift miniature/fake miniature effect that makes a real-world scene appear to be a tiny scale model or diorama. Apply selective focus with a very narrow depth of field band. Increase color saturation and contrast slightly to enhance the toy-like appearance. The viewing angle should ideally be from above. Subjects in the scene should have the slightly static quality of miniature figures.',
      isActive: true
    },
    {
      id: 'impossible-geometry',
      name: 'Impossible Architecture',
      category: 'creative',
      description: 'Escher-style impossible geometric structures',
      tokenCost: 3,
      userPrompt: 'Create an impossible {{structure}} in the style of {{artist}}',
      professionalPrompt: 'Create a visually convincing impossible object or architecture inspired by M.C. Escher\'s impossible constructions. The structure should appear physically plausible at first glance but contain geometric paradoxes upon closer inspection - such as impossible staircases, paradoxical perspectives, or gravity-defying architecture. Use clean lines and professional architectural rendering style. The lighting should be consistent even when the geometry is not.',
      isActive: true
    },
    {
      id: 'style-fusion',
      name: 'Art Style Fusion',
      category: 'creative',
      description: 'Combine two distinct art styles into one image',
      tokenCost: 3,
      userPrompt: 'Create {{subject}} combining {{style1}} and {{style2}} art styles',
      professionalPrompt: 'Generate an artwork that seamlessly blends two distinct artistic styles into a cohesive composition. The fusion should feel intentional and harmonious rather than jarring. Find common elements between the styles that can serve as bridges. The subject matter should be rendered in a way that honors both influences equally. Consider how color palettes, brushwork techniques, and compositional approaches from each style can complement each other.',
      isActive: true
    },
    {
      id: 'coordinates-art',
      name: 'Geographic Coordinates Art',
      category: 'creative',
      description: 'Artistic interpretation of a location\'s coordinates',
      tokenCost: 3,
      userPrompt: 'Create an artistic interpretation of coordinates {{lat}}, {{long}}',
      professionalPrompt: 'Create an artistic visualization inspired by geographic coordinates. Generate an abstract or representational artwork that captures the essence, culture, landscape, or spirit of the location at these coordinates. Consider the geography, climate, local culture, and notable features of the region. The piece should evoke a sense of place while maintaining artistic interpretation and creativity.',
      isActive: true
    },

    // EDUCATION & KNOWLEDGE (1 prompt)
    {
      id: 'infographic-edu',
      name: 'Educational Infographic',
      category: 'education',
      description: 'Clear visual explanations of complex topics',
      tokenCost: 3,
      userPrompt: 'Create an educational infographic about {{topic}}',
      professionalPrompt: 'Design a clear, professional educational infographic that explains a complex topic in an accessible visual format. Use a clean, organized layout with clear visual hierarchy. Include icons, diagrams, and illustrations that aid understanding. Use a cohesive color scheme with good contrast for readability. Break information into digestible chunks with clear headings. Include relevant statistics or data visualized in charts or graphs where appropriate.',
      isActive: true
    },

    // E-COMMERCE & VIRTUAL STUDIO (2 prompts)
    {
      id: 'virtual-tryon',
      name: 'Virtual Try-On',
      category: 'ecommerce',
      description: 'See how clothes or accessories would look when worn',
      tokenCost: 3,
      userPrompt: 'Show {{person}} wearing {{item}} in a try-on visualization',
      professionalPrompt: 'Create a photorealistic virtual try-on visualization showing the specified clothing item or accessory on the subject. The garment should conform naturally to the body shape with realistic fabric draping, wrinkles, and shadows. Lighting should match between the subject and the garment. Show the item from an angle that best displays its features. Include natural fabric texture and material properties.',
      isActive: true
    },
    {
      id: 'product-studio',
      name: 'Product Photography',
      category: 'ecommerce',
      description: 'Professional e-commerce product shots',
      tokenCost: 2,
      userPrompt: 'Create a professional product photo of {{product}} on {{background}}',
      professionalPrompt: 'Identify the main product in the uploaded photo. Isolate it from its original background and place it in a professional e-commerce photography setting. Use a clean studio gradient background with soft shadows. Apply professional product photography lighting: main light at 45 degrees, fill light for shadow detail, and rim light for separation. Ensure the product is sharp, well-exposed, and presented at its most appealing angle. Clean up any imperfections while maintaining realistic appearance.',
      isActive: true
    },

    // WORKPLACE & PRODUCTIVITY (3 prompts)
    {
      id: 'flowchart',
      name: 'Process Flowchart',
      category: 'workplace',
      description: 'Professional flowcharts and process diagrams',
      tokenCost: 2,
      userPrompt: 'Create a flowchart showing {{process}}',
      professionalPrompt: 'Generate a clean, professional flowchart that visually maps out the specified process or workflow. Use standard flowchart symbols: ovals for start/end, rectangles for processes, diamonds for decisions, parallelograms for I/O. Maintain consistent spacing and alignment. Use a limited, professional color palette to indicate different types of steps or departments. Include clear labels and directional arrows. The layout should follow top-to-bottom or left-to-right flow.',
      isActive: true
    },
    {
      id: 'ui-sketch',
      name: 'UI Wireframe Sketch',
      category: 'workplace',
      description: 'Hand-drawn style app wireframes and mockups',
      tokenCost: 2,
      userPrompt: 'Create a hand-drawn UI sketch for {{app_type}} showing {{screens}}',
      professionalPrompt: 'Generate a hand-drawn style UI wireframe sketch that looks like it was created with markers on paper or whiteboard. Include rough but recognizable UI elements: navigation bars, buttons, text placeholders, image areas. Use a sketchy, slightly imperfect line quality that suggests rapid ideation. Add annotations and arrows pointing to key features. Include multiple screen states or flow if relevant. The style should be professional yet approachable.',
      isActive: true
    },
    {
      id: 'magazine-layout',
      name: 'Magazine Page Layout',
      category: 'workplace',
      description: 'Professional magazine spread designs',
      tokenCost: 3,
      userPrompt: 'Design a magazine spread about {{topic}} in {{magazine}} style',
      professionalPrompt: 'Create a professional magazine page layout or spread design. Include sophisticated typography with hierarchy between headlines, subheads, body copy, and captions. Integrate photography or illustrations with dynamic cropping and placement. Use pull quotes, sidebars, or info boxes as design elements. The layout should have visual rhythm with balanced white space. Follow contemporary editorial design principles with attention to grid systems and alignment.',
      isActive: true
    },

    // PHOTO EDITING & RESTORATION (2 prompts)
    {
      id: 'outpainting',
      name: 'Image Outpainting',
      category: 'photoediting',
      description: 'Extend images beyond their original borders',
      tokenCost: 2,
      userPrompt: 'Expand this image to the {{direction}} while maintaining style',
      professionalPrompt: 'Extend the provided image beyond its original borders in the specified direction(s). The generated content must seamlessly blend with the original: match the lighting direction and quality, continue any visible patterns or textures naturally, maintain consistent perspective and scale. The extension should feel like it was always part of the original photograph. Pay special attention to edge blending and tonal consistency.',
      isActive: true
    },
    {
      id: 'crowd-removal',
      name: 'Crowd/Object Removal',
      category: 'photoediting',
      description: 'Remove unwanted people or objects from photos',
      tokenCost: 2,
      userPrompt: 'Remove crowds/people from this {{location}} photo',
      professionalPrompt: 'Intelligently remove crowds, tourists, or unwanted people from the photograph while reconstructing the background naturally. Fill the removed areas with contextually appropriate content that matches the surrounding architecture, landscape, or environment. Maintain consistent lighting, shadows, and perspective. The result should look like the scene was photographed empty, with no artifacts or obvious manipulation.',
      isActive: true
    },

    // INTERIOR DESIGN (1 prompt)
    {
      id: 'floor-plan-3d',
      name: 'Floor Plan to 3D',
      category: 'interior',
      description: 'Transform 2D floor plans into 3D visualizations',
      tokenCost: 3,
      userPrompt: 'Convert this floor plan into a 3D {{style}} interior visualization',
      professionalPrompt: 'Transform the 2D floor plan into a photorealistic 3D interior visualization. Interpret the room dimensions and layout from the plan. Add appropriate furniture placement based on room functions. Apply the specified interior design style with matching materials, colors, and decor. Include realistic lighting from windows and artificial sources. Render with architectural visualization quality including accurate materials and atmospheric effects.',
      isActive: true
    },

    // SOCIAL MEDIA & MARKETING (2 prompts)
    {
      id: 'viral-thumbnail',
      name: 'Viral YouTube Thumbnail',
      category: 'social',
      description: 'Eye-catching thumbnails optimized for clicks',
      tokenCost: 2,
      userPrompt: 'Create a viral thumbnail for a video about {{topic}}',
      professionalPrompt: 'Design an attention-grabbing YouTube thumbnail optimized for maximum click-through rate. Include a human face with exaggerated, expressive emotion (shock, excitement, curiosity). Use bold, contrasting colors that pop against YouTube\'s interface. Leave strategic space for large, readable text overlay. Create visual contrast and focal points that draw the eye. The composition should be readable even at small sizes.',
      isActive: true
    },
    {
      id: 'event-poster',
      name: 'Event Promotional Poster',
      category: 'social',
      description: 'Professional event and concert posters',
      tokenCost: 2,
      userPrompt: 'Design an event poster for {{event}} in {{style}} aesthetic',
      professionalPrompt: 'Create a compelling event promotional poster with strong visual impact. Establish clear visual hierarchy: event name prominent, date/time/location clearly readable, supporting imagery that sets the tone. Use typography that matches the event\'s personality. Include appropriate imagery or graphics that convey the event type and atmosphere. Consider print requirements: bleed areas, safe zones for text, and scalability.',
      isActive: true
    },

    // DAILY LIFE & TRANSLATION (2 prompts)
    {
      id: 'menu-translation',
      name: 'Visual Menu Translation',
      category: 'daily',
      description: 'Translate and visualize foreign language menus',
      tokenCost: 2,
      userPrompt: 'Translate this {{language}} menu to {{target_language}} with food images',
      professionalPrompt: 'Analyze the menu image and create a visual translation guide. Identify each menu item, translate the name and description accurately, and generate an appetizing photograph of what each dish looks like. Present in a clean, organized format that shows: original text, translation, and representative food image side by side. Include any relevant dietary information or common allergens if identifiable.',
      isActive: true
    },
    {
      id: 'comic-localization',
      name: 'Comic/Manga Localization',
      category: 'daily',
      description: 'Translate comics while preserving art style',
      tokenCost: 3,
      userPrompt: 'Translate this comic to {{language}} while keeping the original art style',
      professionalPrompt: 'Localize the comic panel(s) by replacing text with accurate translations while perfectly preserving the original art style. Match the original font style, weight, and character as closely as possible. Adjust text bubble sizes if necessary while maintaining composition. Ensure translated text fits naturally within speech bubbles and text areas. Preserve all visual elements, effects, and sound effects with appropriate localized equivalents.',
      isActive: true
    },

    // SOCIAL NETWORKING & AVATARS (2 prompts)
    {
      id: '3d-avatar',
      name: '3D Character Avatar',
      category: 'avatars',
      description: 'Custom 3D avatars for social media and gaming',
      tokenCost: 3,
      userPrompt: 'Create a 3D avatar based on {{description}} in {{style}} style',
      professionalPrompt: 'Generate a stylized 3D character avatar suitable for social media profiles or gaming. The design should capture the specified characteristics while maintaining appealing stylization. Use clean topology and pleasant proportions. Include customizable elements like hairstyle, accessories, and expression. Render with soft, flattering lighting. The style should be modern and professional while maintaining personality.',
      isActive: true
    },
    {
      id: 'pet-meme',
      name: 'Pet Meme Generator',
      category: 'avatars',
      description: 'Transform pet photos into shareable memes',
      tokenCost: 2,
      userPrompt: 'Turn this pet into a meme with {{expression}} expression',
      professionalPrompt: 'Transform the pet photograph into a meme-worthy image. Enhance or adjust the pet\'s expression to match the desired emotion while keeping it recognizable. Position the image to leave appropriate space for text captions above and/or below. Adjust lighting and color for maximum visual impact. The result should be shareable and engaging while maintaining the pet\'s recognizable features.',
      isActive: true
    },

    // NEW ADDITIONS (4 prompts)
    {
      id: 'memory-palace',
      name: 'Memory Palace Visualization',
      category: 'new',
      description: 'Visual memory aids using the method of loci',
      tokenCost: 3,
      userPrompt: 'Create a memory palace to remember {{items}} using {{location}}',
      professionalPrompt: 'Create a visual memory palace illustration using the method of loci technique. Design a clearly navigable physical space (room, building, path) with distinct locations. Place memorable, exaggerated visual representations of each item to remember at specific points along the route. The items should be interacting with their locations in bizarre, memorable ways. Include visual pathway markers to guide the mental journey through the space.',
      isActive: true
    },
    {
      id: 'googly-eyes',
      name: 'Googly Eyes Addition',
      category: 'new',
      description: 'Add fun googly eyes to any subject',
      tokenCost: 1,
      userPrompt: 'Add googly eyes to {{subject}}',
      professionalPrompt: 'Add photorealistic googly eyes to the subject in a humorous way. The googly eyes should be properly scaled and positioned to replace or enhance existing eyes. Each eye should be pointing in a slightly different direction for comedic effect. Include appropriate shadows and reflections to integrate the googly eyes naturally into the image while maintaining their obviously silly appearance.',
      isActive: true
    },
    {
      id: 'data-infographic',
      name: 'Data Visualization Infographic',
      category: 'new',
      description: 'Transform data into beautiful visual stories',
      tokenCost: 3,
      userPrompt: 'Create a data visualization infographic for {{data}} in {{style}} style',
      professionalPrompt: 'Design a compelling data visualization infographic that tells a story with numbers. Choose appropriate chart types for the data: bar, line, pie, area, scatter, or custom graphics. Use a cohesive color scheme that aids comprehension. Include clear labels, legends, and scale indicators. Create visual hierarchy that guides the viewer through the key insights. Add contextual annotations to highlight important data points.',
      isActive: true
    },
    {
      id: 'weather-card',
      name: 'Stylized Weather Card',
      category: 'new',
      description: 'Beautiful weather forecast visualizations',
      tokenCost: 2,
      userPrompt: 'Create a stylish weather card for {{location}} showing {{conditions}}',
      professionalPrompt: 'Design a beautiful, stylized weather card or widget visualization. Include location name, current temperature, weather condition icon, and relevant metrics (humidity, wind, UV index). Use atmospheric illustration that reflects the weather conditions: sunny scenes should feel warm and bright, rainy scenes should feel moody and wet. Apply a cohesive design language with attention to typography and iconography.',
      isActive: true
    },

    // STRUCTURED SCRIPTS (JSON) - 4 prompts
    {
      id: 'script-2000s-selfie',
      name: '2000s Mirror Selfie Script',
      category: 'scripts',
      description: 'Detailed JSON-structured script for authentic 2000s selfie with precise control',
      tokenCost: 3,
      userPrompt: 'Create a 2000s Mirror Selfie with detailed JSON specification for {{subject}}',
      professionalPrompt: `Create a 2000s Mirror Selfie using this detailed specification:

Subject: {{subject}} taking a mirror selfie. The subject should have the following characteristics:
- Age: young adult
- Expression: confident and slightly playful
- Hair: very long, voluminous waves with soft wispy bangs
- Clothing: fitted cropped t-shirt in cream white featuring a cute graphic
- Face: preserve original features, natural glam makeup with soft pink dewy blush and glossy lips

Accessories:
- Gold geometric hoop earrings
- Smartphone with patterned case visible in hand

Photography Style:
- Camera style: early-2000s digital camera aesthetic
- Lighting: harsh super-flash with bright blown-out highlights but subject still visible
- Angle: mirror selfie
- Shot type: tight selfie composition
- Texture: subtle grain, retro highlights, crisp details, soft shadows

Background Setting:
- Nostalgic early-2000s bedroom atmosphere
- Pastel wall tones
- Period elements: posters of 2000s pop icons, cluttered vanity
- Retro lighting creating authentic 2000s nostalgic vibe`,
      isActive: true
    },
    {
      id: 'script-fashion-shoot',
      name: 'Fashion Photoshoot Script',
      category: 'scripts',
      description: 'Structured script for professional fashion photography with complete styling',
      tokenCost: 3,
      userPrompt: 'Create a fashion photoshoot with detailed specification for {{model}}',
      professionalPrompt: `Create a high-fashion photoshoot using this detailed specification:

Model: {{model}}
- Pose: dynamic editorial stance with confident expression
- Hair: styled professionally, can be flowing or structured
- Makeup: high-fashion editorial makeup, flawless skin

Wardrobe:
- Garment type: specify designer-style piece
- Colors: bold or sophisticated palette
- Accessories: statement jewelry, designer bag or shoes

Photography Setup:
- Lighting: professional three-point studio lighting with dramatic shadows
- Camera: shot on professional medium format, 85mm equivalent
- Background: seamless gradient or styled editorial set
- Post-processing: high-end retouching, skin detail preserved

Mood & Style:
- Editorial fashion magazine quality
- Aspirational and polished
- Strong visual impact`,
      isActive: true
    },
    {
      id: 'script-product-hero',
      name: 'Product Hero Shot Script',
      category: 'scripts',
      description: 'Structured script for hero product photography with precise control',
      tokenCost: 3,
      userPrompt: 'Create a product hero shot with detailed specification for {{product}}',
      professionalPrompt: `Create a professional product hero shot using this detailed specification:

Product: {{product}}
- Position: hero angle showcasing the best features
- Condition: pristine, brand new appearance
- Details: all logos, textures, materials clearly visible

Lighting Setup:
- Key light: soft box at 45 degrees, creating subtle highlight
- Fill light: reducing shadows without flattening
- Rim light: separating product from background
- Reflector: bouncing light into shadow areas

Background & Environment:
- Style: clean gradient or contextual lifestyle setting
- Color: complementary to product colors
- Props: minimal, supporting the hero product

Technical Specifications:
- Lens: macro or 100mm for product detail
- Depth of field: sharp product, subtle background blur
- Post-production: color-accurate, enhanced sharpness`,
      isActive: true
    },
    {
      id: 'script-character',
      name: 'Character Portrait Script',
      category: 'scripts',
      description: 'Detailed character specification for consistent portraits and character design',
      tokenCost: 3,
      userPrompt: 'Create a character portrait with detailed specification for {{character}}',
      professionalPrompt: `Create a detailed character portrait using this specification:

Character: {{character}}

Physical Attributes:
- Face shape: define the face structure
- Eyes: color, shape, expression
- Hair: color, length, style, texture
- Skin tone: natural and realistic
- Age range: approximate visual age
- Build: body type if visible

Expression & Personality:
- Facial expression: specific emotion or mood
- Personality traits: visible in the pose and expression
- Eye contact: direct, avoiding, or looking elsewhere

Attire & Accessories:
- Clothing style: period, culture, or fantasy genre
- Color palette: primary and accent colors
- Accessories: jewelry, glasses, hats, etc.

Artistic Style:
- Rendering: photorealistic, painterly, or stylized
- Lighting: dramatic, soft, or natural
- Background: simple, environmental, or abstract`,
      isActive: true
    }
  ];

  try {
    const batch = db.batch();
    let count = 0;

    for (const prompt of professionalPrompts) {
      const docRef = db.collection('promptTemplates').doc(prompt.id);
      batch.set(docRef, {
        ...prompt,
        createdAt: admin.firestore.FieldValue.serverTimestamp(),
        updatedAt: admin.firestore.FieldValue.serverTimestamp()
      }, { merge: true });
      count++;
    }

    await batch.commit();

    console.log(`Seeded ${count} prompt templates`);

    return {
      success: true,
      count: count,
      message: `Successfully seeded ${count} prompt templates`
    };

  } catch (error) {
    console.error('Seed prompts error:', error);
    throw new functions.https.HttpsError('internal', 'Failed to seed prompts: ' + error.message);
  }
});

// Get prompt templates (public, for Creative Studio)
exports.getPromptTemplates = functions.https.onCall(async (data, context) => {
  const { category, activeOnly } = data || {};

  try {
    let query = db.collection('promptTemplates');

    if (category && category !== 'all') {
      query = query.where('category', '==', category);
    }

    if (activeOnly !== false) {
      query = query.where('isActive', '==', true);
    }

    const snapshot = await query.orderBy('category').orderBy('name').get();
    const templates = [];

    snapshot.forEach(doc => {
      const data = doc.data();
      templates.push({
        id: doc.id,
        name: data.name,
        category: data.category,
        description: data.description,
        tokenCost: data.tokenCost,
        userPrompt: data.userPrompt,
        // Note: professionalPrompt is sent to client but only used on backend for generation
        professionalPrompt: data.professionalPrompt,
        isActive: data.isActive
      });
    });

    return { success: true, templates };

  } catch (error) {
    console.error('Get templates error:', error);
    throw new functions.https.HttpsError('internal', 'Failed to get templates');
  }
});

// =====================================================
// ADMIN: Fix Storage URLs and Make Files Public
// Run this once to migrate existing images
// =====================================================
exports.adminFixStorageUrls = functions.https.onCall(async (data, context) => {
  await requireAdmin(context);

  const { folder, dryRun } = data || {};
  const targetFolder = folder || 'creative-studio';
  const isDryRun = dryRun !== false; // Default to dry run for safety

  const results = {
    folder: targetFolder,
    dryRun: isDryRun,
    filesProcessed: 0,
    filesMadePublic: 0,
    urlsUpdated: 0,
    errors: []
  };

  try {
    const bucket = admin.storage().bucket();

    // List all files in the folder
    const [files] = await bucket.getFiles({ prefix: targetFolder + '/' });
    console.log(`Found ${files.length} files in ${targetFolder}/`);
    results.filesProcessed = files.length;

    if (!isDryRun) {
      // Make each file public
      for (const file of files) {
        try {
          await file.makePublic();
          results.filesMadePublic++;
        } catch (err) {
          results.errors.push(`Failed to make public: ${file.name} - ${err.message}`);
        }
      }
    }

    // Update URLs in Firestore collections
    const collections = ['creativeHistory', 'creativeGallery', 'promptTemplates'];

    for (const collectionName of collections) {
      const snapshot = await db.collection(collectionName).get();

      for (const doc of snapshot.docs) {
        const docData = doc.data();
        let needsUpdate = false;
        const updates = {};

        // Check for firebasestorage URLs and convert them
        const oldUrlPattern = 'firebasestorage.googleapis.com';
        const bucketName = bucket.name;

        // Handle images array (creativeHistory)
        if (docData.images && Array.isArray(docData.images)) {
          const newImages = docData.images.map(img => {
            if (img.url && img.url.includes(oldUrlPattern)) {
              needsUpdate = true;
              // Extract fileName from the encoded URL
              const fileName = img.fileName || decodeURIComponent(
                img.url.split('/o/')[1]?.split('?')[0] || ''
              );
              return {
                ...img,
                url: `https://storage.googleapis.com/${bucketName}/${fileName}`
              };
            }
            return img;
          });
          if (needsUpdate) {
            updates.images = newImages;
          }
        }

        // Handle single imageUrl field (creativeGallery, some history)
        if (docData.imageUrl && docData.imageUrl.includes(oldUrlPattern)) {
          needsUpdate = true;
          const fileName = decodeURIComponent(
            docData.imageUrl.split('/o/')[1]?.split('?')[0] || ''
          );
          updates.imageUrl = `https://storage.googleapis.com/${bucketName}/${fileName}`;
        }

        // Handle coverImage field (promptTemplates)
        if (docData.coverImage && docData.coverImage.includes(oldUrlPattern)) {
          needsUpdate = true;
          const fileName = decodeURIComponent(
            docData.coverImage.split('/o/')[1]?.split('?')[0] || ''
          );
          updates.coverImage = `https://storage.googleapis.com/${bucketName}/${fileName}`;
        }

        if (needsUpdate && !isDryRun) {
          await db.collection(collectionName).doc(doc.id).update(updates);
          results.urlsUpdated++;
        } else if (needsUpdate) {
          results.urlsUpdated++; // Count for dry run
        }
      }
    }

    results.message = isDryRun
      ? `DRY RUN: Would make ${files.length} files public and update ${results.urlsUpdated} URLs`
      : `Made ${results.filesMadePublic} files public and updated ${results.urlsUpdated} URLs`;

    return results;

  } catch (error) {
    console.error('Fix storage URLs error:', error);
    throw new functions.https.HttpsError('internal', 'Failed to fix storage URLs: ' + error.message);
  }
});

// Make a specific file public (for RunPod thumbnails after upload completes)
exports.makeFilePublic = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const { fileName } = data;

  if (!fileName) {
    throw new functions.https.HttpsError('invalid-argument', 'fileName is required');
  }

  // Security: Only allow users to make their own files public
  if (!fileName.includes(`/${uid}/`)) {
    throw new functions.https.HttpsError('permission-denied', 'Cannot access this file');
  }

  try {
    const bucket = admin.storage().bucket();
    const file = bucket.file(fileName);

    // Check if file exists
    const [exists] = await file.exists();
    if (!exists) {
      return { success: false, message: 'File not found - may still be uploading' };
    }

    await file.makePublic();
    const publicUrl = `https://storage.googleapis.com/${bucket.name}/${fileName}`;

    return {
      success: true,
      publicUrl,
      message: 'File is now public'
    };

  } catch (error) {
    console.error('Make file public error:', error);
    throw new functions.https.HttpsError('internal', 'Failed to make file public');
  }
});

// =====================================================
// ADMIN: Set CORS Configuration on Storage Bucket
// Run this ONCE after deployment to enable cross-origin access
// Call from browser console: firebase.functions().httpsCallable('adminSetBucketCors')()
// Added: 2025-12-02 - Fixes CORS errors for images on custom domain
// =====================================================
exports.adminSetBucketCors = functions.https.onCall(async (data, context) => {
  await requireAdmin(context);

  try {
    const bucket = admin.storage().bucket();

    // Define CORS configuration
    const corsConfiguration = [
      {
        origin: [
          'https://ytseo.siteuo.com',
          'https://ytseo-6d1b0.web.app',
          'https://ytseo-6d1b0.firebaseapp.com',
          'http://localhost:5000',
          'http://localhost:5001',
          'http://127.0.0.1:5000',
          'http://127.0.0.1:5001'
        ],
        method: ['GET', 'HEAD', 'OPTIONS'],
        maxAgeSeconds: 3600,
        responseHeader: [
          'Content-Type',
          'Access-Control-Allow-Origin',
          'Access-Control-Allow-Methods',
          'Access-Control-Allow-Headers',
          'Content-Length',
          'Content-Encoding'
        ]
      }
    ];

    // Set CORS on the bucket
    await bucket.setCorsConfiguration(corsConfiguration);

    // Verify it was set
    const [metadata] = await bucket.getMetadata();

    return {
      success: true,
      message: 'CORS configuration applied successfully!',
      bucketName: bucket.name,
      corsConfig: metadata.cors || 'Configuration applied'
    };

  } catch (error) {
    console.error('Set CORS error:', error);
    throw new functions.https.HttpsError('internal',
      'Failed to set CORS: ' + error.message);
  }
});

// =====================================================
// FEATURE 1: CLIENT ACTIVITY TIMELINE / HISTORY LOG
// =====================================================

// Helper function to log user activity (called internally)
async function logUserActivity(userId, activityType, details, adminId = null) {
  try {
    const activityRef = db.collection('users').doc(userId).collection('activityLog');
    await activityRef.add({
      type: activityType,
      details: details,
      adminId: adminId,
      timestamp: admin.firestore.FieldValue.serverTimestamp()
    });
  } catch (error) {
    console.error('Error logging activity:', error);
  }
}

// Get user activity history
exports.adminGetUserActivity = functions.https.onCall(async (data, context) => {
  await requireAdmin(context);
  const { userId, limit: queryLimit = 50 } = data || {};
  if (!userId) throw new functions.https.HttpsError('invalid-argument', 'User ID required');

  try {
    const activityRef = db.collection('users').doc(userId).collection('activityLog');
    const snapshot = await activityRef.orderBy('timestamp', 'desc').limit(Math.min(queryLimit, 100)).get();
    const activities = [];
    snapshot.forEach(doc => {
      const d = doc.data();
      activities.push({
        id: doc.id,
        type: d.type,
        details: d.details,
        adminId: d.adminId,
        timestamp: d.timestamp?.toDate?.()?.toISOString() || null
      });
    });
    return { success: true, activities };
  } catch (error) {
    throw new functions.https.HttpsError('internal', 'Failed to get activity: ' + error.message);
  }
});

// Track user login
exports.trackUserLogin = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  try {
    await db.collection('users').doc(uid).update({ lastLoginAt: admin.firestore.FieldValue.serverTimestamp() });
    await logUserActivity(uid, 'login', { source: data?.source || 'web' });
    return { success: true };
  } catch (error) {
    return { success: false };
  }
});

// =====================================================
// FEATURE 2: BULK OPERATIONS
// =====================================================

// Bulk extend subscriptions
exports.adminBulkExtendSubscriptions = functions.https.onCall(async (data, context) => {
  const adminUid = await requireAdmin(context);
  const { userIds, days } = data || {};

  if (!userIds || !Array.isArray(userIds) || userIds.length === 0) throw new functions.https.HttpsError('invalid-argument', 'User IDs array required');
  if (!days || days < 1 || days > 365) throw new functions.https.HttpsError('invalid-argument', 'Days must be 1-365');
  if (userIds.length > 100) throw new functions.https.HttpsError('invalid-argument', 'Max 100 users per batch');

  const results = { success: 0, failed: 0, errors: [] };

  for (const userId of userIds) {
    try {
      const userRef = db.collection('users').doc(userId);
      const userDoc = await userRef.get();
      if (!userDoc.exists) { results.failed++; continue; }

      const userData = userDoc.data();
      const currentEnd = userData.subscription?.endDate?.toDate?.() || new Date();
      const baseDate = currentEnd > new Date() ? currentEnd : new Date();
      const newEndDate = new Date(baseDate);
      newEndDate.setDate(newEndDate.getDate() + days);

      await userRef.update({ 'subscription.endDate': admin.firestore.Timestamp.fromDate(newEndDate) });
      await logUserActivity(userId, 'subscription_change', { action: 'bulk_extend', days, newEndDate: newEndDate.toISOString() }, adminUid);
      results.success++;
    } catch (error) {
      results.failed++;
      results.errors.push({ userId, error: error.message });
    }
  }
  return { success: true, message: `Extended ${results.success} subscriptions by ${days} days`, results };
});

// Bulk set plan
exports.adminBulkSetPlan = functions.https.onCall(async (data, context) => {
  const adminUid = await requireAdmin(context);
  const { userIds, plan, duration } = data || {};

  const validPlans = ['free', 'lite', 'pro', 'enterprise'];
  if (!validPlans.includes(plan)) throw new functions.https.HttpsError('invalid-argument', 'Invalid plan');
  if (!userIds || !Array.isArray(userIds) || userIds.length === 0) throw new functions.https.HttpsError('invalid-argument', 'User IDs required');
  if (userIds.length > 100) throw new functions.https.HttpsError('invalid-argument', 'Max 100 users');

  const durationDays = { 'week': 7, 'month': 30, '3months': 90, 'year': 365, 'lifetime': null };
  const days = durationDays[duration || 'month'];
  const results = { success: 0, failed: 0, errors: [] };

  const planDoc = await db.collection('plans').doc(plan).get();
  const planLimits = planDoc.exists ? planDoc.data() : {};

  for (const userId of userIds) {
    try {
      const userRef = db.collection('users').doc(userId);
      const userDoc = await userRef.get();
      if (!userDoc.exists) { results.failed++; continue; }

      const now = new Date();
      const endDate = days === null ? null : new Date(now.getTime() + days * 24 * 60 * 60 * 1000);

      const updateData = {
        'subscription.plan': plan,
        'subscription.startDate': admin.firestore.Timestamp.fromDate(now),
        'subscription.updatedAt': admin.firestore.FieldValue.serverTimestamp()
      };
      if (endDate) {
        updateData['subscription.endDate'] = admin.firestore.Timestamp.fromDate(endDate);
      } else {
        updateData['subscription.endDate'] = null;
        updateData['subscription.isLifetime'] = true;
      }
      if (planLimits.warpOptimizer !== undefined) updateData['usage.warpOptimizer.limit'] = planLimits.warpOptimizer;
      if (planLimits.competitorAnalysis !== undefined) updateData['usage.competitorAnalysis.limit'] = planLimits.competitorAnalysis;

      await userRef.update(updateData);
      await logUserActivity(userId, 'subscription_change', { action: 'bulk_set_plan', plan, duration: duration || 'month' }, adminUid);
      results.success++;
    } catch (error) {
      results.failed++;
    }
  }
  return { success: true, message: `Set ${results.success} users to ${plan.toUpperCase()}`, results };
});

// Export users data
exports.adminExportUsers = functions.https.onCall(async (data, context) => {
  await requireAdmin(context);
  const { format = 'json', filters = {} } = data || {};

  try {
    const snapshot = await db.collection('users').get();
    const users = [];

    snapshot.forEach(doc => {
      const userData = doc.data();
      const plan = userData.subscription?.plan || 'free';
      if (filters.plan && plan !== filters.plan) return;
      if (filters.verified && !userData.isFiverrVerified) return;
      if (filters.hasTag && (!userData.tags || !userData.tags.includes(filters.hasTag))) return;

      users.push({
        uid: doc.id,
        email: userData.email || '',
        displayName: userData.displayName || '',
        clientAlias: userData.clientAlias || '',
        isFiverrVerified: userData.isFiverrVerified || false,
        tags: userData.tags || [],
        plan,
        subscriptionEnd: userData.subscription?.endDate?.toDate?.()?.toISOString() || null,
        lastLoginAt: userData.lastLoginAt?.toDate?.()?.toISOString() || null,
        adminNotes: userData.adminNotes || ''
      });
    });

    if (format === 'csv') {
      const headers = ['Email', 'Client Alias', 'Fiverr Verified', 'Tags', 'Plan', 'Subscription End', 'Last Login', 'Admin Notes'];
      const csvRows = [headers.join(',')];
      users.forEach(u => {
        csvRows.push([
          `"${u.email}"`, `"${u.clientAlias}"`, u.isFiverrVerified ? 'Yes' : 'No',
          `"${(u.tags || []).join('; ')}"`, u.plan, u.subscriptionEnd || '', u.lastLoginAt || '',
          `"${(u.adminNotes || '').replace(/"/g, '""')}"`
        ].join(','));
      });
      return { success: true, format: 'csv', data: csvRows.join('\n'), count: users.length };
    }
    return { success: true, format: 'json', data: users, count: users.length };
  } catch (error) {
    throw new functions.https.HttpsError('internal', 'Failed to export: ' + error.message);
  }
});

// =====================================================
// FEATURE 3: CLIENT TAGS SYSTEM
// =====================================================

// Get all available tags
exports.adminGetTags = functions.https.onCall(async (data, context) => {
  await requireAdmin(context);
  try {
    const tagsDoc = await db.collection('settings').doc('tags').get();
    const tagsData = tagsDoc.exists ? tagsDoc.data() : {};
    return { success: true, tags: tagsData.list || [], autoTagRules: tagsData.autoTagRules || [] };
  } catch (error) {
    throw new functions.https.HttpsError('internal', 'Failed to get tags: ' + error.message);
  }
});

// Create a new tag
exports.adminCreateTag = functions.https.onCall(async (data, context) => {
  await requireAdmin(context);
  const { name, color = '#6b7280' } = data || {};
  if (!name || name.trim().length === 0) throw new functions.https.HttpsError('invalid-argument', 'Tag name required');

  const tagName = name.trim().substring(0, 30);
  const tagId = tagName.toLowerCase().replace(/[^a-z0-9]/g, '_');

  try {
    const tagsRef = db.collection('settings').doc('tags');
    const tagsDoc = await tagsRef.get();
    const currentTags = tagsDoc.exists ? (tagsDoc.data().list || []) : [];
    if (currentTags.some(t => t.id === tagId)) throw new functions.https.HttpsError('already-exists', 'Tag exists');

    const newTag = { id: tagId, name: tagName, color, createdAt: new Date().toISOString() };
    currentTags.push(newTag);
    await tagsRef.set({ list: currentTags }, { merge: true });
    return { success: true, tag: newTag };
  } catch (error) {
    if (error.code) throw error;
    throw new functions.https.HttpsError('internal', 'Failed to create tag: ' + error.message);
  }
});

// Delete a tag
exports.adminDeleteTag = functions.https.onCall(async (data, context) => {
  await requireAdmin(context);
  const { tagId } = data || {};
  if (!tagId) throw new functions.https.HttpsError('invalid-argument', 'Tag ID required');

  try {
    const tagsRef = db.collection('settings').doc('tags');
    const tagsDoc = await tagsRef.get();
    const currentTags = tagsDoc.exists ? (tagsDoc.data().list || []) : [];
    await tagsRef.update({ list: currentTags.filter(t => t.id !== tagId) });

    const usersSnapshot = await db.collection('users').where('tags', 'array-contains', tagId).get();
    const batch = db.batch();
    usersSnapshot.forEach(doc => batch.update(doc.ref, { tags: admin.firestore.FieldValue.arrayRemove(tagId) }));
    await batch.commit();
    return { success: true, message: 'Tag deleted' };
  } catch (error) {
    throw new functions.https.HttpsError('internal', 'Failed to delete tag: ' + error.message);
  }
});

// Add tag to user
exports.adminAddUserTag = functions.https.onCall(async (data, context) => {
  const adminUid = await requireAdmin(context);
  const { userId, tagId } = data || {};
  if (!userId || !tagId) throw new functions.https.HttpsError('invalid-argument', 'User ID and Tag ID required');

  try {
    await db.collection('users').doc(userId).update({ tags: admin.firestore.FieldValue.arrayUnion(tagId) });
    await logUserActivity(userId, 'tag_change', { action: 'add', tagId }, adminUid);
    return { success: true };
  } catch (error) {
    throw new functions.https.HttpsError('internal', 'Failed to add tag: ' + error.message);
  }
});

// Remove tag from user
exports.adminRemoveUserTag = functions.https.onCall(async (data, context) => {
  const adminUid = await requireAdmin(context);
  const { userId, tagId } = data || {};
  if (!userId || !tagId) throw new functions.https.HttpsError('invalid-argument', 'User ID and Tag ID required');

  try {
    await db.collection('users').doc(userId).update({ tags: admin.firestore.FieldValue.arrayRemove(tagId) });
    await logUserActivity(userId, 'tag_change', { action: 'remove', tagId }, adminUid);
    return { success: true };
  } catch (error) {
    throw new functions.https.HttpsError('internal', 'Failed to remove tag: ' + error.message);
  }
});

// Set auto-tag rules
exports.adminSetAutoTagRules = functions.https.onCall(async (data, context) => {
  await requireAdmin(context);
  const { rules } = data || {};
  if (!rules || !Array.isArray(rules)) throw new functions.https.HttpsError('invalid-argument', 'Rules array required');

  const validRules = rules.filter(r => r.tagId && r.condition).map(r => ({
    tagId: r.tagId, condition: r.condition, value: r.value, enabled: r.enabled !== false
  }));

  try {
    await db.collection('settings').doc('tags').set({ autoTagRules: validRules }, { merge: true });
    return { success: true, rules: validRules };
  } catch (error) {
    throw new functions.https.HttpsError('internal', 'Failed to save rules: ' + error.message);
  }
});

// Run auto-tag rules manually
exports.adminRunAutoTagRules = functions.https.onCall(async (data, context) => {
  await requireAdmin(context);

  try {
    const tagsDoc = await db.collection('settings').doc('tags').get();
    const rules = tagsDoc.exists ? (tagsDoc.data().autoTagRules || []) : [];
    if (rules.length === 0) return { success: true, tagged: 0 };

    const usersSnapshot = await db.collection('users').get();
    let taggedCount = 0;

    for (const userDoc of usersSnapshot.docs) {
      const userData = userDoc.data();
      const userTags = userData.tags || [];
      const tagsToAdd = [];

      for (const rule of rules) {
        if (!rule.enabled || userTags.includes(rule.tagId)) continue;
        let shouldTag = false;

        if (rule.condition === 'usage_above') {
          const used = userData.usage?.warpOptimizer?.usedToday || 0;
          const limit = userData.usage?.warpOptimizer?.limit || 1;
          shouldTag = (used / limit) * 100 >= rule.value;
        } else if (rule.condition === 'inactive_days') {
          const lastLogin = userData.lastLoginAt?.toDate?.();
          if (lastLogin) {
            const days = Math.floor((Date.now() - lastLogin.getTime()) / (1000 * 60 * 60 * 24));
            shouldTag = days >= rule.value;
          }
        } else if (rule.condition === 'plan_is') {
          shouldTag = (userData.subscription?.plan || 'free') === rule.value;
        }
        if (shouldTag) tagsToAdd.push(rule.tagId);
      }

      if (tagsToAdd.length > 0) {
        await userDoc.ref.update({ tags: admin.firestore.FieldValue.arrayUnion(...tagsToAdd) });
        taggedCount++;
      }
    }
    return { success: true, tagged: taggedCount };
  } catch (error) {
    throw new functions.https.HttpsError('internal', 'Failed to run rules: ' + error.message);
  }
});

// =====================================================
// FEATURE 4: NOTIFICATIONS SYSTEM
// =====================================================

// Get notification settings
exports.adminGetNotificationSettings = functions.https.onCall(async (data, context) => {
  await requireAdmin(context);
  try {
    const doc = await db.collection('settings').doc('notifications').get();
    const s = doc.exists ? doc.data() : {};
    return {
      success: true,
      settings: {
        expiringDays: s.expiringDays || 3,
        inactiveDays: s.inactiveDays || 7,
        highUsagePercent: s.highUsagePercent || 80,
        emailEnabled: s.emailEnabled || false,
        adminEmail: s.adminEmail || ''
      }
    };
  } catch (error) {
    throw new functions.https.HttpsError('internal', 'Failed to get settings: ' + error.message);
  }
});

// Set notification settings
exports.adminSetNotificationSettings = functions.https.onCall(async (data, context) => {
  await requireAdmin(context);
  const { expiringDays, inactiveDays, highUsagePercent, emailEnabled, adminEmail } = data || {};

  try {
    await db.collection('settings').doc('notifications').set({
      expiringDays: Math.max(1, Math.min(30, expiringDays || 3)),
      inactiveDays: Math.max(1, Math.min(90, inactiveDays || 7)),
      highUsagePercent: Math.max(50, Math.min(100, highUsagePercent || 80)),
      emailEnabled: !!emailEnabled,
      adminEmail: (adminEmail || '').trim().substring(0, 100),
      updatedAt: admin.firestore.FieldValue.serverTimestamp()
    }, { merge: true });
    return { success: true };
  } catch (error) {
    throw new functions.https.HttpsError('internal', 'Failed to save: ' + error.message);
  }
});

// Get notifications
exports.adminGetNotifications = functions.https.onCall(async (data, context) => {
  await requireAdmin(context);
  try {
    const snapshot = await db.collection('adminNotifications')
      .where('dismissed', '==', false)
      .orderBy('createdAt', 'desc')
      .limit(50)
      .get();

    const notifications = [];
    snapshot.forEach(doc => {
      const d = doc.data();
      notifications.push({
        id: doc.id, type: d.type, title: d.title, message: d.message,
        userId: d.userId, userEmail: d.userEmail, priority: d.priority || 'normal',
        createdAt: d.createdAt?.toDate?.()?.toISOString() || null
      });
    });
    return { success: true, notifications };
  } catch (error) {
    throw new functions.https.HttpsError('internal', 'Failed to get notifications: ' + error.message);
  }
});

// Dismiss notification
exports.adminDismissNotification = functions.https.onCall(async (data, context) => {
  await requireAdmin(context);
  const { notificationId, dismissAll = false } = data || {};

  try {
    if (dismissAll) {
      const snapshot = await db.collection('adminNotifications').where('dismissed', '==', false).get();
      const batch = db.batch();
      snapshot.forEach(doc => batch.update(doc.ref, { dismissed: true, dismissedAt: admin.firestore.FieldValue.serverTimestamp() }));
      await batch.commit();
      return { success: true, message: 'All dismissed' };
    }
    if (!notificationId) throw new functions.https.HttpsError('invalid-argument', 'ID required');
    await db.collection('adminNotifications').doc(notificationId).update({ dismissed: true, dismissedAt: admin.firestore.FieldValue.serverTimestamp() });
    return { success: true };
  } catch (error) {
    throw new functions.https.HttpsError('internal', 'Failed: ' + error.message);
  }
});

// Check and create notifications (scheduled daily at 8 AM UTC)
exports.checkAndCreateNotifications = functions.pubsub.schedule('0 8 * * *').timeZone('UTC').onRun(async (context) => {
  try {
    const settingsDoc = await db.collection('settings').doc('notifications').get();
    const s = settingsDoc.exists ? settingsDoc.data() : {};
    const expiringDays = s.expiringDays || 3;
    const inactiveDays = s.inactiveDays || 7;
    const highUsagePercent = s.highUsagePercent || 80;

    const now = new Date();
    const expiringThreshold = new Date(now.getTime() + expiringDays * 24 * 60 * 60 * 1000);
    const inactiveThreshold = new Date(now.getTime() - inactiveDays * 24 * 60 * 60 * 1000);

    const usersSnapshot = await db.collection('users').get();
    const existingSnapshot = await db.collection('adminNotifications').where('dismissed', '==', false).get();
    const existingKeys = new Set();
    existingSnapshot.forEach(doc => {
      const d = doc.data();
      if (d.userId && d.type) existingKeys.add(`${d.userId}_${d.type}`);
    });

    for (const userDoc of usersSnapshot.docs) {
      const userData = userDoc.data();
      const userId = userDoc.id;
      const email = userData.clientAlias || userData.email || 'Unknown';
      const plan = userData.subscription?.plan || 'free';

      // Expiring subscriptions
      const endDate = userData.subscription?.endDate?.toDate?.();
      if (endDate && endDate <= expiringThreshold && endDate > now && !existingKeys.has(`${userId}_expiring_subscription`)) {
        const daysLeft = Math.ceil((endDate - now) / (1000 * 60 * 60 * 24));
        await db.collection('adminNotifications').add({
          type: 'expiring_subscription', title: 'Subscription Expiring',
          message: `${email}'s subscription expires in ${daysLeft} day(s)`,
          userId, userEmail: userData.email, priority: daysLeft <= 1 ? 'high' : 'normal',
          dismissed: false, createdAt: admin.firestore.FieldValue.serverTimestamp()
        });
      }

      // Inactive paid users
      if (plan !== 'free') {
        const lastLogin = userData.lastLoginAt?.toDate?.();
        if (lastLogin && lastLogin < inactiveThreshold && !existingKeys.has(`${userId}_inactive_user`)) {
          const daysSince = Math.floor((now - lastLogin) / (1000 * 60 * 60 * 24));
          await db.collection('adminNotifications').add({
            type: 'inactive_user', title: 'Inactive Paid User',
            message: `${email} hasn't logged in for ${daysSince} days`,
            userId, userEmail: userData.email, priority: 'low',
            dismissed: false, createdAt: admin.firestore.FieldValue.serverTimestamp()
          });
        }
      }

      // High usage
      const used = userData.usage?.warpOptimizer?.usedToday || 0;
      const limit = userData.usage?.warpOptimizer?.limit || 1;
      const pct = (used / limit) * 100;
      if (pct >= highUsagePercent && plan !== 'enterprise' && !existingKeys.has(`${userId}_high_usage`)) {
        await db.collection('adminNotifications').add({
          type: 'high_usage', title: 'High Usage - Upsell Opportunity',
          message: `${email} using ${Math.round(pct)}% of quota`,
          userId, userEmail: userData.email, priority: 'normal',
          dismissed: false, createdAt: admin.firestore.FieldValue.serverTimestamp()
        });
      }
    }
    return null;
  } catch (error) {
    console.error('Notification check error:', error);
    return null;
  }
});

// Manual notification check
exports.adminCheckNotifications = functions.https.onCall(async (data, context) => {
  await requireAdmin(context);

  try {
    const settingsDoc = await db.collection('settings').doc('notifications').get();
    const s = settingsDoc.exists ? settingsDoc.data() : {};
    const expiringDays = s.expiringDays || 3;
    const inactiveDays = s.inactiveDays || 7;
    const highUsagePercent = s.highUsagePercent || 80;

    const now = new Date();
    const expiringThreshold = new Date(now.getTime() + expiringDays * 24 * 60 * 60 * 1000);
    const inactiveThreshold = new Date(now.getTime() - inactiveDays * 24 * 60 * 60 * 1000);

    const usersSnapshot = await db.collection('users').get();
    const existingSnapshot = await db.collection('adminNotifications').where('dismissed', '==', false).get();
    const existingKeys = new Set();
    existingSnapshot.forEach(doc => {
      const d = doc.data();
      if (d.userId && d.type) existingKeys.add(`${d.userId}_${d.type}`);
    });

    let created = 0;

    for (const userDoc of usersSnapshot.docs) {
      const userData = userDoc.data();
      const userId = userDoc.id;
      const email = userData.clientAlias || userData.email || 'Unknown';
      const plan = userData.subscription?.plan || 'free';

      const endDate = userData.subscription?.endDate?.toDate?.();
      if (endDate && endDate <= expiringThreshold && endDate > now && !existingKeys.has(`${userId}_expiring_subscription`)) {
        const daysLeft = Math.ceil((endDate - now) / (1000 * 60 * 60 * 24));
        await db.collection('adminNotifications').add({
          type: 'expiring_subscription', title: 'Subscription Expiring',
          message: `${email}'s subscription expires in ${daysLeft} day(s)`,
          userId, userEmail: userData.email, priority: daysLeft <= 1 ? 'high' : 'normal',
          dismissed: false, createdAt: admin.firestore.FieldValue.serverTimestamp()
        });
        created++;
      }

      if (plan !== 'free') {
        const lastLogin = userData.lastLoginAt?.toDate?.();
        if (lastLogin && lastLogin < inactiveThreshold && !existingKeys.has(`${userId}_inactive_user`)) {
          const daysSince = Math.floor((now - lastLogin) / (1000 * 60 * 60 * 24));
          await db.collection('adminNotifications').add({
            type: 'inactive_user', title: 'Inactive Paid User',
            message: `${email} hasn't logged in for ${daysSince} days`,
            userId, userEmail: userData.email, priority: 'low',
            dismissed: false, createdAt: admin.firestore.FieldValue.serverTimestamp()
          });
          created++;
        }
      }

      const used = userData.usage?.warpOptimizer?.usedToday || 0;
      const limit = userData.usage?.warpOptimizer?.limit || 1;
      const pct = (used / limit) * 100;
      if (pct >= highUsagePercent && plan !== 'enterprise' && !existingKeys.has(`${userId}_high_usage`)) {
        await db.collection('adminNotifications').add({
          type: 'high_usage', title: 'High Usage - Upsell',
          message: `${email} using ${Math.round(pct)}% of quota`,
          userId, userEmail: userData.email, priority: 'normal',
          dismissed: false, createdAt: admin.firestore.FieldValue.serverTimestamp()
        });
        created++;
      }
    }
    return { success: true, created };
  } catch (error) {
    throw new functions.https.HttpsError('internal', 'Failed: ' + error.message);
  }
});

// ==============================================
// RENEWAL REQUEST SYSTEM
// ==============================================

// Helper: Create notification for a user
async function createUserNotification(userId, type, title, message, data = {}) {
  try {
    await db.collection('users').doc(userId).collection('notifications').add({
      type,
      title,
      message,
      data,
      read: false,
      createdAt: admin.firestore.FieldValue.serverTimestamp()
    });
    return true;
  } catch (error) {
    console.error('Error creating user notification:', error);
    return false;
  }
}

// User: Submit a renewal request
exports.userSubmitRenewalRequest = functions.https.onCall(async (data, context) => {
  if (!context.auth) {
    throw new functions.https.HttpsError('unauthenticated', 'Must be logged in');
  }

  const userId = context.auth.uid;
  const { preferredPlan, preferredDuration, message } = data || {};

  if (!preferredPlan) {
    throw new functions.https.HttpsError('invalid-argument', 'Preferred plan is required');
  }

  const validPlans = ['lite', 'pro', 'enterprise'];
  if (!validPlans.includes(preferredPlan)) {
    throw new functions.https.HttpsError('invalid-argument', 'Invalid plan');
  }

  const validDurations = ['week', 'month', '3months', 'year', 'lifetime'];
  if (preferredDuration && !validDurations.includes(preferredDuration)) {
    throw new functions.https.HttpsError('invalid-argument', 'Invalid duration');
  }

  // Check for existing pending request
  const existingRequest = await db.collection('renewalRequests')
    .where('userId', '==', userId)
    .where('status', '==', 'pending')
    .limit(1)
    .get();

  if (!existingRequest.empty) {
    throw new functions.https.HttpsError('already-exists', 'You already have a pending renewal request');
  }

  // Get user data for the request
  const userDoc = await db.collection('users').doc(userId).get();
  const userData = userDoc.exists ? userDoc.data() : {};

  const requestData = {
    userId,
    userEmail: userData.email || context.auth.token.email || '',
    userName: userData.displayName || userData.clientAlias || '',
    isFiverrVerified: userData.isFiverrVerified || false,
    currentPlan: userData.subscription?.plan || 'free',
    previousEndDate: userData.subscription?.endDate || null,
    preferredPlan,
    preferredDuration: preferredDuration || 'month',
    message: (message || '').trim().substring(0, 500),
    status: 'pending',
    createdAt: admin.firestore.FieldValue.serverTimestamp(),
    processedAt: null,
    processedBy: null,
    adminResponse: null,
    renewalDuration: null
  };

  const docRef = await db.collection('renewalRequests').add(requestData);

  // Create notification for user
  await createUserNotification(userId, 'request_submitted',
    'Renewal Request Submitted',
    'Your request for ' + preferredPlan.toUpperCase() + ' plan has been submitted. We will review it shortly.',
    { requestId: docRef.id }
  );

  // Log activity
  await logUserActivity(userId, 'renewal_request', { action: 'submitted', preferredPlan, preferredDuration });

  return { success: true, requestId: docRef.id, message: 'Renewal request submitted successfully' };
});

// User: Get their renewal requests
exports.userGetRenewalRequests = functions.https.onCall(async (data, context) => {
  if (!context.auth) {
    throw new functions.https.HttpsError('unauthenticated', 'Must be logged in');
  }

  const userId = context.auth.uid;

  const snapshot = await db.collection('renewalRequests')
    .where('userId', '==', userId)
    .orderBy('createdAt', 'desc')
    .limit(20)
    .get();

  const requests = [];
  snapshot.forEach(doc => {
    const d = doc.data();
    requests.push({
      id: doc.id,
      ...d,
      createdAt: d.createdAt?.toDate?.()?.toISOString() || null,
      processedAt: d.processedAt?.toDate?.()?.toISOString() || null,
      previousEndDate: d.previousEndDate?.toDate?.()?.toISOString() || null
    });
  });

  return { requests };
});

// User: Cancel a pending request
exports.userCancelRenewalRequest = functions.https.onCall(async (data, context) => {
  if (!context.auth) {
    throw new functions.https.HttpsError('unauthenticated', 'Must be logged in');
  }

  const userId = context.auth.uid;
  const { requestId } = data || {};

  if (!requestId) {
    throw new functions.https.HttpsError('invalid-argument', 'Request ID required');
  }

  const requestDoc = await db.collection('renewalRequests').doc(requestId).get();
  if (!requestDoc.exists) {
    throw new functions.https.HttpsError('not-found', 'Request not found');
  }

  const requestData = requestDoc.data();
  if (requestData.userId !== userId) {
    throw new functions.https.HttpsError('permission-denied', 'Not your request');
  }

  if (requestData.status !== 'pending') {
    throw new functions.https.HttpsError('failed-precondition', 'Can only cancel pending requests');
  }

  await db.collection('renewalRequests').doc(requestId).update({
    status: 'cancelled',
    processedAt: admin.firestore.FieldValue.serverTimestamp()
  });

  return { success: true, message: 'Request cancelled' };
});

// User: Get their notifications
exports.userGetNotifications = functions.https.onCall(async (data, context) => {
  if (!context.auth) {
    throw new functions.https.HttpsError('unauthenticated', 'Must be logged in');
  }

  const userId = context.auth.uid;
  const { limit: limitCount = 20, unreadOnly = false } = data || {};

  let query = db.collection('users').doc(userId).collection('notifications');

  // where clause must come before orderBy in Firestore
  if (unreadOnly) {
    query = query.where('read', '==', false);
  }

  query = query.orderBy('createdAt', 'desc').limit(limitCount);

  const snapshot = await query.get();
  const notifications = [];

  snapshot.forEach(doc => {
    const d = doc.data();
    notifications.push({
      id: doc.id,
      ...d,
      createdAt: d.createdAt?.toDate?.()?.toISOString() || null
    });
  });

  // Get total unread count
  const unreadSnapshot = await db.collection('users').doc(userId).collection('notifications')
    .where('read', '==', false)
    .get();

  return { notifications, unreadCount: unreadSnapshot.size };
});

// User: Mark notification as read
exports.userMarkNotificationRead = functions.https.onCall(async (data, context) => {
  if (!context.auth) {
    throw new functions.https.HttpsError('unauthenticated', 'Must be logged in');
  }

  const userId = context.auth.uid;
  const { notificationId, markAllRead = false } = data || {};

  if (markAllRead) {
    const unreadSnapshot = await db.collection('users').doc(userId).collection('notifications')
      .where('read', '==', false)
      .get();

    const batch = db.batch();
    unreadSnapshot.forEach(doc => {
      batch.update(doc.ref, { read: true });
    });
    await batch.commit();

    return { success: true, message: 'All notifications marked as read' };
  }

  if (!notificationId) {
    throw new functions.https.HttpsError('invalid-argument', 'Notification ID required');
  }

  await db.collection('users').doc(userId).collection('notifications').doc(notificationId).update({
    read: true
  });

  return { success: true };
});

// Admin: Get all renewal requests
exports.adminGetRenewalRequests = functions.https.onCall(async (data, context) => {
  await requireAdmin(context);

  try {
    const { status = 'pending', limit: limitCount = 50 } = data || {};

    let query = db.collection('renewalRequests');

    // where clause must come before orderBy in Firestore
    if (status && status !== 'all') {
      query = query.where('status', '==', status);
    }

    query = query.orderBy('createdAt', 'desc').limit(limitCount);

    const snapshot = await query.get();
    const requests = [];

    snapshot.forEach(doc => {
      const d = doc.data();
      requests.push({
        id: doc.id,
        ...d,
        createdAt: d.createdAt?.toDate?.()?.toISOString() || null,
        processedAt: d.processedAt?.toDate?.()?.toISOString() || null,
        previousEndDate: d.previousEndDate?.toDate?.()?.toISOString() || null
      });
    });

    // Get counts by status
    const pendingSnapshot = await db.collection('renewalRequests').where('status', '==', 'pending').get();
    const approvedSnapshot = await db.collection('renewalRequests').where('status', '==', 'approved').get();
    const deniedSnapshot = await db.collection('renewalRequests').where('status', '==', 'denied').get();

    return {
      requests,
      counts: {
        pending: pendingSnapshot.size,
        approved: approvedSnapshot.size,
        denied: deniedSnapshot.size
      }
    };
  } catch (error) {
    console.error('adminGetRenewalRequests error:', error);
    throw new functions.https.HttpsError('internal', 'Failed to get renewal requests: ' + error.message);
  }
});

// Admin: Process (approve/deny) a renewal request
exports.adminProcessRenewalRequest = functions.https.onCall(async (data, context) => {
  const adminUid = await requireAdmin(context);

  const { requestId, action, duration, adminResponse } = data || {};

  if (!requestId) {
    throw new functions.https.HttpsError('invalid-argument', 'Request ID required');
  }

  if (!action || !['approve', 'deny'].includes(action)) {
    throw new functions.https.HttpsError('invalid-argument', 'Action must be approve or deny');
  }

  const requestDoc = await db.collection('renewalRequests').doc(requestId).get();
  if (!requestDoc.exists) {
    throw new functions.https.HttpsError('not-found', 'Request not found');
  }

  const requestData = requestDoc.data();
  if (requestData.status !== 'pending') {
    throw new functions.https.HttpsError('failed-precondition', 'Request already processed');
  }

  const userId = requestData.userId;

  if (action === 'approve') {
    const renewalDuration = duration || requestData.preferredDuration || 'month';
    const plan = requestData.preferredPlan;

    // Calculate new end date
    const now = new Date();
    let endDate = null;

    if (renewalDuration !== 'lifetime') {
      const durationDays = {
        'week': 7,
        'month': 30,
        '3months': 90,
        'year': 365
      };
      endDate = new Date(now.getTime() + (durationDays[renewalDuration] || 30) * 24 * 60 * 60 * 1000);
    }

    // Update user subscription
    const planDoc = await db.collection('subscriptionPlans').doc(plan).get();
    const planLimits = planDoc.exists ? planDoc.data()?.limits || {} : {};
    const defaultToolLimit = 2;

    await db.collection('users').doc(userId).update({
      'subscription.plan': plan,
      'subscription.status': 'active',
      'subscription.duration': renewalDuration,
      'subscription.startDate': admin.firestore.FieldValue.serverTimestamp(),
      'subscription.endDate': endDate ? admin.firestore.Timestamp.fromDate(endDate) : null,
      'usage.warpOptimizer': {
        usedToday: 0,
        limit: planLimits.warpOptimizer?.dailyLimit || defaultToolLimit,
        lastResetAt: admin.firestore.FieldValue.serverTimestamp(),
        cooldownUntil: null
      },
      'usage.competitorAnalysis': {
        usedToday: 0,
        limit: planLimits.competitorAnalysis?.dailyLimit || defaultToolLimit,
        lastResetAt: admin.firestore.FieldValue.serverTimestamp(),
        cooldownUntil: null
      },
      'usage.trendPredictor': {
        usedToday: 0,
        limit: planLimits.trendPredictor?.dailyLimit || defaultToolLimit,
        lastResetAt: admin.firestore.FieldValue.serverTimestamp(),
        cooldownUntil: null
      },
      'usage.thumbnailGenerator': {
        usedToday: 0,
        limit: planLimits.thumbnailGenerator?.dailyLimit || defaultToolLimit,
        lastResetAt: admin.firestore.FieldValue.serverTimestamp(),
        cooldownUntil: null
      }
    });

    // Update request
    await db.collection('renewalRequests').doc(requestId).update({
      status: 'approved',
      processedAt: admin.firestore.FieldValue.serverTimestamp(),
      processedBy: adminUid,
      adminResponse: adminResponse || null,
      renewalDuration
    });

    // Notify user
    const durationLabel = {
      'week': '1 Week',
      'month': '1 Month',
      '3months': '3 Months',
      'year': '1 Year',
      'lifetime': 'Lifetime'
    };

    const endDateStr = endDate ? endDate.toLocaleDateString() : '';
    await createUserNotification(userId, 'request_approved',
      'Subscription Renewed!',
      'Great news! Your ' + plan.toUpperCase() + ' subscription has been renewed for ' + (durationLabel[renewalDuration] || renewalDuration) + '.' + (endDateStr ? ' Valid until ' + endDateStr + '.' : ''),
      { plan, duration: renewalDuration, endDate: endDate?.toISOString() || null }
    );

    // Log activity
    await logUserActivity(userId, 'subscription_change', {
      action: 'renewal_approved',
      plan,
      duration: renewalDuration,
      approvedBy: adminUid
    }, adminUid);

    return {
      success: true,
      message: 'Subscription renewed: ' + plan.toUpperCase() + ' for ' + renewalDuration,
      newEndDate: endDate?.toISOString() || null
    };

  } else {
    // Deny request
    await db.collection('renewalRequests').doc(requestId).update({
      status: 'denied',
      processedAt: admin.firestore.FieldValue.serverTimestamp(),
      processedBy: adminUid,
      adminResponse: adminResponse || null
    });

    // Notify user
    const denyMessage = adminResponse
      ? 'Your renewal request has been reviewed. Response: ' + adminResponse
      : 'Your renewal request has been reviewed. Please contact support for more information.';

    await createUserNotification(userId, 'request_denied',
      'Renewal Request Update',
      denyMessage,
      { reason: adminResponse || null }
    );

    // Log activity
    await logUserActivity(userId, 'renewal_request', { action: 'denied', reason: adminResponse }, adminUid);

    return { success: true, message: 'Request denied' };
  }
});

// ==========================================
// AI TOOLS HUB - CLOUD FUNCTIONS
// ==========================================

// AI Script Studio - Generate full video scripts
exports.generateScript = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  checkRateLimit(uid, 'generateScript', 10);

  const { topic, tone = 'engaging', length = 'medium', includeHook = true, includeCTA = true } = data;

  if (!topic || topic.trim().length < 3) {
    throw new functions.https.HttpsError('invalid-argument', 'Please provide a valid video topic');
  }

  // Token cost: 5 tokens per script
  const tokenCost = 5;

  // Check token balance
  const tokenRef = db.collection('creativeTokens').doc(uid);
  let tokenDoc = await tokenRef.get();
  let balance = 0;

  if (!tokenDoc.exists) {
    const initialTokens = {
      balance: 50,
      rollover: 0,
      plan: 'free',
      monthlyAllocation: 50,
      lastRefresh: admin.firestore.FieldValue.serverTimestamp(),
      createdAt: admin.firestore.FieldValue.serverTimestamp()
    };
    await tokenRef.set(initialTokens);
    balance = 50;
  } else {
    balance = tokenDoc.data().balance || 0;
  }

  if (balance < tokenCost) {
    throw new functions.https.HttpsError('resource-exhausted',
      `Insufficient tokens. Need ${tokenCost}, have ${balance}. Please upgrade your plan.`);
  }

  // Length configurations
  const lengthConfig = {
    short: { minutes: '3-5', words: 500 },
    medium: { minutes: '8-12', words: 1200 },
    long: { minutes: '15-20', words: 2000 }
  };
  const config = lengthConfig[length] || lengthConfig.medium;

  // Tone descriptions
  const toneDescriptions = {
    engaging: 'conversational, energetic, keeps viewers hooked with dynamic pacing and relatable language',
    educational: 'informative, clear explanations, authoritative yet accessible, with structured learning points',
    entertaining: 'fun, humorous, uses storytelling and personality to captivate, includes jokes and pop culture references',
    professional: 'polished, business-appropriate, credible and trustworthy, with data-backed insights'
  };
  const toneDesc = toneDescriptions[tone] || toneDescriptions.engaging;

  try {
    const systemPrompt = `You are a professional YouTube script writer who creates viral, engaging video scripts. Your scripts consistently achieve high watch time and engagement.

Your scripts always include:
- Pattern interrupts to maintain viewer attention
- B-roll suggestions marked with [B-ROLL: description]
- Emphasis markers for key words using *asterisks*
- Natural pauses marked with (pause)
- Speaking pace notes where needed

Format your response as JSON with these exact keys:
{
  "hook": "Opening hook (first 5-10 seconds - the most crucial part)",
  "intro": "Introduction that establishes credibility and previews value",
  "mainContent": "The main body with all key points, transitions, and B-roll markers",
  "cta": "Call-to-action that drives engagement"
}`;

    const userPrompt = `Create a ${config.minutes} minute YouTube script (approximately ${config.words} words) about:

TOPIC: ${topic}

TONE: ${toneDesc}

Requirements:
${includeHook ? '- Start with a powerful hook that creates curiosity or makes a bold claim' : ''}
- Include clear section transitions
- Add [B-ROLL: description] markers for visual suggestions
- Mark emphasis words with *asterisks*
- Include retention markers every 60-90 seconds
${includeCTA ? '- End with a compelling call-to-action for likes, comments, and subscribes' : ''}

Make it feel natural, not scripted. Write like a top YouTuber speaks.`;

    const response = await openai.chat.completions.create({
      model: 'gpt-4',
      messages: [
        { role: 'system', content: systemPrompt },
        { role: 'user', content: userPrompt }
      ],
      temperature: 0.8,
      max_tokens: 3000
    });

    let scriptData;
    const content = response.choices[0]?.message?.content;

    try {
      // Try to parse as JSON
      const jsonMatch = content.match(/\{[\s\S]*\}/);
      if (jsonMatch) {
        scriptData = JSON.parse(jsonMatch[0]);
      } else {
        // Fallback: treat as full script
        scriptData = {
          hook: '',
          intro: '',
          mainContent: content,
          cta: ''
        };
      }
    } catch (parseError) {
      scriptData = {
        hook: '',
        intro: '',
        mainContent: content,
        cta: ''
      };
    }

    // Deduct tokens
    await tokenRef.update({
      balance: admin.firestore.FieldValue.increment(-tokenCost),
      lastUsed: admin.firestore.FieldValue.serverTimestamp()
    });

    // Save to history
    await db.collection('scriptHistory').add({
      userId: uid,
      topic,
      tone,
      length,
      script: scriptData,
      tokenCost,
      createdAt: admin.firestore.FieldValue.serverTimestamp()
    });

    return {
      success: true,
      ...scriptData,
      tokenCost,
      remainingBalance: balance - tokenCost
    };

  } catch (error) {
    console.error('Script generation error:', error);
    throw new functions.https.HttpsError('internal', 'Failed to generate script. Please try again.');
  }
});

// Viral Hook Laboratory - Generate attention-grabbing hooks
exports.generateHooks = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  checkRateLimit(uid, 'generateHooks', 15);

  const { topic, style = 'question', count = 5 } = data;

  if (!topic || topic.trim().length < 3) {
    throw new functions.https.HttpsError('invalid-argument', 'Please provide a valid video topic');
  }

  // Token cost: 3 tokens per generation
  const tokenCost = 3;

  // Check token balance
  const tokenRef = db.collection('creativeTokens').doc(uid);
  let tokenDoc = await tokenRef.get();
  let balance = tokenDoc.exists ? (tokenDoc.data().balance || 0) : 50;

  if (!tokenDoc.exists) {
    await tokenRef.set({
      balance: 50,
      rollover: 0,
      plan: 'free',
      monthlyAllocation: 50,
      lastRefresh: admin.firestore.FieldValue.serverTimestamp(),
      createdAt: admin.firestore.FieldValue.serverTimestamp()
    });
    balance = 50;
  }

  if (balance < tokenCost) {
    throw new functions.https.HttpsError('resource-exhausted',
      `Insufficient tokens. Need ${tokenCost}, have ${balance}.`);
  }

  // Hook style descriptions
  const styleGuides = {
    question: 'Questions that spark curiosity and demand answers. Make viewers think "I need to know this!"',
    controversy: 'Controversial or contrarian statements that challenge common beliefs. Use "Actually, everything you know about X is wrong..."',
    promise: 'Clear value propositions that promise specific outcomes. "In the next X minutes, you\'ll learn..."',
    story: 'Personal story openers that create emotional connection. Start mid-action for maximum impact.',
    statistic: 'Shocking statistics or data points that make viewers stop scrolling. Use specific numbers.',
    challenge: 'Direct challenges to the viewer that engage their ego. "I bet you can\'t..." or "Most people fail at..."'
  };

  const styleGuide = styleGuides[style] || styleGuides.question;

  try {
    const systemPrompt = `You are a viral content expert who specializes in YouTube hooks. You understand that the first 3-5 seconds determine if a viewer stays or leaves.

Your hooks achieve:
- 80%+ retention past the first 30 seconds
- High curiosity gaps that MUST be resolved
- Emotional triggers that stop the scroll

Always provide hooks with predicted effectiveness scores and explanations.

Respond in JSON format:
{
  "hooks": [
    {
      "text": "The hook text",
      "score": 85,
      "explanation": "Why this hook works"
    }
  ]
}`;

    const userPrompt = `Generate ${count} viral YouTube hooks for this video topic:

TOPIC: ${topic}

STYLE: ${styleGuide}

Requirements:
- Each hook should be 1-2 sentences max (speakable in 5 seconds)
- Create curiosity gaps that MUST be resolved
- Use power words that trigger emotional responses
- Make each hook distinctly different
- Score each hook 1-100 based on predicted viral potential
- Explain WHY each hook would work

Think like MrBeast, MKBHD, and other top creators when crafting these.`;

    const response = await openai.chat.completions.create({
      model: 'gpt-4',
      messages: [
        { role: 'system', content: systemPrompt },
        { role: 'user', content: userPrompt }
      ],
      temperature: 0.9,
      max_tokens: 1500
    });

    let hooksData;
    const content = response.choices[0]?.message?.content;

    try {
      const jsonMatch = content.match(/\{[\s\S]*\}/);
      if (jsonMatch) {
        hooksData = JSON.parse(jsonMatch[0]);
      } else {
        hooksData = { hooks: [{ text: content, score: 70, explanation: 'Generated hook' }] };
      }
    } catch (parseError) {
      hooksData = { hooks: [{ text: content, score: 70, explanation: 'Generated hook' }] };
    }

    // Deduct tokens
    await tokenRef.update({
      balance: admin.firestore.FieldValue.increment(-tokenCost),
      lastUsed: admin.firestore.FieldValue.serverTimestamp()
    });

    // Save to history
    await db.collection('hookHistory').add({
      userId: uid,
      topic,
      style,
      count,
      hooks: hooksData.hooks,
      tokenCost,
      createdAt: admin.firestore.FieldValue.serverTimestamp()
    });

    return {
      success: true,
      hooks: hooksData.hooks,
      tokenCost,
      remainingBalance: balance - tokenCost
    };

  } catch (error) {
    console.error('Hook generation error:', error);
    throw new functions.https.HttpsError('internal', 'Failed to generate hooks. Please try again.');
  }
});

// Content Multiplier - Repurpose video content into multiple formats
exports.multiplyContent = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  checkRateLimit(uid, 'multiplyContent', 5);

  const { transcript, formats = ['shorts', 'twitter', 'blog'] } = data;

  if (!transcript || transcript.trim().length < 100) {
    throw new functions.https.HttpsError('invalid-argument', 'Please provide a transcript with at least 100 characters');
  }

  // Token cost: 8 tokens per multiply
  const tokenCost = 8;

  // Check token balance
  const tokenRef = db.collection('creativeTokens').doc(uid);
  let tokenDoc = await tokenRef.get();
  let balance = tokenDoc.exists ? (tokenDoc.data().balance || 0) : 50;

  if (!tokenDoc.exists) {
    await tokenRef.set({
      balance: 50,
      rollover: 0,
      plan: 'free',
      monthlyAllocation: 50,
      lastRefresh: admin.firestore.FieldValue.serverTimestamp(),
      createdAt: admin.firestore.FieldValue.serverTimestamp()
    });
    balance = 50;
  }

  if (balance < tokenCost) {
    throw new functions.https.HttpsError('resource-exhausted',
      `Insufficient tokens. Need ${tokenCost}, have ${balance}.`);
  }

  // Format instructions
  const formatInstructions = {
    shorts: 'Extract 3 viral YouTube Shorts scripts (60 seconds each). Focus on the most shareable, hook-worthy moments. Include visual suggestions.',
    twitter: 'Create a 10-15 tweet thread that tells the story of the video. Each tweet should be standalone but connected. Use hooks and cliffhangers between tweets.',
    blog: 'Write a full SEO-optimized blog post (800-1200 words) with headers, bullet points, and a compelling introduction. Include meta description.',
    quotes: 'Extract 5 quote-worthy statements that would work as shareable graphics. Make them punchy and memorable.',
    email: 'Create a newsletter email summarizing the key insights. Include a compelling subject line, preview text, and clear CTA.',
    linkedin: 'Write a professional LinkedIn post version of the key insights. Include engagement prompts and relevant hashtags.'
  };

  const selectedFormats = formats.filter(f => formatInstructions[f]);
  if (selectedFormats.length === 0) {
    throw new functions.https.HttpsError('invalid-argument', 'Please select at least one valid format');
  }

  try {
    const systemPrompt = `You are a content repurposing expert. You transform long-form video content into multiple formats while maintaining the core message and maximizing engagement for each platform.

Always preserve the creator's voice and key insights while adapting to platform-specific best practices.

Respond in JSON format with each requested format as a key.`;

    let formatPrompts = selectedFormats.map(f => `${f.toUpperCase()}: ${formatInstructions[f]}`).join('\n\n');

    const userPrompt = `Transform this video transcript into multiple content formats:

TRANSCRIPT:
${transcript.substring(0, 8000)}

REQUESTED FORMATS:
${formatPrompts}

Create high-quality, platform-optimized content for each format. Maintain the original insights but adapt the style for each platform.`;

    const response = await openai.chat.completions.create({
      model: 'gpt-4',
      messages: [
        { role: 'system', content: systemPrompt },
        { role: 'user', content: userPrompt }
      ],
      temperature: 0.7,
      max_tokens: 4000
    });

    let contentData;
    const content = response.choices[0]?.message?.content;

    try {
      const jsonMatch = content.match(/\{[\s\S]*\}/);
      if (jsonMatch) {
        contentData = JSON.parse(jsonMatch[0]);
      } else {
        contentData = { content: content };
      }
    } catch (parseError) {
      contentData = { content: content };
    }

    // Deduct tokens
    await tokenRef.update({
      balance: admin.firestore.FieldValue.increment(-tokenCost),
      lastUsed: admin.firestore.FieldValue.serverTimestamp()
    });

    // Save to history
    await db.collection('contentMultiplierHistory').add({
      userId: uid,
      transcriptPreview: transcript.substring(0, 200),
      formats: selectedFormats,
      content: contentData,
      tokenCost,
      createdAt: admin.firestore.FieldValue.serverTimestamp()
    });

    return {
      success: true,
      content: contentData,
      tokenCost,
      remainingBalance: balance - tokenCost
    };

  } catch (error) {
    console.error('Content multiplier error:', error);
    throw new functions.https.HttpsError('internal', 'Failed to multiply content. Please try again.');
  }
});

// Thumbnail A/B Arena - Analyze and predict thumbnail CTR
exports.analyzeThumbnails = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  checkRateLimit(uid, 'analyzeThumbnails', 10);

  const { thumbnailA, thumbnailB } = data;

  if (!thumbnailA?.base64 || !thumbnailB?.base64) {
    throw new functions.https.HttpsError('invalid-argument', 'Please provide two thumbnails to compare');
  }

  // Token cost: 4 tokens per analysis
  const tokenCost = 4;

  // Check token balance
  const tokenRef = db.collection('creativeTokens').doc(uid);
  let tokenDoc = await tokenRef.get();
  let balance = tokenDoc.exists ? (tokenDoc.data().balance || 0) : 50;

  if (!tokenDoc.exists) {
    await tokenRef.set({
      balance: 50,
      rollover: 0,
      plan: 'free',
      monthlyAllocation: 50,
      lastRefresh: admin.firestore.FieldValue.serverTimestamp(),
      createdAt: admin.firestore.FieldValue.serverTimestamp()
    });
    balance = 50;
  }

  if (balance < tokenCost) {
    throw new functions.https.HttpsError('resource-exhausted',
      `Insufficient tokens. Need ${tokenCost}, have ${balance}.`);
  }

  try {
    // Use Gemini Vision for thumbnail analysis
    const geminiApiKey = functions.config().gemini?.key;
    if (!geminiApiKey) {
      throw new functions.https.HttpsError('failed-precondition', 'Vision service not configured');
    }

    const ai = new GoogleGenAI({ apiKey: geminiApiKey });

    const analysisPrompt = `You are a YouTube thumbnail CTR expert. Analyze these two thumbnails and predict which will perform better.

For each thumbnail, evaluate:
1. Visual hierarchy and focal points
2. Color contrast and vibrancy
3. Emotional impact and curiosity triggers
4. Text readability (if any)
5. Face/expression effectiveness (if any)
6. Mobile-friendliness (will it work at small sizes?)
7. Click-worthiness and curiosity gap

Respond in JSON format:
{
  "thumbnailA": {
    "score": 75,
    "strengths": ["Clear focal point", "Good contrast"],
    "weaknesses": ["Text too small", "Low emotional impact"]
  },
  "thumbnailB": {
    "score": 82,
    "strengths": ["Strong emotion", "Vibrant colors"],
    "weaknesses": ["Busy background"]
  },
  "winner": "b",
  "winnerScore": 9.3,
  "recommendations": [
    "Add more contrast to Thumbnail A",
    "Consider larger text for both"
  ]
}`;

    const result = await ai.models.generateContent({
      model: 'gemini-2.0-flash',
      contents: [{
        role: 'user',
        parts: [
          { inlineData: { mimeType: thumbnailA.mimeType || 'image/png', data: thumbnailA.base64 } },
          { text: 'This is Thumbnail A' },
          { inlineData: { mimeType: thumbnailB.mimeType || 'image/png', data: thumbnailB.base64 } },
          { text: 'This is Thumbnail B' },
          { text: analysisPrompt }
        ]
      }]
    });

    let analysisData;
    const content = result.candidates?.[0]?.content?.parts?.[0]?.text || '';

    try {
      const jsonMatch = content.match(/\{[\s\S]*\}/);
      if (jsonMatch) {
        analysisData = JSON.parse(jsonMatch[0]);
      } else {
        analysisData = {
          thumbnailA: { score: 70, strengths: ['Analyzed'], weaknesses: ['See recommendations'] },
          thumbnailB: { score: 70, strengths: ['Analyzed'], weaknesses: ['See recommendations'] },
          winner: 'tie',
          winnerScore: 0,
          recommendations: [content]
        };
      }
    } catch (parseError) {
      analysisData = {
        thumbnailA: { score: 70, strengths: ['Analyzed'], weaknesses: [] },
        thumbnailB: { score: 70, strengths: ['Analyzed'], weaknesses: [] },
        winner: 'tie',
        winnerScore: 0,
        recommendations: ['Analysis completed - see details above']
      };
    }

    // Deduct tokens
    await tokenRef.update({
      balance: admin.firestore.FieldValue.increment(-tokenCost),
      lastUsed: admin.firestore.FieldValue.serverTimestamp()
    });

    // Save to history (without storing full images)
    await db.collection('thumbnailTestHistory').add({
      userId: uid,
      analysis: analysisData,
      winner: analysisData.winner,
      tokenCost,
      createdAt: admin.firestore.FieldValue.serverTimestamp()
    });

    return {
      success: true,
      ...analysisData,
      tokenCost,
      remainingBalance: balance - tokenCost
    };

  } catch (error) {
    console.error('Thumbnail analysis error:', error);
    throw new functions.https.HttpsError('internal', 'Failed to analyze thumbnails. Please try again.');
  }
});

// ==============================================================================
// TREND HIJACKER - Find trending topics in your niche
// ==============================================================================
exports.generateTrendReport = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  checkRateLimit(uid, 'generateTrendReport', 10);

  const { niche, region = 'US', timeframe = 'week' } = data;

  if (!niche || niche.trim().length < 2) {
    throw new functions.https.HttpsError('invalid-argument', 'Please provide a valid niche or topic area');
  }

  // Token cost: 6 tokens per trend analysis
  const tokenCost = 6;

  // Check token balance
  const tokenRef = db.collection('creativeTokens').doc(uid);
  let tokenDoc = await tokenRef.get();
  let balance = tokenDoc.exists ? (tokenDoc.data().balance || 0) : 50;

  if (!tokenDoc.exists) {
    await tokenRef.set({
      balance: 50,
      rollover: 0,
      plan: 'free',
      monthlyAllocation: 50,
      lastRefresh: admin.firestore.FieldValue.serverTimestamp(),
      createdAt: admin.firestore.FieldValue.serverTimestamp()
    });
    balance = 50;
  }

  if (balance < tokenCost) {
    throw new functions.https.HttpsError('resource-exhausted',
      `Insufficient tokens. Need ${tokenCost}, have ${balance}.`);
  }

  try {
    const openaiApiKey = functions.config().openai?.key;
    if (!openaiApiKey) {
      throw new functions.https.HttpsError('failed-precondition', 'AI service not configured');
    }

    const timeframeText = timeframe === 'day' ? 'today (last 24 hours)' :
                          timeframe === 'week' ? 'this week (last 7 days)' :
                          'this month (last 30 days)';

    const regionText = region === 'GLOBAL' ? 'globally' : `in ${region}`;

    const prompt = `You are a trend analysis expert specializing in YouTube content strategy. Analyze current trends ${regionText} for the "${niche}" niche ${timeframeText}.

Identify 5-7 trending topics that a YouTube creator in this niche should create content about RIGHT NOW to capitalize on rising interest.

For each trend, provide:
1. The specific trending topic
2. A trend score (0-100) based on current momentum
3. Urgency level (high/medium/low) - how quickly they need to act
4. A brief description of why this is trending
5. 2-3 content angles they could take
6. A suggested video title that would perform well

Consider:
- Current news and events
- Seasonal relevance
- Platform-specific trends (YouTube, TikTok, Twitter discussions)
- Search volume patterns
- Competitor content gaps

Respond in JSON format:
{
  "trends": [
    {
      "topic": "Topic name",
      "score": 85,
      "urgency": "high",
      "description": "Why this is trending now",
      "angles": ["Angle 1", "Angle 2", "Angle 3"],
      "suggestedTitle": "A clickable video title"
    }
  ],
  "insights": "Overall market insight about the niche right now"
}`;

    const response = await fetch('https://api.openai.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${openaiApiKey}`,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        model: 'gpt-4o-mini',
        messages: [
          {
            role: 'system',
            content: 'You are a YouTube trend analyst who identifies emerging trends and viral opportunities. Always respond with valid JSON.'
          },
          {
            role: 'user',
            content: prompt
          }
        ],
        temperature: 0.8,
        max_tokens: 2000
      })
    });

    if (!response.ok) {
      const errorText = await response.text();
      console.error('OpenAI API error:', errorText);
      throw new Error('AI service error');
    }

    const aiResponse = await response.json();
    const content = aiResponse.choices?.[0]?.message?.content || '';

    let trendData;
    try {
      const jsonMatch = content.match(/\{[\s\S]*\}/);
      if (jsonMatch) {
        trendData = JSON.parse(jsonMatch[0]);
      } else {
        throw new Error('No JSON found');
      }
    } catch (parseError) {
      // Fallback structure if parsing fails
      trendData = {
        trends: [
          {
            topic: niche + ' trends',
            score: 75,
            urgency: 'medium',
            description: 'Current trending topic in your niche',
            angles: ['Educational breakdown', 'News reaction', 'How-to guide'],
            suggestedTitle: `The ${niche} Trend Everyone Is Talking About Right Now`
          }
        ],
        insights: content
      };
    }

    // Deduct tokens
    await tokenRef.update({
      balance: admin.firestore.FieldValue.increment(-tokenCost),
      lastUsed: admin.firestore.FieldValue.serverTimestamp()
    });

    // Save to history
    await db.collection('trendHistory').add({
      userId: uid,
      niche: niche.trim(),
      region,
      timeframe,
      trends: trendData.trends,
      insights: trendData.insights,
      tokenCost,
      createdAt: admin.firestore.FieldValue.serverTimestamp()
    });

    return {
      success: true,
      trends: trendData.trends,
      insights: trendData.insights,
      tokenCost,
      remainingBalance: balance - tokenCost
    };

  } catch (error) {
    console.error('Trend analysis error:', error);
    throw new functions.https.HttpsError('internal', 'Failed to analyze trends. Please try again.');
  }
});

// ==============================================================================
// CONTENT GAP FINDER - Discover untapped content opportunities
// ==============================================================================
exports.findContentGaps = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  checkRateLimit(uid, 'findContentGaps', 10);

  const { niche, competitors = '', depth = 'moderate' } = data;

  if (!niche || niche.trim().length < 2) {
    throw new functions.https.HttpsError('invalid-argument', 'Please provide a valid niche or topic area');
  }

  // Token cost: 6 tokens per gap analysis
  const tokenCost = 6;

  // Check token balance
  const tokenRef = db.collection('creativeTokens').doc(uid);
  let tokenDoc = await tokenRef.get();
  let balance = tokenDoc.exists ? (tokenDoc.data().balance || 0) : 50;

  if (!tokenDoc.exists) {
    await tokenRef.set({
      balance: 50,
      rollover: 0,
      plan: 'free',
      monthlyAllocation: 50,
      lastRefresh: admin.firestore.FieldValue.serverTimestamp(),
      createdAt: admin.firestore.FieldValue.serverTimestamp()
    });
    balance = 50;
  }

  if (balance < tokenCost) {
    throw new functions.https.HttpsError('resource-exhausted',
      `Insufficient tokens. Need ${tokenCost}, have ${balance}.`);
  }

  try {
    const openaiApiKey = functions.config().openai?.key;
    if (!openaiApiKey) {
      throw new functions.https.HttpsError('failed-precondition', 'AI service not configured');
    }

    const depthText = depth === 'quick' ? 'top 4-5 opportunities' :
                      depth === 'deep' ? 'comprehensive analysis with 8-10 opportunities' :
                      '5-7 balanced opportunities';

    const competitorText = competitors.trim()
      ? `\n\nCompetitors to analyze for gaps: ${competitors}`
      : '';

    const prompt = `You are a YouTube content strategy expert specializing in finding untapped content opportunities. Analyze the "${niche}" niche and identify ${depthText}.${competitorText}

Find content gaps - topics that have:
- High search interest but low quality existing content
- Underserved audience segments
- Questions that aren't being answered well
- Emerging subtopics with growth potential
- Unique angles competitors haven't explored

For each gap opportunity, provide:
1. The topic/gap opportunity
2. Difficulty level (easy/medium/hard) to rank for
3. Potential score (0-100) based on opportunity size
4. Why this is a gap (what's missing in current content)
5. Description of the opportunity
6. 2-3 specific video title ideas

Respond in JSON format:
{
  "gaps": [
    {
      "topic": "Gap topic name",
      "difficulty": "easy",
      "potential": 85,
      "reason": "Why this content gap exists",
      "description": "What kind of content would fill this gap",
      "suggestedTitles": [
        "Video Title Idea 1",
        "Video Title Idea 2",
        "Video Title Idea 3"
      ]
    }
  ],
  "summary": "Overall market summary and strategy recommendation"
}`;

    const response = await fetch('https://api.openai.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${openaiApiKey}`,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        model: 'gpt-4o-mini',
        messages: [
          {
            role: 'system',
            content: 'You are a YouTube content strategist who identifies underserved topics and content gaps. Always respond with valid JSON.'
          },
          {
            role: 'user',
            content: prompt
          }
        ],
        temperature: 0.8,
        max_tokens: 2500
      })
    });

    if (!response.ok) {
      const errorText = await response.text();
      console.error('OpenAI API error:', errorText);
      throw new Error('AI service error');
    }

    const aiResponse = await response.json();
    const content = aiResponse.choices?.[0]?.message?.content || '';

    let gapData;
    try {
      const jsonMatch = content.match(/\{[\s\S]*\}/);
      if (jsonMatch) {
        gapData = JSON.parse(jsonMatch[0]);
      } else {
        throw new Error('No JSON found');
      }
    } catch (parseError) {
      // Fallback structure if parsing fails
      gapData = {
        gaps: [
          {
            topic: 'Beginner-friendly ' + niche + ' content',
            difficulty: 'easy',
            potential: 80,
            reason: 'Most content assumes prior knowledge',
            description: 'Create truly beginner-friendly content for newcomers',
            suggestedTitles: [
              `${niche} for Complete Beginners - Everything You Need to Know`,
              `I Tried Learning ${niche} From Scratch - Here's What Happened`,
              `The ${niche} Beginner's Guide Everyone Wishes They Had`
            ]
          }
        ],
        summary: content
      };
    }

    // Deduct tokens
    await tokenRef.update({
      balance: admin.firestore.FieldValue.increment(-tokenCost),
      lastUsed: admin.firestore.FieldValue.serverTimestamp()
    });

    // Save to history
    await db.collection('contentGapHistory').add({
      userId: uid,
      niche: niche.trim(),
      competitors: competitors.trim(),
      depth,
      gaps: gapData.gaps,
      summary: gapData.summary,
      tokenCost,
      createdAt: admin.firestore.FieldValue.serverTimestamp()
    });

    return {
      success: true,
      gaps: gapData.gaps,
      summary: gapData.summary,
      tokenCost,
      remainingBalance: balance - tokenCost
    };

  } catch (error) {
    console.error('Content gap analysis error:', error);
    throw new functions.https.HttpsError('internal', 'Failed to find content gaps. Please try again.');
  }
});

// ==============================================================================
// AUDIENCE DNA ANALYZER - Deep audience insights
// ==============================================================================
exports.analyzeAudienceDNA = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  checkRateLimit(uid, 'analyzeAudienceDNA', 10);

  const { niche, channelUrl = '', depth = 'standard' } = data;

  if (!niche || niche.trim().length < 2) {
    throw new functions.https.HttpsError('invalid-argument', 'Please provide a valid niche or content area');
  }

  // Token cost: 7 tokens per analysis
  const tokenCost = 7;

  // Check token balance
  const tokenRef = db.collection('creativeTokens').doc(uid);
  let tokenDoc = await tokenRef.get();
  let balance = tokenDoc.exists ? (tokenDoc.data().balance || 0) : 50;

  if (!tokenDoc.exists) {
    await tokenRef.set({
      balance: 50,
      rollover: 0,
      plan: 'free',
      monthlyAllocation: 50,
      lastRefresh: admin.firestore.FieldValue.serverTimestamp(),
      createdAt: admin.firestore.FieldValue.serverTimestamp()
    });
    balance = 50;
  }

  if (balance < tokenCost) {
    throw new functions.https.HttpsError('resource-exhausted',
      `Insufficient tokens. Need ${tokenCost}, have ${balance}.`);
  }

  try {
    const openaiApiKey = functions.config().openai?.key;
    if (!openaiApiKey) {
      throw new functions.https.HttpsError('failed-precondition', 'AI service not configured');
    }

    const depthText = depth === 'quick' ? 'key demographics only' :
                      depth === 'deep' ? 'comprehensive psychographic analysis' :
                      'full audience profile';

    const channelContext = channelUrl ? `The creator's channel: ${channelUrl}` : '';

    const prompt = `You are an expert audience research analyst. Create a detailed audience DNA profile for a YouTube creator in the "${niche}" niche. Provide ${depthText}. ${channelContext}

Analyze and provide:
1. Demographics: age range, gender split, primary locations, income level
2. Interests & hobbies related to the niche
3. Pain points and challenges they face
4. Content preferences (formats, length, tone, peak watch times)
5. Actionable recommendations for content

Respond in JSON format:
{
  "demographics": {
    "ageRange": "25-34",
    "gender": "60% male, 40% female",
    "location": "United States, UK, Canada",
    "income": "Middle income"
  },
  "interests": ["Interest 1", "Interest 2", "Interest 3", "Interest 4", "Interest 5"],
  "painPoints": ["Pain point 1", "Pain point 2", "Pain point 3"],
  "contentPreferences": {
    "formats": ["Tutorials", "Reviews", "Vlogs"],
    "length": "10-15 minutes optimal",
    "watchTime": "Evenings and weekends",
    "tone": "Friendly and educational"
  },
  "recommendations": [
    "Recommendation 1",
    "Recommendation 2",
    "Recommendation 3"
  ]
}`;

    const response = await fetch('https://api.openai.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${openaiApiKey}`,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        model: 'gpt-4o-mini',
        messages: [
          {
            role: 'system',
            content: 'You are an audience research expert who creates detailed viewer personas. Always respond with valid JSON.'
          },
          {
            role: 'user',
            content: prompt
          }
        ],
        temperature: 0.7,
        max_tokens: 2000
      })
    });

    if (!response.ok) {
      throw new Error('AI service error');
    }

    const aiResponse = await response.json();
    const content = aiResponse.choices?.[0]?.message?.content || '';

    let audienceData;
    try {
      const jsonMatch = content.match(/\{[\s\S]*\}/);
      if (jsonMatch) {
        audienceData = JSON.parse(jsonMatch[0]);
      } else {
        throw new Error('No JSON found');
      }
    } catch (parseError) {
      audienceData = {
        demographics: { ageRange: '25-44', gender: 'Mixed', location: 'Global', income: 'Varied' },
        interests: ['Related topics', 'Learning', 'Entertainment'],
        painPoints: ['Finding quality content', 'Time management'],
        contentPreferences: { formats: ['Various'], length: '10-20 minutes', watchTime: 'Flexible', tone: 'Engaging' },
        recommendations: [content]
      };
    }

    // Deduct tokens
    await tokenRef.update({
      balance: admin.firestore.FieldValue.increment(-tokenCost),
      lastUsed: admin.firestore.FieldValue.serverTimestamp()
    });

    // Save to history
    await db.collection('audienceHistory').add({
      userId: uid,
      niche: niche.trim(),
      channelUrl,
      depth,
      result: audienceData,
      tokenCost,
      createdAt: admin.firestore.FieldValue.serverTimestamp()
    });

    return {
      success: true,
      ...audienceData,
      tokenCost,
      remainingBalance: balance - tokenCost
    };

  } catch (error) {
    console.error('Audience analysis error:', error);
    throw new functions.https.HttpsError('internal', 'Failed to analyze audience. Please try again.');
  }
});

// ==============================================================================
// COLLAB MATCHMAKER - Find collaboration partners
// ==============================================================================
exports.findCollabPartners = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  checkRateLimit(uid, 'findCollabPartners', 10);

  const { niche, channelSize = 'any', contentStyle = 'any' } = data;

  if (!niche || niche.trim().length < 2) {
    throw new functions.https.HttpsError('invalid-argument', 'Please provide a valid niche or content area');
  }

  // Token cost: 5 tokens per search
  const tokenCost = 5;

  // Check token balance
  const tokenRef = db.collection('creativeTokens').doc(uid);
  let tokenDoc = await tokenRef.get();
  let balance = tokenDoc.exists ? (tokenDoc.data().balance || 0) : 50;

  if (!tokenDoc.exists) {
    await tokenRef.set({
      balance: 50,
      rollover: 0,
      plan: 'free',
      monthlyAllocation: 50,
      lastRefresh: admin.firestore.FieldValue.serverTimestamp(),
      createdAt: admin.firestore.FieldValue.serverTimestamp()
    });
    balance = 50;
  }

  if (balance < tokenCost) {
    throw new functions.https.HttpsError('resource-exhausted',
      `Insufficient tokens. Need ${tokenCost}, have ${balance}.`);
  }

  try {
    const openaiApiKey = functions.config().openai?.key;
    if (!openaiApiKey) {
      throw new functions.https.HttpsError('failed-precondition', 'AI service not configured');
    }

    const sizeText = channelSize === 'any' ? 'any size' :
                     channelSize === 'small' ? '1K-10K subscribers' :
                     channelSize === 'medium' ? '10K-100K subscribers' :
                     '100K+ subscribers';

    const styleText = contentStyle === 'any' ? 'any style' : contentStyle;

    const prompt = `You are a YouTube collaboration strategist. Find 5 ideal collaboration partner TYPES (not specific channels) for a creator in the "${niche}" niche.

Target partner size: ${sizeText}
Content style preference: ${styleText}

For each potential partner type, provide:
1. Type of creator (e.g., "Tech Reviewers", "Lifestyle Vloggers")
2. Compatibility score (0-100)
3. Why they would be a good match
4. Their typical audience size range
5. 3 collaboration video ideas
6. A personalized outreach email template

Respond in JSON format:
{
  "matches": [
    {
      "creatorType": "Creator type name",
      "compatibility": 85,
      "reason": "Why this is a great match",
      "audienceSize": "10K-50K typically",
      "collabIdeas": ["Idea 1", "Idea 2", "Idea 3"],
      "outreachTemplate": "Hi [Name],\\n\\nI love your content about...\\n\\nWould you be interested in...\\n\\nBest,\\n[Your Name]"
    }
  ],
  "tips": "General collaboration tips for this niche"
}`;

    const response = await fetch('https://api.openai.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${openaiApiKey}`,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        model: 'gpt-4o-mini',
        messages: [
          {
            role: 'system',
            content: 'You are a YouTube collaboration expert who matches creators for mutual growth. Always respond with valid JSON.'
          },
          {
            role: 'user',
            content: prompt
          }
        ],
        temperature: 0.8,
        max_tokens: 2500
      })
    });

    if (!response.ok) {
      throw new Error('AI service error');
    }

    const aiResponse = await response.json();
    const content = aiResponse.choices?.[0]?.message?.content || '';

    let collabData;
    try {
      const jsonMatch = content.match(/\{[\s\S]*\}/);
      if (jsonMatch) {
        collabData = JSON.parse(jsonMatch[0]);
      } else {
        throw new Error('No JSON found');
      }
    } catch (parseError) {
      collabData = {
        matches: [{
          creatorType: 'Complementary creators in ' + niche,
          compatibility: 75,
          reason: 'Shared audience interests',
          audienceSize: 'Various',
          collabIdeas: ['Joint video', 'Guest appearance', 'Challenge video'],
          outreachTemplate: 'Hi! I love your content and think we could create something great together. Would you be interested in collaborating?'
        }],
        tips: content
      };
    }

    // Deduct tokens
    await tokenRef.update({
      balance: admin.firestore.FieldValue.increment(-tokenCost),
      lastUsed: admin.firestore.FieldValue.serverTimestamp()
    });

    // Save to history
    await db.collection('collabHistory').add({
      userId: uid,
      niche: niche.trim(),
      channelSize,
      contentStyle,
      matches: collabData.matches,
      tips: collabData.tips,
      tokenCost,
      createdAt: admin.firestore.FieldValue.serverTimestamp()
    });

    return {
      success: true,
      matches: collabData.matches,
      tips: collabData.tips,
      tokenCost,
      remainingBalance: balance - tokenCost
    };

  } catch (error) {
    console.error('Collab matchmaker error:', error);
    throw new functions.https.HttpsError('internal', 'Failed to find collaboration partners. Please try again.');
  }
});

// ==============================================================================
// REVENUE MAXIMIZER PRO - Maximize earnings
// ==============================================================================
exports.analyzeRevenue = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  checkRateLimit(uid, 'analyzeRevenue', 10);

  const { niche, audienceSize = 'small', currentMethods = [] } = data;

  if (!niche || niche.trim().length < 2) {
    throw new functions.https.HttpsError('invalid-argument', 'Please provide a valid niche or content area');
  }

  // Token cost: 8 tokens per analysis
  const tokenCost = 8;

  // Check token balance
  const tokenRef = db.collection('creativeTokens').doc(uid);
  let tokenDoc = await tokenRef.get();
  let balance = tokenDoc.exists ? (tokenDoc.data().balance || 0) : 50;

  if (!tokenDoc.exists) {
    await tokenRef.set({
      balance: 50,
      rollover: 0,
      plan: 'free',
      monthlyAllocation: 50,
      lastRefresh: admin.firestore.FieldValue.serverTimestamp(),
      createdAt: admin.firestore.FieldValue.serverTimestamp()
    });
    balance = 50;
  }

  if (balance < tokenCost) {
    throw new functions.https.HttpsError('resource-exhausted',
      `Insufficient tokens. Need ${tokenCost}, have ${balance}.`);
  }

  try {
    const openaiApiKey = functions.config().openai?.key;
    if (!openaiApiKey) {
      throw new functions.https.HttpsError('failed-precondition', 'AI service not configured');
    }

    const sizeText = audienceSize === 'starter' ? '0-1K subscribers' :
                     audienceSize === 'small' ? '1K-10K subscribers' :
                     audienceSize === 'medium' ? '10K-100K subscribers' :
                     '100K+ subscribers';

    const currentMethodsText = currentMethods.length > 0
      ? `Currently using: ${currentMethods.join(', ')}`
      : 'Not currently monetizing';

    const prompt = `You are a YouTube monetization expert. Create a comprehensive revenue maximization strategy for a creator in the "${niche}" niche.

Audience size: ${sizeText}
${currentMethodsText}

Identify 5-6 revenue opportunities with:
1. Revenue stream name
2. Priority (high/medium/low)
3. Estimated monthly revenue potential
4. Description of the opportunity
5. Step-by-step implementation guide
6. Recommended tools/platforms

Also provide:
- A pricing guide for sponsorships at their level
- A sponsor pitch email template

Respond in JSON format:
{
  "potentialMonthly": "500-2000",
  "opportunities": [
    {
      "name": "Revenue stream name",
      "icon": "ðŸ’°",
      "priority": "high",
      "estimatedRevenue": "200-500",
      "description": "What this opportunity is",
      "steps": ["Step 1", "Step 2", "Step 3"],
      "tools": ["Tool 1", "Tool 2"]
    }
  ],
  "pricingGuide": "Sponsorship pricing guide text...",
  "sponsorPitch": "Email template for reaching out to sponsors..."
}`;

    const response = await fetch('https://api.openai.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${openaiApiKey}`,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        model: 'gpt-4o-mini',
        messages: [
          {
            role: 'system',
            content: 'You are a YouTube monetization expert who helps creators maximize revenue. Always respond with valid JSON.'
          },
          {
            role: 'user',
            content: prompt
          }
        ],
        temperature: 0.7,
        max_tokens: 3000
      })
    });

    if (!response.ok) {
      throw new Error('AI service error');
    }

    const aiResponse = await response.json();
    const content = aiResponse.choices?.[0]?.message?.content || '';

    let revenueData;
    try {
      const jsonMatch = content.match(/\{[\s\S]*\}/);
      if (jsonMatch) {
        revenueData = JSON.parse(jsonMatch[0]);
      } else {
        throw new Error('No JSON found');
      }
    } catch (parseError) {
      revenueData = {
        potentialMonthly: 'Varies',
        opportunities: [{
          name: 'Multiple revenue streams',
          icon: 'ðŸ’°',
          priority: 'high',
          estimatedRevenue: 'Varies',
          description: 'Explore various monetization options',
          steps: ['Research options', 'Start with one method', 'Expand gradually'],
          tools: ['YouTube Studio', 'Various platforms']
        }],
        pricingGuide: content,
        sponsorPitch: 'Contact for personalized template'
      };
    }

    // Deduct tokens
    await tokenRef.update({
      balance: admin.firestore.FieldValue.increment(-tokenCost),
      lastUsed: admin.firestore.FieldValue.serverTimestamp()
    });

    // Save to history
    await db.collection('revenueHistory').add({
      userId: uid,
      niche: niche.trim(),
      audienceSize,
      currentMethods,
      result: revenueData,
      tokenCost,
      createdAt: admin.firestore.FieldValue.serverTimestamp()
    });

    return {
      success: true,
      ...revenueData,
      tokenCost,
      remainingBalance: balance - tokenCost
    };

  } catch (error) {
    console.error('Revenue analysis error:', error);
    throw new functions.https.HttpsError('internal', 'Failed to analyze revenue opportunities. Please try again.');
  }
});

// ==============================================================================
// AI VIDEO COACH - Personal YouTube mentor
// ==============================================================================
exports.getVideoCoaching = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  checkRateLimit(uid, 'getVideoCoaching', 10);

  const { videoUrl = '', transcript = '', challenge = '', focusArea = 'general' } = data;

  if (!challenge && !transcript && !videoUrl) {
    throw new functions.https.HttpsError('invalid-argument', 'Please provide a video URL, transcript, or describe your challenge');
  }

  // Token cost: 10 tokens per coaching session
  const tokenCost = 10;

  // Check token balance
  const tokenRef = db.collection('creativeTokens').doc(uid);
  let tokenDoc = await tokenRef.get();
  let balance = tokenDoc.exists ? (tokenDoc.data().balance || 0) : 50;

  if (!tokenDoc.exists) {
    await tokenRef.set({
      balance: 50,
      rollover: 0,
      plan: 'free',
      monthlyAllocation: 50,
      lastRefresh: admin.firestore.FieldValue.serverTimestamp(),
      createdAt: admin.firestore.FieldValue.serverTimestamp()
    });
    balance = 50;
  }

  if (balance < tokenCost) {
    throw new functions.https.HttpsError('resource-exhausted',
      `Insufficient tokens. Need ${tokenCost}, have ${balance}.`);
  }

  try {
    const openaiApiKey = functions.config().openai?.key;
    if (!openaiApiKey) {
      throw new functions.https.HttpsError('failed-precondition', 'AI service not configured');
    }

    const focusAreaMap = {
      'general': 'overall video quality and strategy',
      'retention': 'viewer retention and watch time optimization',
      'hooks': 'hooks, intros, and first 30 seconds',
      'ctr': 'click-through rate, titles, and thumbnails',
      'engagement': 'comments, likes, and community engagement',
      'growth': 'channel growth and subscriber acquisition',
      'monetization': 'monetization and revenue optimization',
      'scripting': 'script writing and storytelling'
    };

    const focusText = focusAreaMap[focusArea] || focusAreaMap['general'];

    let contextText = '';
    if (videoUrl) contextText += `Video URL: ${videoUrl}\n`;
    if (transcript) contextText += `Transcript/Script:\n${transcript.substring(0, 2000)}\n`;
    if (challenge) contextText += `Creator's Challenge: ${challenge}\n`;

    const prompt = `You are an elite YouTube coach who has helped channels grow from 0 to millions of subscribers. Provide expert coaching focused on ${focusText}.

${contextText}

Analyze and provide:
1. Overall assessment with a score out of 10
2. What they're doing well (strengths)
3. Priority improvements (with specific actions)
4. Step-by-step action plan
5. Pro tips from top creators

Be specific, actionable, and encouraging. Reference specific timestamps or sections if analyzing a transcript.

Respond in JSON format:
{
  "score": 7.5,
  "assessment": "Overall assessment of current performance...",
  "strengths": [
    "Strength 1",
    "Strength 2",
    "Strength 3"
  ],
  "improvements": [
    {
      "title": "Improvement area",
      "action": "Specific action to take"
    }
  ],
  "actionPlan": [
    "Immediate action 1",
    "This week action 2",
    "This month action 3"
  ],
  "proTips": [
    "Pro tip 1",
    "Pro tip 2",
    "Pro tip 3"
  ]
}`;

    const response = await fetch('https://api.openai.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${openaiApiKey}`,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        model: 'gpt-4o-mini',
        messages: [
          {
            role: 'system',
            content: 'You are an expert YouTube coach with deep knowledge of algorithm, retention, and growth strategies. Always respond with valid JSON.'
          },
          {
            role: 'user',
            content: prompt
          }
        ],
        temperature: 0.7,
        max_tokens: 2500
      })
    });

    if (!response.ok) {
      throw new Error('AI service error');
    }

    const aiResponse = await response.json();
    const content = aiResponse.choices?.[0]?.message?.content || '';

    let coachData;
    try {
      const jsonMatch = content.match(/\{[\s\S]*\}/);
      if (jsonMatch) {
        coachData = JSON.parse(jsonMatch[0]);
      } else {
        throw new Error('No JSON found');
      }
    } catch (parseError) {
      coachData = {
        score: 7,
        assessment: content,
        strengths: ['Good effort', 'Room for growth'],
        improvements: [{ title: 'See detailed feedback', action: 'Review the assessment above' }],
        actionPlan: ['Start with one improvement', 'Track your progress', 'Iterate and improve'],
        proTips: ['Consistency is key', 'Focus on your audience', 'Study your analytics']
      };
    }

    // Deduct tokens
    await tokenRef.update({
      balance: admin.firestore.FieldValue.increment(-tokenCost),
      lastUsed: admin.firestore.FieldValue.serverTimestamp()
    });

    // Save to history
    await db.collection('coachHistory').add({
      userId: uid,
      videoUrl,
      hasTranscript: !!transcript,
      challenge,
      focusArea,
      result: coachData,
      tokenCost,
      createdAt: admin.firestore.FieldValue.serverTimestamp()
    });

    return {
      success: true,
      ...coachData,
      tokenCost,
      remainingBalance: balance - tokenCost
    };

  } catch (error) {
    console.error('Video coaching error:', error);
    throw new functions.https.HttpsError('internal', 'Failed to get coaching. Please try again.');
  }
});

// ==========================================
// GET AI TOOLS HISTORY
// ==========================================
exports.getAIToolsHistory = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  checkRateLimit(uid, 'getAIToolsHistory', 10);

  const { limit = 10, type = 'all' } = data || {};
  const safeLimit = Math.min(Math.max(1, parseInt(limit) || 10), 50);

  // Safe query helper
  const safeQuery = async (collectionName) => {
    try {
      return await db.collection(collectionName)
        .where('userId', '==', uid)
        .orderBy('createdAt', 'desc')
        .limit(safeLimit)
        .get();
    } catch (e) {
      console.warn(`Query failed for ${collectionName}:`, e.message);
      return { forEach: () => {}, size: 0 };
    }
  };

  // Safe timestamp handler
  const getTimestamp = (field) => {
    if (!field) return Date.now();
    if (typeof field === 'number') return field;
    if (typeof field.toMillis === 'function') return field.toMillis();
    if (field._seconds) return field._seconds * 1000;
    if (field instanceof Date) return field.getTime();
    return Date.now();
  };

  // Safe serialization
  const sanitize = (obj) => {
    if (obj === null || obj === undefined) return null;
    try {
      return JSON.parse(JSON.stringify(obj));
    } catch (e) {
      return null;
    }
  };

  const formatHistory = (snap, historyType) => {
    const items = [];
    snap.forEach(doc => {
      try {
        const data = doc.data();
        const timestamp = getTimestamp(data.createdAt);

        const item = {
          id: doc.id,
          type: historyType,
          timestamp,
          createdAt: new Date(timestamp).toISOString()
        };

        Object.keys(data).forEach(key => {
          if (key !== 'createdAt' && key !== 'userId') {
            item[key] = sanitize(data[key]) ?? data[key];
          }
        });

        items.push(item);
      } catch (docError) {
        console.error('Error processing history doc:', doc.id, docError);
      }
    });
    return items;
  };

  try {
    // Define collection mappings
    const collections = {
      script: 'scriptHistory',
      hooks: 'hookHistory',
      multiplier: 'contentMultiplierHistory',
      thumbnail: 'thumbnailTestHistory',
      trends: 'trendHistory',
      gaps: 'contentGapHistory',
      audience: 'audienceHistory',
      collab: 'collabHistory',
      revenue: 'revenueHistory',
      coach: 'coachHistory'
    };

    let results = {};

    if (type === 'all') {
      // Fetch all types in parallel
      const queries = Object.entries(collections).map(async ([key, collection]) => {
        const snap = await safeQuery(collection);
        return { key, items: formatHistory(snap, key) };
      });

      const allResults = await Promise.all(queries);
      allResults.forEach(({ key, items }) => {
        results[key] = items;
      });
    } else if (collections[type]) {
      // Fetch single type
      const snap = await safeQuery(collections[type]);
      results[type] = formatHistory(snap, type);
    } else {
      throw new functions.https.HttpsError('invalid-argument', 'Invalid history type');
    }

    return {
      success: true,
      history: results
    };

  } catch (error) {
    if (error instanceof functions.https.HttpsError) throw error;
    console.error('Get AI tools history error:', error);
    throw new functions.https.HttpsError('internal', 'Failed to retrieve history.');
  }
});

// ==========================================
// SPONSORSHIP RATE CALCULATOR
// ==========================================
/**
 * Calculates sponsorship rates for a YouTube channel
 * Analyzes channel metrics, engagement, and niche to generate professional rate cards
 */
exports.calculateSponsorshipRates = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  checkRateLimit(uid, 'calculateSponsorshipRates', 5);
  await checkUsageLimit(uid, 'sponsorshipCalculator');

  const { channelUrl } = data;
  if (!channelUrl) {
    throw new functions.https.HttpsError('invalid-argument', 'Channel URL is required');
  }

  try {
    // Extract channel info
    const channelInfo = extractChannelInfo(channelUrl);

    // Get channel details
    let channelResponse;
    if (channelInfo.type === 'id') {
      channelResponse = await youtube.channels.list({
        part: 'snippet,statistics,topicDetails',
        id: channelInfo.value
      });
    } else if (channelInfo.type === 'handle') {
      channelResponse = await youtube.channels.list({
        part: 'snippet,statistics,topicDetails',
        forHandle: channelInfo.value
      });
    } else {
      const searchResponse = await youtube.search.list({
        part: 'snippet',
        q: channelInfo.value,
        type: 'channel',
        maxResults: 1
      });

      if (!searchResponse.data.items?.length) {
        throw new functions.https.HttpsError('not-found', 'Channel not found');
      }

      channelResponse = await youtube.channels.list({
        part: 'snippet,statistics,topicDetails',
        id: searchResponse.data.items[0].snippet.channelId
      });
    }

    if (!channelResponse.data.items?.length) {
      throw new functions.https.HttpsError('not-found', 'Channel not found');
    }

    const channel = channelResponse.data.items[0];
    const channelId = channel.id;
    const channelName = channel.snippet.title;
    const channelThumbnail = channel.snippet.thumbnails?.medium?.url || channel.snippet.thumbnails?.default?.url;
    const subscriberCount = parseInt(channel.statistics.subscriberCount) || 0;
    const viewCount = parseInt(channel.statistics.viewCount) || 0;
    const videoCount = parseInt(channel.statistics.videoCount) || 0;
    const topicCategories = channel.topicDetails?.topicCategories?.map(t => t.split('/').pop()) || [];

    // Get recent videos for engagement analysis
    const videosResponse = await youtube.search.list({
      part: 'snippet',
      channelId: channelId,
      type: 'video',
      order: 'date',
      maxResults: 20
    });

    const recentVideoIds = videosResponse.data.items?.map(v => v.id.videoId).filter(Boolean) || [];

    let avgViews = 0;
    let avgEngagement = 0;
    if (recentVideoIds.length > 0) {
      const videoDetailsResponse = await youtube.videos.list({
        part: 'statistics',
        id: recentVideoIds.slice(0, 15).join(',')
      });

      const recentStats = videoDetailsResponse.data.items || [];
      const totalViews = recentStats.reduce((sum, v) => sum + parseInt(v.statistics.viewCount || 0), 0);
      const totalLikes = recentStats.reduce((sum, v) => sum + parseInt(v.statistics.likeCount || 0), 0);
      const totalComments = recentStats.reduce((sum, v) => sum + parseInt(v.statistics.commentCount || 0), 0);

      avgViews = Math.round(totalViews / Math.max(recentStats.length, 1));
      avgEngagement = totalViews > 0 ? ((totalLikes + totalComments) / totalViews * 100).toFixed(2) : 0;
    }

    // Determine niche from topics
    const nicheMap = {
      'Finance': 'Finance',
      'Business': 'Finance',
      'Technology': 'Technology',
      'Gaming': 'Gaming',
      'Entertainment': 'Entertainment',
      'Education': 'Education',
      'Lifestyle': 'Lifestyle',
      'Beauty': 'Beauty',
      'Fashion': 'Beauty',
      'Health': 'Health',
      'Fitness': 'Health',
      'Food': 'Food',
      'Travel': 'Travel'
    };

    let detectedNiche = 'General';
    for (const topic of topicCategories) {
      for (const [key, niche] of Object.entries(nicheMap)) {
        if (topic.toLowerCase().includes(key.toLowerCase())) {
          detectedNiche = niche;
          break;
        }
      }
    }

    // Calculate sponsorship rates based on industry standards
    // Base rate: $20-50 per 1,000 subscribers for integration
    // Adjusted by engagement rate and niche multipliers
    const nicheMultipliers = {
      'Finance': 2.5,
      'Technology': 2.0,
      'Business': 2.0,
      'Health': 1.8,
      'Beauty': 1.6,
      'Education': 1.5,
      'Food': 1.4,
      'Travel': 1.4,
      'Lifestyle': 1.3,
      'Gaming': 1.2,
      'Entertainment': 1.0,
      'General': 1.0
    };

    const nicheMultiplier = nicheMultipliers[detectedNiche] || 1.0;
    const engagementBonus = parseFloat(avgEngagement) > 5 ? 1.5 : parseFloat(avgEngagement) > 3 ? 1.2 : 1.0;

    // Base rate per 1000 subscribers
    const baseRatePer1K = 30;
    const baseIntegrationRate = (subscriberCount / 1000) * baseRatePer1K * nicheMultiplier * engagementBonus;

    // Calculate different rate tiers
    const integrationRate = Math.max(100, Math.round(baseIntegrationRate / 50) * 50);
    const dedicatedVideoRate = Math.round(integrationRate * 2.5);
    const shoutoutRate = Math.round(integrationRate * 0.4);

    // Industry averages for comparison
    const industryAverage = Math.round((subscriberCount / 1000) * 25);
    const topCreatorRate = Math.round((subscriberCount / 1000) * 60);

    // Determine position relative to industry
    let position = 'average';
    let insight = 'Your rates are in line with industry averages.';
    if (integrationRate > industryAverage * 1.2) {
      position = 'above';
      insight = 'Your strong engagement justifies premium rates above industry average.';
    } else if (integrationRate < industryAverage * 0.8) {
      position = 'below';
      insight = 'Consider increasing your rates - your content quality may warrant higher pricing.';
    }

    // Use AI to generate negotiation tips
    const prompt = `You are a YouTube sponsorship expert. A creator with ${subscriberCount.toLocaleString()} subscribers in the ${detectedNiche} niche has an average of ${avgViews.toLocaleString()} views per video and ${avgEngagement}% engagement rate.

Generate 4 negotiation strategies to help them get higher sponsorship rates. Each tip should be specific and actionable.

Return as JSON:
{
  "tips": [
    { "title": "Short title", "description": "Detailed strategy explanation" }
  ]
}`;

    const aiResponse = await openai.chat.completions.create({
      model: 'gpt-4o-mini',
      messages: [{ role: 'user', content: prompt }],
      response_format: { type: 'json_object' },
      max_tokens: 800
    });

    let negotiationTips = [];
    try {
      const parsed = JSON.parse(aiResponse.choices[0].message.content);
      negotiationTips = parsed.tips || [];
    } catch (e) {
      negotiationTips = [
        { title: 'Highlight Your Engagement', description: 'Brands value engagement over raw subscriber counts. Emphasize your like-to-view and comment ratios.' },
        { title: 'Create a Media Kit', description: 'A professional media kit with demographics, case studies, and past results can justify higher rates.' },
        { title: 'Bundle Services', description: 'Offer packages that include social media posts, stories, and community posts for added value.' },
        { title: 'Show ROI', description: 'Track and share click-through rates and conversion data from previous sponsorships.' }
      ];
    }

    // Save to history
    const historyData = {
      userId: uid,
      type: 'sponsorship',
      channelUrl,
      channelName,
      channelThumbnail,
      subscribers: subscriberCount,
      niche: detectedNiche,
      avgViews,
      avgEngagement: parseFloat(avgEngagement),
      rates: {
        dedicatedVideo: '$' + dedicatedVideoRate.toLocaleString(),
        integration: '$' + integrationRate.toLocaleString(),
        shoutout: '$' + shoutoutRate.toLocaleString()
      },
      comparison: {
        average: '$' + industryAverage.toLocaleString(),
        top: '$' + topCreatorRate.toLocaleString(),
        position,
        insight
      },
      negotiationTips,
      createdAt: admin.firestore.FieldValue.serverTimestamp()
    };

    await db.collection('sponsorshipHistory').add(historyData);
    await incrementUsage(uid, 'sponsorshipCalculator');
    await logUsage(uid, 'sponsorship_calculator', { channelUrl, subscribers: subscriberCount });

    return {
      success: true,
      channelName,
      channelThumbnail,
      subscribers: subscriberCount,
      niche: detectedNiche,
      avgViews,
      avgEngagement: parseFloat(avgEngagement),
      rates: {
        dedicatedVideo: '$' + dedicatedVideoRate.toLocaleString(),
        integration: '$' + integrationRate.toLocaleString(),
        shoutout: '$' + shoutoutRate.toLocaleString()
      },
      comparison: {
        average: '$' + industryAverage.toLocaleString(),
        top: '$' + topCreatorRate.toLocaleString(),
        position,
        insight
      },
      negotiationTips
    };

  } catch (error) {
    if (error instanceof functions.https.HttpsError) throw error;
    console.error('Sponsorship calculator error:', error);
    throw new functions.https.HttpsError('internal', 'Failed to calculate sponsorship rates.');
  }
});

// ==========================================
// REVENUE DIVERSIFICATION ANALYZER
// ==========================================
/**
 * Analyzes a channel's current revenue sources and identifies gaps
 * Provides recommendations for new income streams
 */
exports.analyzeRevenueDiversification = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  checkRateLimit(uid, 'analyzeRevenueDiversification', 5);
  await checkUsageLimit(uid, 'revenueDiversification');

  const { channelUrl, currentSources = [] } = data;
  if (!channelUrl) {
    throw new functions.https.HttpsError('invalid-argument', 'Channel URL is required');
  }

  try {
    // Extract and fetch channel info
    const channelInfo = extractChannelInfo(channelUrl);

    let channelResponse;
    if (channelInfo.type === 'id') {
      channelResponse = await youtube.channels.list({
        part: 'snippet,statistics,topicDetails',
        id: channelInfo.value
      });
    } else if (channelInfo.type === 'handle') {
      channelResponse = await youtube.channels.list({
        part: 'snippet,statistics,topicDetails',
        forHandle: channelInfo.value
      });
    } else {
      const searchResponse = await youtube.search.list({
        part: 'snippet',
        q: channelInfo.value,
        type: 'channel',
        maxResults: 1
      });

      if (!searchResponse.data.items?.length) {
        throw new functions.https.HttpsError('not-found', 'Channel not found');
      }

      channelResponse = await youtube.channels.list({
        part: 'snippet,statistics,topicDetails',
        id: searchResponse.data.items[0].snippet.channelId
      });
    }

    if (!channelResponse.data.items?.length) {
      throw new functions.https.HttpsError('not-found', 'Channel not found');
    }

    const channel = channelResponse.data.items[0];
    const channelName = channel.snippet.title;
    const channelThumbnail = channel.snippet.thumbnails?.medium?.url || channel.snippet.thumbnails?.default?.url;
    const subscriberCount = parseInt(channel.statistics.subscriberCount) || 0;
    const topicCategories = channel.topicDetails?.topicCategories?.map(t => t.split('/').pop()) || [];

    // Determine niche
    let niche = 'General';
    const nicheKeywords = ['Finance', 'Technology', 'Gaming', 'Education', 'Lifestyle', 'Beauty', 'Health', 'Food', 'Travel', 'Entertainment'];
    for (const topic of topicCategories) {
      for (const keyword of nicheKeywords) {
        if (topic.toLowerCase().includes(keyword.toLowerCase())) {
          niche = keyword;
          break;
        }
      }
    }

    // Define all possible revenue sources with icons
    const allSources = [
      { id: 'adsense', name: 'AdSense', icon: 'ðŸ’°' },
      { id: 'sponsors', name: 'Sponsorships', icon: 'ðŸ¤' },
      { id: 'merch', name: 'Merchandise', icon: 'ðŸ‘•' },
      { id: 'courses', name: 'Courses', icon: 'ðŸ“š' },
      { id: 'affiliate', name: 'Affiliates', icon: 'ðŸ”—' },
      { id: 'memberships', name: 'Memberships', icon: 'â­' },
      { id: 'consulting', name: 'Consulting', icon: 'ðŸ’¼' },
      { id: 'digital', name: 'Digital Products', icon: 'ðŸ“¦' }
    ];

    // Mark active/inactive sources
    const currentSourcesData = allSources.map(source => ({
      ...source,
      active: currentSources.includes(source.id),
      estimated: source.active ? estimateRevenueForSource(source.id, subscriberCount, niche) : null
    }));

    // Calculate diversification score
    const activeCount = currentSources.length;
    const diversificationScore = Math.round((activeCount / allSources.length) * 100);

    // Use AI to generate personalized recommendations
    const prompt = `You are a YouTube monetization expert. Analyze this channel:
- Subscribers: ${subscriberCount.toLocaleString()}
- Niche: ${niche}
- Current revenue sources: ${currentSources.length > 0 ? currentSources.join(', ') : 'Only AdSense'}
- Missing sources: ${allSources.filter(s => !currentSources.includes(s.id)).map(s => s.name).join(', ')}

Generate:
1. An estimate of monthly revenue they're missing (format: "$X,XXX")
2. Top 4 revenue stream recommendations with potential monthly earnings
3. A 5-step prioritized action plan

Return as JSON:
{
  "missingRevenue": "$X,XXX",
  "recommendations": [
    {
      "icon": "emoji",
      "name": "Revenue Stream Name",
      "potential": "$X,XXX",
      "description": "Why this is good for them",
      "effort": "Low/Medium/High",
      "roi": "High/Medium/Low"
    }
  ],
  "actionPlan": [
    { "task": "Action item", "impact": "Expected result" }
  ]
}`;

    const aiResponse = await openai.chat.completions.create({
      model: 'gpt-4o-mini',
      messages: [{ role: 'user', content: prompt }],
      response_format: { type: 'json_object' },
      max_tokens: 1200
    });

    let aiData;
    try {
      aiData = JSON.parse(aiResponse.choices[0].message.content);
    } catch (e) {
      aiData = {
        missingRevenue: '$' + Math.round(subscriberCount / 100) + '/mo',
        recommendations: [
          { icon: 'ðŸ¤', name: 'Brand Sponsorships', potential: '$' + Math.round(subscriberCount / 50), description: 'Partner with brands in your niche', effort: 'Medium', roi: 'High' }
        ],
        actionPlan: [
          { task: 'Create a media kit', impact: 'Professional outreach to brands' }
        ]
      };
    }

    // Save to history
    const historyData = {
      userId: uid,
      type: 'diversification',
      channelUrl,
      channelName,
      channelThumbnail,
      subscribers: subscriberCount,
      niche,
      currentSources: currentSourcesData,
      diversificationScore,
      missingRevenue: aiData.missingRevenue,
      recommendations: aiData.recommendations,
      actionPlan: aiData.actionPlan,
      createdAt: admin.firestore.FieldValue.serverTimestamp()
    };

    await db.collection('diversificationHistory').add(historyData);
    await incrementUsage(uid, 'revenueDiversification');
    await logUsage(uid, 'revenue_diversification', { channelUrl, subscribers: subscriberCount });

    return {
      success: true,
      channelName,
      channelThumbnail,
      subscribers: subscriberCount,
      niche,
      diversificationScore,
      currentSources: currentSourcesData,
      missingRevenue: aiData.missingRevenue,
      recommendations: aiData.recommendations,
      actionPlan: aiData.actionPlan
    };

  } catch (error) {
    if (error instanceof functions.https.HttpsError) throw error;
    console.error('Revenue diversification error:', error);
    throw new functions.https.HttpsError('internal', 'Failed to analyze revenue diversification.');
  }
});

// Helper function for revenue estimation
function estimateRevenueForSource(sourceId, subscribers, niche) {
  const baseRates = {
    adsense: subscribers * 0.002,
    sponsors: subscribers * 0.01,
    merch: subscribers * 0.001,
    courses: subscribers * 0.005,
    affiliate: subscribers * 0.003,
    memberships: subscribers * 0.002,
    consulting: subscribers * 0.001,
    digital: subscribers * 0.004
  };

  const nicheMultipliers = {
    Finance: 2.0,
    Technology: 1.5,
    Education: 1.3,
    Health: 1.4,
    General: 1.0
  };

  const multiplier = nicheMultipliers[niche] || 1.0;
  const estimate = Math.round((baseRates[sourceId] || 0) * multiplier);
  return '$' + estimate.toLocaleString();
}

// ==========================================
// CPM BOOSTER STRATEGIST
// ==========================================
/**
 * Analyzes a channel and provides strategies to increase CPM
 * Identifies high-CPM keywords, topics, and optimal video lengths
 */
exports.analyzeCpmBooster = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  checkRateLimit(uid, 'analyzeCpmBooster', 5);
  await checkUsageLimit(uid, 'cpmBooster');

  const { channelUrl } = data;
  if (!channelUrl) {
    throw new functions.https.HttpsError('invalid-argument', 'Channel URL is required');
  }

  try {
    // Extract and fetch channel info
    const channelInfo = extractChannelInfo(channelUrl);

    let channelResponse;
    if (channelInfo.type === 'id') {
      channelResponse = await youtube.channels.list({
        part: 'snippet,statistics,topicDetails',
        id: channelInfo.value
      });
    } else if (channelInfo.type === 'handle') {
      channelResponse = await youtube.channels.list({
        part: 'snippet,statistics,topicDetails',
        forHandle: channelInfo.value
      });
    } else {
      const searchResponse = await youtube.search.list({
        part: 'snippet',
        q: channelInfo.value,
        type: 'channel',
        maxResults: 1
      });

      if (!searchResponse.data.items?.length) {
        throw new functions.https.HttpsError('not-found', 'Channel not found');
      }

      channelResponse = await youtube.channels.list({
        part: 'snippet,statistics,topicDetails',
        id: searchResponse.data.items[0].snippet.channelId
      });
    }

    if (!channelResponse.data.items?.length) {
      throw new functions.https.HttpsError('not-found', 'Channel not found');
    }

    const channel = channelResponse.data.items[0];
    const channelId = channel.id;
    const channelName = channel.snippet.title;
    const channelDescription = channel.snippet.description || '';
    const topicCategories = channel.topicDetails?.topicCategories?.map(t => t.split('/').pop()) || [];

    // Determine current niche and CPM
    const nicheCPMRates = {
      'Finance': { current: 12, potential: 18 },
      'Insurance': { current: 15, potential: 22 },
      'Legal': { current: 14, potential: 20 },
      'Technology': { current: 8, potential: 12 },
      'Business': { current: 10, potential: 15 },
      'Education': { current: 6, potential: 10 },
      'Health': { current: 7, potential: 11 },
      'Gaming': { current: 4, potential: 6 },
      'Entertainment': { current: 3, potential: 5 },
      'Lifestyle': { current: 5, potential: 8 },
      'General': { current: 4, potential: 7 }
    };

    let detectedNiche = 'General';
    for (const topic of topicCategories) {
      for (const niche of Object.keys(nicheCPMRates)) {
        if (topic.toLowerCase().includes(niche.toLowerCase())) {
          detectedNiche = niche;
          break;
        }
      }
    }

    const cpmData = nicheCPMRates[detectedNiche] || nicheCPMRates.General;

    // Use AI to generate CPM optimization strategies
    const prompt = `You are a YouTube CPM optimization expert. Analyze this channel:
- Channel: ${channelName}
- Niche: ${detectedNiche}
- Description: ${channelDescription.slice(0, 500)}
- Current estimated CPM: $${cpmData.current}

Generate:
1. 8 high-CPM keywords relevant to their niche (with CPM estimates)
2. 5 video topic ideas that would attract higher-paying advertisers
3. Optimal video length recommendation with reasoning
4. Quarterly content calendar showing CPM multipliers

Return as JSON:
{
  "highCpmKeywords": [
    { "keyword": "keyword phrase", "cpm": 15 }
  ],
  "topicIdeas": [
    { "title": "Video title idea", "estimatedCpm": 12, "description": "Why this attracts premium advertisers" }
  ],
  "optimalLength": "8-12 minutes",
  "lengthReason": "Allows 2-3 mid-roll ad placements",
  "contentCalendar": [
    { "period": "Q1", "cpmMultiplier": 0.8, "tip": "Post-holiday dip" },
    { "period": "Q4", "cpmMultiplier": 1.8, "tip": "Holiday ad spend peak" }
  ]
}`;

    const aiResponse = await openai.chat.completions.create({
      model: 'gpt-4o-mini',
      messages: [{ role: 'user', content: prompt }],
      response_format: { type: 'json_object' },
      max_tokens: 1500
    });

    let aiData;
    try {
      aiData = JSON.parse(aiResponse.choices[0].message.content);
    } catch (e) {
      aiData = {
        highCpmKeywords: [
          { keyword: 'best investment strategies', cpm: 15 },
          { keyword: 'how to save money', cpm: 12 }
        ],
        topicIdeas: [
          { title: 'Complete Guide to [Topic]', estimatedCpm: 10, description: 'Educational content attracts premium brands' }
        ],
        optimalLength: '8-12 minutes',
        lengthReason: 'Optimal for mid-roll ad placements',
        contentCalendar: [
          { period: 'Q1', cpmMultiplier: 0.8, tip: 'Lower ad spend' },
          { period: 'Q4', cpmMultiplier: 1.8, tip: 'Holiday peak' }
        ]
      };
    }

    const cpmIncrease = Math.round(((cpmData.potential - cpmData.current) / cpmData.current) * 100) + '%';

    // Save to history
    const historyData = {
      userId: uid,
      type: 'cpmbooster',
      channelUrl,
      channelName,
      niche: detectedNiche,
      currentCPM: '$' + cpmData.current,
      potentialCPM: '$' + cpmData.potential,
      cpmIncrease,
      highCpmKeywords: aiData.highCpmKeywords,
      topicIdeas: aiData.topicIdeas,
      optimalLength: aiData.optimalLength,
      lengthReason: aiData.lengthReason,
      contentCalendar: aiData.contentCalendar,
      createdAt: admin.firestore.FieldValue.serverTimestamp()
    };

    await db.collection('cpmBoosterHistory').add(historyData);
    await incrementUsage(uid, 'cpmBooster');
    await logUsage(uid, 'cpm_booster', { channelUrl, niche: detectedNiche });

    return {
      success: true,
      channelName,
      niche: detectedNiche,
      currentCPM: '$' + cpmData.current,
      potentialCPM: '$' + cpmData.potential,
      cpmIncrease,
      highCpmKeywords: aiData.highCpmKeywords,
      topicIdeas: aiData.topicIdeas,
      optimalLength: aiData.optimalLength,
      lengthReason: aiData.lengthReason,
      contentCalendar: aiData.contentCalendar
    };

  } catch (error) {
    if (error instanceof functions.https.HttpsError) throw error;
    console.error('CPM booster error:', error);
    throw new functions.https.HttpsError('internal', 'Failed to analyze CPM opportunities.');
  }
});

// ==========================================
// AUDIENCE MONETIZATION PROFILER
// ==========================================
/**
 * Analyzes a channel's audience demographics and spending behavior
 * Provides segmentation and targeted offer recommendations
 */
exports.analyzeAudienceProfile = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  checkRateLimit(uid, 'analyzeAudienceProfile', 5);
  await checkUsageLimit(uid, 'audienceProfiler');

  const { channelUrl } = data;
  if (!channelUrl) {
    throw new functions.https.HttpsError('invalid-argument', 'Channel URL is required');
  }

  try {
    // Extract and fetch channel info
    const channelInfo = extractChannelInfo(channelUrl);

    let channelResponse;
    if (channelInfo.type === 'id') {
      channelResponse = await youtube.channels.list({
        part: 'snippet,statistics,topicDetails',
        id: channelInfo.value
      });
    } else if (channelInfo.type === 'handle') {
      channelResponse = await youtube.channels.list({
        part: 'snippet,statistics,topicDetails',
        forHandle: channelInfo.value
      });
    } else {
      const searchResponse = await youtube.search.list({
        part: 'snippet',
        q: channelInfo.value,
        type: 'channel',
        maxResults: 1
      });

      if (!searchResponse.data.items?.length) {
        throw new functions.https.HttpsError('not-found', 'Channel not found');
      }

      channelResponse = await youtube.channels.list({
        part: 'snippet,statistics,topicDetails',
        id: searchResponse.data.items[0].snippet.channelId
      });
    }

    if (!channelResponse.data.items?.length) {
      throw new functions.https.HttpsError('not-found', 'Channel not found');
    }

    const channel = channelResponse.data.items[0];
    const channelId = channel.id;
    const channelName = channel.snippet.title;
    const channelThumbnail = channel.snippet.thumbnails?.medium?.url || channel.snippet.thumbnails?.default?.url;
    const channelDescription = channel.snippet.description || '';
    const subscriberCount = parseInt(channel.statistics.subscriberCount) || 0;
    const topicCategories = channel.topicDetails?.topicCategories?.map(t => t.split('/').pop()) || [];

    // Get recent videos for content analysis
    const videosResponse = await youtube.search.list({
      part: 'snippet',
      channelId: channelId,
      type: 'video',
      order: 'viewCount',
      maxResults: 10
    });

    const topVideoTitles = videosResponse.data.items?.map(v => v.snippet.title).join(', ') || '';

    // Determine niche
    let niche = 'General';
    const nicheKeywords = ['Finance', 'Technology', 'Gaming', 'Education', 'Lifestyle', 'Beauty', 'Health', 'Food', 'Travel', 'Entertainment', 'Business'];
    for (const topic of topicCategories) {
      for (const keyword of nicheKeywords) {
        if (topic.toLowerCase().includes(keyword.toLowerCase())) {
          niche = keyword;
          break;
        }
      }
    }

    // Use AI to generate audience profile
    const prompt = `You are an audience monetization expert. Analyze this YouTube channel:
- Channel: ${channelName}
- Subscribers: ${subscriberCount.toLocaleString()}
- Niche: ${niche}
- Description: ${channelDescription.slice(0, 300)}
- Top videos: ${topVideoTitles.slice(0, 400)}

Create a detailed monetization profile:
1. 4 audience segments with purchasing power analysis
2. 5 products/services this audience would likely buy
3. 4 content recommendations to attract higher-value viewers
4. 3 targeted offer ideas for different segments

Return as JSON:
{
  "segments": [
    {
      "icon": "emoji",
      "name": "Segment Name",
      "percentage": 30,
      "value": "$150",
      "description": "Description of this segment's characteristics and spending habits"
    }
  ],
  "productRecommendations": [
    {
      "icon": "emoji",
      "name": "Product category",
      "reason": "Why they'd buy this",
      "conversionRate": 3.5
    }
  ],
  "contentRecommendations": [
    {
      "title": "Content strategy",
      "impact": "Expected result on audience value"
    }
  ],
  "targetedOffers": [
    {
      "name": "Offer name",
      "segment": "Target segment",
      "description": "Offer details",
      "expectedRevenue": "$X,XXX/month"
    }
  ]
}`;

    const aiResponse = await openai.chat.completions.create({
      model: 'gpt-4o-mini',
      messages: [{ role: 'user', content: prompt }],
      response_format: { type: 'json_object' },
      max_tokens: 1500
    });

    let aiData;
    try {
      aiData = JSON.parse(aiResponse.choices[0].message.content);
    } catch (e) {
      aiData = {
        segments: [
          { icon: 'ðŸ’¼', name: 'Professionals', percentage: 40, value: '$200', description: 'Working professionals interested in career growth' }
        ],
        productRecommendations: [
          { icon: 'ðŸ“š', name: 'Online Courses', reason: 'Educational content viewers value learning', conversionRate: 3.2 }
        ],
        contentRecommendations: [
          { title: 'Create premium tutorials', impact: 'Attracts higher-income viewers' }
        ],
        targetedOffers: [
          { name: 'Premium Course Bundle', segment: 'Professionals', description: 'Advanced training package', expectedRevenue: '$2,000/month' }
        ]
      };
    }

    // Save to history
    const historyData = {
      userId: uid,
      type: 'audienceprofile',
      channelUrl,
      channelName,
      channelThumbnail,
      subscribers: subscriberCount,
      niche,
      segments: aiData.segments,
      productRecommendations: aiData.productRecommendations,
      contentRecommendations: aiData.contentRecommendations,
      targetedOffers: aiData.targetedOffers,
      createdAt: admin.firestore.FieldValue.serverTimestamp()
    };

    await db.collection('audienceProfileHistory').add(historyData);
    await incrementUsage(uid, 'audienceProfiler');
    await logUsage(uid, 'audience_profiler', { channelUrl, subscribers: subscriberCount });

    return {
      success: true,
      channelName,
      channelThumbnail,
      subscribers: subscriberCount,
      niche,
      segments: aiData.segments,
      productRecommendations: aiData.productRecommendations,
      contentRecommendations: aiData.contentRecommendations,
      targetedOffers: aiData.targetedOffers
    };

  } catch (error) {
    if (error instanceof functions.https.HttpsError) throw error;
    console.error('Audience profiler error:', error);
    throw new functions.https.HttpsError('internal', 'Failed to profile audience.');
  }
});

// ============================================================
// DIGITAL PRODUCT ARCHITECT
// Analyzes channel to suggest digital products the creator can sell
// ============================================================
exports.analyzeDigitalProduct = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  checkRateLimit(uid, 'analyzeDigitalProduct', 5);
  await checkUsageLimit(uid, 'digitalProductArchitect');

  const { channelUrl } = data;
  if (!channelUrl) {
    throw new functions.https.HttpsError('invalid-argument', 'Channel URL is required.');
  }

  try {
    // Extract channel info from URL
    const channelInfo = extractChannelInfo(channelUrl);

    // Get channel details based on URL type
    let channelResponse;
    if (channelInfo.type === 'id') {
      channelResponse = await youtube.channels.list({
        part: 'snippet,statistics,topicDetails',
        id: channelInfo.value
      });
    } else if (channelInfo.type === 'handle') {
      channelResponse = await youtube.channels.list({
        part: 'snippet,statistics,topicDetails',
        forHandle: channelInfo.value
      });
    } else {
      // Search for custom/user URLs
      const searchResponse = await youtube.search.list({
        part: 'snippet',
        q: channelInfo.value,
        type: 'channel',
        maxResults: 1
      });

      if (!searchResponse.data.items?.length) {
        throw new functions.https.HttpsError('not-found', 'Channel not found.');
      }

      channelResponse = await youtube.channels.list({
        part: 'snippet,statistics,topicDetails',
        id: searchResponse.data.items[0].snippet.channelId
      });
    }

    if (!channelResponse.data.items || channelResponse.data.items.length === 0) {
      throw new functions.https.HttpsError('not-found', 'Channel not found.');
    }

    const channel = channelResponse.data.items[0];
    const channelId = channel.id;
    const channelName = channel.snippet.title;
    const channelThumbnail = channel.snippet.thumbnails?.medium?.url || channel.snippet.thumbnails?.default?.url;
    const channelDescription = channel.snippet.description || '';
    const subscriberCount = parseInt(channel.statistics.subscriberCount) || 0;
    const topicCategories = channel.topicDetails?.topicCategories?.map(t => t.split('/').pop()) || [];

    // Get popular videos for content analysis
    const videosResponse = await youtube.search.list({
      part: 'snippet',
      channelId: channelId,
      type: 'video',
      order: 'viewCount',
      maxResults: 15
    });

    const videoTitles = videosResponse.data.items?.map(v => v.snippet.title).join(', ') || '';

    // Determine niche
    let niche = 'General';
    const nicheKeywords = ['Finance', 'Technology', 'Gaming', 'Education', 'Lifestyle', 'Beauty', 'Health', 'Food', 'Travel', 'Entertainment', 'Business', 'Fitness', 'Music'];
    for (const topic of topicCategories) {
      for (const keyword of nicheKeywords) {
        if (topic.toLowerCase().includes(keyword.toLowerCase())) {
          niche = keyword;
          break;
        }
      }
    }

    // Use AI to generate digital product ideas
    const prompt = `You are a digital product strategist. Analyze this YouTube channel and create a comprehensive product plan:

Channel: ${channelName}
Subscribers: ${subscriberCount.toLocaleString()}
Niche: ${niche}
Description: ${channelDescription.slice(0, 300)}
Popular videos: ${videoTitles.slice(0, 500)}

Create a digital product strategy with:
1. 5 digital product ideas ranked by potential revenue
2. Pricing strategy with tier recommendations
3. A 90-day launch timeline
4. Skills/expertise this creator can monetize

Return as JSON:
{
  "productIdeas": [
    {
      "icon": "emoji",
      "name": "Product name",
      "type": "Course/Ebook/Template/Community/Tool",
      "description": "What this product offers",
      "targetAudience": "Who would buy this",
      "estimatedPrice": "$XX-$XXX",
      "estimatedMonthlyRevenue": "$X,XXX",
      "difficulty": "Easy/Medium/Hard",
      "priority": 1
    }
  ],
  "pricingStrategy": {
    "tiers": [
      {
        "name": "Tier name",
        "price": "$XX",
        "features": ["Feature 1", "Feature 2"],
        "targetBuyer": "Description of who buys this tier"
      }
    ],
    "recommendation": "Strategic recommendation for pricing"
  },
  "launchTimeline": [
    {
      "week": "Week 1-2",
      "phase": "Phase name",
      "tasks": ["Task 1", "Task 2", "Task 3"],
      "milestone": "Key milestone to achieve"
    }
  ],
  "expertise": [
    {
      "skill": "Skill name",
      "monetizationPotential": "High/Medium/Low",
      "productType": "How to monetize this skill"
    }
  ]
}`;

    const aiResponse = await openai.chat.completions.create({
      model: 'gpt-4o-mini',
      messages: [{ role: 'user', content: prompt }],
      response_format: { type: 'json_object' },
      max_tokens: 2000
    });

    let aiData;
    try {
      aiData = JSON.parse(aiResponse.choices[0].message.content);
    } catch (e) {
      aiData = {
        productIdeas: [
          { icon: 'ðŸ“š', name: 'Comprehensive Course', type: 'Course', description: 'Full training program', targetAudience: 'Beginners', estimatedPrice: '$97-$297', estimatedMonthlyRevenue: '$5,000', difficulty: 'Medium', priority: 1 }
        ],
        pricingStrategy: {
          tiers: [{ name: 'Basic', price: '$47', features: ['Core content'], targetBuyer: 'Budget-conscious learners' }],
          recommendation: 'Start with a low-tier product and upsell'
        },
        launchTimeline: [
          { week: 'Week 1-2', phase: 'Planning', tasks: ['Define product scope', 'Create outline'], milestone: 'Product plan complete' }
        ],
        expertise: [
          { skill: 'Content Creation', monetizationPotential: 'High', productType: 'Online course' }
        ]
      };
    }

    // Save to history
    const historyData = {
      userId: uid,
      type: 'digitalproduct',
      channelUrl,
      channelName,
      channelThumbnail,
      subscribers: subscriberCount,
      niche,
      productIdeas: aiData.productIdeas,
      pricingStrategy: aiData.pricingStrategy,
      launchTimeline: aiData.launchTimeline,
      expertise: aiData.expertise,
      createdAt: admin.firestore.FieldValue.serverTimestamp()
    };

    await db.collection('digitalProductHistory').add(historyData);
    await incrementUsage(uid, 'digitalProductArchitect');
    await logUsage(uid, 'digital_product_architect', { channelUrl, subscribers: subscriberCount });

    return {
      success: true,
      channelName,
      channelThumbnail,
      subscribers: subscriberCount,
      niche,
      productIdeas: aiData.productIdeas,
      pricingStrategy: aiData.pricingStrategy,
      launchTimeline: aiData.launchTimeline,
      expertise: aiData.expertise
    };

  } catch (error) {
    if (error instanceof functions.https.HttpsError) throw error;
    console.error('Digital product architect error:', error);
    throw new functions.https.HttpsError('internal', 'Failed to analyze digital products.');
  }
});

// ============================================================
// AFFILIATE GOLDMINE FINDER
// Finds affiliate programs matching channel's niche
// ============================================================
exports.analyzeAffiliate = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  checkRateLimit(uid, 'analyzeAffiliate', 5);
  await checkUsageLimit(uid, 'affiliateFinder');

  const { channelUrl } = data;
  if (!channelUrl) {
    throw new functions.https.HttpsError('invalid-argument', 'Channel URL is required.');
  }

  try {
    // Extract channel info from URL
    const channelInfo = extractChannelInfo(channelUrl);

    // Get channel details based on URL type
    let channelResponse;
    if (channelInfo.type === 'id') {
      channelResponse = await youtube.channels.list({
        part: 'snippet,statistics,topicDetails',
        id: channelInfo.value
      });
    } else if (channelInfo.type === 'handle') {
      channelResponse = await youtube.channels.list({
        part: 'snippet,statistics,topicDetails',
        forHandle: channelInfo.value
      });
    } else {
      // Search for custom/user URLs
      const searchResponse = await youtube.search.list({
        part: 'snippet',
        q: channelInfo.value,
        type: 'channel',
        maxResults: 1
      });

      if (!searchResponse.data.items?.length) {
        throw new functions.https.HttpsError('not-found', 'Channel not found.');
      }

      channelResponse = await youtube.channels.list({
        part: 'snippet,statistics,topicDetails',
        id: searchResponse.data.items[0].snippet.channelId
      });
    }

    if (!channelResponse.data.items || channelResponse.data.items.length === 0) {
      throw new functions.https.HttpsError('not-found', 'Channel not found.');
    }

    const channel = channelResponse.data.items[0];
    const channelId = channel.id;
    const channelName = channel.snippet.title;
    const channelThumbnail = channel.snippet.thumbnails?.medium?.url || channel.snippet.thumbnails?.default?.url;
    const channelDescription = channel.snippet.description || '';
    const subscriberCount = parseInt(channel.statistics.subscriberCount) || 0;
    const viewCount = parseInt(channel.statistics.viewCount) || 0;
    const topicCategories = channel.topicDetails?.topicCategories?.map(t => t.split('/').pop()) || [];

    // Get popular videos
    const videosResponse = await youtube.search.list({
      part: 'snippet',
      channelId: channelId,
      type: 'video',
      order: 'viewCount',
      maxResults: 15
    });

    const videoTitles = videosResponse.data.items?.map(v => v.snippet.title).join(', ') || '';

    // Determine niche
    let niche = 'General';
    const nicheKeywords = ['Finance', 'Technology', 'Gaming', 'Education', 'Lifestyle', 'Beauty', 'Health', 'Food', 'Travel', 'Entertainment', 'Business', 'Fitness'];
    for (const topic of topicCategories) {
      for (const keyword of nicheKeywords) {
        if (topic.toLowerCase().includes(keyword.toLowerCase())) {
          niche = keyword;
          break;
        }
      }
    }

    // Use AI to find affiliate opportunities
    const prompt = `You are an affiliate marketing expert. Analyze this YouTube channel and find the best affiliate opportunities:

Channel: ${channelName}
Subscribers: ${subscriberCount.toLocaleString()}
Total Views: ${viewCount.toLocaleString()}
Niche: ${niche}
Description: ${channelDescription.slice(0, 300)}
Popular videos: ${videoTitles.slice(0, 500)}

Create a comprehensive affiliate strategy:
1. 6 affiliate programs perfectly matched to this channel
2. Scripts for naturally mentioning affiliate products
3. Earnings breakdown projection
4. Best placement strategies

Return as JSON:
{
  "affiliatePrograms": [
    {
      "icon": "emoji",
      "name": "Program/Company name",
      "network": "Amazon/ShareASale/Impact/Direct/etc",
      "commission": "X% or $XX per sale",
      "cookieDuration": "XX days",
      "avgOrderValue": "$XXX",
      "estimatedEarnings": "$X,XXX/month",
      "fitScore": 95,
      "signupUrl": "General signup info",
      "whyItFits": "Why this is perfect for this channel"
    }
  ],
  "placementScripts": [
    {
      "type": "Intro/Mid-roll/Outro/Description",
      "script": "Natural-sounding script to mention the product",
      "duration": "XX seconds",
      "tips": "How to make it more effective"
    }
  ],
  "earningsBreakdown": {
    "monthly": {
      "conservative": "$X,XXX",
      "moderate": "$X,XXX",
      "optimistic": "$XX,XXX"
    },
    "perVideo": {
      "conservative": "$XXX",
      "moderate": "$XXX",
      "optimistic": "$X,XXX"
    },
    "assumptions": "What these projections are based on"
  },
  "placementStrategy": [
    {
      "location": "Where in video",
      "effectiveness": "High/Medium/Low",
      "conversionRate": "X.X%",
      "tips": "Best practices"
    }
  ]
}`;

    const aiResponse = await openai.chat.completions.create({
      model: 'gpt-4o-mini',
      messages: [{ role: 'user', content: prompt }],
      response_format: { type: 'json_object' },
      max_tokens: 2000
    });

    let aiData;
    try {
      aiData = JSON.parse(aiResponse.choices[0].message.content);
    } catch (e) {
      aiData = {
        affiliatePrograms: [
          { icon: 'ðŸ›’', name: 'Amazon Associates', network: 'Amazon', commission: '1-10%', cookieDuration: '24 hours', avgOrderValue: '$50', estimatedEarnings: '$500/month', fitScore: 85, signupUrl: 'affiliate-program.amazon.com', whyItFits: 'Universal appeal for any niche' }
        ],
        placementScripts: [
          { type: 'Mid-roll', script: 'Speaking of which, I use [Product] for this and you can check it out in the description below.', duration: '10 seconds', tips: 'Keep it natural and brief' }
        ],
        earningsBreakdown: {
          monthly: { conservative: '$300', moderate: '$800', optimistic: '$2,000' },
          perVideo: { conservative: '$30', moderate: '$80', optimistic: '$200' },
          assumptions: 'Based on current subscriber count and typical conversion rates'
        },
        placementStrategy: [
          { location: 'Video description', effectiveness: 'High', conversionRate: '2.5%', tips: 'Put link above the fold' }
        ]
      };
    }

    // Save to history
    const historyData = {
      userId: uid,
      type: 'affiliate',
      channelUrl,
      channelName,
      channelThumbnail,
      subscribers: subscriberCount,
      niche,
      affiliatePrograms: aiData.affiliatePrograms,
      placementScripts: aiData.placementScripts,
      earningsBreakdown: aiData.earningsBreakdown,
      placementStrategy: aiData.placementStrategy,
      createdAt: admin.firestore.FieldValue.serverTimestamp()
    };

    await db.collection('affiliateHistory').add(historyData);
    await incrementUsage(uid, 'affiliateFinder');
    await logUsage(uid, 'affiliate_finder', { channelUrl, subscribers: subscriberCount });

    return {
      success: true,
      channelName,
      channelThumbnail,
      subscribers: subscriberCount,
      niche,
      affiliatePrograms: aiData.affiliatePrograms,
      placementScripts: aiData.placementScripts,
      earningsBreakdown: aiData.earningsBreakdown,
      placementStrategy: aiData.placementStrategy
    };

  } catch (error) {
    if (error instanceof functions.https.HttpsError) throw error;
    console.error('Affiliate finder error:', error);
    throw new functions.https.HttpsError('internal', 'Failed to find affiliate opportunities.');
  }
});

// ============================================================
// VIDEO-TO-MULTI-INCOME CONVERTER
// Analyzes a video to create multiple content pieces for various platforms
// ============================================================
exports.analyzeMultiIncome = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  checkRateLimit(uid, 'analyzeMultiIncome', 5);
  await checkUsageLimit(uid, 'multiIncomeConverter');

  const { videoUrl } = data;
  if (!videoUrl) {
    throw new functions.https.HttpsError('invalid-argument', 'Video URL is required.');
  }

  try {
    // Extract video ID
    const videoId = extractVideoId(videoUrl);
    if (!videoId) {
      throw new functions.https.HttpsError('invalid-argument', 'Invalid YouTube video URL.');
    }

    // Fetch video data
    const videoResponse = await youtube.videos.list({
      part: 'snippet,statistics,contentDetails',
      id: videoId
    });

    if (!videoResponse.data.items || videoResponse.data.items.length === 0) {
      throw new functions.https.HttpsError('not-found', 'Video not found.');
    }

    const video = videoResponse.data.items[0];
    const videoTitle = video.snippet.title;
    const videoThumbnail = video.snippet.thumbnails?.maxres?.url || video.snippet.thumbnails?.high?.url || video.snippet.thumbnails?.medium?.url;
    const videoDescription = video.snippet.description || '';
    const viewCount = parseInt(video.statistics.viewCount) || 0;
    const likeCount = parseInt(video.statistics.likeCount) || 0;
    const channelTitle = video.snippet.channelTitle;
    const duration = video.contentDetails.duration;
    const tags = video.snippet.tags?.slice(0, 10).join(', ') || '';

    // Parse duration
    const durationMatch = duration.match(/PT(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?/);
    const hours = parseInt(durationMatch?.[1] || 0);
    const minutes = parseInt(durationMatch?.[2] || 0);
    const seconds = parseInt(durationMatch?.[3] || 0);
    const totalMinutes = hours * 60 + minutes + Math.round(seconds / 60);

    // Use AI to create multi-platform content strategy
    const prompt = `You are a content repurposing expert. Analyze this YouTube video and create a comprehensive multi-platform income strategy:

Video: ${videoTitle}
Channel: ${channelTitle}
Views: ${viewCount.toLocaleString()}
Likes: ${likeCount.toLocaleString()}
Duration: ${totalMinutes} minutes
Description: ${videoDescription.slice(0, 400)}
Tags: ${tags}

Create a strategy to repurpose this video into multiple income streams:
1. 6 content pieces for different platforms
2. Distribution strategy across platforms
3. Revenue potential for each platform
4. Step-by-step action items

Return as JSON:
{
  "contentPieces": [
    {
      "icon": "emoji",
      "platform": "Platform name",
      "contentType": "Short/Article/Thread/Post/etc",
      "title": "Suggested title or hook",
      "description": "What this content would be",
      "estimatedReach": "X,XXX-XX,XXX",
      "timeToCreate": "X hours",
      "monetization": "How to monetize this"
    }
  ],
  "distributionStrategy": {
    "immediate": ["Platform 1", "Platform 2"],
    "within24Hours": ["Platform 3", "Platform 4"],
    "withinWeek": ["Platform 5", "Platform 6"],
    "schedule": "Recommended posting schedule"
  },
  "revenuePotential": [
    {
      "platform": "Platform name",
      "monthlyPotential": "$XXX-$X,XXX",
      "revenueType": "Ads/Affiliate/Sponsorship/etc",
      "requirements": "What's needed to monetize"
    }
  ],
  "actionItems": [
    {
      "step": 1,
      "action": "What to do",
      "timeRequired": "X hours",
      "tools": "Tools needed",
      "priority": "High/Medium/Low"
    }
  ],
  "summary": {
    "totalPotentialRevenue": "$X,XXX/month",
    "totalTimeInvestment": "X hours",
    "quickestWin": "Platform/content that can generate income fastest"
  }
}`;

    const aiResponse = await openai.chat.completions.create({
      model: 'gpt-4o-mini',
      messages: [{ role: 'user', content: prompt }],
      response_format: { type: 'json_object' },
      max_tokens: 2000
    });

    let aiData;
    try {
      aiData = JSON.parse(aiResponse.choices[0].message.content);
    } catch (e) {
      aiData = {
        contentPieces: [
          { icon: 'ðŸ“±', platform: 'TikTok', contentType: 'Short', title: 'Key moment highlight', description: 'Extract the most engaging 60 seconds', estimatedReach: '5,000-50,000', timeToCreate: '1 hour', monetization: 'Creator fund + affiliate links' }
        ],
        distributionStrategy: {
          immediate: ['YouTube Shorts', 'TikTok'],
          within24Hours: ['Instagram Reels', 'Twitter'],
          withinWeek: ['LinkedIn Article', 'Blog Post'],
          schedule: 'Post shorts immediately, long-form content within a week'
        },
        revenuePotential: [
          { platform: 'TikTok', monthlyPotential: '$100-$500', revenueType: 'Creator Fund', requirements: '10K followers' }
        ],
        actionItems: [
          { step: 1, action: 'Extract key clips', timeRequired: '2 hours', tools: 'Video editor', priority: 'High' }
        ],
        summary: {
          totalPotentialRevenue: '$500-$2,000/month',
          totalTimeInvestment: '10 hours',
          quickestWin: 'YouTube Shorts from existing content'
        }
      };
    }

    // Save to history
    const historyData = {
      userId: uid,
      type: 'multiincome',
      videoUrl,
      videoId,
      videoTitle,
      videoThumbnail,
      channelTitle,
      views: viewCount,
      duration: totalMinutes,
      contentPieces: aiData.contentPieces,
      distributionStrategy: aiData.distributionStrategy,
      revenuePotential: aiData.revenuePotential,
      actionItems: aiData.actionItems,
      summary: aiData.summary,
      createdAt: admin.firestore.FieldValue.serverTimestamp()
    };

    await db.collection('multiIncomeHistory').add(historyData);
    await incrementUsage(uid, 'multiIncomeConverter');
    await logUsage(uid, 'multi_income_converter', { videoUrl, views: viewCount });

    return {
      success: true,
      videoTitle,
      videoThumbnail,
      channelTitle,
      views: viewCount,
      duration: totalMinutes,
      contentPieces: aiData.contentPieces,
      distributionStrategy: aiData.distributionStrategy,
      revenuePotential: aiData.revenuePotential,
      actionItems: aiData.actionItems,
      summary: aiData.summary
    };

  } catch (error) {
    if (error instanceof functions.https.HttpsError) throw error;
    console.error('Multi-income converter error:', error);
    throw new functions.https.HttpsError('internal', 'Failed to analyze video for income streams.');
  }
});

// ============================================================
// BRAND DEAL MATCHMAKER
// Finds brand partnership opportunities for creators
// ============================================================
exports.analyzeBrandDeal = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  checkRateLimit(uid, 'analyzeBrandDeal', 5);
  await checkUsageLimit(uid, 'brandDealMatchmaker');

  const { channelUrl } = data;
  if (!channelUrl) {
    throw new functions.https.HttpsError('invalid-argument', 'Channel URL is required.');
  }

  try {
    // Extract channel info from URL
    const channelInfo = extractChannelInfo(channelUrl);

    // Get channel details based on URL type
    let channelResponse;
    if (channelInfo.type === 'id') {
      channelResponse = await youtube.channels.list({
        part: 'snippet,statistics,topicDetails',
        id: channelInfo.value
      });
    } else if (channelInfo.type === 'handle') {
      channelResponse = await youtube.channels.list({
        part: 'snippet,statistics,topicDetails',
        forHandle: channelInfo.value
      });
    } else {
      // Search for custom/user URLs
      const searchResponse = await youtube.search.list({
        part: 'snippet',
        q: channelInfo.value,
        type: 'channel',
        maxResults: 1
      });

      if (!searchResponse.data.items?.length) {
        throw new functions.https.HttpsError('not-found', 'Channel not found.');
      }

      channelResponse = await youtube.channels.list({
        part: 'snippet,statistics,topicDetails',
        id: searchResponse.data.items[0].snippet.channelId
      });
    }

    if (!channelResponse.data.items || channelResponse.data.items.length === 0) {
      throw new functions.https.HttpsError('not-found', 'Channel not found.');
    }

    const channel = channelResponse.data.items[0];
    const channelId = channel.id;
    const channelName = channel.snippet.title;
    const channelThumbnail = channel.snippet.thumbnails?.medium?.url || channel.snippet.thumbnails?.default?.url;
    const channelDescription = channel.snippet.description || '';
    const subscriberCount = parseInt(channel.statistics.subscriberCount) || 0;
    const viewCount = parseInt(channel.statistics.viewCount) || 0;
    const topicCategories = channel.topicDetails?.topicCategories?.map(t => t.split('/').pop()) || [];

    // Get popular videos
    const videosResponse = await youtube.search.list({
      part: 'snippet',
      channelId: channelId,
      type: 'video',
      order: 'viewCount',
      maxResults: 15
    });

    const videoTitles = videosResponse.data.items?.map(v => v.snippet.title).join(', ') || '';

    // Determine niche
    let niche = 'General';
    const nicheKeywords = ['Finance', 'Technology', 'Gaming', 'Education', 'Lifestyle', 'Beauty', 'Health', 'Food', 'Travel', 'Entertainment', 'Business', 'Fitness', 'Fashion'];
    for (const topic of topicCategories) {
      for (const keyword of nicheKeywords) {
        if (topic.toLowerCase().includes(keyword.toLowerCase())) {
          niche = keyword;
          break;
        }
      }
    }

    // Use AI to find brand matches
    const prompt = `You are a brand partnership expert. Analyze this YouTube channel and find ideal brand partners:

Channel: ${channelName}
Subscribers: ${subscriberCount.toLocaleString()}
Total Views: ${viewCount.toLocaleString()}
Niche: ${niche}
Description: ${channelDescription.slice(0, 300)}
Popular videos: ${videoTitles.slice(0, 500)}

Create a comprehensive brand deal strategy:
1. 6 brands that would be perfect partners for this channel
2. Pitch templates for outreach
3. Negotiation tips specific to this creator's level

Return as JSON:
{
  "matchedBrands": [
    {
      "icon": "emoji",
      "name": "Brand name",
      "industry": "Industry category",
      "matchScore": 95,
      "whyMatch": "Why this brand is perfect for this channel",
      "dealRange": "$X,XXX - $XX,XXX",
      "contactMethod": "How to reach out"
    }
  ],
  "pitchTemplates": [
    {
      "icon": "emoji",
      "type": "Email/DM/Cold Outreach",
      "template": "Full pitch template text with placeholders"
    }
  ],
  "negotiationTips": [
    {
      "title": "Tip title",
      "description": "Detailed negotiation advice"
    }
  ]
}`;

    const aiResponse = await openai.chat.completions.create({
      model: 'gpt-4o-mini',
      messages: [{ role: 'user', content: prompt }],
      response_format: { type: 'json_object' },
      max_tokens: 2000
    });

    let aiData;
    try {
      aiData = JSON.parse(aiResponse.choices[0].message.content);
    } catch (e) {
      aiData = {
        matchedBrands: [
          { icon: 'ðŸ¢', name: 'Sample Brand', industry: 'Technology', matchScore: 85, whyMatch: 'Aligned audience demographics', dealRange: '$500 - $2,000', contactMethod: 'Email marketing team' }
        ],
        pitchTemplates: [
          { icon: 'ðŸ“§', type: 'Email', template: 'Hi [Brand],\n\nI run [Channel Name] with [X] subscribers...' }
        ],
        negotiationTips: [
          { title: 'Know Your Worth', description: 'Research industry rates before negotiating' }
        ]
      };
    }

    // Save to history
    const historyData = {
      userId: uid,
      type: 'branddeal',
      channelUrl,
      channelName,
      channelThumbnail,
      subscribers: subscriberCount,
      niche,
      matchedBrands: aiData.matchedBrands,
      pitchTemplates: aiData.pitchTemplates,
      negotiationTips: aiData.negotiationTips,
      createdAt: admin.firestore.FieldValue.serverTimestamp()
    };

    await db.collection('brandDealHistory').add(historyData);
    await incrementUsage(uid, 'brandDealMatchmaker');
    await logUsage(uid, 'brand_deal_matchmaker', { channelUrl, subscribers: subscriberCount });

    return {
      success: true,
      channelName,
      channelThumbnail,
      subscribers: subscriberCount,
      niche,
      matchedBrands: aiData.matchedBrands,
      pitchTemplates: aiData.pitchTemplates,
      negotiationTips: aiData.negotiationTips
    };

  } catch (error) {
    if (error instanceof functions.https.HttpsError) throw error;
    console.error('Brand deal matchmaker error:', error);
    throw new functions.https.HttpsError('internal', 'Failed to find brand deals.');
  }
});

// ============================================================
// LICENSING & SYNDICATION SCOUT
// Finds licensing and syndication opportunities for content
// ============================================================
exports.analyzeLicensing = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  checkRateLimit(uid, 'analyzeLicensing', 5);
  await checkUsageLimit(uid, 'licensingScout');

  const { channelUrl } = data;
  if (!channelUrl) {
    throw new functions.https.HttpsError('invalid-argument', 'Channel URL is required.');
  }

  try {
    // Extract channel info from URL
    const channelInfo = extractChannelInfo(channelUrl);

    // Get channel details based on URL type
    let channelResponse;
    if (channelInfo.type === 'id') {
      channelResponse = await youtube.channels.list({
        part: 'snippet,statistics,topicDetails',
        id: channelInfo.value
      });
    } else if (channelInfo.type === 'handle') {
      channelResponse = await youtube.channels.list({
        part: 'snippet,statistics,topicDetails',
        forHandle: channelInfo.value
      });
    } else {
      // Search for custom/user URLs
      const searchResponse = await youtube.search.list({
        part: 'snippet',
        q: channelInfo.value,
        type: 'channel',
        maxResults: 1
      });

      if (!searchResponse.data.items?.length) {
        throw new functions.https.HttpsError('not-found', 'Channel not found.');
      }

      channelResponse = await youtube.channels.list({
        part: 'snippet,statistics,topicDetails',
        id: searchResponse.data.items[0].snippet.channelId
      });
    }

    if (!channelResponse.data.items || channelResponse.data.items.length === 0) {
      throw new functions.https.HttpsError('not-found', 'Channel not found.');
    }

    const channel = channelResponse.data.items[0];
    const channelId = channel.id;
    const channelName = channel.snippet.title;
    const channelThumbnail = channel.snippet.thumbnails?.medium?.url || channel.snippet.thumbnails?.default?.url;
    const channelDescription = channel.snippet.description || '';
    const subscriberCount = parseInt(channel.statistics.subscriberCount) || 0;
    const videoCount = parseInt(channel.statistics.videoCount) || 0;
    const topicCategories = channel.topicDetails?.topicCategories?.map(t => t.split('/').pop()) || [];

    // Get popular videos
    const videosResponse = await youtube.search.list({
      part: 'snippet',
      channelId: channelId,
      type: 'video',
      order: 'viewCount',
      maxResults: 15
    });

    const videoTitles = videosResponse.data.items?.map(v => v.snippet.title).join(', ') || '';

    // Determine niche
    let niche = 'General';
    const nicheKeywords = ['Finance', 'Technology', 'Gaming', 'Education', 'Lifestyle', 'Beauty', 'Health', 'Food', 'Travel', 'Entertainment', 'Business', 'News', 'Sports'];
    for (const topic of topicCategories) {
      for (const keyword of nicheKeywords) {
        if (topic.toLowerCase().includes(keyword.toLowerCase())) {
          niche = keyword;
          break;
        }
      }
    }

    // Use AI to find licensing opportunities
    const prompt = `You are a content licensing expert. Analyze this YouTube channel and find licensing/syndication opportunities:

Channel: ${channelName}
Subscribers: ${subscriberCount.toLocaleString()}
Videos: ${videoCount}
Niche: ${niche}
Description: ${channelDescription.slice(0, 300)}
Popular videos: ${videoTitles.slice(0, 500)}

Create a comprehensive licensing strategy:
1. 5 licensing opportunities for this content
2. 4 syndication networks to join
3. Step-by-step action plan

Return as JSON:
{
  "opportunities": [
    {
      "icon": "emoji",
      "platform": "Platform/Company name",
      "type": "Licensing/Syndication/Compilation/Stock",
      "description": "What this opportunity involves",
      "potentialRevenue": "$X,XXX/month",
      "requirements": "What's needed to qualify"
    }
  ],
  "syndicationNetworks": [
    {
      "icon": "emoji",
      "name": "Network name",
      "description": "What this network does",
      "revenueModel": "How you earn money"
    }
  ],
  "actionSteps": [
    {
      "action": "What to do",
      "details": "How to do it"
    }
  ]
}`;

    const aiResponse = await openai.chat.completions.create({
      model: 'gpt-4o-mini',
      messages: [{ role: 'user', content: prompt }],
      response_format: { type: 'json_object' },
      max_tokens: 2000
    });

    let aiData;
    try {
      aiData = JSON.parse(aiResponse.choices[0].message.content);
    } catch (e) {
      aiData = {
        opportunities: [
          { icon: 'ðŸ“º', platform: 'TV Networks', type: 'Licensing', description: 'License clips to news channels', potentialRevenue: '$500/month', requirements: 'High-quality original content' }
        ],
        syndicationNetworks: [
          { icon: 'ðŸŒ', name: 'Jukin Media', description: 'Viral video licensing network', revenueModel: 'Revenue share on licensed content' }
        ],
        actionSteps: [
          { action: 'Register content with ID systems', details: 'Sign up for Content ID to track usage' }
        ]
      };
    }

    // Save to history
    const historyData = {
      userId: uid,
      type: 'licensing',
      channelUrl,
      channelName,
      channelThumbnail,
      subscribers: subscriberCount,
      niche,
      opportunities: aiData.opportunities,
      syndicationNetworks: aiData.syndicationNetworks,
      actionSteps: aiData.actionSteps,
      createdAt: admin.firestore.FieldValue.serverTimestamp()
    };

    await db.collection('licensingHistory').add(historyData);
    await incrementUsage(uid, 'licensingScout');
    await logUsage(uid, 'licensing_scout', { channelUrl, subscribers: subscriberCount });

    return {
      success: true,
      channelName,
      channelThumbnail,
      subscribers: subscriberCount,
      niche,
      opportunities: aiData.opportunities,
      syndicationNetworks: aiData.syndicationNetworks,
      actionSteps: aiData.actionSteps
    };

  } catch (error) {
    if (error instanceof functions.https.HttpsError) throw error;
    console.error('Licensing scout error:', error);
    throw new functions.https.HttpsError('internal', 'Failed to find licensing opportunities.');
  }
});

// ============================================================
// REVENUE AUTOMATION PIPELINE
// Creates automated revenue systems for creators
// ============================================================
exports.analyzeAutomation = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  checkRateLimit(uid, 'analyzeAutomation', 5);
  await checkUsageLimit(uid, 'automationPipeline');

  const { channelUrl } = data;
  if (!channelUrl) {
    throw new functions.https.HttpsError('invalid-argument', 'Channel URL is required.');
  }

  try {
    // Extract channel info from URL
    const channelInfo = extractChannelInfo(channelUrl);

    // Get channel details based on URL type
    let channelResponse;
    if (channelInfo.type === 'id') {
      channelResponse = await youtube.channels.list({
        part: 'snippet,statistics,topicDetails',
        id: channelInfo.value
      });
    } else if (channelInfo.type === 'handle') {
      channelResponse = await youtube.channels.list({
        part: 'snippet,statistics,topicDetails',
        forHandle: channelInfo.value
      });
    } else {
      // Search for custom/user URLs
      const searchResponse = await youtube.search.list({
        part: 'snippet',
        q: channelInfo.value,
        type: 'channel',
        maxResults: 1
      });

      if (!searchResponse.data.items?.length) {
        throw new functions.https.HttpsError('not-found', 'Channel not found.');
      }

      channelResponse = await youtube.channels.list({
        part: 'snippet,statistics,topicDetails',
        id: searchResponse.data.items[0].snippet.channelId
      });
    }

    if (!channelResponse.data.items || channelResponse.data.items.length === 0) {
      throw new functions.https.HttpsError('not-found', 'Channel not found.');
    }

    const channel = channelResponse.data.items[0];
    const channelId = channel.id;
    const channelName = channel.snippet.title;
    const channelThumbnail = channel.snippet.thumbnails?.medium?.url || channel.snippet.thumbnails?.default?.url;
    const channelDescription = channel.snippet.description || '';
    const subscriberCount = parseInt(channel.statistics.subscriberCount) || 0;
    const videoCount = parseInt(channel.statistics.videoCount) || 0;
    const topicCategories = channel.topicDetails?.topicCategories?.map(t => t.split('/').pop()) || [];

    // Get recent videos
    const videosResponse = await youtube.search.list({
      part: 'snippet',
      channelId: channelId,
      type: 'video',
      order: 'date',
      maxResults: 10
    });

    const videoTitles = videosResponse.data.items?.map(v => v.snippet.title).join(', ') || '';

    // Determine niche
    let niche = 'General';
    const nicheKeywords = ['Finance', 'Technology', 'Gaming', 'Education', 'Lifestyle', 'Beauty', 'Health', 'Food', 'Travel', 'Entertainment', 'Business'];
    for (const topic of topicCategories) {
      for (const keyword of nicheKeywords) {
        if (topic.toLowerCase().includes(keyword.toLowerCase())) {
          niche = keyword;
          break;
        }
      }
    }

    // Calculate automation score based on channel size
    let automationScore = 50;
    if (subscriberCount > 100000) automationScore = 90;
    else if (subscriberCount > 50000) automationScore = 80;
    else if (subscriberCount > 10000) automationScore = 70;
    else if (subscriberCount > 1000) automationScore = 60;

    // Use AI to create automation pipeline
    const prompt = `You are a revenue automation expert for content creators. Analyze this YouTube channel and create an automation pipeline:

Channel: ${channelName}
Subscribers: ${subscriberCount.toLocaleString()}
Videos: ${videoCount}
Niche: ${niche}
Description: ${channelDescription.slice(0, 300)}
Recent videos: ${videoTitles.slice(0, 400)}

Create a comprehensive automation strategy:
1. Revenue summary (current vs automated potential)
2. 5 automation workflows to implement
3. Recommended tool stack
4. Implementation timeline

Return as JSON:
{
  "revenueSummary": {
    "currentManual": "$X,XXX/month",
    "afterAutomation": "$XX,XXX/month",
    "timeSaved": "XX hours/week"
  },
  "workflows": [
    {
      "icon": "emoji",
      "name": "Workflow name",
      "category": "Content/Sales/Marketing/Admin",
      "description": "What this workflow automates",
      "difficulty": "Easy/Medium/Hard",
      "revenueImpact": "+$X,XXX/month",
      "tools": "Tools needed"
    }
  ],
  "toolStack": [
    {
      "icon": "emoji",
      "name": "Tool name",
      "purpose": "What it does",
      "pricing": "Free/$XX/month"
    }
  ],
  "timeline": [
    {
      "week": "Week 1-2",
      "focus": "What to focus on",
      "tasks": "Specific tasks to complete"
    }
  ]
}`;

    const aiResponse = await openai.chat.completions.create({
      model: 'gpt-4o-mini',
      messages: [{ role: 'user', content: prompt }],
      response_format: { type: 'json_object' },
      max_tokens: 2000
    });

    let aiData;
    try {
      aiData = JSON.parse(aiResponse.choices[0].message.content);
    } catch (e) {
      aiData = {
        revenueSummary: {
          currentManual: '$1,000/month',
          afterAutomation: '$3,000/month',
          timeSaved: '15 hours/week'
        },
        workflows: [
          { icon: 'ðŸ“§', name: 'Email Automation', category: 'Marketing', description: 'Automated email sequences', difficulty: 'Easy', revenueImpact: '+$500/month', tools: 'ConvertKit' }
        ],
        toolStack: [
          { icon: 'ðŸ“§', name: 'ConvertKit', purpose: 'Email marketing automation', pricing: '$29/month' }
        ],
        timeline: [
          { week: 'Week 1-2', focus: 'Set up foundation', tasks: 'Create accounts, connect integrations' }
        ]
      };
    }

    // Save to history
    const historyData = {
      userId: uid,
      type: 'automation',
      channelUrl,
      channelName,
      channelThumbnail,
      subscribers: subscriberCount,
      niche,
      automationScore,
      revenueSummary: aiData.revenueSummary,
      workflows: aiData.workflows,
      toolStack: aiData.toolStack,
      timeline: aiData.timeline,
      createdAt: admin.firestore.FieldValue.serverTimestamp()
    };

    await db.collection('automationHistory').add(historyData);
    await incrementUsage(uid, 'automationPipeline');
    await logUsage(uid, 'automation_pipeline', { channelUrl, subscribers: subscriberCount });

    return {
      success: true,
      channelName,
      channelThumbnail,
      subscribers: subscriberCount,
      niche,
      automationScore,
      revenueSummary: aiData.revenueSummary,
      workflows: aiData.workflows,
      toolStack: aiData.toolStack,
      timeline: aiData.timeline
    };

  } catch (error) {
    if (error instanceof functions.https.HttpsError) throw error;
    console.error('Automation pipeline error:', error);
    throw new functions.https.HttpsError('internal', 'Failed to build automation pipeline.');
  }
});

// ==============================================
// VIDEO-TO-SHORTS WIZARD FUNCTIONS
// ==============================================

/**
 * Extract video ID from various YouTube URL formats
 */
function extractYouTubeVideoId(url) {
  if (!url) return null;
  const patterns = [
    /(?:youtube\.com\/watch\?v=|youtu\.be\/|youtube\.com\/embed\/|youtube\.com\/v\/)([a-zA-Z0-9_-]{11})/,
    /^([a-zA-Z0-9_-]{11})$/
  ];
  for (const pattern of patterns) {
    const match = url.match(pattern);
    if (match) return match[1];
  }
  return null;
}

/**
 * Parse ISO 8601 duration to seconds
 */
function parseDurationToSeconds(duration) {
  if (!duration) return 0;
  const match = duration.match(/PT(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?/);
  if (!match) return 0;
  return parseInt(match[1] || 0) * 3600 + parseInt(match[2] || 0) * 60 + parseInt(match[3] || 0);
}

/**
 * wizardAnalyzeVideo - Analyzes video and finds potential viral clips
 * Uses transcript analysis for better clip identification
 */
exports.wizardAnalyzeVideo = functions
  .runWith({ timeoutSeconds: 300, memory: '1GB' })
  .https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  checkRateLimit(uid, 'wizardAnalyzeVideo', 3);

  // Check and deduct tokens for video analysis
  const tokenCosts = await getWizardTokenCosts();
  const analyzeCost = tokenCosts.analyzeVideo || 5;

  const tokenResult = await deductWizardTokens(uid, analyzeCost, 'analyzeVideo', {
    videoUrl: data.videoUrl || 'uploaded_file'
  });

  if (!tokenResult.success) {
    throw new functions.https.HttpsError(
      'resource-exhausted',
      `Insufficient tokens. This operation requires ${analyzeCost} tokens, but you have ${tokenResult.available || 0}.`
    );
  }

  const { videoUrl, options, uploadedVideoUrl, uploadedVideoPath, uploadedVideoName, extensionData, useExtension, contentType, platformPreset } = data;
  const isUploadedFile = !!uploadedVideoUrl;
  const hasExtensionData = useExtension && extensionData && extensionData.videoInfo;

  // Platform preset settings with defaults
  const platform = contentType || 'youtube-shorts';
  const presetConfig = platformPreset || {
    minDuration: 45,
    maxDuration: 60,
    targetDuration: 55,
    durationRange: '50-60',
    aiPrompt: 'Each clip should be 50-60 seconds to maximize YouTube Shorts watch time. Focus on complete story arcs with satisfying conclusions.'
  };

  console.log('[wizardAnalyzeVideo] Platform settings:', { platform, targetDuration: presetConfig.targetDuration, durationRange: presetConfig.durationRange });

  // Log extension data if provided
  if (hasExtensionData) {
    console.log('[wizardAnalyzeVideo] Extension data provided:', {
      hasVideoInfo: !!extensionData.videoInfo,
      hasStreamData: !!extensionData.streamData,
      videoId: extensionData.videoInfo?.videoId
    });
  }

  // Validate input - need either YouTube URL or uploaded file
  if (!videoUrl && !uploadedVideoUrl) {
    throw new functions.https.HttpsError('invalid-argument', 'Video URL or uploaded file is required');
  }

  // Handle uploaded file
  if (isUploadedFile) {
    console.log('Processing uploaded video file:', uploadedVideoName);

    // Generate a unique ID for uploaded videos
    const videoId = `upload_${Date.now()}_${uid.substring(0, 8)}`;
    const videoData = {
      videoId,
      title: uploadedVideoName || 'Uploaded Video',
      description: 'Uploaded video file',
      channelTitle: 'User Upload',
      thumbnail: '', // No thumbnail for uploaded files initially
      duration: 0, // Will be determined during processing
      viewCount: 0,
      likeCount: 0,
      isUpload: true,
      uploadedVideoUrl,
      uploadedVideoPath
    };

    // For uploaded files, we'll create a project and process it differently
    // The video processor will handle extracting duration and generating clips
    // IMPORTANT: Create sourceAsset from the uploaded file - this is the canonical source for export
    const sourceAsset = {
      storageUrl: uploadedVideoUrl,
      storagePath: uploadedVideoPath || null,
      duration: 0, // Will be updated by video processor
      format: 'video/mp4',
      fileSize: 0,
      capturedAt: Date.now(),
      source: 'direct_upload'
    };

    const projectData = {
      userId: uid,
      videoId,
      videoUrl: uploadedVideoUrl,
      videoData,
      clips: [], // Will be populated by video processor
      isUpload: true,
      uploadedVideoPath,
      uploadedVideoName: uploadedVideoName || 'Uploaded Video',
      sourceAsset, // Canonical source for export - uses the uploaded file
      options: options || {},
      status: 'pending_processing', // Needs video processor to analyze
      createdAt: admin.firestore.FieldValue.serverTimestamp(),
      updatedAt: admin.firestore.FieldValue.serverTimestamp()
    };

    // Enforce max projects limit before creating new project
    const maxProjects = await getMaxProjectsLimit();
    await enforceMaxProjects(uid, maxProjects);

    const projectRef = await db.collection('wizardProjects').add(projectData);
    await logUsage(uid, 'wizard_analyze_upload', { videoId, fileName: uploadedVideoName });

    // Call video processor to analyze the uploaded file
    try {
      const videoProcessorUrl = functions.config().videoprocessor?.url;
      if (videoProcessorUrl) {
        // Trigger async processing
        const processorResponse = await fetch(`${videoProcessorUrl}/analyze-upload`, {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({
            projectId: projectRef.id,
            videoUrl: uploadedVideoUrl,
            storagePath: uploadedVideoPath,
            userId: uid
          })
        });

        if (processorResponse.ok) {
          const result = await processorResponse.json();
          // Update project with analysis results
          if (result.clips && result.duration) {
            await projectRef.update({
              clips: result.clips,
              'videoData.duration': result.duration,
              status: 'analyzed',
              updatedAt: admin.firestore.FieldValue.serverTimestamp()
            });
            projectData.clips = result.clips;
            projectData.videoData.duration = result.duration;
            projectData.status = 'analyzed';
          }
        }
      }
    } catch (processorError) {
      console.log('Video processor not available for uploaded file:', processorError.message);
      // Continue without processor - clips will be generated on export
    }

    return {
      success: true,
      projectId: projectRef.id,
      videoData: projectData.videoData,
      clips: projectData.clips,
      isUpload: true,
      sourceAsset: sourceAsset, // Return sourceAsset so frontend can store it
      message: 'Uploaded video ready for processing'
    };
  }

  // Handle YouTube URL (existing logic)
  const videoId = extractYouTubeVideoId(videoUrl);
  if (!videoId) {
    throw new functions.https.HttpsError('invalid-argument', 'Invalid YouTube URL');
  }

  try {
    let videoData;
    let snippet = {};
    let stats = {};
    let durationSeconds = 0;

    // Check if extension provided USEFUL video info (not just empty/sparse data)
    // We need at least a real title AND duration to skip the YouTube API
    const extInfo = extensionData?.videoInfo;
    const hasUsefulExtensionData = hasExtensionData && extInfo &&
      extInfo.title && extInfo.title !== 'YouTube Video' && // Has real title
      (extInfo.duration && extInfo.duration !== 0); // Has real duration

    console.log('[wizardAnalyzeVideo] Extension data check:', {
      hasExtensionData,
      hasVideoInfo: !!extInfo,
      title: extInfo?.title || 'none',
      duration: extInfo?.duration || 'none',
      hasUsefulExtensionData
    });

    // If extension provided USEFUL video info, use it; otherwise fetch from YouTube API
    if (hasUsefulExtensionData) {
      console.log('[wizardAnalyzeVideo] Using extension-provided video info:', extInfo.title);

      // Parse duration from extension format (e.g., "10:30" or "1:05:30" or seconds)
      if (typeof extInfo.duration === 'number') {
        durationSeconds = extInfo.duration;
      } else if (typeof extInfo.duration === 'string' && extInfo.duration.includes(':')) {
        const parts = extInfo.duration.split(':').map(Number);
        if (parts.length === 3) {
          durationSeconds = parts[0] * 3600 + parts[1] * 60 + parts[2];
        } else if (parts.length === 2) {
          durationSeconds = parts[0] * 60 + parts[1];
        }
      }

      videoData = {
        videoId,
        title: extInfo.title,
        description: '', // Extension doesn't capture description
        channelTitle: extInfo.channel || extInfo.channelTitle || 'Unknown Channel',
        thumbnail: extInfo.thumbnail || `https://img.youtube.com/vi/${videoId}/maxresdefault.jpg`,
        duration: durationSeconds || 300, // Default to 5 mins if unknown
        viewCount: 0,
        likeCount: 0,
        fromExtension: true
      };

      // Store extension stream data for later use in processing
      // CRITICAL: Preserve ALL fields from streamData, especially source and uploadedToStorage
      // These fields are needed by the video processor to determine the download method
      if (extensionData.streamData) {
        videoData.extensionStreamData = {
          ...extensionData.streamData,  // Preserve ALL fields from extension
          capturedAt: extensionData.streamData.capturedAt || Date.now()
        };
        console.log('[wizardAnalyzeVideo] Extension stream data stored:', {
          hasVideoUrl: !!extensionData.streamData.videoUrl,
          hasAudioUrl: !!extensionData.streamData.audioUrl,
          quality: extensionData.streamData.quality,
          source: extensionData.streamData.source,
          uploadedToStorage: extensionData.streamData.uploadedToStorage,
          captureStartTime: extensionData.streamData.captureStartTime,
          captureEndTime: extensionData.streamData.captureEndTime
        });
      }

      // Also set snippet for the AI prompt
      snippet = {
        title: videoData.title,
        description: videoData.description,
        channelTitle: videoData.channelTitle
      };
      stats = { viewCount: 0, likeCount: 0 };
    } else {
      // Fallback to YouTube API (extension didn't provide useful data)
      console.log('[wizardAnalyzeVideo] Fetching video metadata from YouTube API (extension data was sparse or missing)');
      const videoResponse = await youtube.videos.list({
        part: ['snippet', 'statistics', 'contentDetails'],
        id: [videoId]
      });

      if (!videoResponse.data.items || videoResponse.data.items.length === 0) {
        throw new functions.https.HttpsError('not-found', 'Video not found');
      }

      const video = videoResponse.data.items[0];
      snippet = video.snippet;
      stats = video.statistics;
      durationSeconds = parseDurationToSeconds(video.contentDetails.duration);

      videoData = {
        videoId,
        title: snippet.title,
        description: snippet.description?.substring(0, 1000) || '',
        channelTitle: snippet.channelTitle,
        thumbnail: snippet.thumbnails?.maxres?.url || snippet.thumbnails?.high?.url || `https://img.youtube.com/vi/${videoId}/hqdefault.jpg`,
        duration: durationSeconds,
        viewCount: parseInt(stats.viewCount || 0),
        likeCount: parseInt(stats.likeCount || 0)
      };

      console.log('[wizardAnalyzeVideo] YouTube API returned:', {
        title: videoData.title,
        duration: videoData.duration,
        channel: videoData.channelTitle
      });
    }

    // Get actual transcript using the working getVideoTranscript function
    // Add 30 second timeout for transcript fetch
    let transcriptData = { segments: [], fullText: '' };
    try {
      const TRANSCRIPT_TIMEOUT_MS = 30000;
      transcriptData = await Promise.race([
        getVideoTranscript(videoId),
        new Promise((_, reject) =>
          setTimeout(() => reject(new Error('Transcript fetch timeout')), TRANSCRIPT_TIMEOUT_MS)
        )
      ]);
      console.log(`Fetched transcript: ${transcriptData.segments.length} segments, ${transcriptData.fullText.length} chars`);
    } catch (transcriptError) {
      console.log('Transcript fetch note:', transcriptError.message);
      // Continue without transcript - AI can still analyze based on title/description
    }

    const transcriptSegments = transcriptData.segments;
    const fullTranscript = transcriptData.fullText;

    // ============================================
    // PHASE 1: Dynamic Clip Limits Based on Duration
    // ============================================
    // Calculate how many clips to request based on video length
    // Longer videos = more potential viral moments
    function calculateClipCount(durationSecs) {
      if (durationSecs < 600) {          // < 10 min
        return { min: 4, max: 8 };
      } else if (durationSecs < 1800) {  // 10-30 min
        return { min: 8, max: 15 };
      } else if (durationSecs < 3600) {  // 30-60 min
        return { min: 12, max: 25 };
      } else if (durationSecs < 7200) {  // 1-2 hours
        return { min: 20, max: 40 };
      } else {                            // 2+ hours
        return { min: 30, max: 60 };
      }
    }

    const clipLimits = calculateClipCount(durationSeconds);
    console.log(`[wizardAnalyzeVideo] Video duration: ${Math.floor(durationSeconds / 60)}min, requesting ${clipLimits.min}-${clipLimits.max} clips`);

    // ============================================
    // PHASE 2: Smart Transcript Sampling
    // ============================================
    // For long videos, sample transcript from different parts instead of just the beginning
    // This ensures AI sees content from intro, middle, and end
    function getSmartTranscriptSample(segments, fullText, maxChars, durationSecs) {
      if (!segments || segments.length === 0) {
        return fullText ? fullText.substring(0, maxChars) : '';
      }

      // For short videos (< 30 min), just use the full transcript up to limit
      if (durationSecs < 1800) {
        return fullText ? fullText.substring(0, maxChars) : '';
      }

      // For longer videos, sample from multiple segments
      const numSamples = durationSecs > 7200 ? 6 : durationSecs > 3600 ? 4 : 3; // 6 for 2h+, 4 for 1-2h, 3 for 30m-1h
      const charsPerSample = Math.floor(maxChars / numSamples);

      // Calculate time boundaries for each sample
      const sampleDuration = durationSecs / numSamples;
      const samples = [];

      for (let i = 0; i < numSamples; i++) {
        const sampleStart = i * sampleDuration;
        const sampleEnd = (i + 1) * sampleDuration;

        // Find transcript segments in this time range
        const segmentsInRange = segments.filter(seg => {
          const segStart = seg.start || seg.offset || 0;
          return segStart >= sampleStart && segStart < sampleEnd;
        });

        // Get text from these segments
        const sampleText = segmentsInRange.map(seg => seg.text || seg.snippet || '').join(' ');
        const truncatedSample = sampleText.substring(0, charsPerSample);

        if (truncatedSample.length > 50) {
          const timeLabel = `[${Math.floor(sampleStart / 60)}-${Math.floor(sampleEnd / 60)} min]`;
          samples.push(`${timeLabel}: ${truncatedSample}`);
        }
      }

      const result = samples.join('\n\n');
      console.log(`[wizardAnalyzeVideo] Smart transcript: ${numSamples} samples, ${result.length} total chars`);
      return result;
    }

    // Use smart sampling for transcript
    const transcriptCharLimit = Math.min(12000, Math.max(4000, Math.floor(durationSeconds * 2)));
    const transcriptForPrompt = getSmartTranscriptSample(transcriptSegments, fullTranscript, transcriptCharLimit, durationSeconds);
    console.log(`[wizardAnalyzeVideo] Transcript for prompt: ${transcriptForPrompt.length} chars (limit: ${transcriptCharLimit})`);

    const clipAnalysisPrompt = `You are an expert viral content analyst specializing in short-form video content. Analyze this YouTube video and identify ${clipLimits.min}-${clipLimits.max} DISTINCT, NON-OVERLAPPING viral clip opportunities.

VIDEO INFORMATION:
- Title: "${snippet.title}"
- Channel: ${snippet.channelTitle}
- Description: ${snippet.description?.substring(0, 800) || 'No description'}
- Total Duration: ${Math.floor(durationSeconds / 60)} minutes ${durationSeconds % 60} seconds (${durationSeconds} total seconds)
- Views: ${parseInt(stats.viewCount || 0).toLocaleString()}
- Likes: ${parseInt(stats.likeCount || 0).toLocaleString()}
${transcriptForPrompt ? `
ACTUAL VIDEO TRANSCRIPT:
${transcriptForPrompt}
${fullTranscript && fullTranscript.length > 4000 ? '\n[Transcript truncated...]' : ''}
` : ''}

CRITICAL REQUIREMENTS:
1. DIVERSITY: Each clip must focus on a DIFFERENT topic, moment, or theme - NO similar clips
2. SPREAD: Distribute clips across the ENTIRE video duration (beginning, middle, end)
3. NO OVERLAP: Clips must NOT overlap in time - minimum 60 second gap between clips
4. TARGET DURATION: ${presetConfig.aiPrompt}
5. CLIP LENGTH: Each clip should be ${presetConfig.durationRange} seconds (minimum ${presetConfig.minDuration}s, maximum ${presetConfig.maxDuration}s)
6. DIFFERENT START TIMES: Clips should NOT all start at similar timestamps

CLIP SELECTION CRITERIA (prioritize variety):
- Opening hooks (first 60 seconds) - max 1 clip
- Key turning points or revelations
- Emotional peaks (humor, inspiration, shock)
- Quotable statements or one-liners
- Actionable tips or advice
- Story climaxes
- Controversial or debate-worthy moments
- Behind-the-scenes insights
- Closing thoughts or calls-to-action

For each clip, analyze:
- What makes this moment UNIQUE from other clips
- Why this specific timestamp would perform well on short-form platforms
- The emotional hook that will stop scrolling

RESPOND IN VALID JSON:
{
  "clips": [
    {
      "startTime": <integer seconds from video start>,
      "endTime": <integer seconds from video start>,
      "duration": <clip length in seconds, between ${presetConfig.minDuration}-${presetConfig.maxDuration}>,
      "transcript": "The key quote or summary of what's said in this moment (be specific)",
      "viralityScore": <0-100 based on viral potential>,
      "uniqueAngle": "What makes THIS clip different from others",
      "emotionalHook": "The emotion this triggers (curiosity, shock, inspiration, etc.)",
      "platforms": ["youtube", "tiktok", "instagram"],
      "reason": "Detailed explanation of viral potential"
    }
  ],
  "overallPotential": "High/Medium/Low with explanation",
  "bestTopics": ["main theme 1", "main theme 2", "main theme 3"],
  "contentType": "educational/entertainment/motivational/tutorial/vlog/other"
}

IMPORTANT:
- startTime must be >= 0 and < ${durationSeconds}
- endTime must be > startTime and <= ${durationSeconds}
- You MUST provide at least ${clipLimits.min} clips, ideally ${clipLimits.max} clips
- For this ${Math.floor(durationSeconds / 60)}-minute video, distribute clips evenly:
${durationSeconds > 3600 ? `  * Segment 1 (0-30min): ${Math.ceil(clipLimits.min / 4)} clips minimum
  * Segment 2 (30-60min): ${Math.ceil(clipLimits.min / 4)} clips minimum
  * Segment 3 (60-90min): ${Math.ceil(clipLimits.min / 4)} clips minimum
  * Segment 4 (90min+): ${Math.ceil(clipLimits.min / 4)} clips minimum` :
durationSeconds > 1800 ? `  * First third (0-${Math.floor(durationSeconds * 0.33)}s): ${Math.ceil(clipLimits.min / 3)} clips minimum
  * Middle third (${Math.floor(durationSeconds * 0.33)}-${Math.floor(durationSeconds * 0.66)}s): ${Math.ceil(clipLimits.min / 3)} clips minimum
  * Final third (${Math.floor(durationSeconds * 0.66)}-${durationSeconds}s): ${Math.ceil(clipLimits.min / 3)} clips minimum` :
`  * Spread evenly across: 0-${Math.floor(durationSeconds * 0.33)}s, ${Math.floor(durationSeconds * 0.33)}-${Math.floor(durationSeconds * 0.66)}s, ${Math.floor(durationSeconds * 0.66)}-${durationSeconds}s`}`;

    // Scale max_tokens based on expected clips (more clips = more JSON output)
    // ~250 tokens per clip in JSON format
    const maxTokens = Math.min(8000, Math.max(3500, clipLimits.max * 200));

    // Add 120 second timeout for main analysis (GPT-4o can be slow with long transcripts)
    const AI_ANALYSIS_TIMEOUT_MS = 120000;
    console.log('[wizardAnalyzeVideo] Starting AI clip analysis with 120s timeout...');

    // Helper function to generate fallback clips
    function generateFallbackClips() {
      const numClips = Math.min(clipLimits.max, Math.max(clipLimits.min, Math.floor(durationSeconds / 60)));
      console.log(`[wizardAnalyzeVideo] Fallback: generating ${numClips} clips`);
      const clips = [];
      const segmentSize = Math.floor(durationSeconds / numClips);

      for (let i = 0; i < numClips; i++) {
        const minDur = presetConfig.minDuration || 45;
        const maxDur = presetConfig.maxDuration || 60;
        const targetDur = presetConfig.targetDuration || 55;
        const variance = Math.floor((maxDur - minDur) / 2);
        const clipDuration = Math.min(maxDur, Math.max(minDur, targetDur + Math.floor(Math.random() * variance * 2) - variance));
        const segmentStart = segmentSize * i;
        const startOffset = Math.floor(Math.random() * (segmentSize - clipDuration - 10)) + 5;
        const startTime = Math.max(0, segmentStart + startOffset);

        clips.push({
          startTime,
          endTime: Math.min(startTime + clipDuration, durationSeconds),
          duration: clipDuration,
          transcript: `Segment ${i + 1} of "${snippet.title}"`,
          viralityScore: Math.floor(60 + Math.random() * 35),
          uniqueAngle: `Key moment ${i + 1}`,
          emotionalHook: ['curiosity', 'inspiration', 'humor', 'shock'][i % 4],
          platforms: ['youtube', 'tiktok', 'instagram'],
          reason: 'Potential viral moment'
        });
      }
      return { clips, overallPotential: 'Good', bestTopics: [], contentType: 'general' };
    }

    let analysisResult;
    try {
      const aiResponse = await Promise.race([
        openai.chat.completions.create({
          model: 'gpt-4o',  // Use GPT-4o for better analysis
          messages: [{ role: 'user', content: clipAnalysisPrompt }],
          response_format: { type: 'json_object' },
          max_tokens: maxTokens,
          temperature: 0.7
        }),
        new Promise((_, reject) =>
          setTimeout(() => reject(new Error('AI analysis timeout')), AI_ANALYSIS_TIMEOUT_MS)
        )
      ]);

      analysisResult = JSON.parse(aiResponse.choices[0].message.content);
      console.log(`[wizardAnalyzeVideo] AI returned ${analysisResult.clips?.length || 0} clips`);
    } catch (e) {
      console.error('[wizardAnalyzeVideo] AI analysis failed, using fallback:', e.message);
      analysisResult = generateFallbackClips();
    }

    // Helper function to get actual transcript for a time range
    function getTranscriptForTimeRange(segments, startTime, endTime) {
      if (!segments || segments.length === 0) return null;
      const relevantSegments = segments.filter(seg =>
        seg.timestamp >= startTime && seg.timestamp < endTime
      );
      if (relevantSegments.length === 0) return null;
      return relevantSegments.map(s => s.text).join(' ');
    }

    // Validate and process clips - ensure no overlaps and proper spread
    // ENFORCE platform-specific durations
    const minDuration = presetConfig.minDuration || 45;
    const maxDuration = presetConfig.maxDuration || 60;
    const targetDuration = presetConfig.targetDuration || 55;

    let processedClips = (analysisResult.clips || [])
      .filter(clip => {
        // Validate timestamps
        const start = parseInt(clip.startTime) || 0;
        return start >= 0 && start < durationSeconds;
      })
      .map((clip, index) => {
        const start = parseInt(clip.startTime) || 0;
        // ENFORCE platform-specific duration: use AI's duration if within range, otherwise use target
        let clipDuration = parseInt(clip.duration) || targetDuration;
        if (clipDuration < minDuration) clipDuration = targetDuration;
        if (clipDuration > maxDuration) clipDuration = maxDuration;
        // Calculate end based on enforced duration
        const end = Math.min(start + clipDuration, durationSeconds);
        // Recalculate duration in case end was capped by video length
        const finalDuration = end - start;

        // Get actual transcript for this clip's time range
        const actualTranscript = getTranscriptForTimeRange(transcriptSegments, start, end);
        return {
          id: `clip_${videoId}_${index}_${Date.now()}`,
          startTime: start,
          endTime: end,
          duration: finalDuration,
          // Use actual transcript if available, otherwise use AI-generated summary
          transcript: actualTranscript || clip.transcript || `Clip ${index + 1}`,
          aiSummary: clip.transcript || '', // Keep AI summary as additional context
          thumbnail: `https://img.youtube.com/vi/${videoId}/hqdefault.jpg`,
          score: Math.min(100, Math.max(0, clip.viralityScore || 75)),
          uniqueAngle: clip.uniqueAngle || '',
          emotionalHook: clip.emotionalHook || '',
          platforms: clip.platforms || ['youtube', 'tiktok', 'instagram'],
          reason: clip.reason || ''
        };
      })
      .sort((a, b) => a.startTime - b.startTime);

    // Remove overlapping clips (keep higher scoring one)
    const nonOverlappingClips = [];
    for (const clip of processedClips) {
      const overlaps = nonOverlappingClips.some(existing =>
        (clip.startTime >= existing.startTime && clip.startTime < existing.endTime) ||
        (clip.endTime > existing.startTime && clip.endTime <= existing.endTime) ||
        (clip.startTime <= existing.startTime && clip.endTime >= existing.endTime)
      );

      if (!overlaps) {
        nonOverlappingClips.push(clip);
      }
    }

    // Sort by score for final output
    processedClips = nonOverlappingClips.sort((a, b) => b.score - a.score);

    // ENHANCED VIRALITY SCORING (OpusClip-style)
    // This adds detailed breakdown and predictions for top clips
    // Add timeout protection to prevent function timeout (60 second limit for scoring)
    try {
      console.log('[wizardAnalyzeVideo] Running enhanced virality scoring on top clips...');
      const videoContext = {
        title: snippet.title || videoData.title,
        channelTitle: snippet.channelTitle || videoData.channelTitle,
        viewCount: parseInt(stats.viewCount || videoData.viewCount || 0),
        contentType: analysisResult.contentType || 'general'
      };

      // Race between enhanced scoring and a 60-second timeout
      const SCORING_TIMEOUT_MS = 60000; // 60 seconds max for enhanced scoring
      const scoringPromise = batchCalculateViralityScores(processedClips, videoContext);
      const timeoutPromise = new Promise((_, reject) =>
        setTimeout(() => reject(new Error('Enhanced scoring timeout')), SCORING_TIMEOUT_MS)
      );

      processedClips = await Promise.race([scoringPromise, timeoutPromise]);
      console.log('[wizardAnalyzeVideo] Enhanced virality scoring complete');
    } catch (scoringError) {
      console.log('[wizardAnalyzeVideo] Enhanced scoring skipped:', scoringError.message);
      // Continue with basic scores - add basic predictions for all clips
      processedClips = processedClips.map(clip => ({
        ...clip,
        viralPrediction: clip.score >= 80 ? 'HIGH' : clip.score >= 60 ? 'MEDIUM' : 'LOW'
      }));
    }

    const projectData = {
      userId: uid,
      videoId,
      videoUrl,
      videoData,
      clips: processedClips,
      // Platform preset used for clip generation
      platform: platform,
      platformPreset: {
        minDuration: presetConfig.minDuration,
        maxDuration: presetConfig.maxDuration,
        targetDuration: presetConfig.targetDuration,
        durationRange: presetConfig.durationRange
      },
      // Store transcript data for SEO generation and other features
      transcriptSegments: transcriptSegments.slice(0, 500), // Limit to 500 segments
      fullTranscript: fullTranscript ? fullTranscript.substring(0, 10000) : '', // First 10k chars
      hasTranscript: transcriptSegments.length > 0,
      options: options || {},
      overallPotential: analysisResult.overallPotential,
      bestTopics: analysisResult.bestTopics || [],
      contentType: analysisResult.contentType || 'general',
      status: 'analyzed',
      createdAt: admin.firestore.FieldValue.serverTimestamp(),
      updatedAt: admin.firestore.FieldValue.serverTimestamp(),

      // CANONICAL SOURCE ASSET - for reliable export
      // This is the single source of truth for video data during export
      // If present, export will use this instead of re-capturing
      sourceAsset: null,  // Will be populated by frontend after capture upload
      isUpload: false     // Will be set to true for uploaded videos
    };

    // Enforce max projects limit before creating new project
    const maxProjects = await getMaxProjectsLimit();
    await enforceMaxProjects(uid, maxProjects);

    const projectRef = await db.collection('wizardProjects').add(projectData);
    await logUsage(uid, 'wizard_analyze_video', { videoId, clipCount: processedClips.length });

    return {
      success: true,
      projectId: projectRef.id,
      videoData,
      clips: processedClips,
      overallPotential: analysisResult.overallPotential,
      bestTopics: analysisResult.bestTopics || [],
      contentType: analysisResult.contentType || 'general'
    };

  } catch (error) {
    console.error('Wizard analyze video error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to analyze video.'));
  }
});

/**
 * wizardSmartCrop - AI-powered subject detection for optimal crop positioning
 * Analyzes a video thumbnail to detect the main subject (person/face) and
 * suggests the optimal horizontal crop position for 9:16 format.
 */
exports.wizardSmartCrop = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const { thumbnailUrl, videoId, clipStartTime } = data;

  if (!thumbnailUrl && !videoId) {
    throw new functions.https.HttpsError('invalid-argument', 'Thumbnail URL or video ID required');
  }

  try {
    // Get the thumbnail URL
    let imageUrl = thumbnailUrl;
    if (!imageUrl && videoId) {
      imageUrl = `https://img.youtube.com/vi/${videoId}/maxresdefault.jpg`;
    }

    // Use GPT-4 Vision to analyze the image and detect subject position
    const response = await openai.chat.completions.create({
      model: 'gpt-4o',
      messages: [
        {
          role: 'system',
          content: `You are an expert video editor analyzing frames for optimal crop positioning.
Your task is to identify where the main subject (typically a person) is located horizontally in the frame.
The goal is to crop a 16:9 video to 9:16 (vertical) format while keeping the main subject centered.

Analyze the image and determine the optimal horizontal crop position as a percentage from 0-100:
- 0% = crop from the LEFT edge (subject is on the left)
- 50% = crop from CENTER (subject is centered)
- 100% = crop from the RIGHT edge (subject is on the right)

Consider:
1. Face/person position
2. Important visual elements
3. Text/graphics that should remain visible
4. Rule of thirds composition

Respond with ONLY a JSON object, no markdown or other text.`
        },
        {
          role: 'user',
          content: [
            {
              type: 'text',
              text: 'Analyze this video frame and determine the optimal horizontal crop position (0-100) to best capture the main subject when converting from 16:9 to 9:16 format. Return JSON with: cropPosition (0-100), confidence (low/medium/high), subject (what you detected), and reasoning.'
            },
            {
              type: 'image_url',
              image_url: {
                url: imageUrl,
                detail: 'low'
              }
            }
          ]
        }
      ],
      max_tokens: 300
    });

    const content = response.choices[0]?.message?.content || '';

    // Parse the response
    let result;
    try {
      // Try to extract JSON from the response
      const jsonMatch = content.match(/\{[\s\S]*\}/);
      if (jsonMatch) {
        result = JSON.parse(jsonMatch[0]);
      } else {
        throw new Error('No JSON found in response');
      }
    } catch (parseError) {
      console.log('[wizardSmartCrop] Failed to parse AI response, using fallback:', content);
      // Fallback: try to extract number from response
      const numberMatch = content.match(/(\d+)/);
      result = {
        cropPosition: numberMatch ? parseInt(numberMatch[1], 10) : 50,
        confidence: 'low',
        subject: 'unknown',
        reasoning: 'Fallback analysis'
      };
    }

    // Validate crop position
    let cropPosition = parseInt(result.cropPosition, 10);
    if (isNaN(cropPosition) || cropPosition < 0 || cropPosition > 100) {
      cropPosition = 50; // Default to center
    }

    console.log(`[wizardSmartCrop] AI detected subject at ${cropPosition}%:`, result.subject);

    return {
      success: true,
      cropPosition,
      confidence: result.confidence || 'medium',
      subject: result.subject || 'person',
      reasoning: result.reasoning || 'AI analysis'
    };

  } catch (error) {
    console.error('[wizardSmartCrop] Error:', error);

    // Return a reasonable fallback instead of throwing
    return {
      success: true,
      cropPosition: 50,
      confidence: 'low',
      subject: 'unknown',
      reasoning: 'Fallback to center (AI analysis unavailable)'
    };
  }
});

/**
 * wizardUpdateSourceAsset - Updates project with canonical source asset
 * Called by frontend after successfully capturing and uploading video during analysis
 *
 * The sourceAsset is the single source of truth for export operations.
 * Once set, export will use this asset instead of re-capturing.
 */
exports.wizardUpdateSourceAsset = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const { projectId, sourceAsset } = data;

  if (!projectId) {
    throw new functions.https.HttpsError('invalid-argument', 'Project ID required');
  }

  if (!sourceAsset || !sourceAsset.storageUrl) {
    throw new functions.https.HttpsError('invalid-argument', 'Valid sourceAsset with storageUrl required');
  }

  try {
    // Verify project ownership
    const projectRef = db.collection('wizardProjects').doc(projectId);
    const projectDoc = await projectRef.get();

    if (!projectDoc.exists) {
      throw new functions.https.HttpsError('not-found', 'Project not found');
    }

    const project = projectDoc.data();
    if (project.userId !== uid) {
      throw new functions.https.HttpsError('permission-denied', 'Not authorized');
    }

    // Validate sourceAsset structure
    const validatedSourceAsset = {
      storageUrl: sourceAsset.storageUrl,        // Firebase Storage download URL
      storagePath: sourceAsset.storagePath || null,  // gs:// path if available
      duration: sourceAsset.duration || project.videoData?.duration || 0,
      format: sourceAsset.format || 'video/mp4',
      fileSize: sourceAsset.fileSize || 0,
      capturedAt: sourceAsset.capturedAt || Date.now(),
      source: sourceAsset.source || 'extension_capture'  // 'extension_capture' | 'direct_upload' | 'server_download'
    };

    console.log(`[wizardUpdateSourceAsset] Updating project ${projectId} with sourceAsset:`, {
      storageUrl: validatedSourceAsset.storageUrl.substring(0, 80) + '...',
      duration: validatedSourceAsset.duration,
      source: validatedSourceAsset.source
    });

    // Update project with sourceAsset
    await projectRef.update({
      sourceAsset: validatedSourceAsset,
      updatedAt: admin.firestore.FieldValue.serverTimestamp()
    });

    console.log(`[wizardUpdateSourceAsset] Project ${projectId} sourceAsset updated successfully`);

    return {
      success: true,
      message: 'Source asset saved. Video is ready for export.',
      sourceAsset: validatedSourceAsset
    };

  } catch (error) {
    console.error('[wizardUpdateSourceAsset] Error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', 'Failed to update source asset');
  }
});

/**
 * wizardGenerateClipSEO - Generates SEO for a clip
 */
exports.wizardGenerateClipSEO = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  checkRateLimit(uid, 'wizardGenerateClipSEO', 10);

  // Check and deduct tokens for SEO generation
  const tokenCosts = await getWizardTokenCosts();
  const seoCost = tokenCosts.generateSEO || 2;

  const tokenResult = await deductWizardTokens(uid, seoCost, 'generateSEO', {
    clipId: data.clipId,
    projectId: data.projectId
  });

  if (!tokenResult.success) {
    throw new functions.https.HttpsError(
      'resource-exhausted',
      `Insufficient tokens. This operation requires ${seoCost} tokens, but you have ${tokenResult.available || 0}.`
    );
  }

  const { clipId, transcript, platform, projectId, videoTitle } = data;
  if (!transcript) {
    throw new functions.https.HttpsError('invalid-argument', 'Transcript is required');
  }

  try {
    // Fetch project data for more context
    let videoDescription = '';
    let channelTitle = '';
    let bestTopics = [];
    let contentType = '';

    if (projectId) {
      const projectDoc = await db.collection('wizardProjects').doc(projectId).get();
      if (projectDoc.exists) {
        const project = projectDoc.data();
        videoDescription = project.videoData?.description || '';
        channelTitle = project.videoData?.channelTitle || '';
        bestTopics = project.bestTopics || [];
        contentType = project.contentType || '';
      }
    }

    const seoPrompt = `Generate viral ${platform || 'YouTube Shorts'} SEO metadata for this video clip.

CLIP TRANSCRIPT (what's actually said):
"${transcript}"

VIDEO CONTEXT:
- Original Video Title: ${videoTitle || 'Not provided'}
- Channel: ${channelTitle || 'Not provided'}
- Content Type: ${contentType || 'general'}
- Main Topics: ${bestTopics.length > 0 ? bestTopics.join(', ') : 'Not specified'}
${videoDescription ? `- Video Description Preview: ${videoDescription.substring(0, 300)}...` : ''}

PLATFORM: ${platform || 'YouTube Shorts'}

Generate SEO that:
1. Captures the ACTUAL content from the transcript
2. Uses hooks and keywords that match what's spoken
3. Targets ${platform || 'YouTube Shorts'} audience specifically
4. Includes relevant hashtags for discoverability

RESPOND IN VALID JSON:
{
  "title": "Catchy, hook-driven title based on actual content (max 100 chars)",
  "description": "Engaging description that summarizes the clip content with CTA and 3-5 relevant hashtags",
  "tags": ["tag1", "tag2", "tag3", "tag4", "tag5", "tag6", "tag7", "tag8"],
  "hashtags": ["#hashtag1", "#hashtag2", "#hashtag3", "#hashtag4", "#hashtag5"]
}`;

    const aiResponse = await openai.chat.completions.create({
      model: 'gpt-4o-mini',
      messages: [{ role: 'user', content: seoPrompt }],
      response_format: { type: 'json_object' },
      max_tokens: 1000
    });

    let seoData;
    try {
      seoData = JSON.parse(aiResponse.choices[0].message.content);
    } catch (e) {
      seoData = {
        title: transcript.substring(0, 60) + '...',
        description: transcript + '\n\nðŸ”” Follow for more!',
        tags: ['shorts', 'viral', 'trending'],
        hashtags: ['#shorts', '#viral', '#fyp']
      };
    }

    if (projectId && clipId) {
      await db.collection('wizardProjects').doc(projectId).update({
        [`clipSEO.${clipId}`]: { ...seoData, platform: platform || 'youtube', generatedAt: admin.firestore.FieldValue.serverTimestamp() },
        updatedAt: admin.firestore.FieldValue.serverTimestamp()
      });
    }

    await logUsage(uid, 'wizard_generate_seo', { platform, clipId });
    return { success: true, ...seoData };

  } catch (error) {
    console.error('Wizard generate SEO error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to generate SEO.'));
  }
});

/**
 * wizardGenerateThumbnails - Generates 2 high-quality thumbnail concepts
 * Uses main video context and video frames as reference for consistent style
 */
exports.wizardGenerateThumbnails = functions
  .runWith({ timeoutSeconds: 180, memory: '1GB' })
  .https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  checkRateLimit(uid, 'wizardGenerateThumbnails', 5);

  const { clipId, transcript, projectId, videoTitle } = data;
  if (!transcript) {
    throw new functions.https.HttpsError('invalid-argument', 'Transcript is required');
  }

  try {
    // Get FULL project data for comprehensive video context
    let videoId = null;
    let videoThumbnailUrl = null;
    let mainVideoTitle = videoTitle || '';
    let mainVideoDescription = '';
    let channelName = '';
    let clipStartTime = 0;
    let clipEndTime = 0;

    if (projectId) {
      const projectDoc = await db.collection('wizardProjects').doc(projectId).get();
      if (projectDoc.exists) {
        const project = projectDoc.data();
        videoId = project.videoData?.videoId || project.videoId;
        videoThumbnailUrl = project.videoData?.thumbnail;
        mainVideoTitle = project.videoData?.title || videoTitle || '';
        mainVideoDescription = project.videoData?.description || '';
        channelName = project.videoData?.channelTitle || '';

        // Get clip timing for frame extraction
        const clip = project.clips?.find(c => c.id === clipId);
        if (clip) {
          clipStartTime = clip.startTime || 0;
          clipEndTime = clip.endTime || clipStartTime + 30;
        }
      }
    }

    // Use Gemini API key for Nano Banana Pro
    const geminiApiKey = functions.config().gemini?.key;
    if (!geminiApiKey) {
      throw new functions.https.HttpsError('failed-precondition', 'Gemini API key not configured');
    }

    const ai = new GoogleGenAI({ apiKey: geminiApiKey });
    const geminiModelId = 'gemini-3-pro-image-preview'; // Nano Banana Pro

    // Fetch ACTUAL video frames from YouTube's standard thumbnail endpoints
    // YouTube provides multiple thumbnails that ARE actual frames from the video:
    // - 0.jpg: Main thumbnail (full quality)
    // - 1.jpg: Frame at ~25% of video
    // - 2.jpg: Frame at ~50% of video (middle)
    // - 3.jpg: Frame at ~75% of video
    const referenceImages = [];

    if (videoId) {
      // YouTube's numbered thumbnails are ACTUAL FRAMES from the video at different timestamps
      const frameUrls = [
        `https://img.youtube.com/vi/${videoId}/maxresdefault.jpg`,  // HD main thumbnail
        `https://img.youtube.com/vi/${videoId}/0.jpg`,              // Full-size main frame
        `https://img.youtube.com/vi/${videoId}/1.jpg`,              // Frame at ~25%
        `https://img.youtube.com/vi/${videoId}/2.jpg`,              // Frame at ~50%
        `https://img.youtube.com/vi/${videoId}/3.jpg`,              // Frame at ~75%
        `https://img.youtube.com/vi/${videoId}/hqdefault.jpg`,      // HQ fallback
      ];

      console.log(`[wizardGenerateThumbnails] Fetching video frames for ${videoId}`);

      // Fetch multiple frames in parallel for better reference
      const framePromises = frameUrls.slice(0, 4).map(async (url, index) => {
        try {
          const imageResponse = await axios.get(url, {
            responseType: 'arraybuffer',
            timeout: 8000,
            validateStatus: (status) => status === 200
          });

          // Check if it's a valid image (not a placeholder)
          if (imageResponse.data && imageResponse.data.length > 5000) {
            return {
              base64: Buffer.from(imageResponse.data).toString('base64'),
              type: 'video_frame',
              source: url.includes('maxres') ? 'maxres' : `frame_${index}`
            };
          }
        } catch (err) {
          console.log(`[wizardGenerateThumbnails] Could not fetch ${url.split('/').pop()}`);
        }
        return null;
      });

      const fetchedFrames = (await Promise.all(framePromises)).filter(f => f !== null);
      referenceImages.push(...fetchedFrames);

      console.log(`[wizardGenerateThumbnails] Fetched ${referenceImages.length} actual video frames`);
    }

    // Use custom thumbnail URL if provided and we don't have enough frames
    if (referenceImages.length < 2 && videoThumbnailUrl) {
      try {
        const imageResponse = await axios.get(videoThumbnailUrl, {
          responseType: 'arraybuffer',
          timeout: 5000,
          validateStatus: (status) => status === 200
        });
        if (imageResponse.data && imageResponse.data.length > 1000) {
          referenceImages.push({
            base64: Buffer.from(imageResponse.data).toString('base64'),
            type: 'custom_thumbnail'
          });
        }
      } catch (err) {
        // Ignore
      }
    }

    const hasReference = referenceImages.length > 0;
    console.log(`[wizardGenerateThumbnails] Total reference images: ${referenceImages.length}`);

    // Build comprehensive context from main video
    const videoContext = `
MAIN VIDEO CONTEXT:
- Title: "${mainVideoTitle}"
- Channel: "${channelName}"
- Description excerpt: "${mainVideoDescription.substring(0, 300)}"

CLIP CONTENT (what this short is about):
"${transcript.substring(0, 400)}"
`.trim();

    // Generate 2 thumbnail variations with different high-impact styles
    const thumbnailConcepts = [
      {
        name: 'Hero Shot',
        prompt: `Generate a professional YouTube thumbnail that captures the essence of this video content.

${videoContext}

DESIGN REQUIREMENTS:
1. Create a HERO SHOT thumbnail - the most impactful, eye-catching frame that represents this content
2. Feature a compelling focal point (person, object, or scene) that relates to the video topic
3. Use dramatic lighting: bright highlights on the subject, darker background for contrast
4. Composition: Subject positioned using rule of thirds (not dead center)
5. Leave 30% space on one side for potential text overlay
6. Colors: Vibrant, high saturation, complementary color scheme

STYLE: Ultra high quality, photorealistic, cinematic lighting, 9:16 vertical/portrait aspect ratio (for YouTube Shorts, TikTok, Instagram Reels), 4K resolution, professional short-form video thumbnail that gets clicks. Magazine cover quality with depth and dimension.

TEXT RULES: If including any text overlay, use ONLY simple ASCII characters (A-Z, a-z, 0-9). Do NOT use checkmarks, special symbols, emojis, or Unicode characters like âœ“ âœ— â†’ â˜…. Keep text minimal and impactful.

CRITICAL: The thumbnail must visually represent the VIDEO TOPIC, not just generic graphics. Make it specific to the content described above.`
      },
      {
        name: 'Dynamic Action',
        prompt: `Generate a professional YouTube thumbnail that creates intrigue and energy for this video content.

${videoContext}

DESIGN REQUIREMENTS:
1. Create a DYNAMIC ACTION thumbnail - convey movement, energy, and excitement
2. Use visual elements that create a sense of anticipation or reveal
3. Dramatic perspective: slight angle, dynamic framing, not flat/static
4. High contrast with bold colors that pop on both desktop and mobile
5. Include visual elements specific to the topic (icons, objects, expressions related to the content)
6. Background: either blurred/bokeh or gradient that makes subject pop

STYLE: High energy, bold contrast, vibrant colors, professional short-form video thumbnail, 9:16 vertical/portrait aspect ratio (for YouTube Shorts, TikTok, Instagram Reels), 4K resolution. The kind of thumbnail that stops scroll and demands attention.

TEXT RULES: If including any text overlay, use ONLY simple ASCII characters (A-Z, a-z, 0-9). Do NOT use checkmarks, special symbols, emojis, or Unicode characters like âœ“ âœ— â†’ â˜…. Keep text minimal and impactful.

CRITICAL: The thumbnail must visually represent the VIDEO TOPIC with specific relevant imagery. Make viewers understand what the video is about at a glance.`
      }
    ];

    const storage = admin.storage().bucket();
    const timestamp = Date.now();
    const generatedThumbnails = [];

    // Generate only 2 thumbnails
    for (let i = 0; i < thumbnailConcepts.length; i++) {
      const concept = thumbnailConcepts[i];

      try {
        // Build content parts with reference images FIRST for better context
        const contentParts = [];

        // Add multiple reference images from the actual video
        if (hasReference) {
          // Add up to 3 reference images for comprehensive style matching
          const imagesToAdd = referenceImages.slice(0, 3);
          for (const refImg of imagesToAdd) {
            contentParts.push({
              inlineData: {
                mimeType: 'image/jpeg',
                data: refImg.base64
              }
            });
          }
          console.log(`[wizardGenerateThumbnails] Added ${imagesToAdd.length} reference images to prompt`);
        }

        // Build enhanced prompt with strong reference instructions
        let finalPrompt;
        if (hasReference) {
          finalPrompt = `REFERENCE FRAMES: I've provided ${Math.min(referenceImages.length, 3)} actual frame(s) from the original video. These show the REAL content of the video.

CRITICAL - YOU MUST FOLLOW THESE RULES:
1. The thumbnail MUST match the visual content shown in these reference frames
2. If the reference shows animation/cartoon - create an animated/cartoon style thumbnail
3. If the reference shows a real person - create a thumbnail featuring a similar-looking person
4. If the reference shows a specific scene/setting - use that same setting
5. MATCH the color palette, art style, and visual aesthetic of the reference frames exactly
6. Do NOT create unrelated imagery - the thumbnail must represent what's actually in the video

${concept.prompt}`;
        } else {
          finalPrompt = concept.prompt;
        }

        contentParts.push({ text: finalPrompt });

        // Generate image - using exact same pattern as working generateThumbnailPro/generateCreativeImage
        console.log(`[wizardGenerateThumbnails] Generating thumbnail ${i + 1}/2 with model: ${geminiModelId}`);
        console.log(`[wizardGenerateThumbnails] Prompt length: ${finalPrompt.length}, hasReference: ${hasReference}`);

        const result = await ai.models.generateContent({
          model: geminiModelId,
          contents: [{ role: 'user', parts: contentParts }],
          config: {
            responseModalities: ['image', 'text']
          }
        });

        // Extract image from response - handle both SDK response structures (same as working code)
        const candidates = result.candidates || (result.response && result.response.candidates);
        console.log(`[wizardGenerateThumbnails] Got ${candidates?.length || 0} candidates`);

        if (candidates && candidates.length > 0) {
          const candidate = candidates[0];
          const parts = candidate.content?.parts || candidate.parts || [];
          console.log(`[wizardGenerateThumbnails] Candidate has ${parts.length} parts`);

          for (const part of parts) {
            const inlineData = part.inlineData || part.inline_data;
            if (inlineData && (inlineData.data || inlineData.bytesBase64Encoded)) {
              const imageBytes = inlineData.data || inlineData.bytesBase64Encoded;
              const mimeType = inlineData.mimeType || inlineData.mime_type || 'image/png';
              const extension = mimeType.includes('jpeg') ? 'jpg' : 'png';

              console.log(`[wizardGenerateThumbnails] Found image data, mimeType: ${mimeType}`);

              // Upload to Firebase Storage
              const fileName = `wizard-thumbnails/${uid}/${timestamp}-${clipId}-${i}.${extension}`;
              const file = storage.file(fileName);

              const buffer = Buffer.from(imageBytes, 'base64');
              await file.save(buffer, {
                metadata: {
                  contentType: mimeType,
                  metadata: {
                    concept: concept.name,
                    clipId: clipId,
                    model: geminiModelId
                  }
                }
              });

              await file.makePublic();
              const publicUrl = `https://storage.googleapis.com/${storage.name}/${fileName}`;

              generatedThumbnails.push({
                id: `thumb_${clipId}_${i}`,
                concept: concept.name,
                previewUrl: publicUrl,
                storagePath: fileName,
                generatedAt: new Date().toISOString()
              });

              console.log(`[wizardGenerateThumbnails] Saved thumbnail: ${publicUrl}`);

              console.log(`[wizardGenerateThumbnails] Generated thumbnail ${i + 1}/2: ${concept.name}`);
              break; // Only need first image from response
            }
          }
        }
      } catch (genError) {
        console.error(`[wizardGenerateThumbnails] Error generating thumbnail ${i + 1}/2:`, genError.message);
        // Add placeholder for failed generation - use video thumbnail as fallback
        const fallbackUrl = videoThumbnailUrl || `https://img.youtube.com/vi/${videoId}/hqdefault.jpg`;
        generatedThumbnails.push({
          id: `thumb_${clipId}_${i}`,
          concept: concept.name,
          previewUrl: fallbackUrl,
          error: 'Generation failed - using video thumbnail',
          generatedAt: new Date().toISOString()
        });
      }
    }

    // Save to project
    if (projectId && clipId && generatedThumbnails.length > 0) {
      await db.collection('wizardProjects').doc(projectId).update({
        [`clipThumbnails.${clipId}`]: {
          thumbnails: generatedThumbnails,
          selectedIndex: 0,
          generatedAt: admin.firestore.FieldValue.serverTimestamp()
        },
        updatedAt: admin.firestore.FieldValue.serverTimestamp()
      });
    }

    await logUsage(uid, 'wizard_generate_thumbnails', { clipId, count: generatedThumbnails.length });
    return { success: true, thumbnails: generatedThumbnails };

  } catch (error) {
    console.error('Wizard generate thumbnails error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to generate thumbnails.'));
  }
});

/**
 * wizardSaveClipSettings - Saves customization settings
 */
exports.wizardSaveClipSettings = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const { projectId, clipId, settings, seo } = data;

  if (!projectId || !clipId) {
    throw new functions.https.HttpsError('invalid-argument', 'Project ID and Clip ID required');
  }

  try {
    const projectDoc = await db.collection('wizardProjects').doc(projectId).get();
    if (!projectDoc.exists || projectDoc.data().userId !== uid) {
      throw new functions.https.HttpsError('permission-denied', 'Not authorized');
    }

    const updateData = {
      updatedAt: admin.firestore.FieldValue.serverTimestamp()
    };

    // Save clip settings if provided
    if (settings) {
      updateData[`clipSettings.${clipId}`] = { ...settings, updatedAt: admin.firestore.FieldValue.serverTimestamp() };
    }

    // Save clip SEO if provided
    if (seo) {
      updateData[`clipSEO.${clipId}`] = { ...seo, updatedAt: admin.firestore.FieldValue.serverTimestamp() };
    }

    await db.collection('wizardProjects').doc(projectId).update(updateData);

    return { success: true };
  } catch (error) {
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', 'Failed to save settings.');
  }
});

/**
 * wizardGetProject - Retrieves a project by ID
 */
exports.wizardGetProject = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const { projectId } = data;

  if (!projectId) {
    throw new functions.https.HttpsError('invalid-argument', 'Project ID required');
  }

  try {
    const projectDoc = await db.collection('wizardProjects').doc(projectId).get();
    if (!projectDoc.exists) {
      throw new functions.https.HttpsError('not-found', 'Project not found');
    }
    if (projectDoc.data().userId !== uid) {
      throw new functions.https.HttpsError('permission-denied', 'Not authorized');
    }

    return { success: true, project: { id: projectDoc.id, ...projectDoc.data() } };
  } catch (error) {
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', 'Failed to get project.');
  }
});

/**
 * wizardGetProjects - Retrieves all projects for user
 */
exports.wizardGetProjects = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const { limit = 20 } = data || {};

  try {
    let snapshot;
    try {
      // Try with orderBy (requires composite index)
      snapshot = await db.collection('wizardProjects')
        .where('userId', '==', uid)
        .orderBy('createdAt', 'desc')
        .limit(Math.min(limit, 50))
        .get();
    } catch (indexError) {
      // Fallback: if index doesn't exist, query without orderBy and sort in memory
      console.log('Index not available, using fallback query:', indexError.message);
      snapshot = await db.collection('wizardProjects')
        .where('userId', '==', uid)
        .limit(Math.min(limit, 50))
        .get();
    }

    let projects = snapshot.docs.map(doc => {
      const data = doc.data();
      return {
        id: doc.id,
        videoTitle: data.videoData?.title || 'Untitled',
        videoThumbnail: data.videoData?.thumbnail,
        clipCount: data.clips?.length || 0,
        status: data.status || 'draft',
        createdAt: data.createdAt,
        updatedAt: data.updatedAt
      };
    });

    // Sort by createdAt in memory (fallback for when index isn't available)
    projects.sort((a, b) => {
      const timeA = a.createdAt?.toMillis ? a.createdAt.toMillis() : 0;
      const timeB = b.createdAt?.toMillis ? b.createdAt.toMillis() : 0;
      return timeB - timeA;
    });

    return { success: true, projects, hasMore: projects.length === limit };
  } catch (error) {
    console.error('wizardGetProjects error:', error);
    throw new functions.https.HttpsError('internal', 'Failed to get projects: ' + error.message);
  }
});

/**
 * wizardDeleteProject - Deletes a project
 */
exports.wizardDeleteProject = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const { projectId } = data;

  if (!projectId) {
    throw new functions.https.HttpsError('invalid-argument', 'Project ID required');
  }

  try {
    const projectDoc = await db.collection('wizardProjects').doc(projectId).get();
    if (!projectDoc.exists || projectDoc.data().userId !== uid) {
      throw new functions.https.HttpsError('permission-denied', 'Not authorized');
    }

    await db.collection('wizardProjects').doc(projectId).delete();
    return { success: true };
  } catch (error) {
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', 'Failed to delete project.');
  }
});

/**
 * wizardGenerateAllSEO - Batch generates SEO for all clips
 */
exports.wizardGenerateAllSEO = functions
  .runWith({ timeoutSeconds: 120 })
  .https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  checkRateLimit(uid, 'wizardGenerateAllSEO', 3);

  const { projectId, clipIds, platform } = data;
  if (!projectId || !clipIds?.length) {
    throw new functions.https.HttpsError('invalid-argument', 'Project ID and clip IDs required');
  }

  try {
    const projectDoc = await db.collection('wizardProjects').doc(projectId).get();
    if (!projectDoc.exists || projectDoc.data().userId !== uid) {
      throw new functions.https.HttpsError('permission-denied', 'Not authorized');
    }

    const clips = projectDoc.data().clips.filter(c => clipIds.includes(c.id));
    const seoResults = {};

    for (let i = 0; i < clips.length; i += 3) {
      const batch = clips.slice(i, i + 3);
      const promises = batch.map(async (clip) => {
        const response = await openai.chat.completions.create({
          model: 'gpt-4o-mini',
          messages: [{ role: 'user', content: `Generate viral ${platform || 'YouTube Shorts'} metadata: "${clip.transcript}"\nRESPOND IN JSON: {"title":"","description":"","tags":[],"hashtags":[]}` }],
          response_format: { type: 'json_object' },
          max_tokens: 500
        });
        try {
          return { clipId: clip.id, seo: JSON.parse(response.choices[0].message.content) };
        } catch {
          return { clipId: clip.id, seo: { title: clip.transcript.substring(0, 60), description: clip.transcript, tags: ['shorts'], hashtags: ['#shorts'] } };
        }
      });
      const results = await Promise.all(promises);
      results.forEach(r => { seoResults[r.clipId] = r.seo; });
    }

    const updateData = { updatedAt: admin.firestore.FieldValue.serverTimestamp() };
    Object.entries(seoResults).forEach(([clipId, seo]) => {
      updateData[`clipSEO.${clipId}`] = { ...seo, platform: platform || 'youtube', generatedAt: admin.firestore.FieldValue.serverTimestamp() };
    });
    await db.collection('wizardProjects').doc(projectId).update(updateData);

    await logUsage(uid, 'wizard_generate_all_seo', { projectId, clipCount: clipIds.length });
    return { success: true, seoData: seoResults };

  } catch (error) {
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to generate SEO.'));
  }
});

/**
 * wizardExportProject - Exports project data as CSV/JSON
 */
exports.wizardExportProject = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const { projectId, format = 'csv' } = data;

  if (!projectId) {
    throw new functions.https.HttpsError('invalid-argument', 'Project ID required');
  }

  try {
    const projectDoc = await db.collection('wizardProjects').doc(projectId).get();
    if (!projectDoc.exists || projectDoc.data().userId !== uid) {
      throw new functions.https.HttpsError('permission-denied', 'Not authorized');
    }

    const project = projectDoc.data();
    const clips = project.clips || [];
    const clipSEO = project.clipSEO || {};

    let exportData, filename;

    if (format === 'json') {
      exportData = JSON.stringify({
        videoTitle: project.videoData?.title,
        videoUrl: project.videoUrl,
        exportedAt: new Date().toISOString(),
        clips: clips.map(clip => ({ ...clip, seo: clipSEO[clip.id] || {} }))
      }, null, 2);
      filename = `shorts-export-${projectDoc.id}.json`;
    } else {
      const headers = ['Clip ID', 'Start Time', 'End Time', 'Duration', 'Score', 'Title', 'Description', 'Tags', 'Platforms'];
      const rows = clips.map(clip => {
        const seo = clipSEO[clip.id] || {};
        return [clip.id, clip.startTime, clip.endTime, clip.duration, clip.score,
          `"${(seo.title || '').replace(/"/g, '""')}"`,
          `"${(seo.description || '').replace(/"/g, '""').substring(0, 200)}"`,
          `"${(seo.tags || []).join(', ')}"`,
          `"${(clip.platforms || []).join(', ')}"`
        ].join(',');
      });
      exportData = [headers.join(','), ...rows].join('\n');
      filename = `shorts-export-${projectDoc.id}.csv`;
    }

    return { success: true, data: exportData, filename, format };
  } catch (error) {
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', 'Failed to export project.');
  }
});

// ============================================
// PHASE 4: ADVANCED AI FEATURES
// ============================================

/**
 * wizardGenerateBRoll - Generates AI B-Roll suggestions for clips
 * Uses OpenAI to analyze transcript and suggest relevant B-Roll footage
 */
exports.wizardGenerateBRoll = functions.runWith({ timeoutSeconds: 120 }).https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const { projectId, clipId } = data;

  if (!projectId || !clipId) {
    throw new functions.https.HttpsError('invalid-argument', 'Project ID and Clip ID required');
  }

  // Check and deduct tokens for B-Roll generation
  const tokenCosts = await getWizardTokenCosts();
  const brollCost = tokenCosts.generateBRoll || 4;

  const tokenResult = await deductWizardTokens(uid, brollCost, 'generateBRoll', {
    clipId: clipId,
    projectId: projectId
  });

  if (!tokenResult.success) {
    throw new functions.https.HttpsError(
      'resource-exhausted',
      `Insufficient tokens. This operation requires ${brollCost} tokens, but you have ${tokenResult.available || 0}.`
    );
  }

  try {
    // Verify project ownership
    const projectDoc = await db.collection('wizardProjects').doc(projectId).get();
    if (!projectDoc.exists || projectDoc.data().userId !== uid) {
      throw new functions.https.HttpsError('permission-denied', 'Not authorized');
    }

    const project = projectDoc.data();
    const clip = (project.clips || []).find(c => c.id === clipId);

    if (!clip) {
      throw new functions.https.HttpsError('not-found', 'Clip not found');
    }

    // Get OpenAI API key
    const settingsDoc = await db.collection('settings').doc('openai').get();
    const openaiKey = settingsDoc.exists ? settingsDoc.data().apiKey : null;

    if (!openaiKey) {
      throw new functions.https.HttpsError('failed-precondition', 'OpenAI API key not configured');
    }

    // Generate B-Roll suggestions using GPT
    const prompt = `Analyze this video clip transcript and suggest 5 B-Roll footage ideas that would enhance the visual storytelling. For each suggestion, provide:
1. A brief description of the footage
2. Suggested duration (2-5 seconds)
3. When in the clip it should appear (beginning, middle, end, or specific phrase)
4. Search keywords for stock footage

Clip transcript: "${clip.transcript}"

Video context: ${project.videoData?.title || 'Business/Educational content'}

Respond in JSON format:
{
  "suggestions": [
    {
      "id": "broll_1",
      "description": "Description of the B-Roll footage",
      "duration": 3,
      "placement": "beginning",
      "triggerPhrase": "optional specific phrase",
      "keywords": ["keyword1", "keyword2", "keyword3"],
      "category": "stock" | "ai-generated" | "screen-recording"
    }
  ]
}`;

    const response = await fetch('https://api.openai.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${openaiKey}`,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        model: 'gpt-4o-mini',
        messages: [{ role: 'user', content: prompt }],
        temperature: 0.7,
        max_tokens: 1000,
        response_format: { type: 'json_object' }
      })
    });

    if (!response.ok) {
      throw new Error(`OpenAI API error: ${response.status}`);
    }

    const result = await response.json();
    const brollData = JSON.parse(result.choices[0].message.content);

    // Add AI-generated image prompts for each suggestion
    brollData.suggestions = brollData.suggestions.map(suggestion => ({
      ...suggestion,
      imagePrompt: `Professional cinematic B-Roll footage: ${suggestion.description}. High quality, 4K, smooth motion, ${suggestion.keywords.join(', ')}`
    }));

    // Save B-Roll suggestions to project
    const clipBRoll = project.clipBRoll || {};
    clipBRoll[clipId] = brollData.suggestions;

    await db.collection('wizardProjects').doc(projectId).update({
      clipBRoll,
      updatedAt: admin.firestore.FieldValue.serverTimestamp()
    });

    return {
      success: true,
      clipId,
      brollSuggestions: brollData.suggestions
    };

  } catch (error) {
    console.error('B-Roll generation error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to generate B-Roll suggestions.'));
  }
});

/**
 * wizardRemoveFillers - Analyzes transcript and marks filler words for removal
 * Returns timestamps and cleaned transcript
 */
exports.wizardRemoveFillers = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const { projectId, clipId } = data;

  if (!projectId || !clipId) {
    throw new functions.https.HttpsError('invalid-argument', 'Project ID and Clip ID required');
  }

  try {
    // Verify project ownership
    const projectDoc = await db.collection('wizardProjects').doc(projectId).get();
    if (!projectDoc.exists || projectDoc.data().userId !== uid) {
      throw new functions.https.HttpsError('permission-denied', 'Not authorized');
    }

    const project = projectDoc.data();
    const clip = (project.clips || []).find(c => c.id === clipId);

    if (!clip) {
      throw new functions.https.HttpsError('not-found', 'Clip not found');
    }

    // Common filler words and phrases
    const fillerPatterns = [
      { pattern: /\b(um|uh|uhm|umm)\b/gi, type: 'hesitation' },
      { pattern: /\b(like)\b(?!\s+(to|a|the|this|that|it|when|if|because))/gi, type: 'filler' },
      { pattern: /\b(you know)\b/gi, type: 'filler' },
      { pattern: /\b(I mean)\b/gi, type: 'filler' },
      { pattern: /\b(basically)\b/gi, type: 'filler' },
      { pattern: /\b(literally)\b/gi, type: 'filler' },
      { pattern: /\b(actually)\b(?!\s+(is|was|are|were|do|did|have|has))/gi, type: 'filler' },
      { pattern: /\b(so)\b(?=\s*,|\s*\.|\s*$)/gi, type: 'trailing' },
      { pattern: /\b(right)\b(?=\s*,|\s*\?)/gi, type: 'tag' },
      { pattern: /\b(kind of|sort of)\b/gi, type: 'hedge' },
      { pattern: /\b(just)\b(?!\s+(now|then|because|in|on|at))/gi, type: 'minimizer' }
    ];

    const transcript = clip.transcript || '';
    const fillers = [];
    let cleanedTranscript = transcript;
    let totalFillerDuration = 0;

    // Find all filler occurrences
    fillerPatterns.forEach(({ pattern, type }) => {
      let match;
      const regex = new RegExp(pattern.source, pattern.flags);
      while ((match = regex.exec(transcript)) !== null) {
        // Estimate timing based on word position (rough estimate)
        const wordsBefore = transcript.substring(0, match.index).split(/\s+/).length;
        const estimatedTime = clip.startTime + (wordsBefore * 0.4); // ~0.4s per word

        const fillerDuration = match[0].split(/\s+/).length * 0.3; // ~0.3s per filler word
        const fillerStart = Math.min(estimatedTime, clip.endTime - fillerDuration);
        const fillerEnd = Math.min(fillerStart + fillerDuration, clip.endTime);

        fillers.push({
          word: match[0],
          type,
          position: match.index,
          estimatedTimestamp: fillerStart,  // Legacy field for compatibility
          duration: fillerDuration,         // Legacy field for compatibility
          // New fields for audio silencing during export
          start: Math.max(0, fillerStart - clip.startTime),  // Relative to clip start (0-based)
          end: Math.max(0, fillerEnd - clip.startTime)       // Relative to clip start (0-based)
        });

        totalFillerDuration += fillerDuration;
      }
    });

    // Clean the transcript
    fillerPatterns.forEach(({ pattern }) => {
      cleanedTranscript = cleanedTranscript.replace(pattern, '').replace(/\s+/g, ' ').trim();
    });

    // Sort fillers by position
    fillers.sort((a, b) => a.position - b.position);

    // Calculate statistics
    const stats = {
      originalWordCount: transcript.split(/\s+/).length,
      cleanedWordCount: cleanedTranscript.split(/\s+/).length,
      fillersRemoved: fillers.length,
      estimatedTimeSaved: Math.round(totalFillerDuration * 10) / 10,
      fillersByType: fillers.reduce((acc, f) => {
        acc[f.type] = (acc[f.type] || 0) + 1;
        return acc;
      }, {})
    };

    // Save to project
    const clipFillers = project.clipFillers || {};
    clipFillers[clipId] = {
      fillers,
      cleanedTranscript,
      stats,
      processedAt: new Date().toISOString()
    };

    await db.collection('wizardProjects').doc(projectId).update({
      clipFillers,
      updatedAt: admin.firestore.FieldValue.serverTimestamp()
    });

    return {
      success: true,
      clipId,
      originalTranscript: transcript,
      cleanedTranscript,
      fillers,
      stats
    };

  } catch (error) {
    console.error('Filler removal error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to process fillers.'));
  }
});

/**
 * wizardGenerateHook - Generates viral hook variations for clips
 * Creates attention-grabbing opening lines
 */
exports.wizardGenerateHook = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const { projectId, clipId, hookStyle } = data;

  if (!projectId || !clipId) {
    throw new functions.https.HttpsError('invalid-argument', 'Project ID and Clip ID required');
  }

  try {
    // Verify project ownership
    const projectDoc = await db.collection('wizardProjects').doc(projectId).get();
    if (!projectDoc.exists || projectDoc.data().userId !== uid) {
      throw new functions.https.HttpsError('permission-denied', 'Not authorized');
    }

    const project = projectDoc.data();
    const clip = (project.clips || []).find(c => c.id === clipId);

    if (!clip) {
      throw new functions.https.HttpsError('not-found', 'Clip not found');
    }

    // Get OpenAI API key
    const settingsDoc = await db.collection('settings').doc('openai').get();
    const openaiKey = settingsDoc.exists ? settingsDoc.data().apiKey : null;

    if (!openaiKey) {
      throw new functions.https.HttpsError('failed-precondition', 'OpenAI API key not configured');
    }

    const styleDescriptions = {
      curiosity: 'Create curiosity gaps that make viewers desperate to know more',
      controversy: 'Make bold, slightly controversial statements that spark debate',
      story: 'Start with a compelling personal story or narrative hook',
      question: 'Ask thought-provoking questions that viewers want answered',
      shock: 'Lead with surprising facts or statistics that stop the scroll',
      promise: 'Make a clear value promise about what viewers will learn'
    };

    const style = hookStyle || 'curiosity';
    const styleGuide = styleDescriptions[style] || styleDescriptions.curiosity;

    const prompt = `You are a viral content expert. Generate 5 attention-grabbing hook variations for this short-form video clip.

CLIP TRANSCRIPT:
"${clip.transcript}"

VIDEO CONTEXT: ${project.videoData?.title || 'Content creator video'}

HOOK STYLE: ${style}
STYLE GUIDE: ${styleGuide}

Requirements:
- Each hook should be 2-8 words (super punchy)
- Hooks should be speakable in under 3 seconds
- Create FOMO or curiosity
- Match the energy and topic of the content
- Make viewers stop scrolling immediately

Respond in JSON format:
{
  "hooks": [
    {
      "id": "hook_1",
      "text": "The hook text here",
      "style": "${style}",
      "speakingDuration": 2.5,
      "emotionalTrigger": "curiosity|fear|excitement|surprise",
      "captionOverlay": "Optional text to show on screen",
      "voiceDirection": "excited|mysterious|urgent|casual"
    }
  ],
  "recommendedHook": "hook_1",
  "explanation": "Why this hook works best for this content"
}`;

    const response = await fetch('https://api.openai.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${openaiKey}`,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        model: 'gpt-4o-mini',
        messages: [{ role: 'user', content: prompt }],
        temperature: 0.8,
        max_tokens: 800,
        response_format: { type: 'json_object' }
      })
    });

    if (!response.ok) {
      throw new Error(`OpenAI API error: ${response.status}`);
    }

    const result = await response.json();
    const hookData = JSON.parse(result.choices[0].message.content);

    // Save hooks to project
    const clipHooks = project.clipHooks || {};
    clipHooks[clipId] = {
      ...hookData,
      generatedAt: new Date().toISOString(),
      style
    };

    await db.collection('wizardProjects').doc(projectId).update({
      clipHooks,
      updatedAt: admin.firestore.FieldValue.serverTimestamp()
    });

    return {
      success: true,
      clipId,
      ...hookData
    };

  } catch (error) {
    console.error('Hook generation error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to generate hooks.'));
  }
});

/**
 * wizardDetectSpeakers - Analyzes video/audio to detect and label speakers
 * Uses transcript analysis to identify speaker changes
 */
exports.wizardDetectSpeakers = functions.runWith({ timeoutSeconds: 120 }).https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const { projectId, clipId } = data;

  if (!projectId || !clipId) {
    throw new functions.https.HttpsError('invalid-argument', 'Project ID and Clip ID required');
  }

  // Check and deduct tokens for speaker detection
  const tokenCosts = await getWizardTokenCosts();
  const speakerCost = tokenCosts.detectSpeakers || 3;

  const tokenResult = await deductWizardTokens(uid, speakerCost, 'detectSpeakers', {
    clipId: clipId,
    projectId: projectId
  });

  if (!tokenResult.success) {
    throw new functions.https.HttpsError(
      'resource-exhausted',
      `Insufficient tokens. This operation requires ${speakerCost} tokens, but you have ${tokenResult.available || 0}.`
    );
  }

  try {
    // Verify project ownership
    const projectDoc = await db.collection('wizardProjects').doc(projectId).get();
    if (!projectDoc.exists || projectDoc.data().userId !== uid) {
      throw new functions.https.HttpsError('permission-denied', 'Not authorized');
    }

    const project = projectDoc.data();
    const clip = (project.clips || []).find(c => c.id === clipId);

    if (!clip) {
      throw new functions.https.HttpsError('not-found', 'Clip not found');
    }

    // Get OpenAI API key
    const settingsDoc = await db.collection('settings').doc('openai').get();
    const openaiKey = settingsDoc.exists ? settingsDoc.data().apiKey : null;

    if (!openaiKey) {
      throw new functions.https.HttpsError('failed-precondition', 'OpenAI API key not configured');
    }

    // Use GPT to analyze transcript for speaker patterns
    const prompt = `Analyze this transcript and identify if there are multiple speakers. Look for:
- Changes in speaking style or vocabulary
- Question/answer patterns
- Interview dynamics
- Different perspectives being expressed

TRANSCRIPT:
"${clip.transcript}"

VIDEO CONTEXT: ${project.videoData?.title || 'Video content'}

Respond in JSON format:
{
  "speakerCount": 1,
  "speakers": [
    {
      "id": "speaker_1",
      "label": "Host" | "Guest" | "Interviewer" | "Speaker 1",
      "estimatedRole": "main_speaker" | "interviewer" | "guest" | "narrator",
      "characteristics": ["energetic", "expert", "casual"],
      "segments": [
        {
          "text": "Part of transcript spoken by this speaker",
          "estimatedStart": 0,
          "estimatedEnd": 15
        }
      ]
    }
  ],
  "isSingleSpeaker": true,
  "isInterview": false,
  "isPodcast": false,
  "confidence": 0.85,
  "analysis": "Brief explanation of the speaker detection"
}`;

    const response = await fetch('https://api.openai.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${openaiKey}`,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        model: 'gpt-4o-mini',
        messages: [{ role: 'user', content: prompt }],
        temperature: 0.3,
        max_tokens: 1000,
        response_format: { type: 'json_object' }
      })
    });

    if (!response.ok) {
      throw new Error(`OpenAI API error: ${response.status}`);
    }

    const result = await response.json();
    const speakerData = JSON.parse(result.choices[0].message.content);

    // Recommend reframe mode based on speaker count
    let recommendedReframe = 'auto_center';
    if (speakerData.speakerCount === 2) {
      recommendedReframe = 'split_screen';
    } else if (speakerData.speakerCount >= 3) {
      recommendedReframe = 'three_person';
    }

    speakerData.recommendedReframe = recommendedReframe;

    // Save speaker detection to project
    const clipSpeakers = project.clipSpeakers || {};
    clipSpeakers[clipId] = {
      ...speakerData,
      detectedAt: new Date().toISOString()
    };

    await db.collection('wizardProjects').doc(projectId).update({
      clipSpeakers,
      updatedAt: admin.firestore.FieldValue.serverTimestamp()
    });

    return {
      success: true,
      clipId,
      ...speakerData
    };

  } catch (error) {
    console.error('Speaker detection error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to detect speakers.'));
  }
});

/**
 * wizardApplyAIEnhancements - Batch applies AI enhancements to a clip
 * Combines multiple AI features in one call
 */
exports.wizardApplyAIEnhancements = functions.runWith({ timeoutSeconds: 180, memory: '1GB' }).https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const { projectId, clipId, enhancements = [] } = data;

  if (!projectId || !clipId) {
    throw new functions.https.HttpsError('invalid-argument', 'Project ID and Clip ID required');
  }

  const validEnhancements = ['broll', 'fillers', 'hook', 'speakers', 'captions', 'reframe'];
  const requestedEnhancements = enhancements.filter(e => validEnhancements.includes(e));

  if (requestedEnhancements.length === 0) {
    throw new functions.https.HttpsError('invalid-argument', 'At least one valid enhancement required');
  }

  try {
    const results = {
      success: true,
      clipId,
      applied: [],
      failed: [],
      data: {}
    };

    // Apply each enhancement
    for (const enhancement of requestedEnhancements) {
      try {
        let result;
        switch (enhancement) {
          case 'broll':
            result = await exports.wizardGenerateBRoll.run({ projectId, clipId }, context);
            results.data.broll = result.brollSuggestions;
            break;
          case 'fillers':
            result = await exports.wizardRemoveFillers.run({ projectId, clipId }, context);
            results.data.fillers = result.stats;
            break;
          case 'hook':
            result = await exports.wizardGenerateHook.run({ projectId, clipId }, context);
            results.data.hooks = result.hooks;
            break;
          case 'speakers':
            result = await exports.wizardDetectSpeakers.run({ projectId, clipId }, context);
            results.data.speakers = result.speakers;
            break;
        }
        results.applied.push(enhancement);
      } catch (err) {
        console.error(`Enhancement ${enhancement} failed:`, err);
        results.failed.push({ enhancement, error: err.message });
      }
    }

    return results;

  } catch (error) {
    console.error('AI enhancements error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to apply AI enhancements.'));
  }
});

/**
 * wizardGenerateAICaptions - Generates styled captions with timing
 * Creates word-by-word captions with animation cues
 */
exports.wizardGenerateAICaptions = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const { projectId, clipId, style = 'karaoke' } = data;

  if (!projectId || !clipId) {
    throw new functions.https.HttpsError('invalid-argument', 'Project ID and Clip ID required');
  }

  try {
    // Verify project ownership
    const projectDoc = await db.collection('wizardProjects').doc(projectId).get();
    if (!projectDoc.exists || projectDoc.data().userId !== uid) {
      throw new functions.https.HttpsError('permission-denied', 'Not authorized');
    }

    const project = projectDoc.data();
    const clip = (project.clips || []).find(c => c.id === clipId);

    if (!clip) {
      throw new functions.https.HttpsError('not-found', 'Clip not found');
    }

    // Parse transcript into words with estimated timing
    const words = (clip.transcript || '').split(/\s+/).filter(w => w.length > 0);

    if (words.length === 0) {
      throw new functions.https.HttpsError('failed-precondition', 'Clip has no transcript text to generate captions from');
    }

    const clipDuration = clip.duration || (clip.endTime - clip.startTime);
    const avgWordDuration = clipDuration / words.length;

    const captionStyles = {
      karaoke: { highlightColor: '#FBBF24', animation: 'scale', wordsPerGroup: 3 },
      beasty: { highlightColor: '#FBBF24', animation: 'pop', wordsPerGroup: 2, uppercase: true },
      hormozi: { highlightColor: '#22C55E', animation: 'highlight', wordsPerGroup: 4 },
      minimal: { highlightColor: '#FFFFFF', animation: 'fade', wordsPerGroup: 5 },
      ali: { highlightColor: '#EC4899', animation: 'glow', wordsPerGroup: 3 }
    };

    const styleConfig = captionStyles[style] || captionStyles.karaoke;

    // Generate word-by-word captions with timing
    const captions = [];
    let currentTime = clip.startTime;

    for (let i = 0; i < words.length; i++) {
      const word = styleConfig.uppercase ? words[i].toUpperCase() : words[i];
      const isKeyword = word.length > 5 || /[!?]/.test(word); // Simple keyword detection

      captions.push({
        word,
        startTime: currentTime,
        endTime: currentTime + avgWordDuration,
        isHighlight: isKeyword,
        animation: isKeyword ? styleConfig.animation : 'none',
        color: isKeyword ? styleConfig.highlightColor : '#FFFFFF',
        groupIndex: Math.floor(i / styleConfig.wordsPerGroup)
      });

      currentTime += avgWordDuration;
    }

    // Group captions for display
    const captionGroups = [];
    for (let i = 0; i < captions.length; i += styleConfig.wordsPerGroup) {
      const group = captions.slice(i, i + styleConfig.wordsPerGroup);
      captionGroups.push({
        text: group.map(c => c.word).join(' '),
        startTime: group[0].startTime,
        endTime: group[group.length - 1].endTime,
        words: group
      });
    }

    // Save captions to project
    const clipCaptions = project.clipCaptions || {};
    clipCaptions[clipId] = {
      style,
      styleConfig,
      captions,
      captionGroups,
      generatedAt: new Date().toISOString()
    };

    await db.collection('wizardProjects').doc(projectId).update({
      clipCaptions,
      updatedAt: admin.firestore.FieldValue.serverTimestamp()
    });

    return {
      success: true,
      clipId,
      style,
      captionCount: captions.length,
      groupCount: captionGroups.length,
      captions,
      captionGroups
    };

  } catch (error) {
    console.error('Caption generation error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to generate captions.'));
  }
});

// ============================================
// PHASE 5: EXPORT & INTEGRATION
// ============================================

/**
 * wizardExportFullProject - Comprehensive export with all AI data
 * Exports complete project data including settings, SEO, hooks, B-Roll, etc.
 */
exports.wizardExportFullProject = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const { projectId, format = 'json', includeAIData = true } = data;

  if (!projectId) {
    throw new functions.https.HttpsError('invalid-argument', 'Project ID required');
  }

  try {
    const projectDoc = await db.collection('wizardProjects').doc(projectId).get();
    if (!projectDoc.exists || projectDoc.data().userId !== uid) {
      throw new functions.https.HttpsError('permission-denied', 'Not authorized');
    }

    const project = projectDoc.data();
    const clips = project.clips || [];

    // Build comprehensive export
    const exportPackage = {
      meta: {
        exportVersion: '1.0',
        exportedAt: new Date().toISOString(),
        projectId: projectDoc.id
      },
      video: {
        title: project.videoData?.title,
        url: project.videoUrl,
        duration: project.videoData?.duration,
        thumbnail: project.videoData?.thumbnail
      },
      clips: clips.map(clip => {
        const clipData = {
          id: clip.id,
          startTime: clip.startTime,
          endTime: clip.endTime,
          duration: clip.duration,
          score: clip.score,
          transcript: clip.transcript,
          platforms: clip.platforms || [],
          thumbnail: clip.thumbnail
        };

        // Add settings
        if (project.clipSettings && project.clipSettings[clip.id]) {
          clipData.settings = project.clipSettings[clip.id];
        }

        // Add SEO
        if (project.clipSEO && project.clipSEO[clip.id]) {
          clipData.seo = project.clipSEO[clip.id];
        }

        // Add AI data if requested
        if (includeAIData) {
          if (project.clipHooks && project.clipHooks[clip.id]) {
            clipData.hooks = project.clipHooks[clip.id];
          }
          if (project.clipBRoll && project.clipBRoll[clip.id]) {
            clipData.broll = project.clipBRoll[clip.id];
          }
          if (project.clipFillers && project.clipFillers[clip.id]) {
            clipData.fillers = project.clipFillers[clip.id];
          }
          if (project.clipSpeakers && project.clipSpeakers[clip.id]) {
            clipData.speakers = project.clipSpeakers[clip.id];
          }
          if (project.clipCaptions && project.clipCaptions[clip.id]) {
            clipData.captions = project.clipCaptions[clip.id];
          }
        }

        return clipData;
      }),
      summary: {
        totalClips: clips.length,
        totalDuration: clips.reduce((sum, c) => sum + (c.duration || 0), 0),
        averageScore: clips.length > 0
          ? Math.round(clips.reduce((sum, c) => sum + (c.score || 0), 0) / clips.length)
          : 0,
        platforms: [...new Set(clips.flatMap(c => c.platforms || []))]
      }
    };

    let exportData, filename;

    if (format === 'json') {
      exportData = JSON.stringify(exportPackage, null, 2);
      filename = `shorts-project-${projectDoc.id}.json`;
    } else if (format === 'csv') {
      // Multi-sheet style CSV with sections
      let csvContent = '';

      // Video Info
      csvContent += '# VIDEO INFO\n';
      csvContent += 'Title,URL,Duration\n';
      csvContent += `"${exportPackage.video.title || ''}","${exportPackage.video.url || ''}",${exportPackage.video.duration || 0}\n\n`;

      // Clips
      csvContent += '# CLIPS\n';
      csvContent += 'Clip ID,Start,End,Duration,Score,Platforms,SEO Title,SEO Description,Tags\n';
      exportPackage.clips.forEach(clip => {
        const seo = clip.seo || {};
        csvContent += [
          clip.id,
          clip.startTime,
          clip.endTime,
          clip.duration,
          clip.score,
          `"${(clip.platforms || []).join('; ')}"`,
          `"${(seo.title || '').replace(/"/g, '""')}"`,
          `"${(seo.description || '').replace(/"/g, '""').substring(0, 200)}"`,
          `"${(seo.tags || []).join('; ')}"`
        ].join(',') + '\n';
      });

      // Hooks section
      if (includeAIData) {
        csvContent += '\n# AI HOOKS\n';
        csvContent += 'Clip ID,Hook Text,Style,Duration,Trigger\n';
        exportPackage.clips.forEach(clip => {
          if (clip.hooks && clip.hooks.hooks) {
            clip.hooks.hooks.forEach(hook => {
              csvContent += [
                clip.id,
                `"${(hook.text || '').replace(/"/g, '""')}"`,
                hook.style || '',
                hook.speakingDuration || '',
                hook.emotionalTrigger || ''
              ].join(',') + '\n';
            });
          }
        });
      }

      exportData = csvContent;
      filename = `shorts-project-${projectDoc.id}.csv`;
    }

    return {
      success: true,
      data: exportData,
      filename,
      format,
      summary: exportPackage.summary
    };

  } catch (error) {
    console.error('Full export error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to export project.'));
  }
});

/**
 * wizardGetProjectsList - Gets list of user's wizard projects with summaries
 */
exports.wizardGetProjectsList = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const { limit = 20, startAfter = null } = data;

  try {
    let query = db.collection('wizardProjects')
      .where('userId', '==', uid)
      .orderBy('updatedAt', 'desc')
      .limit(Math.min(limit, 50));

    if (startAfter) {
      const startDoc = await db.collection('wizardProjects').doc(startAfter).get();
      if (startDoc.exists) {
        query = query.startAfter(startDoc);
      }
    }

    const snapshot = await query.get();

    const projects = snapshot.docs.map(doc => {
      const data = doc.data();
      const clips = data.clips || [];

      return {
        id: doc.id,
        videoTitle: data.videoData?.title || 'Untitled Project',
        videoUrl: data.videoUrl,
        videoThumbnail: data.videoData?.thumbnail,
        clipCount: clips.length,
        totalDuration: clips.reduce((sum, c) => sum + (c.duration || 0), 0),
        averageScore: clips.length > 0
          ? Math.round(clips.reduce((sum, c) => sum + (c.score || 0), 0) / clips.length)
          : 0,
        status: data.status || 'draft',
        createdAt: data.createdAt?.toDate?.()?.toISOString() || null,
        updatedAt: data.updatedAt?.toDate?.()?.toISOString() || null
      };
    });

    return {
      success: true,
      projects,
      hasMore: snapshot.docs.length === limit,
      lastId: snapshot.docs.length > 0 ? snapshot.docs[snapshot.docs.length - 1].id : null
    };

  } catch (error) {
    console.error('Get projects list error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to get projects.'));
  }
});

/**
 * wizardLoadProject - Loads a complete project for editing
 */
exports.wizardLoadProject = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const { projectId } = data;

  if (!projectId) {
    throw new functions.https.HttpsError('invalid-argument', 'Project ID required');
  }

  try {
    const projectDoc = await db.collection('wizardProjects').doc(projectId).get();
    if (!projectDoc.exists) {
      throw new functions.https.HttpsError('not-found', 'Project not found');
    }

    const project = projectDoc.data();
    if (project.userId !== uid) {
      throw new functions.https.HttpsError('permission-denied', 'Not authorized');
    }

    return {
      success: true,
      project: {
        id: projectDoc.id,
        videoUrl: project.videoUrl,
        videoData: project.videoData,
        clips: project.clips || [],
        clipSettings: project.clipSettings || {},
        clipSEO: project.clipSEO || {},
        clipThumbnails: project.clipThumbnails || {},
        clipHooks: project.clipHooks || {},
        clipBRoll: project.clipBRoll || {},
        clipFillers: project.clipFillers || {},
        clipSpeakers: project.clipSpeakers || {},
        clipCaptions: project.clipCaptions || {},
        status: project.status || 'draft',
        createdAt: project.createdAt?.toDate?.()?.toISOString() || null,
        updatedAt: project.updatedAt?.toDate?.()?.toISOString() || null
      }
    };

  } catch (error) {
    console.error('Load project error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to load project.'));
  }
});

/**
 * wizardDuplicateProject - Creates a copy of an existing project
 */
exports.wizardDuplicateProject = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const { projectId } = data;

  if (!projectId) {
    throw new functions.https.HttpsError('invalid-argument', 'Project ID required');
  }

  try {
    const projectDoc = await db.collection('wizardProjects').doc(projectId).get();
    if (!projectDoc.exists) {
      throw new functions.https.HttpsError('not-found', 'Project not found');
    }

    const project = projectDoc.data();
    if (project.userId !== uid) {
      throw new functions.https.HttpsError('permission-denied', 'Not authorized');
    }

    // Enforce max projects limit before creating duplicate
    const maxProjects = await getMaxProjectsLimit();
    await enforceMaxProjects(uid, maxProjects);

    // Create duplicate with new ID
    // Note: Clear storage references since they point to original project's files
    // User will need to re-capture/upload for the duplicate
    const newProject = {
      ...project,
      videoData: {
        ...project.videoData,
        title: `${project.videoData?.title || 'Project'} (Copy)`,
        // Clear upload-specific fields from videoData
        uploadedVideoUrl: null,
        uploadedVideoPath: null
      },
      // Clear storage references - duplicate doesn't own original's files
      sourceAsset: null,
      uploadedVideoPath: null,
      uploadedVideoName: null,
      // For YouTube videos, keep the videoId so they can re-capture
      // For uploads, generate new videoId to avoid storage path conflicts
      videoId: project.isUpload ? `upload_${Date.now()}_${uid}` : project.videoId,
      status: 'draft',
      createdAt: admin.firestore.FieldValue.serverTimestamp(),
      updatedAt: admin.firestore.FieldValue.serverTimestamp()
    };

    const newDoc = await db.collection('wizardProjects').add(newProject);

    return {
      success: true,
      newProjectId: newDoc.id,
      message: 'Project duplicated successfully. Note: You may need to re-capture the video source.'
    };

  } catch (error) {
    console.error('Duplicate project error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to duplicate project.'));
  }
});

/**
 * wizardUpdateProjectStatus - Updates project status (draft/complete/archived)
 */
exports.wizardUpdateProjectStatus = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const { projectId, status } = data;

  if (!projectId || !status) {
    throw new functions.https.HttpsError('invalid-argument', 'Project ID and status required');
  }

  const validStatuses = ['draft', 'complete', 'archived'];
  if (!validStatuses.includes(status)) {
    throw new functions.https.HttpsError('invalid-argument', 'Invalid status');
  }

  try {
    const projectDoc = await db.collection('wizardProjects').doc(projectId).get();
    if (!projectDoc.exists || projectDoc.data().userId !== uid) {
      throw new functions.https.HttpsError('permission-denied', 'Not authorized');
    }

    await db.collection('wizardProjects').doc(projectId).update({
      status,
      updatedAt: admin.firestore.FieldValue.serverTimestamp()
    });

    return { success: true, status };

  } catch (error) {
    console.error('Update status error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to update status.'));
  }
});

// ============================================
// VIDEO PROCESSING FUNCTIONS
// ============================================

/**
 * wizardProcessClip - Creates a video processing job for a clip
 * This sets up the infrastructure for FFmpeg-based video processing
 *
 * CANONICAL SOURCE ASSET ARCHITECTURE:
 * Export REQUIRES a sourceAsset (video file stored in our storage).
 * The sourceAsset is created during analysis when user captures video.
 * This eliminates unreliable re-capture at export time.
 */
exports.wizardProcessClip = functions
  .runWith({ timeoutSeconds: 540, memory: '2GB' })
  .https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const { projectId, clipId, quality, settings, extensionCaptureData } = data;
  // extensionCaptureData is used as fallback when sourceAsset is missing (e.g., extension capture failed during analysis)

  if (!projectId || !clipId) {
    throw new functions.https.HttpsError('invalid-argument', 'Project ID and Clip ID required');
  }

  const validQualities = ['720p', '1080p'];
  const outputQuality = validQualities.includes(quality) ? quality : '720p';

  try {
    // Get project and clip data
    const projectDoc = await db.collection('wizardProjects').doc(projectId).get();
    if (!projectDoc.exists) {
      throw new functions.https.HttpsError('not-found', 'Project not found');
    }

    const project = projectDoc.data();
    if (project.userId !== uid) {
      throw new functions.https.HttpsError('permission-denied', 'Not authorized');
    }

    const clip = (project.clips || []).find(c => c.id === clipId);
    if (!clip) {
      throw new functions.https.HttpsError('not-found', 'Clip not found');
    }

    // Get clip settings (from project or from request)
    const clipSettings = settings || project.clipSettings?.[clipId] || {};

    // Get filler word data if available (from wizardRemoveFillers analysis)
    // Note: For filler removal to work during export, the filler data must include
    // word-level timestamps. The current analysis stores character positions,
    // which would need to be enhanced to include timestamps from Whisper transcription.
    const clipFillerData = project.clipFillers?.[clipId];
    let fillerTimestamps = null;
    if (clipFillerData && clipFillerData.fillers && clipFillerData.fillers.length > 0) {
      // Extract timestamps from filler data if available
      // Format: [{start: seconds, end: seconds}, ...]
      fillerTimestamps = clipFillerData.fillers
        .filter(f => f.start !== undefined && f.end !== undefined)
        .map(f => ({ start: f.start, end: f.end }));

      console.log(`[wizardProcessClip] Found ${clipFillerData.fillers.length} filler words for clip ${clipId}`);
      console.log(`[wizardProcessClip] Filler timestamps available: ${fillerTimestamps.length > 0 ? fillerTimestamps.length : 'none (timestamps not in data)'}`);
    }

    // Log cropPosition for debugging export issues
    console.log(`[wizardProcessClip] ========== CROP POSITION DEBUG ==========`);
    console.log(`[wizardProcessClip] Clip ID: ${clipId}`);
    console.log(`[wizardProcessClip] Settings source: ${settings ? 'from request' : (project.clipSettings?.[clipId] ? 'from project' : 'default empty')}`);
    console.log(`[wizardProcessClip] clipSettings.cropPosition: ${clipSettings.cropPosition} (type: ${typeof clipSettings.cropPosition})`);
    console.log(`[wizardProcessClip] clipSettings.reframeMode: ${clipSettings.reframeMode}`);
    console.log(`[wizardProcessClip] ==========================================`);

    // Log multi-source settings for three_person debugging
    console.log(`[wizardProcessClip] ========== MULTI-SOURCE DEBUG ==========`);
    console.log(`[wizardProcessClip] secondarySource exists: ${!!clipSettings.secondarySource}`);
    console.log(`[wizardProcessClip] secondarySource.enabled: ${clipSettings.secondarySource?.enabled}`);
    console.log(`[wizardProcessClip] secondarySource.type: ${clipSettings.secondarySource?.type}`);
    console.log(`[wizardProcessClip] secondarySource.youtubeVideoId: ${clipSettings.secondarySource?.youtubeVideoId}`);
    console.log(`[wizardProcessClip] secondarySource.uploadedUrl: ${clipSettings.secondarySource?.uploadedUrl ? 'SET (' + clipSettings.secondarySource.uploadedUrl.substring(0, 50) + '...)' : 'NOT SET'}`);
    console.log(`[wizardProcessClip] tertiarySource exists: ${!!clipSettings.tertiarySource}`);
    console.log(`[wizardProcessClip] tertiarySource.enabled: ${clipSettings.tertiarySource?.enabled}`);
    console.log(`[wizardProcessClip] tertiarySource.type: ${clipSettings.tertiarySource?.type}`);
    console.log(`[wizardProcessClip] tertiarySource.youtubeVideoId: ${clipSettings.tertiarySource?.youtubeVideoId}`);
    console.log(`[wizardProcessClip] tertiarySource.uploadedUrl: ${clipSettings.tertiarySource?.uploadedUrl ? 'SET (' + clipSettings.tertiarySource.uploadedUrl.substring(0, 50) + '...)' : 'NOT SET'}`);
    // Check if conditions for three_person mode are met
    const hasSecondaryForLog = clipSettings.secondarySource?.enabled && (clipSettings.secondarySource?.uploadedUrl || clipSettings.secondarySource?.youtubeVideoId);
    const hasTertiaryForLog = clipSettings.tertiarySource?.enabled && (clipSettings.tertiarySource?.uploadedUrl || clipSettings.tertiarySource?.youtubeVideoId);
    console.log(`[wizardProcessClip] THREE_PERSON conditions: reframeMode=${clipSettings.reframeMode}, hasSecondary=${hasSecondaryForLog}, hasTertiary=${hasTertiaryForLog}`);
    console.log(`[wizardProcessClip] Will processor use three_person?: ${clipSettings.reframeMode === 'three_person' && hasSecondaryForLog && hasTertiaryForLog}`);
    console.log(`[wizardProcessClip] ==========================================`);

    // SOURCE ASSET PRIORITY:
    // 1. extensionCaptureData - ALWAYS preferred when provided (clip-specific capture)
    // 2. sourceAsset - full source video stored during analysis
    // 3. uploadedVideoUrl - for user-uploaded videos
    //
    // CRITICAL FIX: extensionCaptureData contains the clip-specific captured segment.
    // We must use it when provided, NOT fall back to sourceAsset (which may be a different clip's segment).
    // Do NOT save clip-specific captures as sourceAsset - that's for the full source video only.

    let sourceAsset = project.sourceAsset;
    const isUploadedVideo = project.isUpload && project.videoData?.uploadedVideoUrl;

    // Check if we have clip-specific capture data from the extension
    const hasClipSpecificCapture = extensionCaptureData &&
      extensionCaptureData.streamData &&
      extensionCaptureData.streamData.uploadedToStorage &&
      extensionCaptureData.streamData.videoUrl;

    console.log(`[wizardProcessClip] Checking source for ${clipId}:`, {
      hasClipSpecificCapture,
      hasSourceAsset: !!sourceAsset,
      sourceAssetUrl: sourceAsset?.storageUrl?.substring(0, 60) + '...' || 'none',
      isUploadedVideo,
      projectId
    });

    // Determine the video source URL for processing
    // PRIORITY: clip-specific capture > sourceAsset > uploaded video
    let videoSourceUrl = null;
    let videoSourceType = 'unknown';

    if (hasClipSpecificCapture) {
      // HIGHEST PRIORITY: Use clip-specific captured segment from extension
      // This is the video that was captured specifically for THIS clip at export time
      videoSourceUrl = extensionCaptureData.streamData.videoUrl;
      videoSourceType = 'clip_capture';
      console.log(`[wizardProcessClip] Using clip-specific capture for ${clipId}: ${videoSourceUrl.substring(0, 60)}...`);

      // NOTE: We intentionally do NOT save this as sourceAsset.
      // sourceAsset should only contain the full source video, not clip segments.
      // Each clip gets its own captured segment via extensionCaptureData.
    } else if (sourceAsset && sourceAsset.storageUrl) {
      // SECOND PRIORITY: Use full source video from analysis
      videoSourceUrl = sourceAsset.storageUrl;
      videoSourceType = 'source_asset';
      console.log(`[wizardProcessClip] Using sourceAsset: ${videoSourceUrl.substring(0, 60)}...`);
    } else if (isUploadedVideo) {
      // THIRD PRIORITY: Use user-uploaded video
      videoSourceUrl = project.videoData.uploadedVideoUrl;
      videoSourceType = 'uploaded_video';
      console.log(`[wizardProcessClip] Using uploaded video: ${videoSourceUrl.substring(0, 60)}...`);
    }

    // Validate we have a video source
    if (!videoSourceUrl) {
      console.error(`[wizardProcessClip] ERROR: No video source for project ${projectId}, clip ${clipId}`);
      throw new functions.https.HttpsError(
        'failed-precondition',
        'Video source not available. The video capture may have failed. Please try re-analyzing the video.'
      );
    }

    // Create processing job record with canonical source
    const processingJob = {
      userId: uid,
      projectId,
      clipId,
      videoId: project.videoId,
      videoUrl: project.videoUrl,

      // CANONICAL SOURCE - single source of truth for video data
      videoSourceUrl: videoSourceUrl,
      videoSourceType: videoSourceType,

      // Legacy fields (for backward compatibility)
      isUpload: project.isUpload || false,
      uploadedVideoUrl: videoSourceUrl,
      uploadedVideoPath: project.uploadedVideoPath || null,

      // Mark that we have a valid source (replaces extensionStreamData logic)
      hasExtensionStream: true,  // Always true now since we require sourceAsset
      extensionStreamData: {
        videoUrl: videoSourceUrl,
        source: videoSourceType,
        uploadedToStorage: true,
        capturedAt: sourceAsset?.capturedAt || Date.now()
      },

      // Clip timing
      startTime: clip.startTime,
      endTime: clip.endTime,
      duration: clip.duration,

      // Processing settings
      quality: outputQuality,
      settings: {
        captionStyle: clipSettings.captionStyle || 'karaoke',
        captionPosition: clipSettings.captionPosition || 'bottom',  // Caption position: 'bottom', 'middle', 'top'
        captionSize: clipSettings.captionSize || 1,                 // Caption size multiplier: 0.7-1.5
        captionSource: clipSettings.captionSource || 'primary',  // Which video's audio to use for captions
        customCaptionStyle: clipSettings.customCaptionStyle || null,
        reframeMode: clipSettings.reframeMode || 'auto_center',
        cropPosition: clipSettings.cropPosition !== undefined ? clipSettings.cropPosition : 50,
        zoom: clipSettings.zoom || 100, // Single video zoom: 100 = no zoom, 200 = 2x zoom
        trimStart: clipSettings.trimStart || 0,
        trimEnd: clipSettings.trimEnd || clip.duration,
        introTransition: clipSettings.introTransition || 'none',
        outroTransition: clipSettings.outroTransition || 'none',
        autoZoom: clipSettings.autoZoom || false,
        vignette: clipSettings.vignette || false,
        colorGrade: clipSettings.colorGrade || false,
        enhanceAudio: clipSettings.enhanceAudio !== false,
        removeFiller: clipSettings.removeFiller || false,
        audioDucking: clipSettings.audioDucking || false,  // Smart audio ducking - auto-lower music during speech
        fillerTimestamps: fillerTimestamps && fillerTimestamps.length > 0 ? fillerTimestamps : null,  // Timestamps of filler words to silence
        voiceVolume: clipSettings.voiceVolume || 100,
        addMusic: clipSettings.addMusic || false,
        musicVolume: clipSettings.musicVolume || 30,
        selectedTrack: clipSettings.selectedTrack || null,

        // Multi-source split screen settings
        secondarySource: clipSettings.secondarySource && clipSettings.secondarySource.enabled ? {
          enabled: true,
          type: clipSettings.secondarySource.type || null,
          uploadedUrl: clipSettings.secondarySource.uploadedUrl || null,
          youtubeUrl: clipSettings.secondarySource.youtubeUrl || null,
          youtubeVideoId: clipSettings.secondarySource.youtubeVideoId || null,
          position: clipSettings.secondarySource.position || 'bottom',
          timeOffset: clipSettings.secondarySource.timeOffset || 0
        } : null,

        // Tertiary source for three_person mode (third video)
        tertiarySource: clipSettings.tertiarySource && clipSettings.tertiarySource.enabled ? {
          enabled: true,
          type: clipSettings.tertiarySource.type || null,
          uploadedUrl: clipSettings.tertiarySource.uploadedUrl || null,
          youtubeUrl: clipSettings.tertiarySource.youtubeUrl || null,
          youtubeVideoId: clipSettings.tertiarySource.youtubeVideoId || null,
          timeOffset: clipSettings.tertiarySource.timeOffset || 0
        } : null,

        // Audio mixing settings (for multi-source)
        audioMix: clipSettings.audioMix ? {
          primaryVolume: clipSettings.audioMix.primaryVolume ?? 100,
          secondaryVolume: clipSettings.audioMix.secondaryVolume ?? 0,
          tertiaryVolume: clipSettings.audioMix.tertiaryVolume ?? 0,
          primaryMuted: clipSettings.audioMix.primaryMuted ?? false,
          secondaryMuted: clipSettings.audioMix.secondaryMuted ?? true,
          tertiaryMuted: clipSettings.audioMix.tertiaryMuted ?? true
        } : null,

        // Split screen speaker position settings
        splitScreenSettings: clipSettings.splitScreenSettings ? {
          speaker1: {
            cropPosition: clipSettings.splitScreenSettings.speaker1?.cropPosition ?? 50,
            cropWidth: clipSettings.splitScreenSettings.speaker1?.cropWidth ?? 33,
            zoom: clipSettings.splitScreenSettings.speaker1?.zoom ?? 100
          },
          speaker2: {
            cropPosition: clipSettings.splitScreenSettings.speaker2?.cropPosition ?? 50,
            cropWidth: clipSettings.splitScreenSettings.speaker2?.cropWidth ?? 33,
            zoom: clipSettings.splitScreenSettings.speaker2?.zoom ?? 100
          },
          preset: clipSettings.splitScreenSettings.preset || 'center',
          layoutRatio: clipSettings.splitScreenSettings.layoutRatio || '50-50',
          border: clipSettings.splitScreenSettings.border ? {
            enabled: clipSettings.splitScreenSettings.border.enabled ?? false,
            thickness: clipSettings.splitScreenSettings.border.thickness ?? 2,
            color: clipSettings.splitScreenSettings.border.color || '#ffffff'
          } : { enabled: false, thickness: 2, color: '#ffffff' }
        } : null,

        // Three person mode position settings
        threePersonSettings: clipSettings.threePersonSettings ? {
          main: {
            cropPosition: clipSettings.threePersonSettings.main?.cropPosition ?? 50,
            zoom: clipSettings.threePersonSettings.main?.zoom ?? 100
          },
          left: {
            cropPosition: clipSettings.threePersonSettings.left?.cropPosition ?? 50,
            zoom: clipSettings.threePersonSettings.left?.zoom ?? 100
          },
          right: {
            cropPosition: clipSettings.threePersonSettings.right?.cropPosition ?? 50,
            zoom: clipSettings.threePersonSettings.right?.zoom ?? 100
          },
          preset: clipSettings.threePersonSettings.preset || 'center',
          layoutRatio: clipSettings.threePersonSettings.layoutRatio || '50-50'
        } : null
      },

      // Output specifications - use frame rate from settings
      output: {
        format: 'mp4',
        aspectRatio: '9:16',
        resolution: outputQuality === '1080p' ? { width: 1080, height: 1920 } : { width: 720, height: 1280 },
        fps: parseInt(clipSettings.frameRate) || 30,
        codec: 'h264'
      },

      // Status tracking
      status: 'queued',
      progress: 0,
      createdAt: admin.firestore.FieldValue.serverTimestamp(),
      updatedAt: admin.firestore.FieldValue.serverTimestamp()
    };

    // Save job to Firestore
    const jobRef = await db.collection('wizardProcessingJobs').add(processingJob);

    // Update project with processing status
    await db.collection('wizardProjects').doc(projectId).update({
      [`clipProcessing.${clipId}`]: {
        jobId: jobRef.id,
        status: 'queued',
        quality: outputQuality,
        queuedAt: admin.firestore.FieldValue.serverTimestamp()
      },
      updatedAt: admin.firestore.FieldValue.serverTimestamp()
    });

    // Get user's YouTube credentials if available
    let youtubeCredentials = null;
    try {
      youtubeCredentials = await getYouTubeCredentialsForUser(uid);
      if (youtubeCredentials) {
        // Store credentials reference in job (not the actual tokens for security)
        await jobRef.update({
          hasYouTubeAuth: true
        });
        console.log(`[${jobRef.id}] User has YouTube credentials available`);
      } else {
        console.log(`[${jobRef.id}] No YouTube credentials - will use fallback download method`);
      }
    } catch (credError) {
      console.log(`[${jobRef.id}] Could not fetch YouTube credentials:`, credError.message);
    }

    // Trigger Cloud Run video processor service (fire and forget)
    const videoProcessorUrl = functions.config().videoprocessor?.url;
    if (videoProcessorUrl) {
      try {
        // Prepare request body with optional YouTube credentials
        const requestBody = {
          jobId: jobRef.id
        };

        // Include YouTube credentials if available (passed securely)
        if (youtubeCredentials) {
          requestBody.youtubeAuth = {
            accessToken: youtubeCredentials.accessToken
          };
        }

        // Async call to Cloud Run - don't await
        axios.post(`${videoProcessorUrl}/process`, requestBody, {
          timeout: 5000,
          headers: { 'Content-Type': 'application/json' }
        }).catch(err => {
          console.log('Video processor trigger sent (async):', err.message || 'pending');
        });
        console.log(`Triggered video processor for job: ${jobRef.id}`);
      } catch (triggerError) {
        // Log but don't fail - job is queued and can be picked up by scheduler
        console.log('Video processor trigger note:', triggerError.message);
      }
    } else {
      console.log('Video processor URL not configured - job queued for manual processing');
    }

    await logUsage(uid, 'wizard_process_clip', { projectId, clipId, quality: outputQuality });

    return {
      success: true,
      jobId: jobRef.id,
      status: 'queued',
      message: 'Video processing job created. Processing will be available soon.',
      estimatedTime: outputQuality === '1080p' ? '3-5 minutes' : '2-3 minutes'
    };

  } catch (error) {
    console.error('Process clip error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to create processing job.'));
  }
});

/**
 * wizardGetProcessingStatus - Gets the status of a processing job
 */
exports.wizardGetProcessingStatus = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const { jobId } = data;

  if (!jobId) {
    throw new functions.https.HttpsError('invalid-argument', 'Job ID required');
  }

  try {
    const jobDoc = await db.collection('wizardProcessingJobs').doc(jobId).get();
    if (!jobDoc.exists) {
      throw new functions.https.HttpsError('not-found', 'Job not found');
    }

    const job = jobDoc.data();
    if (job.userId !== uid) {
      throw new functions.https.HttpsError('permission-denied', 'Not authorized');
    }

    return {
      success: true,
      job: {
        id: jobId,
        clipId: job.clipId,
        status: job.status,
        progress: job.progress || 0,
        quality: job.quality,
        outputUrl: job.outputUrl || null,
        error: job.error || null,
        createdAt: job.createdAt?.toDate?.()?.toISOString() || null,
        completedAt: job.completedAt?.toDate?.()?.toISOString() || null
      }
    };

  } catch (error) {
    console.error('Get processing status error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to get job status.'));
  }
});

// ============================================
// BATCH EXPORT TRACKING FUNCTIONS
// ============================================

/**
 * wizardCreateBatchExport - Create a batch export tracking document
 * This allows users to resume viewing progress if they refresh/leave the page
 */
exports.wizardCreateBatchExport = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const { projectId, clips } = data;

  if (!projectId || !clips || !Array.isArray(clips)) {
    throw new functions.https.HttpsError('invalid-argument', 'Project ID and clips array required');
  }

  try {
    // Create batch export document
    const batchData = {
      userId: uid,
      projectId: projectId,
      status: 'processing',
      clips: clips.map(clip => ({
        clipId: clip.clipId,
        title: clip.title || '',
        jobId: null,
        status: 'pending',
        progress: 0,
        outputUrl: null,
        error: null
      })),
      totalClips: clips.length,
      completedClips: 0,
      failedClips: 0,
      startedAt: admin.firestore.FieldValue.serverTimestamp(),
      completedAt: null,
      lastUpdated: admin.firestore.FieldValue.serverTimestamp()
    };

    const batchRef = await db.collection('wizardBatchExports').add(batchData);

    console.log(`[wizardCreateBatchExport] Created batch ${batchRef.id} for project ${projectId} with ${clips.length} clips`);

    return {
      success: true,
      batchId: batchRef.id
    };
  } catch (error) {
    console.error('[wizardCreateBatchExport] Error:', error);
    throw new functions.https.HttpsError('internal', error.message);
  }
});

/**
 * wizardUpdateBatchExportClip - Update a single clip's status in a batch export
 */
exports.wizardUpdateBatchExportClip = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const { batchId, clipId, jobId, status, progress, outputUrl, error } = data;

  if (!batchId || !clipId) {
    throw new functions.https.HttpsError('invalid-argument', 'Batch ID and clip ID required');
  }

  try {
    const batchRef = db.collection('wizardBatchExports').doc(batchId);
    const batchDoc = await batchRef.get();

    if (!batchDoc.exists) {
      throw new functions.https.HttpsError('not-found', 'Batch export not found');
    }

    const batch = batchDoc.data();
    if (batch.userId !== uid) {
      throw new functions.https.HttpsError('permission-denied', 'Not authorized');
    }

    // Update the specific clip in the array
    const updatedClips = batch.clips.map(clip => {
      if (clip.clipId === clipId) {
        return {
          ...clip,
          jobId: jobId || clip.jobId,
          status: status || clip.status,
          progress: progress !== undefined ? progress : clip.progress,
          outputUrl: outputUrl || clip.outputUrl,
          error: error || clip.error
        };
      }
      return clip;
    });

    // Calculate completion stats
    const completedClips = updatedClips.filter(c => c.status === 'completed').length;
    const failedClips = updatedClips.filter(c => c.status === 'error').length;
    const allDone = (completedClips + failedClips) === batch.totalClips;

    const updateData = {
      clips: updatedClips,
      completedClips,
      failedClips,
      lastUpdated: admin.firestore.FieldValue.serverTimestamp()
    };

    if (allDone) {
      updateData.status = failedClips === batch.totalClips ? 'failed' : (failedClips > 0 ? 'partial' : 'completed');
      updateData.completedAt = admin.firestore.FieldValue.serverTimestamp();
    }

    await batchRef.update(updateData);

    return {
      success: true,
      completedClips,
      failedClips,
      isComplete: allDone
    };
  } catch (error) {
    console.error('[wizardUpdateBatchExportClip] Error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', error.message);
  }
});

/**
 * wizardGetBatchExport - Get batch export status
 */
exports.wizardGetBatchExport = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const { batchId } = data;

  if (!batchId) {
    throw new functions.https.HttpsError('invalid-argument', 'Batch ID required');
  }

  try {
    const batchDoc = await db.collection('wizardBatchExports').doc(batchId).get();

    if (!batchDoc.exists) {
      throw new functions.https.HttpsError('not-found', 'Batch export not found');
    }

    const batch = batchDoc.data();
    if (batch.userId !== uid) {
      throw new functions.https.HttpsError('permission-denied', 'Not authorized');
    }

    return {
      success: true,
      batch: {
        id: batchId,
        projectId: batch.projectId,
        status: batch.status,
        clips: batch.clips,
        totalClips: batch.totalClips,
        completedClips: batch.completedClips,
        failedClips: batch.failedClips,
        startedAt: batch.startedAt?.toDate?.()?.toISOString() || null,
        completedAt: batch.completedAt?.toDate?.()?.toISOString() || null
      }
    };
  } catch (error) {
    console.error('[wizardGetBatchExport] Error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', error.message);
  }
});

/**
 * wizardGetPendingBatchExport - Get any pending batch export for a project
 * Called on page load to check if there's an export in progress
 */
exports.wizardGetPendingBatchExport = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const { projectId } = data;

  if (!projectId) {
    throw new functions.https.HttpsError('invalid-argument', 'Project ID required');
  }

  try {
    // Find any processing batch for this project
    const batchSnapshot = await db.collection('wizardBatchExports')
      .where('userId', '==', uid)
      .where('projectId', '==', projectId)
      .where('status', '==', 'processing')
      .orderBy('startedAt', 'desc')
      .limit(1)
      .get();

    if (batchSnapshot.empty) {
      return { success: true, batch: null };
    }

    const batchDoc = batchSnapshot.docs[0];
    const batch = batchDoc.data();

    // Check if any jobs are still actually processing
    // If all jobs are done but status wasn't updated, fix it
    const pendingClips = batch.clips.filter(c => c.status === 'pending' || c.status === 'capturing' || c.status === 'processing');
    const completedClips = batch.clips.filter(c => c.status === 'completed').length;
    const failedClips = batch.clips.filter(c => c.status === 'error').length;

    // Refresh job statuses from wizardProcessingJobs collection
    const updatedClips = await Promise.all(batch.clips.map(async (clip) => {
      if (clip.jobId && (clip.status === 'processing' || clip.status === 'capturing')) {
        try {
          const jobDoc = await db.collection('wizardProcessingJobs').doc(clip.jobId).get();
          if (jobDoc.exists) {
            const job = jobDoc.data();
            return {
              ...clip,
              status: job.status,
              progress: job.progress || clip.progress,
              outputUrl: job.outputUrl || clip.outputUrl,
              error: job.error || clip.error
            };
          }
        } catch (e) {
          console.warn(`Could not fetch job ${clip.jobId}:`, e.message);
        }
      }
      return clip;
    }));

    // Recalculate stats
    const actualCompleted = updatedClips.filter(c => c.status === 'completed').length;
    const actualFailed = updatedClips.filter(c => c.status === 'error').length;
    const allDone = (actualCompleted + actualFailed) === batch.totalClips;

    // Update the batch if stats changed
    if (actualCompleted !== completedClips || actualFailed !== failedClips || allDone) {
      const updateData = {
        clips: updatedClips,
        completedClips: actualCompleted,
        failedClips: actualFailed,
        lastUpdated: admin.firestore.FieldValue.serverTimestamp()
      };

      if (allDone) {
        updateData.status = actualFailed === batch.totalClips ? 'failed' : (actualFailed > 0 ? 'partial' : 'completed');
        updateData.completedAt = admin.firestore.FieldValue.serverTimestamp();
      }

      await batchDoc.ref.update(updateData);
    }

    return {
      success: true,
      batch: {
        id: batchDoc.id,
        projectId: batch.projectId,
        status: allDone ? (actualFailed === batch.totalClips ? 'failed' : (actualFailed > 0 ? 'partial' : 'completed')) : 'processing',
        clips: updatedClips,
        totalClips: batch.totalClips,
        completedClips: actualCompleted,
        failedClips: actualFailed,
        startedAt: batch.startedAt?.toDate?.()?.toISOString() || null,
        completedAt: allDone ? new Date().toISOString() : null
      }
    };
  } catch (error) {
    console.error('[wizardGetPendingBatchExport] Error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', error.message);
  }
});

/**
 * wizardCancelBatchExport - Cancel a batch export
 */
exports.wizardCancelBatchExport = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const { batchId } = data;

  if (!batchId) {
    throw new functions.https.HttpsError('invalid-argument', 'Batch ID required');
  }

  try {
    const batchRef = db.collection('wizardBatchExports').doc(batchId);
    const batchDoc = await batchRef.get();

    if (!batchDoc.exists) {
      throw new functions.https.HttpsError('not-found', 'Batch export not found');
    }

    const batch = batchDoc.data();
    if (batch.userId !== uid) {
      throw new functions.https.HttpsError('permission-denied', 'Not authorized');
    }

    await batchRef.update({
      status: 'cancelled',
      completedAt: admin.firestore.FieldValue.serverTimestamp(),
      lastUpdated: admin.firestore.FieldValue.serverTimestamp()
    });

    return { success: true };
  } catch (error) {
    console.error('[wizardCancelBatchExport] Error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', error.message);
  }
});

// ============================================
// YOUTUBE OAUTH FUNCTIONS
// ============================================

/**
 * YouTube OAuth2 Client Configuration
 * These credentials must be configured in Firebase Functions config:
 * firebase functions:config:set youtube.client_id="YOUR_CLIENT_ID" youtube.client_secret="YOUR_CLIENT_SECRET"
 */
function getYouTubeOAuth2Client(redirectUri) {
  const clientId = functions.config().youtube?.client_id || process.env.YOUTUBE_CLIENT_ID;
  const clientSecret = functions.config().youtube?.client_secret || process.env.YOUTUBE_CLIENT_SECRET;

  if (!clientId || !clientSecret) {
    throw new functions.https.HttpsError(
      'failed-precondition',
      'YouTube OAuth is not configured. Please contact support.'
    );
  }

  return new google.auth.OAuth2(clientId, clientSecret, redirectUri);
}

/**
 * getYouTubeOAuthUrl - Generate OAuth URL for user to authorize YouTube access
 * This allows the app to use the user's YouTube session for video downloads
 */
exports.getYouTubeOAuthUrl = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);

  try {
    // Use the frontend URL as the redirect - it will handle the callback
    const redirectUri = data.redirectUri || 'https://ytseo.siteuo.com/video-wizard.html';

    const oauth2Client = getYouTubeOAuth2Client(redirectUri);

    // Generate state parameter for security (CSRF protection)
    const state = Buffer.from(JSON.stringify({
      uid: uid,
      timestamp: Date.now(),
      nonce: Math.random().toString(36).substring(7)
    })).toString('base64');

    // Store state in Firestore for validation
    await db.collection('youtubeOAuthStates').doc(state).set({
      uid: uid,
      createdAt: admin.firestore.FieldValue.serverTimestamp(),
      expiresAt: new Date(Date.now() + 10 * 60 * 1000) // 10 minutes
    });

    // Generate the OAuth URL
    // We need access to YouTube to download videos on behalf of the user
    const authUrl = oauth2Client.generateAuthUrl({
      access_type: 'offline', // Get refresh token
      prompt: 'consent', // Always show consent screen to get refresh token
      scope: [
        'https://www.googleapis.com/auth/youtube.readonly', // Read-only access to YouTube account
        'https://www.googleapis.com/auth/youtube.force-ssl'  // Force SSL for all requests
      ],
      state: state,
      include_granted_scopes: true
    });

    console.log(`[YouTubeOAuth] Generated auth URL for user ${uid}`);

    return {
      success: true,
      authUrl: authUrl,
      state: state
    };

  } catch (error) {
    console.error('[YouTubeOAuth] Error generating auth URL:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', 'Failed to generate YouTube authorization URL');
  }
});

/**
 * handleYouTubeOAuthCallback - Process the OAuth callback and store tokens
 */
exports.handleYouTubeOAuthCallback = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const { code, state, redirectUri } = data;

  if (!code || !state) {
    throw new functions.https.HttpsError('invalid-argument', 'Authorization code and state are required');
  }

  try {
    // Validate state parameter
    const stateDoc = await db.collection('youtubeOAuthStates').doc(state).get();

    if (!stateDoc.exists) {
      throw new functions.https.HttpsError('invalid-argument', 'Invalid or expired state parameter');
    }

    const stateData = stateDoc.data();

    // Verify state belongs to this user
    if (stateData.uid !== uid) {
      throw new functions.https.HttpsError('permission-denied', 'State parameter does not match user');
    }

    // Check if state has expired
    if (stateData.expiresAt && stateData.expiresAt.toDate() < new Date()) {
      throw new functions.https.HttpsError('invalid-argument', 'Authorization request has expired. Please try again.');
    }

    // Delete state to prevent reuse
    await db.collection('youtubeOAuthStates').doc(state).delete();

    // Exchange code for tokens
    const actualRedirectUri = redirectUri || 'https://ytseo.siteuo.com/video-wizard.html';
    const oauth2Client = getYouTubeOAuth2Client(actualRedirectUri);

    const { tokens } = await oauth2Client.getToken(code);

    if (!tokens.access_token) {
      throw new functions.https.HttpsError('internal', 'Failed to obtain access token from YouTube');
    }

    // Store tokens securely in Firestore (encrypted in production)
    const youtubeConnection = {
      accessToken: tokens.access_token,
      refreshToken: tokens.refresh_token || null,
      tokenType: tokens.token_type || 'Bearer',
      expiresAt: tokens.expiry_date ? new Date(tokens.expiry_date) : null,
      scope: tokens.scope || '',
      connectedAt: admin.firestore.FieldValue.serverTimestamp(),
      updatedAt: admin.firestore.FieldValue.serverTimestamp(),
      status: 'connected'
    };

    // Get YouTube channel info to display to user
    oauth2Client.setCredentials(tokens);
    const youtubeApi = google.youtube({ version: 'v3', auth: oauth2Client });

    try {
      const channelResponse = await youtubeApi.channels.list({
        part: ['snippet'],
        mine: true
      });

      if (channelResponse.data.items && channelResponse.data.items.length > 0) {
        const channel = channelResponse.data.items[0];
        youtubeConnection.channelId = channel.id;
        youtubeConnection.channelTitle = channel.snippet?.title || 'Unknown Channel';
        youtubeConnection.channelThumbnail = channel.snippet?.thumbnails?.default?.url || null;
      }
    } catch (channelError) {
      console.log('[YouTubeOAuth] Could not fetch channel info:', channelError.message);
      // Continue without channel info - tokens are still valid
    }

    // Store in user's document
    await db.collection('users').doc(uid).set({
      youtubeConnection: youtubeConnection
    }, { merge: true });

    console.log(`[YouTubeOAuth] Successfully connected YouTube for user ${uid}`);

    await logUsage(uid, 'youtube_oauth_connect', {
      channelId: youtubeConnection.channelId
    });

    return {
      success: true,
      message: 'YouTube account connected successfully',
      channel: {
        id: youtubeConnection.channelId || null,
        title: youtubeConnection.channelTitle || 'YouTube Account',
        thumbnail: youtubeConnection.channelThumbnail || null
      }
    };

  } catch (error) {
    console.error('[YouTubeOAuth] Callback error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', 'Failed to connect YouTube account. Please try again.');
  }
});

/**
 * getYouTubeConnectionStatus - Check if user has connected YouTube
 */
exports.getYouTubeConnectionStatus = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);

  try {
    const userDoc = await db.collection('users').doc(uid).get();

    if (!userDoc.exists) {
      return {
        connected: false,
        channel: null
      };
    }

    const userData = userDoc.data();
    const connection = userData.youtubeConnection;

    if (!connection || connection.status !== 'connected' || !connection.accessToken) {
      return {
        connected: false,
        channel: null
      };
    }

    // Check if token is expired
    const isExpired = connection.expiresAt &&
      connection.expiresAt.toDate &&
      connection.expiresAt.toDate() < new Date();

    // If we have a refresh token, we can refresh expired access tokens
    const canRefresh = !!connection.refreshToken;

    return {
      connected: true,
      needsRefresh: isExpired && canRefresh,
      expired: isExpired && !canRefresh,
      channel: {
        id: connection.channelId || null,
        title: connection.channelTitle || 'YouTube Account',
        thumbnail: connection.channelThumbnail || null
      },
      connectedAt: connection.connectedAt?.toDate?.()?.toISOString() || null
    };

  } catch (error) {
    console.error('[YouTubeOAuth] Status check error:', error);
    throw new functions.https.HttpsError('internal', 'Failed to check YouTube connection status');
  }
});

/**
 * disconnectYouTube - Remove YouTube connection
 */
exports.disconnectYouTube = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);

  try {
    const userDoc = await db.collection('users').doc(uid).get();

    if (userDoc.exists && userDoc.data().youtubeConnection) {
      // Optionally revoke token at Google
      const connection = userDoc.data().youtubeConnection;
      if (connection.accessToken) {
        try {
          await axios.post(`https://oauth2.googleapis.com/revoke?token=${connection.accessToken}`);
        } catch (revokeError) {
          console.log('[YouTubeOAuth] Token revoke note:', revokeError.message);
          // Continue even if revoke fails
        }
      }
    }

    // Remove connection from user document
    await db.collection('users').doc(uid).update({
      youtubeConnection: admin.firestore.FieldValue.delete()
    });

    console.log(`[YouTubeOAuth] Disconnected YouTube for user ${uid}`);

    await logUsage(uid, 'youtube_oauth_disconnect', {});

    return {
      success: true,
      message: 'YouTube account disconnected'
    };

  } catch (error) {
    console.error('[YouTubeOAuth] Disconnect error:', error);
    throw new functions.https.HttpsError('internal', 'Failed to disconnect YouTube account');
  }
});

/**
 * refreshYouTubeToken - Refresh expired YouTube access token
 * Called internally or when token is about to expire
 */
exports.refreshYouTubeToken = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);

  try {
    const userDoc = await db.collection('users').doc(uid).get();

    if (!userDoc.exists || !userDoc.data().youtubeConnection) {
      throw new functions.https.HttpsError('not-found', 'No YouTube connection found');
    }

    const connection = userDoc.data().youtubeConnection;

    if (!connection.refreshToken) {
      throw new functions.https.HttpsError(
        'failed-precondition',
        'No refresh token available. Please reconnect your YouTube account.'
      );
    }

    // Create OAuth client and refresh
    const oauth2Client = getYouTubeOAuth2Client('https://ytseo.siteuo.com/video-wizard.html');
    oauth2Client.setCredentials({
      refresh_token: connection.refreshToken
    });

    const { credentials } = await oauth2Client.refreshAccessToken();

    // Update stored tokens
    await db.collection('users').doc(uid).update({
      'youtubeConnection.accessToken': credentials.access_token,
      'youtubeConnection.expiresAt': credentials.expiry_date ? new Date(credentials.expiry_date) : null,
      'youtubeConnection.updatedAt': admin.firestore.FieldValue.serverTimestamp()
    });

    console.log(`[YouTubeOAuth] Refreshed token for user ${uid}`);

    return {
      success: true,
      message: 'YouTube token refreshed successfully'
    };

  } catch (error) {
    console.error('[YouTubeOAuth] Token refresh error:', error);

    // If refresh fails, mark connection as needing reconnection
    try {
      await db.collection('users').doc(uid).update({
        'youtubeConnection.status': 'expired',
        'youtubeConnection.updatedAt': admin.firestore.FieldValue.serverTimestamp()
      });
    } catch (updateError) {
      console.error('[YouTubeOAuth] Failed to update status:', updateError);
    }

    throw new functions.https.HttpsError(
      'unauthenticated',
      'Failed to refresh YouTube token. Please reconnect your account.'
    );
  }
});

/**
 * Internal helper to get valid YouTube credentials for a user
 * Used by video processing functions
 */
async function getYouTubeCredentialsForUser(uid) {
  const userDoc = await db.collection('users').doc(uid).get();

  if (!userDoc.exists || !userDoc.data().youtubeConnection) {
    return null;
  }

  const connection = userDoc.data().youtubeConnection;

  if (connection.status !== 'connected' || !connection.accessToken) {
    return null;
  }

  // Check if token needs refresh
  const needsRefresh = connection.expiresAt &&
    connection.expiresAt.toDate &&
    connection.expiresAt.toDate() < new Date(Date.now() + 5 * 60 * 1000); // 5 min buffer

  if (needsRefresh && connection.refreshToken) {
    try {
      const oauth2Client = getYouTubeOAuth2Client('https://ytseo.siteuo.com/video-wizard.html');
      oauth2Client.setCredentials({
        refresh_token: connection.refreshToken
      });

      const { credentials } = await oauth2Client.refreshAccessToken();

      // Update stored tokens
      await db.collection('users').doc(uid).update({
        'youtubeConnection.accessToken': credentials.access_token,
        'youtubeConnection.expiresAt': credentials.expiry_date ? new Date(credentials.expiry_date) : null,
        'youtubeConnection.updatedAt': admin.firestore.FieldValue.serverTimestamp()
      });

      return {
        accessToken: credentials.access_token,
        refreshToken: connection.refreshToken
      };
    } catch (refreshError) {
      console.error('[YouTubeOAuth] Auto-refresh failed:', refreshError);
      return null;
    }
  }

  return {
    accessToken: connection.accessToken,
    refreshToken: connection.refreshToken
  };
}

// ============================================
// YOUTUBE OAUTH CALLBACK PAGE
// ============================================

/**
 * HTTP endpoint that serves the OAuth callback page
 * This eliminates the need to upload a separate HTML file
 * URL: https://us-central1-ytseo-6d1b0.cloudfunctions.net/youtubeOAuthCallbackPage
 */
exports.youtubeOAuthCallbackPage = functions.https.onRequest((req, res) => {
  // Set CORS headers
  res.set('Access-Control-Allow-Origin', '*');

  const callbackHTML = `<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>YouTube Authorization</title>
    <style>
        * { box-sizing: border-box; margin: 0; padding: 0; }
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            background: linear-gradient(135deg, #0f0f1a 0%, #1a1a2e 50%, #16213e 100%);
            min-height: 100vh;
            display: flex;
            align-items: center;
            justify-content: center;
            color: white;
        }
        .container { text-align: center; padding: 2rem; max-width: 400px; }
        .icon {
            width: 80px; height: 80px; margin: 0 auto 1.5rem;
            background: rgba(255, 0, 0, 0.1); border-radius: 50%;
            display: flex; align-items: center; justify-content: center;
        }
        .icon svg { width: 40px; height: 40px; }
        h1 { font-size: 1.5rem; margin-bottom: 0.5rem; }
        p { color: rgba(255, 255, 255, 0.6); margin-bottom: 1.5rem; }
        .spinner {
            width: 40px; height: 40px;
            border: 3px solid rgba(255, 255, 255, 0.1);
            border-top-color: #ff0000; border-radius: 50%;
            animation: spin 1s linear infinite; margin: 0 auto;
        }
        @keyframes spin { to { transform: rotate(360deg); } }
        .success { color: #10b981; }
        .error { color: #ef4444; }
        .status-icon { font-size: 3rem; margin-bottom: 1rem; }
    </style>
</head>
<body>
    <div class="container" id="content">
        <div class="icon">
            <svg viewBox="0 0 24 24" fill="#ff0000">
                <path d="M23.498 6.186a3.016 3.016 0 0 0-2.122-2.136C19.505 3.545 12 3.545 12 3.545s-7.505 0-9.377.505A3.017 3.017 0 0 0 .502 6.186C0 8.07 0 12 0 12s0 3.93.502 5.814a3.016 3.016 0 0 0 2.122 2.136c1.871.505 9.376.505 9.376.505s7.505 0 9.377-.505a3.015 3.015 0 0 0 2.122-2.136C24 15.93 24 12 24 12s0-3.93-.502-5.814zM9.545 15.568V8.432L15.818 12l-6.273 3.568z"/>
            </svg>
        </div>
        <h1>Connecting YouTube...</h1>
        <p>Please wait while we complete the authorization.</p>
        <div class="spinner"></div>
    </div>
    <script>
        (function() {
            var params = new URLSearchParams(window.location.search);
            var code = params.get('code');
            var state = params.get('state');
            var error = params.get('error');
            var container = document.getElementById('content');

            function showSuccess() {
                container.innerHTML = '<div class="status-icon success">âœ“</div>' +
                    '<h1 class="success">Connected!</h1>' +
                    '<p>YouTube account connected. This window will close automatically.</p>';
            }
            function showError(msg) {
                container.innerHTML = '<div class="status-icon error">âœ—</div>' +
                    '<h1 class="error">Connection Failed</h1>' +
                    '<p>' + (msg || 'Authorization failed.') + '</p>' +
                    '<p style="margin-top:1rem">You can close this window and try again.</p>';
            }

            if (error) { showError(error === 'access_denied' ? 'Authorization was cancelled.' : error); return; }
            if (!code || !state) { showError('Missing authorization parameters.'); return; }

            // Check if opener exists
            if (window.opener && !window.opener.closed) {
                // Always use postMessage first - it works cross-origin
                // The try/catch for direct function call would fail on cross-origin
                try {
                    // Send via postMessage (cross-origin compatible)
                    window.opener.postMessage({
                        type: 'youtube-oauth-callback',
                        code: code,
                        state: state
                    }, '*');

                    showSuccess();
                    setTimeout(function() { window.close(); }, 2000);
                } catch (e) {
                    console.error('postMessage failed:', e);
                    // Fallback to localStorage
                    try {
                        localStorage.setItem('youtube_oauth_pending', JSON.stringify({ code: code, state: state, timestamp: Date.now() }));
                        showSuccess();
                        container.innerHTML += '<p style="margin-top:1rem;font-size:0.85rem">Return to Video Wizard to complete connection.</p>';
                    } catch (e2) {
                        showError('Could not communicate with main window.');
                    }
                }
            } else {
                // No opener - store in localStorage for main app to pick up
                try {
                    localStorage.setItem('youtube_oauth_pending', JSON.stringify({ code: code, state: state, timestamp: Date.now() }));
                    showSuccess();
                    container.innerHTML += '<p style="margin-top:1rem;font-size:0.85rem">Return to Video Wizard to complete connection.</p>';
                } catch (e) { showError('Could not save authorization.'); }
            }
        })();
    </script>
</body>
</html>`;

  res.status(200).send(callbackHTML);
});

// ==============================================
// WIZARD: AI-POWERED METADATA EXTRACTION
// Extract title, description, tags from uploaded video
// Uses Whisper for transcription + GPT-4 for metadata generation
// ==============================================

exports.wizardExtractVideoMetadata = functions
  .runWith({
    timeoutSeconds: 300,  // 5 minutes for transcription
    memory: '1GB'         // Need memory for video processing
  })
  .https.onCall(async (data, context) => {
    const uid = await verifyAuth(context);

    const { videoUrl, fileName } = data;

    if (!videoUrl) {
      throw new functions.https.HttpsError('invalid-argument', 'Video URL is required');
    }

    console.log(`[MetadataExtract] Starting for user ${uid}, file: ${fileName || 'unknown'}`);

    try {
      // Step 1: Download video to temporary file
      console.log(`[MetadataExtract] Downloading video from: ${videoUrl.substring(0, 100)}...`);

      const fetch = (await import('node-fetch')).default;
      const fs = require('fs');
      const os = require('os');
      const path = require('path');

      const tempDir = os.tmpdir();
      const tempVideoFile = path.join(tempDir, `metadata_${Date.now()}.mp4`);

      const response = await fetch(videoUrl);
      if (!response.ok) {
        throw new Error(`Failed to download video: ${response.status}`);
      }

      const buffer = await response.buffer();
      fs.writeFileSync(tempVideoFile, buffer);

      const fileSizeMB = (buffer.length / (1024 * 1024)).toFixed(2);
      console.log(`[MetadataExtract] Downloaded ${fileSizeMB} MB`);

      // Check file size limit for Whisper (25MB for audio, but video files are larger)
      // Whisper API has a 25MB limit - we'll need to extract just audio for larger files
      // For now, let's try with the video file directly (Whisper extracts audio)

      let transcription = '';

      try {
        // Step 2: Transcribe with Whisper
        console.log(`[MetadataExtract] Transcribing with Whisper...`);

        const whisperResponse = await openai.audio.transcriptions.create({
          file: fs.createReadStream(tempVideoFile),
          model: 'whisper-1',
          response_format: 'text',
          language: 'en'  // Can be auto-detected by removing this
        });

        transcription = whisperResponse;
        console.log(`[MetadataExtract] Transcription complete: ${transcription.length} chars`);

      } catch (whisperError) {
        console.error(`[MetadataExtract] Whisper error:`, whisperError.message);

        // If file too large, try with a shorter segment
        if (whisperError.message.includes('too large') || whisperError.message.includes('25 MB')) {
          console.log(`[MetadataExtract] File too large for Whisper, skipping transcription`);
          transcription = '(Video too large for automatic transcription)';
        } else {
          throw whisperError;
        }
      } finally {
        // Clean up temp file
        try {
          fs.unlinkSync(tempVideoFile);
        } catch (e) {
          console.log(`[MetadataExtract] Could not delete temp file:`, e.message);
        }
      }

      // Step 3: Generate metadata with GPT-4
      console.log(`[MetadataExtract] Generating metadata with GPT-4...`);

      const gptPrompt = `You are an expert video content analyst. Based on the following transcript from a video, generate SEO-optimized metadata.

TRANSCRIPT:
${transcription.substring(0, 4000)}${transcription.length > 4000 ? '...(truncated)' : ''}

FILE NAME (may contain hints): ${fileName || 'unknown'}

Generate the following in JSON format:
{
  "title": "A compelling, SEO-friendly title (max 60 chars)",
  "description": "A detailed description summarizing the content (100-200 words). Include key points and make it engaging for viewers.",
  "tags": ["array", "of", "relevant", "tags", "for", "discovery"],
  "category": "Best fitting category (e.g., Education, Entertainment, Gaming, How-to, Vlog, etc.)",
  "keyTopics": ["main", "topics", "discussed"],
  "suggestedHashtags": ["#relevant", "#hashtags"]
}

Respond ONLY with valid JSON, no markdown or explanation.`;

      const gptResponse = await openai.chat.completions.create({
        model: 'gpt-4o-mini',  // Cost-efficient for this task
        messages: [
          {
            role: 'system',
            content: 'You are a video content analyst that generates SEO metadata. Always respond with valid JSON only.'
          },
          { role: 'user', content: gptPrompt }
        ],
        temperature: 0.7,
        max_tokens: 1000
      });

      const gptContent = gptResponse.choices[0]?.message?.content || '{}';
      console.log(`[MetadataExtract] GPT response: ${gptContent.substring(0, 200)}...`);

      // Parse the JSON response
      let metadata;
      try {
        // Clean up potential markdown formatting
        let cleanJson = gptContent.trim();
        if (cleanJson.startsWith('```json')) {
          cleanJson = cleanJson.replace(/^```json\n?/, '').replace(/\n?```$/, '');
        } else if (cleanJson.startsWith('```')) {
          cleanJson = cleanJson.replace(/^```\n?/, '').replace(/\n?```$/, '');
        }

        metadata = JSON.parse(cleanJson);
      } catch (parseError) {
        console.error(`[MetadataExtract] Failed to parse GPT response:`, parseError.message);
        metadata = {
          title: fileName?.replace(/\.[^/.]+$/, '') || 'Untitled Video',
          description: transcription.substring(0, 500) || 'No description available',
          tags: [],
          category: 'Other',
          keyTopics: [],
          suggestedHashtags: []
        };
      }

      // Add transcription to metadata for reference
      metadata.transcription = transcription.substring(0, 2000);
      metadata.hasFullTranscription = transcription.length > 0 && !transcription.includes('too large');

      console.log(`[MetadataExtract] Success! Title: "${metadata.title}"`);

      return {
        success: true,
        metadata: metadata
      };

    } catch (error) {
      console.error(`[MetadataExtract] Error:`, error.message);
      throw new functions.https.HttpsError('internal', `Failed to extract metadata: ${error.message}`);
    }
  });

// ==============================================
// EXTENSION: Video Upload Endpoint
// Receives captured video from browser extension and stores in Firebase Storage
// This is a fallback when Cloud Run video-processor is not available
// ==============================================

const Busboy = require('busboy');

exports.extensionUploadVideo = functions
  .runWith({
    timeoutSeconds: 300,
    memory: '1GB'
  })
  .https.onRequest(async (req, res) => {
    // Set CORS headers
    res.set('Access-Control-Allow-Origin', '*');
    res.set('Access-Control-Allow-Methods', 'POST, OPTIONS');
    res.set('Access-Control-Allow-Headers', 'Content-Type');

    if (req.method === 'OPTIONS') {
      return res.status(204).send('');
    }

    if (req.method !== 'POST') {
      return res.status(405).json({ error: 'Method not allowed' });
    }

    console.log('[ExtensionUpload] Received upload request');

    try {
      const busboy = Busboy({ headers: req.headers });

      let videoBuffer = null;
      let videoId = null;
      let fileType = 'video';
      let mimeType = 'video/webm';
      let captureStart = 0;
      let captureEnd = 0;

      const filePromise = new Promise((resolve, reject) => {
        busboy.on('field', (name, val) => {
          if (name === 'videoId') videoId = val;
          if (name === 'type') fileType = val;
          if (name === 'captureStart') captureStart = parseFloat(val) || 0;
          if (name === 'captureEnd') captureEnd = parseFloat(val) || 0;
        });

        busboy.on('file', (name, file, info) => {
          mimeType = info.mimeType || 'video/webm';
          const chunks = [];
          file.on('data', (data) => chunks.push(data));
          file.on('end', () => {
            videoBuffer = Buffer.concat(chunks);
            console.log(`[ExtensionUpload] File received: ${(videoBuffer.length / 1024 / 1024).toFixed(2)}MB`);
          });
        });

        busboy.on('finish', () => resolve());
        busboy.on('error', reject);
      });

      if (req.rawBody) {
        busboy.end(req.rawBody);
      } else {
        req.pipe(busboy);
      }

      await filePromise;

      if (!videoBuffer) {
        return res.status(400).json({ error: 'No video file provided' });
      }

      if (!videoId) {
        return res.status(400).json({ error: 'videoId is required' });
      }

      const timestamp = Date.now();
      const extension = mimeType.includes('webm') ? 'webm' : 'mp4';
      const fileName = `extension-uploads/${videoId}/${fileType}_${timestamp}.${extension}`;

      console.log(`[ExtensionUpload] Uploading to storage: ${fileName}`);

      const bucket = admin.storage().bucket();
      const file = bucket.file(fileName);

      await file.save(videoBuffer, {
        metadata: {
          contentType: mimeType,
          metadata: {
            videoId: videoId,
            type: fileType,
            captureStart: String(captureStart),
            captureEnd: String(captureEnd),
            uploadedAt: new Date().toISOString(),
            source: 'browser-extension'
          }
        }
      });

      await file.makePublic();

      const publicUrl = `https://storage.googleapis.com/${bucket.name}/${fileName}`;
      console.log(`[ExtensionUpload] Upload successful: ${publicUrl}`);

      return res.status(200).json({
        success: true,
        url: publicUrl,
        videoId: videoId,
        type: fileType,
        size: videoBuffer.length
      });

    } catch (error) {
      console.error('[ExtensionUpload] Error:', error);
      return res.status(500).json({ error: error.message || 'Upload failed' });
    }
  });

// ============================================================================
// VIDEO WIZARD ADMIN MANAGEMENT FUNCTIONS
// ============================================================================

/**
 * adminGetWizardStorageStats - Get storage usage statistics
 * Returns total storage used, video count, and breakdown by folder
 */
exports.adminGetWizardStorageStats = functions.https.onCall(async (data, context) => {
  await requireAdmin(context);

  try {
    const bucket = admin.storage().bucket();

    // Get all files in storage
    const [files] = await bucket.getFiles();

    let totalSize = 0;
    let totalFiles = 0;
    const folderStats = {};

    for (const file of files) {
      const [metadata] = await file.getMetadata();
      const size = parseInt(metadata.size || 0);
      totalSize += size;
      totalFiles++;

      // Group by top-level folder
      const folder = file.name.split('/')[0] || 'root';
      if (!folderStats[folder]) {
        folderStats[folder] = { files: 0, size: 0 };
      }
      folderStats[folder].files++;
      folderStats[folder].size += size;
    }

    // Get project count
    const projectsSnapshot = await db.collection('wizardProjects').get();
    const totalProjects = projectsSnapshot.size;

    // Get processing jobs count
    const activeJobsSnapshot = await db.collection('processingJobs')
      .where('status', 'in', ['pending', 'processing'])
      .get();
    const activeJobs = activeJobsSnapshot.size;

    // Get failed jobs count
    const failedJobsSnapshot = await db.collection('processingJobs')
      .where('status', '==', 'failed')
      .limit(100)
      .get();
    const failedJobs = failedJobsSnapshot.size;

    // Get config
    const configDoc = await db.collection('settings').doc('wizardConfig').get();
    const config = configDoc.exists ? configDoc.data() : {};

    return {
      success: true,
      stats: {
        totalSize,
        totalSizeGB: (totalSize / (1024 * 1024 * 1024)).toFixed(2),
        totalFiles,
        totalProjects,
        activeJobs,
        failedJobs,
        folderStats
      },
      config: {
        maxProjectsPerUser: config.maxProjectsPerUser || 8,
        retentionDays: config.retentionDays || 14,
        autoCleanupEnabled: config.autoCleanupEnabled || false
      }
    };
  } catch (error) {
    console.error('[adminGetWizardStorageStats] Error:', error);
    throw new functions.https.HttpsError('internal', error.message);
  }
});

/**
 * adminGetWizardVideos - List all videos/projects with user info
 */
exports.adminGetWizardVideos = functions.https.onCall(async (data, context) => {
  await requireAdmin(context);

  const { limit: queryLimit = 50, startAfter, filterByUser, olderThanDays } = data || {};

  try {
    let query = db.collection('wizardProjects')
      .orderBy('createdAt', 'desc');

    if (filterByUser) {
      query = query.where('userId', '==', filterByUser);
    }

    if (startAfter) {
      const startDoc = await db.collection('wizardProjects').doc(startAfter).get();
      if (startDoc.exists) {
        query = query.startAfter(startDoc);
      }
    }

    query = query.limit(queryLimit);
    const snapshot = await query.get();

    const videos = [];
    const userIds = new Set();

    snapshot.forEach(doc => {
      const data = doc.data();
      userIds.add(data.userId);

      const createdAt = data.createdAt?.toDate?.() || new Date();
      const ageInDays = Math.floor((Date.now() - createdAt.getTime()) / (1000 * 60 * 60 * 24));

      // Filter by age if specified
      if (olderThanDays && ageInDays < olderThanDays) {
        return;
      }

      videos.push({
        id: doc.id,
        userId: data.userId,
        videoId: data.videoId,
        title: data.videoData?.title || data.uploadedVideoName || 'Unknown',
        clipCount: data.clips?.length || 0,
        isUpload: data.isUpload || false,
        hasSourceAsset: !!data.sourceAsset?.storageUrl,
        createdAt: createdAt.toISOString(),
        ageInDays,
        status: data.status
      });
    });

    // Get user emails for display
    const userEmails = {};
    for (const userId of userIds) {
      try {
        const userRecord = await admin.auth().getUser(userId);
        userEmails[userId] = userRecord.email || 'Unknown';
      } catch (e) {
        userEmails[userId] = 'Deleted User';
      }
    }

    // Add emails to videos
    videos.forEach(v => {
      v.userEmail = userEmails[v.userId] || 'Unknown';
    });

    return {
      success: true,
      videos,
      hasMore: snapshot.size === queryLimit,
      lastId: videos.length > 0 ? videos[videos.length - 1].id : null
    };
  } catch (error) {
    console.error('[adminGetWizardVideos] Error:', error);
    throw new functions.https.HttpsError('internal', error.message);
  }
});

/**
 * adminGetTopStorageUsers - Get users consuming most storage
 */
exports.adminGetTopStorageUsers = functions.https.onCall(async (data, context) => {
  await requireAdmin(context);

  try {
    const projectsSnapshot = await db.collection('wizardProjects').get();

    const userStats = {};

    projectsSnapshot.forEach(doc => {
      const data = doc.data();
      const userId = data.userId;

      if (!userStats[userId]) {
        userStats[userId] = { projectCount: 0, clipCount: 0 };
      }

      userStats[userId].projectCount++;
      userStats[userId].clipCount += data.clips?.length || 0;
    });

    // Get user emails and sort by project count
    const userList = [];
    for (const [userId, stats] of Object.entries(userStats)) {
      try {
        const userRecord = await admin.auth().getUser(userId);
        userList.push({
          userId,
          email: userRecord.email || 'Unknown',
          ...stats
        });
      } catch (e) {
        userList.push({
          userId,
          email: 'Deleted User',
          ...stats
        });
      }
    }

    userList.sort((a, b) => b.projectCount - a.projectCount);

    return {
      success: true,
      users: userList.slice(0, 20) // Top 20 users
    };
  } catch (error) {
    console.error('[adminGetTopStorageUsers] Error:', error);
    throw new functions.https.HttpsError('internal', error.message);
  }
});

/**
 * adminDeleteWizardProject - Delete a specific project and its storage
 */
exports.adminDeleteWizardProject = functions.https.onCall(async (data, context) => {
  await requireAdmin(context);

  const { projectId } = data;
  if (!projectId) {
    throw new functions.https.HttpsError('invalid-argument', 'Project ID required');
  }

  try {
    const projectDoc = await db.collection('wizardProjects').doc(projectId).get();
    if (!projectDoc.exists) {
      throw new functions.https.HttpsError('not-found', 'Project not found');
    }

    const projectData = projectDoc.data();
    // Use the default bucket (most reliable) - Firebase admin SDK knows the correct bucket
    const bucket = admin.storage().bucket();
    const STORAGE_BUCKET = bucket.name;

    console.log(`[adminDeleteWizardProject] Using bucket: ${STORAGE_BUCKET}, videoId: ${projectData.videoId}`);

    // Delete associated storage files
    const deletedFiles = [];

    // Delete sourceAsset
    if (projectData.sourceAsset?.storagePath) {
      try {
        await bucket.file(projectData.sourceAsset.storagePath).delete();
        deletedFiles.push(projectData.sourceAsset.storagePath);
      } catch (e) { /* ignore */ }
    }

    // Delete uploaded video
    if (projectData.uploadedVideoPath) {
      try {
        await bucket.file(projectData.uploadedVideoPath).delete();
        deletedFiles.push(projectData.uploadedVideoPath);
      } catch (e) { /* ignore */ }
    }

    // Delete extension uploads for this video
    if (projectData.videoId) {
      const prefix = `extension-uploads/${projectData.videoId}/`;
      const [files] = await bucket.getFiles({ prefix });
      for (const file of files) {
        await file.delete().catch(() => {});
        deletedFiles.push(file.name);
      }
    }

    // Delete project document
    await db.collection('wizardProjects').doc(projectId).delete();

    // Delete related processing jobs
    const jobsSnapshot = await db.collection('processingJobs')
      .where('projectId', '==', projectId)
      .get();

    const batch = db.batch();
    jobsSnapshot.forEach(doc => batch.delete(doc.ref));
    await batch.commit();

    return {
      success: true,
      deletedProject: projectId,
      deletedFiles: deletedFiles.length,
      deletedJobs: jobsSnapshot.size
    };
  } catch (error) {
    console.error('[adminDeleteWizardProject] Error:', error);
    throw new functions.https.HttpsError('internal', error.message);
  }
});

/**
 * adminBulkDeleteWizardProjects - Delete multiple projects or all old projects
 */
exports.adminBulkDeleteWizardProjects = functions.https.onCall(async (data, context) => {
  await requireAdmin(context);

  const { projectIds, olderThanDays, userId, deleteAll, cleanAllStorage } = data;

  try {
    let query = db.collection('wizardProjects');
    let targetDocs = [];

    if (projectIds && projectIds.length > 0) {
      // Delete specific projects
      for (const id of projectIds) {
        const doc = await db.collection('wizardProjects').doc(id).get();
        if (doc.exists) targetDocs.push(doc);
      }
    } else if (olderThanDays) {
      // Delete projects older than X days
      const cutoffDate = new Date();
      cutoffDate.setDate(cutoffDate.getDate() - olderThanDays);

      const snapshot = await query
        .where('createdAt', '<', cutoffDate)
        .get();
      targetDocs = snapshot.docs;
    } else if (userId) {
      // Delete all projects for a specific user
      const snapshot = await query
        .where('userId', '==', userId)
        .get();
      targetDocs = snapshot.docs;
    } else if (deleteAll) {
      // Delete ALL projects (dangerous!)
      const snapshot = await query.get();
      targetDocs = snapshot.docs;
    }

    // CRITICAL: Try multiple bucket name formats - Firebase can use different formats
    const BUCKET_FORMATS = [
      'ytseo-6d1b0.firebasestorage.app',  // New format
      'ytseo-6d1b0.appspot.com',           // Old format
    ];

    let bucket;
    let STORAGE_BUCKET;

    // First, try the default bucket (most reliable)
    console.log('[adminBulkDeleteWizardProjects] Detecting correct bucket...');
    const defaultBucket = admin.storage().bucket();
    const defaultBucketName = defaultBucket.name;
    console.log(`[adminBulkDeleteWizardProjects] Default bucket name: ${defaultBucketName}`);

    // Test if default bucket has files
    try {
      const [testFiles] = await defaultBucket.getFiles({ prefix: 'processed-clips/', maxResults: 5 });
      if (testFiles.length > 0) {
        bucket = defaultBucket;
        STORAGE_BUCKET = defaultBucketName;
        console.log(`[adminBulkDeleteWizardProjects] Using default bucket: ${STORAGE_BUCKET} (found ${testFiles.length} files)`);
      }
    } catch (e) {
      console.log(`[adminBulkDeleteWizardProjects] Default bucket test failed: ${e.message}`);
    }

    // If default bucket didn't have files, try explicit bucket names
    if (!bucket) {
      for (const bucketName of BUCKET_FORMATS) {
        try {
          const testBucket = admin.storage().bucket(bucketName);
          const [testFiles] = await testBucket.getFiles({ prefix: 'processed-clips/', maxResults: 5 });
          if (testFiles.length > 0) {
            bucket = testBucket;
            STORAGE_BUCKET = bucketName;
            console.log(`[adminBulkDeleteWizardProjects] Using bucket: ${STORAGE_BUCKET} (found ${testFiles.length} files)`);
            break;
          }
        } catch (e) {
          console.log(`[adminBulkDeleteWizardProjects] ${bucketName} failed: ${e.message}`);
        }
      }
    }

    // If still no bucket, use default
    if (!bucket) {
      bucket = defaultBucket;
      STORAGE_BUCKET = defaultBucketName;
      console.log(`[adminBulkDeleteWizardProjects] No files found, using default bucket: ${STORAGE_BUCKET}`);
    }

    let deletedCount = 0;
    let deletedFilesCount = 0;

    console.log(`[adminBulkDeleteWizardProjects] Using bucket: ${STORAGE_BUCKET}`);

    // Delete project-associated files
    for (const doc of targetDocs) {
      const projectData = doc.data();

      // Delete storage files
      if (projectData.sourceAsset?.storagePath) {
        await bucket.file(projectData.sourceAsset.storagePath).delete().catch(() => {});
        deletedFilesCount++;
      }
      if (projectData.uploadedVideoPath) {
        await bucket.file(projectData.uploadedVideoPath).delete().catch(() => {});
        deletedFilesCount++;
      }
      if (projectData.videoId) {
        const prefix = `extension-uploads/${projectData.videoId}/`;
        const [files] = await bucket.getFiles({ prefix });
        for (const file of files) {
          await file.delete().catch(() => {});
          deletedFilesCount++;
        }
      }

      // Delete project
      await db.collection('wizardProjects').doc(doc.id).delete();
      deletedCount++;
    }

    // If deleteAll or cleanAllStorage flag is set, also delete ALL files in storage
    // This catches orphaned files not referenced by any project document
    if (deleteAll || cleanAllStorage) {
      console.log('[adminBulkDeleteWizardProjects] Cleaning ALL storage files...');

      // All storage folders that the Video Wizard uses
      const storageFolders = [
        'processed-clips/',     // Exported processed clips (video-processor)
        'extension-uploads/',   // Extension video/audio captures
        'uploads/',             // Frontend: file uploads, export captures, parallel exports
        'video-cache/',         // FULL SOURCE VIDEOS cached by video-processor (HUGE!)
        'thumbnails-pro/',      // Pro thumbnail generator (Gemini, DALL-E, Imagen)
        'wizard-thumbnails/',   // Wizard clip AI thumbnails
        'wizard-videos/',       // Legacy - may not exist
        'video-uploads/',       // Legacy - may not exist
      ];

      for (const folder of storageFolders) {
        const [files] = await bucket.getFiles({ prefix: folder });
        for (const file of files) {
          await file.delete().catch((e) => console.warn(`Failed to delete ${file.name}:`, e.message));
          deletedFilesCount++;
        }
        if (files.length > 0) {
          console.log(`[adminBulkDeleteWizardProjects] Deleted ${files.length} files from ${folder}`);
        }
      }
    }

    console.log(`[adminBulkDeleteWizardProjects] Total: Deleted ${deletedCount} projects, ${deletedFilesCount} files`);

    return {
      success: true,
      deleted: deletedCount,
      deletedFiles: deletedFilesCount
    };
  } catch (error) {
    console.error('[adminBulkDeleteWizardProjects] Error:', error);
    throw new functions.https.HttpsError('internal', error.message);
  }
});

/**
 * adminCleanWizardStorage - Clean ALL Video Wizard storage files (orphan cleanup)
 * This deletes ALL files in wizard-related storage folders regardless of project references
 */
exports.adminCleanWizardStorage = functions.https.onCall(async (data, context) => {
  await requireAdmin(context);

  const { dryRun = false } = data;

  try {
    // Try multiple bucket name formats - Firebase can use different formats
    const BUCKET_FORMATS = [
      'ytseo-6d1b0.firebasestorage.app',  // New format
      'ytseo-6d1b0.appspot.com',           // Old format
    ];

    let bucket;
    let STORAGE_BUCKET;
    let foundFiles = false;

    // First, try the default bucket (most reliable)
    console.log('[adminCleanWizardStorage] Trying default bucket first...');
    const defaultBucket = admin.storage().bucket();
    const defaultBucketName = defaultBucket.name;
    console.log(`[adminCleanWizardStorage] Default bucket name: ${defaultBucketName}`);

    try {
      const [testFiles] = await defaultBucket.getFiles({ prefix: 'processed-clips/', maxResults: 5 });
      console.log(`[adminCleanWizardStorage] Default bucket: found ${testFiles.length} files in processed-clips/`);
      if (testFiles.length > 0) {
        bucket = defaultBucket;
        STORAGE_BUCKET = defaultBucketName;
        foundFiles = true;
        console.log(`[adminCleanWizardStorage] Using default bucket: ${STORAGE_BUCKET}`);
      }
    } catch (e) {
      console.log(`[adminCleanWizardStorage] Default bucket test failed: ${e.message}`);
    }

    // If default bucket didn't have files, try explicit bucket names
    if (!foundFiles) {
      for (const bucketName of BUCKET_FORMATS) {
        console.log(`[adminCleanWizardStorage] Trying bucket: ${bucketName}`);
        try {
          const testBucket = admin.storage().bucket(bucketName);
          const [testFiles] = await testBucket.getFiles({ prefix: 'processed-clips/', maxResults: 5 });
          console.log(`[adminCleanWizardStorage] ${bucketName}: found ${testFiles.length} files in processed-clips/`);
          if (testFiles.length > 0) {
            bucket = testBucket;
            STORAGE_BUCKET = bucketName;
            foundFiles = true;
            console.log(`[adminCleanWizardStorage] Using bucket: ${STORAGE_BUCKET}`);
            break;
          }
        } catch (e) {
          console.log(`[adminCleanWizardStorage] ${bucketName} failed: ${e.message}`);
        }
      }
    }

    // If still no files found, use default bucket anyway and report what we find
    if (!bucket) {
      bucket = defaultBucket;
      STORAGE_BUCKET = defaultBucketName;
      console.log(`[adminCleanWizardStorage] No files found in any bucket, using default: ${STORAGE_BUCKET}`);
    }

    console.log(`[adminCleanWizardStorage] Starting cleanup, bucket: ${STORAGE_BUCKET}, dryRun: ${dryRun}`);

    // Check all possible storage locations used by Video Wizard
    // IMPORTANT: These must match the ACTUAL folder names in Firebase Storage
    const storagePrefixes = [
      'processed-clips/',     // Exported processed clips (video-processor)
      'extension-uploads/',   // Extension video/audio captures
      'uploads/',             // Frontend: file uploads, export captures, parallel exports (HIGH IMPACT!)
      'video-cache/',         // FULL SOURCE VIDEOS cached by video-processor (HUGE!)
      'thumbnails-pro/',      // Pro thumbnail generator (Gemini, DALL-E, Imagen)
      'wizard-thumbnails/',   // Wizard clip AI thumbnails
      'wizard-videos/',       // Legacy - may not exist
      'video-uploads/',       // Legacy - may not exist
      'temp/'                 // Temporary files
    ];

    let totalFiles = 0;
    let totalSize = 0;
    const results = {};

    for (const prefix of storagePrefixes) {
      console.log(`[adminCleanWizardStorage] Scanning: ${prefix}`);
      const [files] = await bucket.getFiles({ prefix });
      let folderSize = 0;

      for (const file of files) {
        try {
          const [metadata] = await file.getMetadata();
          folderSize += parseInt(metadata.size || 0);
        } catch (e) {
          // Ignore metadata errors
        }

        if (!dryRun) {
          await file.delete().catch((e) => console.warn(`Failed to delete ${file.name}:`, e.message));
        }
      }

      results[prefix] = {
        fileCount: files.length,
        sizeBytes: folderSize,
        sizeMB: (folderSize / (1024 * 1024)).toFixed(2)
      };

      if (files.length > 0) {
        console.log(`[adminCleanWizardStorage] ${prefix}: ${files.length} files, ${(folderSize / (1024 * 1024)).toFixed(2)} MB`);
      }

      totalFiles += files.length;
      totalSize += folderSize;
    }

    // Also try to list ALL files in the bucket to find any unexpected locations
    console.log(`[adminCleanWizardStorage] Scanning entire bucket for any remaining files...`);
    const [allFiles] = await bucket.getFiles({ maxResults: 1000 });

    // Find files that weren't in our prefixes
    const knownPrefixes = storagePrefixes;
    const otherFiles = allFiles.filter(f => !knownPrefixes.some(p => f.name.startsWith(p)));

    if (otherFiles.length > 0) {
      let otherSize = 0;
      const otherPaths = new Set();

      for (const file of otherFiles) {
        const folder = file.name.split('/')[0] + '/';
        otherPaths.add(folder);

        try {
          const [metadata] = await file.getMetadata();
          otherSize += parseInt(metadata.size || 0);
        } catch (e) {}

        if (!dryRun) {
          await file.delete().catch((e) => console.warn(`Failed to delete ${file.name}:`, e.message));
        }
      }

      results['OTHER (unexpected)'] = {
        fileCount: otherFiles.length,
        sizeBytes: otherSize,
        sizeMB: (otherSize / (1024 * 1024)).toFixed(2),
        paths: Array.from(otherPaths)
      };

      console.log(`[adminCleanWizardStorage] Found ${otherFiles.length} files in unexpected locations: ${Array.from(otherPaths).join(', ')}`);

      totalFiles += otherFiles.length;
      totalSize += otherSize;
    }

    console.log(`[adminCleanWizardStorage] ${dryRun ? 'DRY RUN - ' : ''}Total: ${totalFiles} files, ${(totalSize / (1024 * 1024)).toFixed(2)} MB`);

    // Also clean up the videoSourceCache Firestore collection (references to video-cache/ files)
    let cacheDocsDeleted = 0;
    try {
      const cacheSnapshot = await db.collection('videoSourceCache').get();
      if (!cacheSnapshot.empty) {
        console.log(`[adminCleanWizardStorage] Found ${cacheSnapshot.size} videoSourceCache documents`);
        if (!dryRun) {
          const batch = db.batch();
          cacheSnapshot.docs.forEach(doc => batch.delete(doc.ref));
          await batch.commit();
          cacheDocsDeleted = cacheSnapshot.size;
          console.log(`[adminCleanWizardStorage] Deleted ${cacheDocsDeleted} videoSourceCache documents`);
        } else {
          cacheDocsDeleted = cacheSnapshot.size;
        }
      }
    } catch (cacheError) {
      console.log('[adminCleanWizardStorage] videoSourceCache cleanup note:', cacheError.message);
    }

    return {
      success: true,
      dryRun,
      bucket: STORAGE_BUCKET,
      totalFiles,
      totalSizeBytes: totalSize,
      totalSizeMB: (totalSize / (1024 * 1024)).toFixed(2),
      totalSizeGB: (totalSize / (1024 * 1024 * 1024)).toFixed(3),
      breakdown: results,
      cacheDocsDeleted,
      message: dryRun
        ? `Found ${totalFiles} files (${(totalSize / (1024 * 1024)).toFixed(2)} MB) that would be deleted`
        : `Deleted ${totalFiles} files (${(totalSize / (1024 * 1024)).toFixed(2)} MB)`
    };
  } catch (error) {
    console.error('[adminCleanWizardStorage] Error:', error);
    throw new functions.https.HttpsError('internal', error.message);
  }
});

/**
 * adminSetWizardConfig - Set Video Wizard configuration
 */
exports.adminSetWizardConfig = functions.https.onCall(async (data, context) => {
  await requireAdmin(context);

  const { maxProjectsPerUser, retentionDays, autoCleanupEnabled } = data;

  try {
    const updateData = {
      updatedAt: admin.firestore.FieldValue.serverTimestamp(),
      updatedBy: context.auth.uid
    };

    if (maxProjectsPerUser !== undefined) {
      updateData.maxProjectsPerUser = Math.max(1, Math.min(50, parseInt(maxProjectsPerUser)));
    }
    if (retentionDays !== undefined) {
      updateData.retentionDays = Math.max(1, Math.min(365, parseInt(retentionDays)));
    }
    if (autoCleanupEnabled !== undefined) {
      updateData.autoCleanupEnabled = !!autoCleanupEnabled;
    }

    await db.collection('settings').doc('wizardConfig').set(updateData, { merge: true });

    return {
      success: true,
      config: updateData
    };
  } catch (error) {
    console.error('[adminSetWizardConfig] Error:', error);
    throw new functions.https.HttpsError('internal', error.message);
  }
});

/**
 * adminGetProcessingJobs - Get processing job status
 */
exports.adminGetProcessingJobs = functions.https.onCall(async (data, context) => {
  await requireAdmin(context);

  const { status, limit: queryLimit = 50 } = data || {};

  try {
    let query = db.collection('processingJobs')
      .orderBy('createdAt', 'desc');

    if (status) {
      query = query.where('status', '==', status);
    }

    const snapshot = await query.limit(queryLimit).get();

    const jobs = [];
    snapshot.forEach(doc => {
      const data = doc.data();
      jobs.push({
        id: doc.id,
        projectId: data.projectId,
        clipId: data.clipId,
        status: data.status,
        error: data.error,
        createdAt: data.createdAt?.toDate?.().toISOString(),
        completedAt: data.completedAt?.toDate?.().toISOString()
      });
    });

    return { success: true, jobs };
  } catch (error) {
    console.error('[adminGetProcessingJobs] Error:', error);
    throw new functions.https.HttpsError('internal', error.message);
  }
});

/**
 * adminRetryFailedJob - Retry a failed processing job
 */
exports.adminRetryFailedJob = functions.https.onCall(async (data, context) => {
  await requireAdmin(context);

  const { jobId } = data;
  if (!jobId) {
    throw new functions.https.HttpsError('invalid-argument', 'Job ID required');
  }

  try {
    const jobDoc = await db.collection('processingJobs').doc(jobId).get();
    if (!jobDoc.exists) {
      throw new functions.https.HttpsError('not-found', 'Job not found');
    }

    const jobData = jobDoc.data();
    if (jobData.status !== 'failed') {
      throw new functions.https.HttpsError('failed-precondition', 'Job is not in failed state');
    }

    // Reset job status to pending
    await db.collection('processingJobs').doc(jobId).update({
      status: 'pending',
      error: null,
      retryCount: (jobData.retryCount || 0) + 1,
      retriedAt: admin.firestore.FieldValue.serverTimestamp()
    });

    return { success: true, message: 'Job queued for retry' };
  } catch (error) {
    console.error('[adminRetryFailedJob] Error:', error);
    throw new functions.https.HttpsError('internal', error.message);
  }
});

/**
 * adminClearFailedJobs - Delete all failed processing jobs
 */
exports.adminClearFailedJobs = functions.https.onCall(async (data, context) => {
  await requireAdmin(context);

  try {
    const snapshot = await db.collection('processingJobs')
      .where('status', '==', 'failed')
      .get();

    const batch = db.batch();
    snapshot.forEach(doc => batch.delete(doc.ref));
    await batch.commit();

    return { success: true, deleted: snapshot.size };
  } catch (error) {
    console.error('[adminClearFailedJobs] Error:', error);
    throw new functions.https.HttpsError('internal', error.message);
  }
});

/**
 * scheduledWizardCleanup - Scheduled function to clean up old projects
 * Runs daily at 3 AM UTC
 */
exports.scheduledWizardCleanup = functions.pubsub
  .schedule('0 3 * * *')
  .timeZone('UTC')
  .onRun(async (context) => {
    console.log('[scheduledWizardCleanup] Starting scheduled cleanup...');

    try {
      // Check if auto-cleanup is enabled
      const configDoc = await db.collection('settings').doc('wizardConfig').get();
      const config = configDoc.exists ? configDoc.data() : {};

      if (!config.autoCleanupEnabled) {
        console.log('[scheduledWizardCleanup] Auto-cleanup is disabled, skipping');
        return null;
      }

      const retentionDays = config.retentionDays || 14;
      const cutoffDate = new Date();
      cutoffDate.setDate(cutoffDate.getDate() - retentionDays);

      console.log(`[scheduledWizardCleanup] Deleting projects older than ${retentionDays} days (before ${cutoffDate.toISOString()})`);

      // Find old projects
      const snapshot = await db.collection('wizardProjects')
        .where('createdAt', '<', cutoffDate)
        .get();

      if (snapshot.empty) {
        console.log('[scheduledWizardCleanup] No old projects to delete');
        return null;
      }

      const bucket = admin.storage().bucket();
      let deletedCount = 0;
      let deletedFilesCount = 0;

      for (const doc of snapshot.docs) {
        const projectData = doc.data();

        // Delete storage files
        if (projectData.sourceAsset?.storagePath) {
          await bucket.file(projectData.sourceAsset.storagePath).delete().catch(() => {});
          deletedFilesCount++;
        }
        if (projectData.uploadedVideoPath) {
          await bucket.file(projectData.uploadedVideoPath).delete().catch(() => {});
          deletedFilesCount++;
        }
        if (projectData.videoId) {
          const prefix = `extension-uploads/${projectData.videoId}/`;
          const [files] = await bucket.getFiles({ prefix });
          for (const file of files) {
            await file.delete().catch(() => {});
            deletedFilesCount++;
          }
        }

        // Delete project
        await db.collection('wizardProjects').doc(doc.id).delete();
        deletedCount++;
      }

      console.log(`[scheduledWizardCleanup] Completed: deleted ${deletedCount} projects, ${deletedFilesCount} files`);

      // Log cleanup action
      await db.collection('adminLogs').add({
        action: 'scheduled_wizard_cleanup',
        deletedProjects: deletedCount,
        deletedFiles: deletedFilesCount,
        retentionDays,
        timestamp: admin.firestore.FieldValue.serverTimestamp()
      });

      return null;
    } catch (error) {
      console.error('[scheduledWizardCleanup] Error:', error);
      return null;
    }
  });

/**
 * adminManualCleanup - Manually trigger cleanup
 */
exports.adminManualCleanup = functions.https.onCall(async (data, context) => {
  await requireAdmin(context);

  const { olderThanDays } = data;

  if (!olderThanDays || olderThanDays < 1) {
    throw new functions.https.HttpsError('invalid-argument', 'olderThanDays must be at least 1');
  }

  try {
    const cutoffDate = new Date();
    cutoffDate.setDate(cutoffDate.getDate() - olderThanDays);

    const snapshot = await db.collection('wizardProjects')
      .where('createdAt', '<', cutoffDate)
      .get();

    if (snapshot.empty) {
      return { success: true, deleted: 0, message: 'No old projects found' };
    }

    const bucket = admin.storage().bucket();
    let deletedCount = 0;
    let deletedFilesCount = 0;

    for (const doc of snapshot.docs) {
      const projectData = doc.data();

      if (projectData.sourceAsset?.storagePath) {
        await bucket.file(projectData.sourceAsset.storagePath).delete().catch(() => {});
        deletedFilesCount++;
      }
      if (projectData.uploadedVideoPath) {
        await bucket.file(projectData.uploadedVideoPath).delete().catch(() => {});
        deletedFilesCount++;
      }
      if (projectData.videoId) {
        const prefix = `extension-uploads/${projectData.videoId}/`;
        const [files] = await bucket.getFiles({ prefix });
        for (const file of files) {
          await file.delete().catch(() => {});
          deletedFilesCount++;
        }
      }

      await db.collection('wizardProjects').doc(doc.id).delete();
      deletedCount++;
    }

    await db.collection('adminLogs').add({
      action: 'manual_wizard_cleanup',
      deletedProjects: deletedCount,
      deletedFiles: deletedFilesCount,
      olderThanDays,
      adminId: context.auth.uid,
      timestamp: admin.firestore.FieldValue.serverTimestamp()
    });

    return {
      success: true,
      deleted: deletedCount,
      deletedFiles: deletedFilesCount
    };
  } catch (error) {
    console.error('[adminManualCleanup] Error:', error);
    throw new functions.https.HttpsError('internal', error.message);
  }
});

// ============================================================================
// VIDEO CREATION WIZARD FUNCTIONS
// For the new AI Video Creation feature (separate from video-to-shorts wizard)
// ============================================================================

/**
 * creationWizardSaveProject - Creates or updates a video creation project
 */
exports.creationWizardSaveProject = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const { projectId, projectData } = data;

  if (!projectData) {
    throw new functions.https.HttpsError('invalid-argument', 'Project data required');
  }

  try {
    const now = admin.firestore.FieldValue.serverTimestamp();

    // Prepare the project document - SAVE COMPLETE STATE
    const projectDoc = {
      userId: uid,
      type: 'creation', // Distinguish from video-to-shorts wizard
      name: projectData.name || 'Untitled Video',
      status: projectData.status || 'draft',

      // CRITICAL: Save wizard step progress for proper restoration
      currentStep: projectData.currentStep || 1,
      maxReachedStep: projectData.maxReachedStep || 1,

      // Platform configuration - complete
      platform: projectData.platform ? {
        selected: projectData.platform.selected || null,
        aspectRatio: projectData.platform.aspectRatio || '16:9',
        targetDuration: projectData.platform.targetDuration || 60,
        preset: projectData.platform.preset || null
      } : null,

      // Content configuration - complete with all fields
      content: projectData.content ? {
        niche: projectData.content.niche || null,
        subniche: projectData.content.subniche || null,
        style: projectData.content.style || null,
        topic: projectData.content.topic || '',
        genre: projectData.content.genre || null,
        pacing: projectData.content.pacing || null,
        voiceCharacter: projectData.content.voiceCharacter || null,
        productionMode: projectData.content.productionMode || null,
        targetAudience: projectData.content.targetAudience || null,
        targetAudienceCustom: projectData.content.targetAudienceCustom || null,
        narrativePreset: projectData.content.narrativePreset || null,
        mood: projectData.content.mood || null,
        storyArc: projectData.content.storyArc || null,
        emotionalJourney: projectData.content.emotionalJourney || null
      } : null,

      // NEW WIZARD FLOW: Production Type System
      production: projectData.production ? {
        format: projectData.production.format || null,
        formatConfig: projectData.production.formatConfig || null,
        type: projectData.production.type || null,
        typeConfig: projectData.production.typeConfig || null,
        subType: projectData.production.subType || null,
        subTypeConfig: projectData.production.subTypeConfig || null,
        targetDuration: projectData.production.targetDuration || 180,
        episodeConfig: projectData.production.episodeConfig || null
      } : null,

      // NEW WIZARD FLOW: Concept Development
      concept: projectData.concept ? {
        status: projectData.concept.status || 'idle',
        keywords: projectData.concept.keywords || [],
        rawInput: projectData.concept.rawInput || '',
        ideas: projectData.concept.ideas || [],
        selectedIdea: projectData.concept.selectedIdea,
        refinedConcept: projectData.concept.refinedConcept || null,
        styleReference: projectData.concept.styleReference || '',
        uniqueElements: projectData.concept.uniqueElements || [],
        avoidElements: projectData.concept.avoidElements || [],
        logline: projectData.concept.logline || '',
        worldBuilding: projectData.concept.worldBuilding || null
      } : null,

      // NEW WIZARD FLOW: Character Intelligence
      characterIntelligence: projectData.characterIntelligence ? {
        status: projectData.characterIntelligence.status || 'idle',
        characterType: projectData.characterIntelligence.characterType || 'auto',
        narrationMode: projectData.characterIntelligence.narrationMode || 'auto',
        suggestedCount: projectData.characterIntelligence.suggestedCount || 0,
        characters: projectData.characterIntelligence.characters || [],
        narratorConfig: projectData.characterIntelligence.narratorConfig || null,
        recommendations: projectData.characterIntelligence.recommendations || null
      } : null,

      // Script data - complete
      script: projectData.script ? {
        text: projectData.script.text || '',
        scenes: projectData.script.scenes || [],
        generatedAt: projectData.script.generatedAt || null,
        fullScript: projectData.script.fullScript || null,
        metadata: projectData.script.metadata || null
      } : null,

      // Storyboard data - complete (including Phase 4 Scene Memory & Phase 8-11 Prompt Chain)
      storyboard: projectData.storyboard ? {
        scenes: projectData.storyboard.scenes || [],
        visualStyle: projectData.storyboard.visualStyle || null,
        selectedAspectRatio: projectData.storyboard.selectedAspectRatio || null,
        imageModel: projectData.storyboard.imageModel || 'hidream',
        // Phase 4: Scene Memory System
        styleBible: projectData.storyboard.styleBible || null,
        characterBible: projectData.storyboard.characterBible || null,
        technicalSpecs: projectData.storyboard.technicalSpecs || null,
        // Phase 8-11: Prompt Chain Architecture
        promptChain: projectData.storyboard.promptChain ? {
          enabled: projectData.storyboard.promptChain.enabled !== false,
          status: projectData.storyboard.promptChain.status || 'idle',
          processedAt: projectData.storyboard.promptChain.processedAt || null,
          scenes: projectData.storyboard.promptChain.scenes || []
        } : null
      } : null,

      // Animation data - complete
      animation: projectData.animation ? {
        engine: projectData.animation.engine || 'runpod',
        scenes: projectData.animation.scenes || [],
        selectedEngine: projectData.animation.selectedEngine || null,
        floatingPreview: projectData.animation.floatingPreview || null
      } : null,

      // Assembly data - complete with all audio/caption settings
      assembly: projectData.assembly ? {
        status: projectData.assembly.status || 'pending',
        sceneOrder: projectData.assembly.sceneOrder || [],
        transitions: projectData.assembly.transitions || {},
        defaultTransition: projectData.assembly.defaultTransition || 'fade',
        // Music settings
        music: projectData.assembly.music || { enabled: false, trackId: null, volume: 30, fadeIn: 2, fadeOut: 3 },
        musicLibrary: projectData.assembly.musicLibrary || [],
        // Caption settings
        captions: projectData.assembly.captions || null,
        // Audio mix
        audioMix: projectData.assembly.audioMix || null,
        // Audio intelligence
        audioMood: projectData.assembly.audioMood || null,
        voiceProfile: projectData.assembly.voiceProfile || null,
        transitionSound: projectData.assembly.transitionSound || null,
        ambienceLayer: projectData.assembly.ambienceLayer || null,
        audioProfile: projectData.assembly.audioProfile || null,
        // Beat sync
        beatSync: projectData.assembly.beatSync || null,
        beatSyncApplied: projectData.assembly.beatSyncApplied || false,
        // Assembly presets
        assemblyPreset: projectData.assembly.assemblyPreset || null,
        pacingProfile: projectData.assembly.pacingProfile || null,
        bRollStrategy: projectData.assembly.bRollStrategy || null
      } : null,

      // Export data - complete
      export: projectData.export ? {
        status: projectData.export.status || 'pending',
        outputUrl: projectData.export.outputUrl || null,
        settings: projectData.export.settings || {},
        showModal: false // Don't persist modal state
      } : null,

      updatedAt: now
    };

    let docId;

    if (projectId) {
      // Update existing project
      const existingDoc = await db.collection('creationProjects').doc(projectId).get();
      if (!existingDoc.exists) {
        throw new functions.https.HttpsError('not-found', 'Project not found');
      }
      if (existingDoc.data().userId !== uid) {
        throw new functions.https.HttpsError('permission-denied', 'Not authorized');
      }

      await db.collection('creationProjects').doc(projectId).update(projectDoc);
      docId = projectId;
    } else {
      // Create new project
      projectDoc.createdAt = now;
      const newDoc = await db.collection('creationProjects').add(projectDoc);
      docId = newDoc.id;
    }

    return {
      success: true,
      projectId: docId,
      message: projectId ? 'Project updated' : 'Project created'
    };

  } catch (error) {
    console.error('[creationWizardSaveProject] Error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to save project'));
  }
});

/**
 * creationWizardLoadProject - Loads a complete creation wizard project
 */
exports.creationWizardLoadProject = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const { projectId } = data;

  if (!projectId) {
    throw new functions.https.HttpsError('invalid-argument', 'Project ID required');
  }

  try {
    const projectDoc = await db.collection('creationProjects').doc(projectId).get();

    if (!projectDoc.exists) {
      throw new functions.https.HttpsError('not-found', 'Project not found');
    }

    const project = projectDoc.data();
    if (project.userId !== uid) {
      throw new functions.https.HttpsError('permission-denied', 'Not authorized');
    }

    return {
      success: true,
      project: {
        id: projectDoc.id,
        name: project.name,
        status: project.status,
        // CRITICAL: Include step progress for proper restoration
        currentStep: project.currentStep || null,
        maxReachedStep: project.maxReachedStep || null,
        platform: project.platform,
        content: project.content,
        // NEW WIZARD FLOW: Include production, concept, characterIntelligence
        production: project.production || null,
        concept: project.concept || null,
        characterIntelligence: project.characterIntelligence || null,
        // End new wizard flow fields
        script: project.script,
        storyboard: project.storyboard,
        animation: project.animation,
        assembly: project.assembly,
        export: project.export,
        createdAt: project.createdAt?.toDate?.()?.toISOString() || null,
        updatedAt: project.updatedAt?.toDate?.()?.toISOString() || null
      }
    };

  } catch (error) {
    console.error('[creationWizardLoadProject] Error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to load project'));
  }
});

/**
 * creationWizardGetProjects - Gets list of user's creation wizard projects
 */
exports.creationWizardGetProjects = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const { limit = 20, startAfter = null } = data || {};

  try {
    let query = db.collection('creationProjects')
      .where('userId', '==', uid)
      .orderBy('updatedAt', 'desc')
      .limit(limit);

    if (startAfter) {
      const startDoc = await db.collection('creationProjects').doc(startAfter).get();
      if (startDoc.exists) {
        query = query.startAfter(startDoc);
      }
    }

    const snapshot = await query.get();

    const projects = snapshot.docs.map(doc => {
      const data = doc.data();
      return {
        id: doc.id,
        name: data.name,
        status: data.status,
        platform: data.platform?.selected || null,
        aspectRatio: data.platform?.aspectRatio || null,
        niche: data.content?.niche || null,
        style: data.content?.style || null,
        topic: data.content?.topic || '',
        sceneCount: data.script?.scenes?.length || 0,
        createdAt: data.createdAt?.toDate?.()?.toISOString() || null,
        updatedAt: data.updatedAt?.toDate?.()?.toISOString() || null
      };
    });

    return {
      success: true,
      projects,
      hasMore: projects.length === limit
    };

  } catch (error) {
    console.error('[creationWizardGetProjects] Error:', error);
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to get projects'));
  }
});

/**
 * creationWizardDeleteProject - Deletes a creation wizard project
 */
exports.creationWizardDeleteProject = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const { projectId } = data;

  if (!projectId) {
    throw new functions.https.HttpsError('invalid-argument', 'Project ID required');
  }

  try {
    const projectDoc = await db.collection('creationProjects').doc(projectId).get();

    if (!projectDoc.exists) {
      throw new functions.https.HttpsError('not-found', 'Project not found');
    }

    const project = projectDoc.data();
    if (project.userId !== uid) {
      throw new functions.https.HttpsError('permission-denied', 'Not authorized');
    }

    // Delete any associated storage files
    const bucket = admin.storage().bucket();
    const prefix = `creation-projects/${projectId}/`;
    try {
      const [files] = await bucket.getFiles({ prefix });
      for (const file of files) {
        await file.delete().catch(() => {});
      }
    } catch (storageError) {
      console.warn('[creationWizardDeleteProject] Storage cleanup warning:', storageError);
    }

    // Delete the project document
    await db.collection('creationProjects').doc(projectId).delete();

    return {
      success: true,
      message: 'Project deleted'
    };

  } catch (error) {
    console.error('[creationWizardDeleteProject] Error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to delete project'));
  }
});

/**
 * creationWizardImproveIdea - AI-powered idea enhancement
 *
 * Takes a rough user concept and transforms it into a detailed, rich concept
 * with automatically extracted style references, things to avoid, and a polished description.
 *
 * Input:
 * - rawInput: User's rough concept description
 * - productionType: The type of production (movie, series, social, etc.)
 * - productionSubType: Sub-type (action, drama, documentary, etc.)
 *
 * Returns:
 * - improvedConcept: Detailed, expanded concept description
 * - extractedStyles: Array of visual style inspirations extracted from input
 * - thingsToAvoid: Inferred things to avoid for originality
 * - suggestedMood: Detected mood/atmosphere
 * - suggestedTone: Detected tone
 * - keyElements: Core creative elements identified
 * - worldBuilding: World rules for fantasy/scifi concepts
 * - characters: Suggested original character archetypes
 */
exports.creationWizardImproveIdea = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const { rawInput, productionType, productionSubType, currentStyleReference } = data;

  if (!rawInput || rawInput.trim().length < 10) {
    throw new functions.https.HttpsError('invalid-argument', 'Please provide a concept description (at least 10 characters)');
  }

  console.log('[creationWizardImproveIdea] Improving idea:', {
    rawInput: rawInput.substring(0, 100),
    productionType,
    productionSubType,
    uid
  });

  // === NARRATIVE ARCHITECTURE REFERENCE ===
  // These are the building blocks for sophisticated storytelling
  const NARRATIVE_STRUCTURES_REF = `
AVAILABLE NARRATIVE STRUCTURES (choose the BEST fit for the concept):
- heros_journey: Single protagonist transforms through trials (Lord of the Rings, Star Wars)
- ensemble: Group of 3-6 characters with equal weight (Ocean's Eleven, Guardians of Galaxy, Avengers)
- dual_protagonist: Two leads with different/conflicting goals (Heat, The Prestige, Marriage Story)
- mystery_procedural: Uncovering truth clue by clue (Knives Out, Sherlock, Only Murders)
- tragedy: Protagonist undone by fatal flaw (Breaking Bad, Scarface, The Godfather)
- rebirth: Dark character transforms to redemption (A Christmas Carol, Iron Man, Groundhog Day)
- comedy_of_errors: Misunderstandings lead to chaos then resolution (The Hangover, Bridesmaids)
- anthology_connected: Multiple stories connected by theme (Love Actually, Crash, Black Mirror)
- siege_survival: Group trapped against threat (Die Hard, Alien, A Quiet Place)
- quest_journey: Travel toward goal with obstacles (Finding Nemo, Mad Max, Wizard of Oz)
- nonlinear_puzzle: Story out of order, audience pieces truth (Pulp Fiction, Memento)`;

  const STORY_ENGINES_REF = `
STORY ENGINES (what drives scenes forward):
- case_of_week: Each segment presents new problem to solve
- relationship_dynamics: Character interactions drive plot
- escalating_threat: Outside force grows stronger continuously
- mystery_layers: Each scene peels back layer of mystery
- transformation_arc: Character change is the engine
- quest_progress: Movement toward clear objective
- survival_pressure: Constant threat keeps tension
- social_dynamics: Shifting alliances and power`;

  const CHARACTER_ARCHETYPES_REF = `
CHARACTER ARCHETYPES (use MULTIPLE for depth):
PROTAGONISTS: reluctant_hero, chosen_one, anti_hero, tragic_hero, everyman, wounded_healer
SUPPORTING: mentor, trickster, loyal_friend, love_interest, rival, innocent
ANTAGONISTS: mastermind, force_of_nature, fallen_hero, mirror_villain, system, inner_demon
ENSEMBLE: leader, specialist, heart, wildcard, skeptic, newbie`;

  const prompt = `You are a GENIUS Hollywood creative director with 30+ years of experience developing blockbuster concepts.
You understand that great stories come in MANY forms - not just the hero's journey.

USER'S ROUGH IDEA:
"${rawInput}"

PRODUCTION TYPE: ${productionType || 'Video Content'} ${productionSubType ? `(${productionSubType})` : ''}
${currentStyleReference ? `CURRENT STYLE REFERENCE: ${currentStyleReference}` : ''}

${NARRATIVE_STRUCTURES_REF}

${STORY_ENGINES_REF}

${CHARACTER_ARCHETYPES_REF}

YOUR TASK:
Transform this rough idea into a BRILLIANT, detailed concept with a COMPLETE PRODUCTION BIBLE foundation.

CRITICAL ANALYSIS:
1. What NARRATIVE STRUCTURE fits this idea best? (NOT always hero's journey!)
2. What STORY ENGINE should drive scenes?
3. How many protagonists? (1? 2? Ensemble of 4-6?)
4. What character ARCHETYPES create the richest dynamics?
5. What's the THEME beneath the surface?
6. What visual/audio style defines this world?

RULES FOR IMPROVEMENT:
- EXPAND terse descriptions into vivid, evocative prose
- CHOOSE the narrative structure that BEST serves this story (ensembles, dual protagonists, mysteries, etc.)
- CREATE multiple characters with clear archetypes, flaws, and arcs
- DEFINE character RELATIONSHIPS and tensions
- ESTABLISH world rules that affect the story
- Include specific visual and audio style guidelines
- Make it feel like a PRODUCTION BIBLE that a whole team could work from

Return EXACTLY this JSON structure:
{
  "improvedConcept": "A rich, detailed 3-5 sentence description capturing the ESSENCE with vivid, cinematic language. Include the hook, central conflict, and emotional stakes.",

  "narrativeArchitecture": {
    "structure": "KEY from narrative structures (e.g., 'ensemble', 'dual_protagonist', 'mystery_procedural')",
    "structureReason": "Why this structure serves the story best",
    "storyEngine": "KEY from story engines (e.g., 'relationship_dynamics', 'escalating_threat')",
    "protagonistCount": "Number (1, 2, '3-4', etc.)",
    "keyBeats": ["Beat 1", "Beat 2", "Beat 3", "...specific to chosen structure"]
  },

  "characters": [
    {
      "name": "Original character name",
      "archetype": "KEY from archetypes (e.g., 'anti_hero', 'leader')",
      "role": "protagonist/supporting/antagonist/ensemble",
      "flaw": "Specific character flaw",
      "arc": "How they change through the story",
      "uniqueTwist": "What makes them fresh and original",
      "visualDescription": "Detailed visual concept",
      "voiceDescription": "How they speak, verbal mannerisms"
    }
  ],

  "characterRelationships": [
    {
      "char1": "Character name",
      "char2": "Character name",
      "relationshipType": "rivals/allies/lovers/mentor-student/enemies/family",
      "tension": "Source of conflict or connection",
      "evolution": "How relationship changes"
    }
  ],

  "worldBuilding": {
    "setting": "Where and when - be specific",
    "uniqueMechanic": "What makes this world special (rules, tech, magic, society)",
    "limitations": ["What CAN'T happen", "Constraint 2"],
    "visualLanguage": "How this world should LOOK - colors, textures, style",
    "atmosphere": "The pervading feeling"
  },

  "visualStyle": {
    "colorPalette": "Specific colors and their meaning",
    "lightingApproach": "High key/low key/natural/stylized and why",
    "cameraPhilosophy": "Handheld/steady, close/wide, movement style",
    "referenceFilms": ["Film 1 for visual reference", "Film 2"]
  },

  "audioStyle": {
    "musicGenre": "Score style (orchestral, electronic, hybrid, etc.)",
    "dialogueApproach": "Naturalistic/stylized/sparse",
    "signatureSounds": ["Recurring sound 1", "Sound motif 2"]
  },

  "extractedStyles": ["Visual style reference 1", "Visual style reference 2", "Visual style reference 3"],
  "thingsToAvoid": ["Thing to avoid 1", "Thing to avoid 2", "ClichÃ© to avoid"],
  "suggestedMood": "Primary atmospheric mood",
  "suggestedTone": "Narrative tone",
  "keyElements": ["Core element 1", "Core element 2", "Core element 3"],
  "genreFusion": "Description of genre combination",
  "visualSignature": "The defining visual style in one sentence",
  "hookLine": "A single powerful sentence that hooks the audience",
  "theme": "What this story is REALLY about beneath the surface (e.g., 'the cost of ambition', 'found family vs blood', 'redemption through sacrifice')",
  "premise": "The 'what if' question that drives everything"
}

CRITICAL:
- Do NOT default to hero's journey - choose the structure that BEST fits
- Create AT LEAST 3-4 characters with distinct archetypes
- Define relationships BETWEEN characters
- Be SPECIFIC in every field - this is a production bible, not a vague pitch`;


  try {
    const openaiResponse = await fetch('https://api.openai.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${process.env.OPENAI_API_KEY || functions.config().openai?.key}`
      },
      body: JSON.stringify({
        model: 'gpt-4o',
        messages: [
          {
            role: 'system',
            content: 'You are a legendary creative director and showrunner known for creating hit TV series and films. You understand that great stories use diverse narrative structures - ensembles, dual protagonists, mysteries, tragedies - not just the hero\'s journey. You create rich production bibles with multiple characters, defined relationships, and clear world rules. Always return valid JSON with complete, specific details.'
          },
          { role: 'user', content: prompt }
        ],
        response_format: { type: 'json_object' }, // Force valid JSON output
        max_tokens: 4000,
        temperature: 0.85
      })
    });

    if (!openaiResponse.ok) {
      console.error('[creationWizardImproveIdea] OpenAI error:', openaiResponse.status);
      throw new Error(`OpenAI API error: ${openaiResponse.status}`);
    }

    const openaiData = await openaiResponse.json();
    let responseText = openaiData.choices?.[0]?.message?.content || '';

    // Clean up response - extract JSON
    responseText = responseText.replace(/```json\s*/g, '').replace(/```\s*/g, '').trim();

    // Parse the JSON response
    let result;
    try {
      result = JSON.parse(responseText);
    } catch (parseError) {
      console.error('[creationWizardImproveIdea] JSON parse error, trying to extract:', parseError);
      const jsonMatch = responseText.match(/\{[\s\S]*"improvedConcept"[\s\S]*\}/);
      if (jsonMatch) {
        result = JSON.parse(jsonMatch[0]);
      } else {
        throw parseError;
      }
    }

    console.log('[creationWizardImproveIdea] Successfully improved idea');

    return {
      success: true,
      ...result
    };

  } catch (error) {
    console.error('[creationWizardImproveIdea] Error:', error);

    // Provide a reasonable fallback
    return {
      success: true,
      fallback: true,
      improvedConcept: `${rawInput}\n\nThis concept blends multiple creative influences into a unique visual experience. The story follows original characters through a world that captures the essence of the referenced styles while forging its own identity.`,
      extractedStyles: ['Cinematic action sequences', 'Epic visual scale', 'Immersive atmosphere'],
      thingsToAvoid: ['Direct character copies', 'Trademarked elements', 'Overused clichÃ©s'],
      suggestedMood: 'Epic',
      suggestedTone: 'Dramatic',
      keyElements: ['Original characters', 'Unique visual style', 'Compelling narrative'],
      genreFusion: 'Action-Adventure with Fantasy elements',
      visualSignature: 'Dynamic cinematography with immersive world-building',
      hookLine: 'A journey unlike any other awaits.',
      characters: [{ archetype: 'The Protagonist', uniqueTwist: 'Original backstory', visualDescription: 'Distinctive look' }],
      worldBuilding: { setting: 'A unique world', rules: 'Original concept', atmosphere: 'Immersive' }
    };
  }
});

/**
 * creationWizardGenerateConcepts - Generates unique video concepts using GPT-4o
 *
 * CRITICAL: This function distinguishes between STYLE REFERENCES and SUBJECT MATTER
 * - Style references are for VISUAL STYLE only (cinematography, color grading, mood)
 * - We generate 100% ORIGINAL content inspired by the style, never copying characters/plots
 *
 * Takes:
 * - rawInput: User's concept description
 * - styleReference: Visual style inspiration (e.g., "Breaking Bad cinematography")
 * - avoidElements: Things to explicitly avoid
 * - production: Production type context (movie, series, educational, etc.)
 *
 * Returns:
 * - ideas: Array of 3 unique concept options with titles, loglines, unique elements
 */
exports.creationWizardGenerateConcepts = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const { rawInput, styleReference, avoidElements, production, enrichment } = data;

  console.log('[creationWizardGenerateConcepts] Generating concepts for:', {
    rawInput: rawInput?.substring(0, 100),
    styleReference,
    production: production?.type,
    hasEnrichment: !!enrichment,
    uid,
    creativeSeed: Date.now() % 1000000 // Log seed for debugging
  });

  // === CREATIVE DIVERSITY INJECTION ===
  // Generate random creative "seeds" to force unique outputs each time
  const creativeSeed = Date.now() % 1000000; // 6-digit random seed

  // Random character name pools from different cultures/eras
  const namePools = [
    ['Kai', 'Zara', 'Marcus', 'Yuki', 'Elena', 'Dmitri', 'Amara', 'Finn'],
    ['Nova', 'Jax', 'Priya', 'Alejandro', 'Mei', 'Kofi', 'Ingrid', 'Rashid'],
    ['Atlas', 'Seraphina', 'Rowan', 'Nadia', 'Cedric', 'Xiomara', 'Dante', 'Isolde'],
    ['Phoenix', 'Cassius', 'Thalia', 'Magnus', 'Zuri', 'Orion', 'Kira', 'Ajax'],
    ['River', 'Octavia', 'Soren', 'Valentina', 'Malik', 'Astrid', 'Joaquin', 'Neve']
  ];

  // Random setting flavors
  const settingFlavors = [
    ['neon-lit', 'ancient', 'floating', 'underground', 'abandoned', 'sacred'],
    ['crystalline', 'war-torn', 'utopian', 'decaying', 'frontier', 'forbidden'],
    ['oceanic', 'volcanic', 'arctic', 'desert', 'jungle', 'mountain'],
    ['steampunk', 'biopunk', 'solarpunk', 'retrofuture', 'post-collapse', 'alternate']
  ];

  // Random story hooks
  const storyHooks = [
    'What if the hero discovers they are the prophecied villain?',
    'What if every choice creates a parallel version that the characters can glimpse?',
    'What if the magical system is actually ancient technology misunderstood?',
    'What if the enemy they must defeat was once their greatest ally?',
    'What if winning the battle would cause a worse catastrophe?',
    'What if the world is a simulation and some characters suspect it?',
    'What if memories can be transferred but with unintended consequences?',
    'What if time moves differently for different characters?',
    'What if the monster they hunt is actually protecting something precious?',
    'What if the power comes at a cost that escalates with each use?'
  ];

  // Random genre mashups
  const genreMashups = [
    ['noir', 'fantasy'], ['horror', 'comedy'], ['western', 'sci-fi'],
    ['romance', 'thriller'], ['mystery', 'supernatural'], ['war', 'coming-of-age'],
    ['heist', 'period drama'], ['survival', 'philosophical'], ['sports', 'crime'],
    ['disaster', 'family drama'], ['espionage', 'supernatural'], ['revenge', 'redemption']
  ];

  // Select random elements based on seed
  const selectedNamePool = namePools[creativeSeed % namePools.length];
  const selectedSettings = settingFlavors[creativeSeed % settingFlavors.length];
  const selectedHook = storyHooks[creativeSeed % storyHooks.length];
  const selectedMashup = genreMashups[creativeSeed % genreMashups.length];

  // Creative divergence prompt section
  const creativeDivergence = `
=== CREATIVE DIVERGENCE SEED: ${creativeSeed} ===
To ensure MAXIMUM CREATIVITY, consider these random inspirations (adapt freely):

SUGGESTED CHARACTER NAME POOL (use or transform): ${selectedNamePool.join(', ')}
SETTING TEXTURE IDEAS: ${selectedSettings.join(', ')}
UNEXPECTED TWIST TO CONSIDER: "${selectedHook}"
GENRE FUSION POSSIBILITY: ${selectedMashup.join(' + ')}

IMPORTANT: These are STARTING POINTS. Transform them. Surprise the user.
DO NOT create the same story you created last time.
Each generation should feel like a FRESH creative mind is approaching the concept.

MANDATORY VARIATION RULES:
1. If the user input mentions "action" - one concept must subvert action tropes
2. If mentioned "sci-fi" - one concept should ground sci-fi in intimate human drama
3. If mentioned "fantasy" - one concept should feel grounded/realistic despite fantasy elements
4. Make characters SPECIFIC (age, background, specific flaw) not generic
5. Give at least one character an UNUSUAL profession or background
6. Include at least one SURPRISING character dynamic (enemies forced to cooperate, etc.)
`;

  // Build the prompt for concept generation
  const productionContext = production ? `
Production Type: ${production.type}
Sub-Type: ${production.subType || 'Not specified'}
Visual Style Guidance: ${production.visualStyle || 'Cinematic'}
Style References (for VISUAL inspiration only): ${(production.references || []).join(', ')}
Characteristics: ${(production.characteristics || []).join(', ')}
Target Duration: ${production.duration || 180} seconds
` : '';

  const styleWarning = styleReference ? `
CRITICAL STYLE INSTRUCTION:
The user mentioned "${styleReference}" - this is for VISUAL STYLE ONLY.
- DO NOT create content about ${styleReference.replace(/cinematography|style|palette|look|feel|aesthetic|visuals/gi, '').trim()}
- DO NOT use any characters, plots, or specific elements from ${styleReference}
- DO capture the FEELING, MOOD, and VISUAL APPROACH of ${styleReference}
- Create 100% ORIGINAL characters, settings, and stories
` : '';

  const avoidList = avoidElements && avoidElements.length > 0 ?
    `\nExplicitly AVOID: ${avoidElements.join(', ')}` : '';

  // Build AI Enhancement context if available
  let enrichmentContext = '';
  if (enrichment) {
    enrichmentContext = `
=== AI-ENHANCED CONCEPT DATA (USE THIS!) ===
The user has already developed their idea with AI assistance. Use this enriched data:

ENHANCED CONCEPT: ${enrichment.improvedConcept || rawInput}
${enrichment.hookLine ? `HOOK LINE: "${enrichment.hookLine}"` : ''}
${enrichment.genreFusion ? `GENRE FUSION: ${enrichment.genreFusion}` : ''}
${enrichment.visualSignature ? `VISUAL SIGNATURE: ${enrichment.visualSignature}` : ''}
${enrichment.suggestedMood ? `MOOD: ${enrichment.suggestedMood}` : ''}
${enrichment.suggestedTone ? `TONE: ${enrichment.suggestedTone}` : ''}
${enrichment.keyElements?.length > 0 ? `KEY ELEMENTS TO INCLUDE:\n${enrichment.keyElements.map((e, i) => `  ${i+1}. ${e}`).join('\n')}` : ''}
${enrichment.characters?.length > 0 ? `
CHARACTER ARCHETYPES TO USE:
${enrichment.characters.map((c, i) => `  ${i+1}. ${c.archetype}: ${c.uniqueTwist} (Visual: ${c.visualDescription})`).join('\n')}` : ''}
${enrichment.worldBuilding ? `
WORLD-BUILDING:
  Setting: ${enrichment.worldBuilding.setting}
  Rules: ${enrichment.worldBuilding.rules}
  Atmosphere: ${enrichment.worldBuilding.atmosphere}` : ''}

IMPORTANT: Generate concepts that EXPAND on this enriched data, not ignore it.
Each concept should be a different creative direction but ALL should incorporate:
- The visual signature and mood
- At least some of the key elements
- The character archetypes (can be adapted)
- The world-building rules
`;
  }

  // === NARRATIVE DIVERSITY REQUIREMENT ===
  // Each concept MUST use a DIFFERENT narrative structure
  const narrativeDiversityGuide = `
=== MANDATORY NARRATIVE DIVERSITY ===
You MUST generate 3 concepts using DIFFERENT narrative structures.

NARRATIVE STRUCTURES TO CHOOSE FROM:
1. heros_journey - Single protagonist transforms through trials (1 protagonist)
2. ensemble - Group of 3-6 characters with equal weight, interconnected (3-6 protagonists)
3. dual_protagonist - Two leads with conflicting goals (2 protagonists)
4. mystery_procedural - Uncovering truth clue by clue (1-2 protagonists)
5. tragedy - Character undone by fatal flaw (1 protagonist)
6. rebirth - Dark character transforms to redemption (1 protagonist)
7. comedy_of_errors - Misunderstandings lead to chaos (2-4 protagonists)
8. anthology_connected - Multiple stories connected by theme (4-8 characters)
9. siege_survival - Group trapped against threat (3-8 protagonists)
10. quest_journey - Travel toward goal with obstacles (1-4 protagonists)
11. nonlinear_puzzle - Story out of order (1-3 protagonists)

STORY ENGINES (pair with structure):
- relationship_dynamics: Character interactions drive plot
- escalating_threat: Outside force grows stronger
- mystery_layers: Each scene reveals new layer
- transformation_arc: Character change drives story
- quest_progress: Movement toward clear goal
- survival_pressure: Constant threat tension
- social_dynamics: Shifting alliances and power

CHARACTER ARCHETYPES (use multiple per concept):
PROTAGONISTS: reluctant_hero, chosen_one, anti_hero, tragic_hero, everyman, wounded_healer
SUPPORTING: mentor, trickster, loyal_friend, love_interest, rival, innocent
ANTAGONISTS: mastermind, force_of_nature, fallen_hero, mirror_villain
ENSEMBLE: leader, specialist, heart, wildcard, skeptic, newbie

CRITICAL: The 3 concepts must use 3 DIFFERENT structures!
Example valid distribution:
- Concept 1: ensemble structure (team of 4)
- Concept 2: mystery_procedural (detective + partner)
- Concept 3: tragedy (single anti-hero)

INVALID (too similar):
- Concept 1: heros_journey (one hero)
- Concept 2: heros_journey (one hero)
- Concept 3: rebirth (one hero)
`;

  const prompt = `You are a Hollywood showrunner who creates diverse, sophisticated stories.
You understand that the best pitches offer RADICALLY DIFFERENT approaches to the same core idea.
CRITICAL: You MUST generate FRESH, UNIQUE content every time. Never repeat patterns from previous generations.

USER'S CORE IDEA:
${rawInput || 'Create something engaging and original'}

${creativeDivergence}
${productionContext}
${enrichmentContext}
${narrativeDiversityGuide}
${styleWarning}
${avoidList}

YOUR TASK:
Generate 3 concepts that are GENUINELY DIFFERENT in structure and approach.
Each concept should explore the user's idea through a DIFFERENT LENS.
Use the CREATIVE DIVERGENCE SEED above to inspire fresh, unexpected directions.

REQUIREMENTS:
1. Each concept uses a DIFFERENT narrative structure (mandatory!)
2. Each concept has a DIFFERENT number/type of protagonists
3. Create rich characters with archetypes, flaws, and arcs
4. Define character relationships and dynamics
5. Establish clear world rules
6. Include visual and tonal direction
${enrichment ? `7. Build on the enriched production bible data (narrative architecture, characters, relationships)
8. Use the detected story engine and adapt it for each structure` : ''}

Return EXACTLY this JSON structure:
{
  "ideas": [
    {
      "title": "Short, catchy title",
      "logline": "One compelling sentence (include the hook!)",
      "description": "2-3 sentences expanding on the concept with emotional stakes",

      "narrativeStructure": "KEY from structures above (e.g., 'ensemble', 'dual_protagonist')",
      "storyEngine": "KEY from engines above (e.g., 'relationship_dynamics')",
      "protagonistCount": "Number or range (1, 2, '3-4', etc.)",

      "characters": [
        {
          "name": "Original character name",
          "archetype": "KEY from archetypes",
          "role": "protagonist/supporting/antagonist/ensemble",
          "flaw": "Specific flaw",
          "arc": "Transformation"
        }
      ],

      "keyRelationships": [
        {
          "between": "Character 1 & Character 2",
          "type": "rivals/allies/lovers/mentor-student/family",
          "tension": "Source of conflict"
        }
      ],

      "worldSetting": {
        "location": "Where this takes place",
        "uniqueRule": "What makes this world special",
        "atmosphere": "The feeling of this world"
      },

      "uniqueElements": ["Element 1", "Element 2", "Element 3"],
      "mood": "Primary mood",
      "tone": "Narrative tone",
      "visualApproach": "Cinematic visual style",
      "theme": "What it's REALLY about beneath the surface"
    }
  ]
}

CRITICAL DIVERSITY CHECK before responding:
- Are all 3 structures DIFFERENT? (Required!)
- Are protagonist counts VARIED? (Required!)
- Do the stories feel like different shows/films? (Required!)`;

  try {
    // Use GPT-4o for concept generation
    const openaiResponse = await fetch('https://api.openai.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${process.env.OPENAI_API_KEY || functions.config().openai?.key}`
      },
      body: JSON.stringify({
        model: 'gpt-4o',
        messages: [
          {
            role: 'system',
            content: `You are a showrunner known for creating diverse, acclaimed series. You approach every brief with FRESH EYES.

CORE MANDATE: Generate genuinely NEW, SURPRISING content each time:
- NEVER use the same character archetypes in the same configuration twice
- NEVER default to the "obvious" interpretation of a genre
- ALWAYS include at least one element that subverts expectations
- VARY character names, ages, backgrounds, and professions
- Each concept should feel like it came from a DIFFERENT creative mind

You NEVER pitch the same story structure twice - each concept explores fundamentally different narrative territory (ensemble vs single protagonist, mystery vs character study, etc.). You create rich multi-character stories with clear archetypes, relationships, and world rules.

When given a creative seed, use it as a launching point for UNEXPECTED directions.
Always return valid JSON with genuinely diverse, surprising concepts.`
          },
          { role: 'user', content: prompt }
        ],
        response_format: { type: 'json_object' }, // Force valid JSON output
        max_tokens: 4000,
        temperature: 1.0 // Increased from 0.95 for more creativity
      })
    });

    if (!openaiResponse.ok) {
      console.error('[creationWizardGenerateConcepts] OpenAI error:', openaiResponse.status);
      throw new Error(`OpenAI API error: ${openaiResponse.status}`);
    }

    const openaiData = await openaiResponse.json();
    let responseText = openaiData.choices?.[0]?.message?.content || '';

    // Clean up response - extract JSON
    responseText = responseText.replace(/```json\s*/g, '').replace(/```\s*/g, '').trim();

    // Parse the JSON response
    let parsedResponse;
    try {
      parsedResponse = JSON.parse(responseText);
    } catch (parseError) {
      console.error('[creationWizardGenerateConcepts] JSON parse error, trying to extract:', parseError);
      // Try to find JSON in the response
      const jsonMatch = responseText.match(/\{[\s\S]*"ideas"[\s\S]*\}/);
      if (jsonMatch) {
        parsedResponse = JSON.parse(jsonMatch[0]);
      } else {
        throw parseError;
      }
    }

    console.log('[creationWizardGenerateConcepts] Generated', parsedResponse.ideas?.length, 'concepts');

    return {
      success: true,
      ideas: parsedResponse.ideas || []
    };

  } catch (error) {
    console.error('[creationWizardGenerateConcepts] Error:', error);

    // Return fallback ideas if AI fails
    return {
      success: true,
      fallback: true,
      ideas: [
        {
          title: 'Original Concept 1',
          logline: `A unique exploration of ${rawInput || 'an untold story'}`,
          description: 'An original narrative with compelling characters',
          uniqueElements: ['Original protagonist', 'Fresh setting', 'Unique twist'],
          mood: 'Engaging',
          tone: 'Dynamic'
        },
        {
          title: 'Original Concept 2',
          logline: `A dramatic journey through ${rawInput || 'new territory'}`,
          description: 'A character-driven story with emotional depth',
          uniqueElements: ['Complex characters', 'Unexpected development', 'Strong visuals'],
          mood: 'Dramatic',
          tone: 'Intense'
        },
        {
          title: 'Original Concept 3',
          logline: `An exploration of ${rawInput || 'human experience'}`,
          description: 'A thoughtful piece with universal themes',
          uniqueElements: ['Relatable themes', 'Visual storytelling', 'Memorable moments'],
          mood: 'Thoughtful',
          tone: 'Contemplative'
        }
      ]
    };
  }
});

/**
 * creationWizardGenerateScript - Generates a video script using GPT-4o
 *
 * Takes project configuration and generates a structured script with:
 * - Title and hook
 * - Multiple scenes with narration and visual descriptions
 * - Call-to-action
 * - Timing for each scene
 */
exports.creationWizardGenerateScript = functions
  .runWith({
    timeoutSeconds: 300, // 5 minutes - needed for complex prompts with rich concept data
    memory: '1GB' // Increase memory for large prompt processing
  })
  .https.onCall(async (data, context) => {
  try {
    const uid = await verifyAuth(context);
    const { projectId, config } = data;

    if (!config) {
      throw new functions.https.HttpsError('invalid-argument', 'Script configuration required');
    }

  const {
    platform,
    aspectRatio,
    targetDuration,
    niche,
    subniche,
    style,
    topic,
    tone = 'engaging',
    pacing = 'medium', // 'fast' | 'medium' | 'slow' - controls visual breathing room
    contentDepth = 'standard', // 'minimal' | 'standard' | 'detailed' | 'comprehensive' - content richness
    additionalInstructions = '',
    // Phase 2: Hollywood Production Mode
    productionMode = 'standard', // 'standard' | 'documentary' | 'thriller' | 'inspirational' | 'story' | 'cinematic'
    // Phase 3A: Genre Reference System
    genre = null, // Genre key from GENRE_REFERENCE_LIBRARY (e.g., 'documentary-nature', 'educational-explainer')
    contentFormat = 'medium-form', // 'short-form' | 'medium-form' | 'long-form' | 'episodic'
    // Video Model Configuration (for scene duration optimization)
    videoModel = { duration: '10s', resolution: '768p' },  // 10s default for richer action, 768p (1080p only for 6s)
    // Phase 5: AI Concept Enhancement - Deep Story Architecture
    conceptEnrichment = null
  } = config;

  if (!topic || topic.trim().length < 3) {
    throw new functions.https.HttpsError('invalid-argument', 'Topic must be at least 3 characters');
  }

    // === CONTENT DEPTH CONFIGURATION ===
    // Controls how rich and detailed the narration content is
    const contentDepthConfig = {
      minimal: {
        description: 'Quick overview, key points only',
        wordsMultiplier: 0.7,
        requirements: 'brief points, no elaboration',
        includeStats: false,
        includeExamples: false,
        includeStory: false
      },
      standard: {
        description: 'Balanced content with good explanation',
        wordsMultiplier: 1.0,
        requirements: 'clear explanations with context',
        includeStats: true,
        includeExamples: false,
        includeStory: false
      },
      detailed: {
        description: 'Rich content with examples and data',
        wordsMultiplier: 1.4,
        requirements: 'thorough explanations, statistics, and examples',
        includeStats: true,
        includeExamples: true,
        includeStory: true
      },
      comprehensive: {
        description: 'Maximum depth - educational/documentary style',
        wordsMultiplier: 1.8,
        requirements: 'in-depth analysis, multiple examples, storytelling, expert-level detail',
        includeStats: true,
        includeExamples: true,
        includeStory: true
      }
    };

    const depthSettings = contentDepthConfig[contentDepth] || contentDepthConfig.standard;

    // ============================================================
    // PHASE 3A: GENRE REFERENCE LIBRARY
    // Real-world production references for premium content creation
    // ============================================================

    /**
     * GENRE REFERENCE LIBRARY
     * Each genre contains references to successful productions and what makes them work.
     * This ensures content never feels like a "basic presentation" regardless of assets used.
     */
    const GENRE_REFERENCE_LIBRARY = {
      // === DOCUMENTARY GENRES ===
      'documentary-nature': {
        name: 'Nature Documentary',
        category: 'documentary',
        references: ['Planet Earth (BBC)', 'Our Planet (Netflix)', 'Blue Planet', 'Life (BBC)'],
        whatMakesItWork: 'Epic scale contrasted with intimate moments. Anthropomorphizing animals without being childish. The drama of survival. Patience in letting moments breathe.',
        narrativeVoice: 'Warm, authoritative, filled with wonder. David Attenborough energy. Never condescending. Treats subject with reverence.',
        signatureTechniques: [
          'The patient reveal - let beauty unfold slowly',
          'Micro to macro transitions (dewdrop â†’ forest â†’ planet)',
          'Behavior sequences that tell mini-stories',
          'Silence before majesty - let visuals speak'
        ],
        hookStyles: [
          'The impossible image: Start with something viewers have never seen',
          'The countdown: "Every X seconds, something remarkable happens..."',
          'The hidden world: "Just beneath the surface..."',
          'The journey: "10,000 miles. One destination."'
        ],
        visualGrammar: 'Sweeping aerials, extreme close-ups, golden hour lighting, long takes that reward patience. 16:9 or wider aspect ratios.',
        pacing: 'Slow build with punctuated moments of action. Breathing room between sequences.',
        emotionalBeats: ['wonder', 'tension', 'relief', 'awe', 'hope'],
        avoidAtAllCosts: ['Rushed narration', 'over-explaining visuals', 'cheap stock footage feeling', 'generic transitions']
      },

      'documentary-true-crime': {
        name: 'True Crime',
        category: 'documentary',
        references: ['Making a Murderer', 'The Jinx', 'Tiger King', 'Serial (podcast)', 'Mindhunter'],
        whatMakesItWork: 'The puzzle. Unreliable narrators. Information revealed strategically. The feeling that YOU are solving the case. Moral ambiguity.',
        narrativeVoice: 'Investigative, measured, occasionally ominous. Let interview subjects reveal themselves. Narrator as guide, not judge.',
        signatureTechniques: [
          'The timeline reveal - show how pieces connect',
          'Contradicting testimonies back-to-back',
          'The detail that doesn\'t fit',
          'Archival footage that takes on new meaning',
          'The cliffhanger cut'
        ],
        hookStyles: [
          'The missing piece: "One detail never made sense..."',
          'The reversal: "Everything pointed to him. Until it didn\'t."',
          'The witness: "She saw something that night. She\'s never told anyone."',
          'The cold open: Start with the crime, no context'
        ],
        visualGrammar: 'Moody lighting, evidence close-ups, talking heads in shadowy settings, newspaper clippings, crime scene photos (tastefully), maps and timelines.',
        pacing: 'Build tension, release with revelation, immediately raise new questions. Cliffhangers between scenes.',
        emotionalBeats: ['intrigue', 'suspicion', 'shock', 'doubt', 'unsettling realization'],
        avoidAtAllCosts: ['Sensationalism without substance', 'giving away the twist early', 'one-dimensional villains', 'fake dramatic pauses']
      },

      'documentary-social': {
        name: 'Social Documentary',
        category: 'documentary',
        references: ['The Social Dilemma', 'Blackfish', '13th', 'Icarus', 'Won\'t You Be My Neighbor'],
        whatMakesItWork: 'Personal stories that reveal systemic issues. Expert credibility. The "I had no idea" moment. Call to awareness.',
        narrativeVoice: 'Urgent but not preachy. Let subjects speak for themselves. Data made human. Righteous anger earned through evidence.',
        signatureTechniques: [
          'The insider perspective - someone who was there',
          'Statistics made visceral',
          'The juxtaposition (what they say vs what happens)',
          'The tipping point moment',
          'The "this affects YOU" bridge'
        ],
        hookStyles: [
          'The confession: "I helped build this. Now I regret it."',
          'The statistic that shocks: "Every X minutes..."',
          'The hidden connection: "Your morning routine funds..."',
          'The question you\'ve never asked'
        ],
        visualGrammar: 'Intimate interviews, B-roll of everyday life affected, data visualizations, archival footage, behind-the-scenes access.',
        pacing: 'Build the case methodically. Emotional peaks tied to human stories.',
        emotionalBeats: ['curiosity', 'concern', 'anger', 'empathy', 'determination'],
        avoidAtAllCosts: ['Preaching', 'one-sided without addressing counter-arguments', 'statistics without human context', 'guilt-tripping']
      },

      'documentary-historical': {
        name: 'Historical Documentary',
        category: 'documentary',
        references: ['The Civil War (Ken Burns)', 'The Last Dance', 'Apollo 11', 'They Shall Not Grow Old'],
        whatMakesItWork: 'Making the past feel alive and relevant. Finding the human stories within history. The "you are there" feeling. Connecting then to now.',
        narrativeVoice: 'Authoritative but warm. Reading primary sources brings voices to life. Historians as storytellers, not lecturers.',
        signatureTechniques: [
          'The Ken Burns effect - life in still images',
          'Letters and diaries read aloud',
          'Then-and-now transitions',
          'The overlooked perspective',
          'The moment that changed everything'
        ],
        hookStyles: [
          'The forgotten story: "History forgot them. Until now."',
          'The parallel: "It happened before. It\'s happening again."',
          'The artifact: "This letter was never meant to be found."',
          'The eyewitness: "He was the last person alive who saw it."'
        ],
        visualGrammar: 'Archival photos with subtle movement, documents and maps, recreated scenes (tastefully), locations as they are today, artifacts in museums.',
        pacing: 'Chronological with strategic flashbacks. Build to the pivotal moment.',
        emotionalBeats: ['context', 'immersion', 'tension', 'tragedy or triumph', 'reflection'],
        avoidAtAllCosts: ['Dry recitation of dates', 'assuming viewer knowledge', 'cheesy reenactments', 'ignoring human cost']
      },

      // === EDUCATIONAL GENRES ===
      'educational-explainer': {
        name: 'Explainer',
        category: 'educational',
        references: ['Kurzgesagt', 'Vox', 'Wendover Productions', 'Real Engineering', 'Veritasium'],
        whatMakesItWork: 'Complex made simple without being dumbed down. Visual metaphors that click. The "aha" moment. Respecting viewer intelligence.',
        narrativeVoice: 'Curious, enthusiastic, slightly nerdy. Explaining to a smart friend. Admitting what we don\'t know. Building mental models.',
        signatureTechniques: [
          'The unexpected connection (A relates to B how?!)',
          'Scale comparisons that click',
          'The common misconception corrected',
          'Building complexity layer by layer',
          'The satisfying callback'
        ],
        hookStyles: [
          'The question you didn\'t know you had: "Why is X actually Y?"',
          'The mind-blowing fact: "There\'s more X than Y on Earth."',
          'The misconception: "Everything you know about X is wrong."',
          'The challenge: "Can you explain X? Most people can\'t."'
        ],
        visualGrammar: 'Clean animations, infographics, real-world footage to ground concepts, visual metaphors, color-coded systems.',
        pacing: 'Quick but not rushed. Each concept lands before the next. Occasional breathers for absorption.',
        emotionalBeats: ['curiosity', 'confusion (brief)', 'understanding', 'wonder', 'satisfaction'],
        avoidAtAllCosts: ['Talking down to viewers', 'jargon without explanation', 'boring visuals for complex topics', 'info-dumping']
      },

      'educational-tutorial': {
        name: 'Tutorial/How-To',
        category: 'educational',
        references: ['Mark Rober', 'Simone Giertz', 'Adam Savage', 'Binging with Babish', 'DIY Perks'],
        whatMakesItWork: 'Personality-driven teaching. Showing the process AND the struggle. Results that inspire. Making the viewer feel capable.',
        narrativeVoice: 'Encouraging, patient, celebrating small wins. Acknowledging difficulty. The mentor vibe.',
        signatureTechniques: [
          'The "here\'s what I messed up" moment',
          'Time-lapses of tedious parts',
          'Close-ups at crucial steps',
          'The reveal of finished product',
          'Pro tips dropped casually'
        ],
        hookStyles: [
          'The impossible result: "I built X in my garage."',
          'The challenge: "They said it couldn\'t be done."',
          'The problem solver: "I was tired of X, so I fixed it."',
          'The transformation: "From junk to [amazing thing]"'
        ],
        visualGrammar: 'Clean workspace shots, hands in frame doing the work, before/after comparisons, materials laid out satisfyingly.',
        pacing: 'Varies - quick for simple steps, slow for crucial moments. Always show the result early to hook.',
        emotionalBeats: ['inspiration', 'follow-along confidence', 'problem-solving satisfaction', 'pride in result'],
        avoidAtAllCosts: ['Skipping crucial steps', 'making it look too easy', 'boring tool explanations', 'no personality']
      },

      'educational-science': {
        name: 'Science/Tech',
        category: 'educational',
        references: ['Veritasium', 'SmarterEveryDay', 'Vsauce', 'Numberphile', 'Physics Girl'],
        whatMakesItWork: 'Wonder at the universe. Experiments that prove concepts. The joy of discovery. Making viewers feel smart.',
        narrativeVoice: 'Genuinely excited by knowledge. Asking questions with viewers. "Isn\'t that weird?" energy.',
        signatureTechniques: [
          'The experiment that proves it',
          'Slow-motion reveals',
          'The intuition that\'s wrong',
          'Expert interviews that humanize science',
          'The "but wait, there\'s more" escalation'
        ],
        hookStyles: [
          'The paradox: "This shouldn\'t be possible. And yet..."',
          'The demo: [Show something impossible-looking first]',
          'The question: "What would happen if..."',
          'The mistake everyone makes'
        ],
        visualGrammar: 'Lab settings, real experiments, diagrams that build, slow-motion, microscopic/telescopic footage.',
        pacing: 'Build curiosity, test hypothesis, reveal answer, explore implications.',
        emotionalBeats: ['curiosity', 'prediction', 'surprise', 'understanding', 'wonder'],
        avoidAtAllCosts: ['Being a boring lecture', 'no visual demonstration', 'assuming prior knowledge', 'no payoff to setup']
      },

      // === ENTERTAINMENT GENRES ===
      'entertainment-comedy': {
        name: 'Comedy',
        category: 'entertainment',
        references: ['The Office', 'Brooklyn 99', 'Key & Peele', 'SNL Digital Shorts', 'Ryan George (Pitch Meetings)'],
        whatMakesItWork: 'Timing. Subverted expectations. Specificity over generality. Callbacks. Commitment to the bit.',
        narrativeVoice: 'Depends on style - deadpan, manic, observational. The character\'s voice IS the comedy.',
        signatureTechniques: [
          'The callback - setup early, payoff late',
          'The escalation - each beat more absurd',
          'The cut - comedic timing through editing',
          'The straight man - ground absurdity in reality',
          'The pause - let the joke land'
        ],
        hookStyles: [
          'The absurd premise stated matter-of-factly',
          'The relatable situation pushed to extreme',
          'The fish-out-of-water setup',
          'Cold open with no context (context comes later for payoff)'
        ],
        visualGrammar: 'Timing-aware editing, reaction shots, awkward silence holds, visual gags in background, motivated camera moves for punchlines.',
        pacing: 'Rapid-fire OR slow burn. Rarely in between. Jokes need breathing room.',
        emotionalBeats: ['setup', 'misdirection', 'punchline', 'callback', 'button'],
        avoidAtAllCosts: ['Explaining the joke', 'rushing punchlines', 'no straight man', 'trying too hard']
      },

      'entertainment-drama': {
        name: 'Drama',
        category: 'entertainment',
        references: ['Breaking Bad', 'Succession', 'The Crown', 'Better Call Saul', 'Chernobyl'],
        whatMakesItWork: 'Stakes. Character conflict. Moral complexity. Tension in silence. Making viewers root for flawed people.',
        narrativeVoice: 'Measured, allowing subtext. Dialogue with layers. What isn\'t said matters.',
        signatureTechniques: [
          'The moral dilemma with no good answer',
          'Slow push-in during revelation',
          'Silence that speaks',
          'The scene before the storm',
          'Parallel editing building to collision'
        ],
        hookStyles: [
          'In media res - middle of conflict',
          'The aftermath - show destruction, then flashback',
          'The choice - protagonist facing impossible decision',
          'The lie that will unravel'
        ],
        visualGrammar: 'Cinematic compositions, meaningful blocking, shadow and light for moral ambiguity, close-ups in emotional moments.',
        pacing: 'Slow burn punctuated by explosive moments. Tension is the currency.',
        emotionalBeats: ['normalcy', 'disturbance', 'escalation', 'crisis', 'transformation'],
        avoidAtAllCosts: ['Melodrama without earned emotion', 'on-the-nose dialogue', 'convenient resolutions', 'flat characters']
      },

      'entertainment-horror': {
        name: 'Horror/Thriller',
        category: 'entertainment',
        references: ['Black Mirror', 'Get Out', 'Hereditary', 'The Haunting of Hill House', 'A Quiet Place'],
        whatMakesItWork: 'Dread over jump scares. The unknown. Making familiar things threatening. Psychological unease. Rules that make it scarier.',
        narrativeVoice: 'Unreliable, paranoid, or ominously calm. The voice itself can unsettle.',
        signatureTechniques: [
          'The long take building dread',
          'Something wrong in the frame (viewer notices before character)',
          'Sound design that unsettles',
          'The fake-out relief before real scare',
          'The rules that make it worse'
        ],
        hookStyles: [
          'The mundane made wrong - something is off',
          'The warning ignored',
          'The discovery that shouldn\'t exist',
          'The voice that shouldn\'t be there'
        ],
        visualGrammar: 'Deep shadows, unsettling framing, slow zooms, negative space that threatens, practical effects feeling.',
        pacing: 'Slow build, release, LONGER slow build, bigger release. Escalating cycle.',
        emotionalBeats: ['unease', 'tension', 'false relief', 'dread', 'horror', 'lingering discomfort'],
        avoidAtAllCosts: ['Jump scares without buildup', 'over-explaining the threat', 'gore as substitute for tension', 'breaking established rules cheaply']
      },

      // === BUSINESS/MARKETING GENRES ===
      'business-brand': {
        name: 'Brand Story',
        category: 'business',
        references: ['Apple keynotes', 'Nike campaigns', 'Patagonia', 'Dollar Shave Club', 'Mailchimp'],
        whatMakesItWork: 'Values over features. Aspiration over information. The customer as hero. Emotional truth.',
        narrativeVoice: 'Confident without arrogance. Speaks to identity, not just needs. "We believe" energy.',
        signatureTechniques: [
          'The manifesto moment - what we stand for',
          'The customer transformation story',
          'The origin story with purpose',
          'Show don\'t tell the value',
          'The rallying cry close'
        ],
        hookStyles: [
          'The bold statement: "We believe..."',
          'The enemy: "Most companies do X. We don\'t."',
          'The movement: "Join the [type of] people who..."',
          'The question of identity: "Are you the kind of person who..."'
        ],
        visualGrammar: 'Aspirational lifestyle imagery, real customers, behind-the-scenes authenticity, product in context of life.',
        pacing: 'Building emotional crescendo. End on high.',
        emotionalBeats: ['recognition', 'belonging', 'aspiration', 'conviction', 'action'],
        avoidAtAllCosts: ['Features lists', 'corporate speak', 'inauthentic diversity', 'hard sell']
      },

      'business-product': {
        name: 'Product Launch',
        category: 'business',
        references: ['Apple product reveals', 'Tesla unveilings', 'Dyson', 'MKBHD reviews', 'Unbox Therapy'],
        whatMakesItWork: 'Building anticipation. Strategic reveals. Technical excellence made emotional. The moment of truth.',
        narrativeVoice: 'Authoritative, precise, building to enthusiasm. Facts that impress.',
        signatureTechniques: [
          'The problem no one knew they had',
          'The "one more thing" reveal',
          'Specs made meaningful',
          'The satisfying unboxing',
          'Real-world demo over marketing claims'
        ],
        hookStyles: [
          'The tease: Show result before showing product',
          'The problem: Start with frustration, then solution',
          'The comparison: "While others do X..."',
          'The numbers: One spec that stops scrolling'
        ],
        visualGrammar: 'Clean product shots, 360 views, detail macro shots, size/scale references, dramatic lighting.',
        pacing: 'Build anticipation, reveal, explore, demonstrate, close with aspiration.',
        emotionalBeats: ['curiosity', 'anticipation', 'reveal satisfaction', 'want', 'justified desire'],
        avoidAtAllCosts: ['Specs without context', 'overpromising', 'no real demo', 'ignoring competition dishonestly']
      },

      'business-testimonial': {
        name: 'Testimonial/Case Study',
        category: 'business',
        references: ['Salesforce customer stories', 'Apple "Shot on iPhone"', 'Squarespace creators'],
        whatMakesItWork: 'Real people, real results. Specific over generic. The transformation story. Credible authenticity.',
        narrativeVoice: 'Let the customer speak. Minimal brand voice. Genuine, unscripted feeling.',
        signatureTechniques: [
          'The before/after with specifics',
          'The unexpected benefit',
          'Day-in-the-life authenticity',
          'Numbers that prove it',
          'The moment of realization'
        ],
        hookStyles: [
          'The transformation: "I went from X to Y"',
          'The skeptic converted: "I didn\'t believe it until..."',
          'The specific result: "In 3 months, we..."',
          'The peer recommendation: "If you\'re like me..."'
        ],
        visualGrammar: 'Real settings, authentic lighting, the person in their element, B-roll of them working/succeeding.',
        pacing: 'Problem â†’ Discovery â†’ Skepticism â†’ Trial â†’ Success â†’ Advocacy',
        emotionalBeats: ['relatability', 'recognition', 'hope', 'proof', 'inspiration'],
        avoidAtAllCosts: ['Script-reading', 'too polished', 'vague claims', 'no specific results']
      },

      // === SOCIAL MEDIA NATIVE GENRES ===
      'social-viral': {
        name: 'Viral/Hook-Driven',
        category: 'social',
        references: ['MrBeast hooks', 'TikTok trending sounds', 'Instagram Reels top performers'],
        whatMakesItWork: 'First 1 second stops the scroll. Pattern interrupt. Curiosity gap. Instant value promise.',
        narrativeVoice: 'High energy, direct, no preamble. "Here\'s the thing" energy.',
        signatureTechniques: [
          'The mid-action start',
          'The impossible thumbnail moment',
          'The countdown/list format',
          'The result shown first',
          'The loop - ending that makes you rewatch'
        ],
        hookStyles: [
          'The POV: "POV: You just discovered..."',
          'The controversial take: "Unpopular opinion..."',
          'The hack: "Stop doing X, do Y instead"',
          'The reaction: Start with the payoff expression'
        ],
        visualGrammar: 'Vertical, in-your-face, fast cuts, text overlays, trending effects.',
        pacing: 'No slow moments. Every second earns the next.',
        emotionalBeats: ['interrupt', 'hook', 'deliver', 'payoff/twist', 'call to action'],
        avoidAtAllCosts: ['Slow starts', 'burying the lead', 'inside jokes', 'horizontal thinking']
      },

      'social-storytime': {
        name: 'Storytime',
        category: 'social',
        references: ['Reddit stories on TikTok', 'True crime TikTok', 'Commentary channels'],
        whatMakesItWork: 'Bingeable narrative. Strategic cliffhangers. Personality of the teller. "And then it got worse" energy.',
        narrativeVoice: 'Conversational, expressive, reactive. Like telling a friend the craziest thing.',
        signatureTechniques: [
          'The teased ending: "What happened next changed everything"',
          'The reaction inserts',
          'Building to "the part"',
          'Strategic pauses for drama',
          'The satisfying callback'
        ],
        hookStyles: [
          'The chaos preview: "So there I was..."',
          'The rating: "This is a 10/10 crazy story"',
          'The category: "Insane customer stories, part 47"',
          'The question: "Has this ever happened to you?"'
        ],
        visualGrammar: 'Face-to-camera, expressive reactions, simple visuals that don\'t distract, maybe relevant B-roll.',
        pacing: 'Build tension, milk the drama, deliver the payoff, button with reaction.',
        emotionalBeats: ['hook', 'context', 'escalation', 'climax', 'reaction/callback'],
        avoidAtAllCosts: ['Spoiling the ending', 'too much context', 'underselling the payoff', 'no personality']
      },

      // === SERIES/EPISODIC GENRES ===
      'series-docuseries': {
        name: 'Docuseries',
        category: 'series',
        references: ['The Last Dance', 'Formula 1: Drive to Survive', 'Chef\'s Table', 'Abstract'],
        whatMakesItWork: 'Each episode complete but connected. Overarching narrative. Character development across episodes. The binge factor.',
        narrativeVoice: 'Consistent tone, evolving perspective. Characters become familiar.',
        signatureTechniques: [
          'The episode-end cliffhanger',
          'The season arc tease',
          'Character intro episodes that set up payoffs',
          'The callback to earlier episodes',
          'The previously on...'
        ],
        hookStyles: [
          'The season tease: Montage of what\'s coming',
          'The immediate drama: Drop into conflict',
          'The character hook: Who is this person?',
          'The question the season will answer'
        ],
        visualGrammar: 'Consistent visual language across episodes, signature shots, evolving as story evolves.',
        pacing: 'Each episode: setup, development, mini-climax, tease next. Season: build to finale.',
        emotionalBeats: ['per episode arc within larger season arc'],
        avoidAtAllCosts: ['Episodes that feel standalone', 'no payoff to setups', 'inconsistent quality', 'filler episodes']
      }
    };

    /**
     * CONTENT FORMAT MODIFIERS
     * How the genre adapts to different content lengths/platforms
     */
    const CONTENT_FORMATS = {
      'short-form': {
        name: 'Short-Form (< 60s)',
        platforms: ['TikTok', 'Reels', 'Shorts'],
        adaptations: 'Compress to essence. Hook in 0.5s. One key idea. Strong close loop.',
        pacingMultiplier: 1.5, // Faster
        sceneCount: '3-5 scenes',
        hookCritical: true
      },
      'medium-form': {
        name: 'Medium-Form (1-5 min)',
        platforms: ['YouTube', 'Instagram', 'LinkedIn'],
        adaptations: 'Full narrative arc possible. Multiple beats. Room for nuance.',
        pacingMultiplier: 1.0,
        sceneCount: '5-12 scenes',
        hookCritical: true
      },
      'long-form': {
        name: 'Long-Form (5-20 min)',
        platforms: ['YouTube', 'Podcast video', 'Course content'],
        adaptations: 'Deep exploration. Multiple sub-sections. Varied pacing. Retention strategies throughout.',
        pacingMultiplier: 0.8,
        sceneCount: '10-25 scenes',
        hookCritical: true
      },
      'episodic': {
        name: 'Episodic (Series)',
        platforms: ['YouTube series', 'Netflix-style', 'Course modules'],
        adaptations: 'Each episode complete but connected. Cliffhangers. Character development. Previously on...',
        pacingMultiplier: 0.9,
        sceneCount: 'Varies by episode',
        hookCritical: true
      }
    };

    /**
     * HOOK ARCHETYPES
     * Universal hook structures that work across genres
     */
    const HOOK_ARCHETYPES = {
      'curiosity-gap': {
        name: 'Curiosity Gap',
        structure: 'Reveal partial information that demands completion',
        examples: ['There\'s one thing about X that nobody talks about...', 'What happened next surprised everyone...'],
        bestFor: ['documentary', 'educational', 'mystery']
      },
      'pattern-interrupt': {
        name: 'Pattern Interrupt',
        structure: 'Start with something unexpected that breaks mental autopilot',
        examples: ['[Unexpected visual/sound]', 'Forget everything you know about X...'],
        bestFor: ['social', 'comedy', 'educational']
      },
      'identity-hook': {
        name: 'Identity Hook',
        structure: 'Appeal to who the viewer wants to be',
        examples: ['If you\'re the kind of person who...', 'Most people will scroll past this...'],
        bestFor: ['business', 'inspirational', 'tutorial']
      },
      'result-first': {
        name: 'Result First',
        structure: 'Show the payoff immediately, then explain how',
        examples: ['[Show amazing result] Want to know how?', 'I made $X in Y days. Here\'s exactly how.'],
        bestFor: ['tutorial', 'business', 'transformation']
      },
      'controversy': {
        name: 'Controversial Take',
        structure: 'State an opinion that challenges common belief',
        examples: ['X is actually bad for you. Here\'s why.', 'Unpopular opinion: [take]'],
        bestFor: ['educational', 'social', 'commentary']
      },
      'story-drop': {
        name: 'Story Drop',
        structure: 'Begin mid-story with high stakes',
        examples: ['So there I was, $10,000 in debt, when...', 'The day I almost lost everything...'],
        bestFor: ['story', 'testimonial', 'drama']
      },
      'the-list': {
        name: 'The List',
        structure: 'Promise a specific number of valuable items',
        examples: ['7 things I wish I knew before...', '3 mistakes that are costing you...'],
        bestFor: ['educational', 'tutorial', 'business']
      },
      'demonstration': {
        name: 'Demonstration',
        structure: 'Show don\'t tell - lead with action',
        examples: ['[Start with the experiment/demo]', 'Watch what happens when I...'],
        bestFor: ['science', 'tutorial', 'product']
      }
    };

    /**
     * ANTI-GENERIC RULES
     * Rules to ensure content never feels like a basic presentation
     */
    const ANTI_GENERIC_RULES = [
      'Never start with "Hey guys" or "What\'s up everyone" - earn attention first',
      'Never use stock transitions without purpose',
      'Never explain what you\'re about to explain - just explain it',
      'Never use generic background music that doesn\'t match mood',
      'Never have talking head without B-roll for more than 10 seconds',
      'Never use bullet points on screen - visualize concepts instead',
      'Never end with "Don\'t forget to like and subscribe" as the main CTA',
      'Never have dead air without intentional purpose',
      'Never use the first take if authenticity isn\'t the point',
      'Never describe what viewers can already see',
      'Never rush through the payoff after slow buildup',
      'Never have consistent energy throughout - vary rhythm'
    ];

    // Get genre settings if specified
    const genreKey = config.genre || null;
    const genreSettings = genreKey ? GENRE_REFERENCE_LIBRARY[genreKey] : null;
    const formatKey = config.contentFormat || 'medium-form';
    const formatSettings = CONTENT_FORMATS[formatKey] || CONTENT_FORMATS['medium-form'];

    // === HOLLYWOOD PRODUCTION MODES ===
    // Cinematic templates for professional-quality video storytelling
    const productionModes = {
      standard: {
        name: 'Standard',
        description: 'Clean, professional content',
        narrativeStyle: 'straightforward informative narration',
        visualApproach: 'clean compositions with good lighting',
        emotionalArc: 'neutral to mildly engaging',
        openingStyle: 'direct hook with value proposition',
        closingStyle: 'clear call-to-action',
        cameraDirections: [],
        musicMood: 'upbeat corporate',
        specialInstructions: ''
      },
      documentary: {
        name: 'Documentary',
        description: 'Ken Burns style - educational, authoritative, visually rich',
        narrativeStyle: 'authoritative narrator voice, like David Attenborough or Morgan Freeman. Use present tense for immediacy. Include moments of wonder and discovery.',
        visualApproach: 'Ken Burns effect on historical images, slow zooms on details, wide establishing shots, intimate close-ups. Think National Geographic or BBC Earth.',
        emotionalArc: 'curiosity â†’ discovery â†’ understanding â†’ appreciation',
        openingStyle: 'Start with a powerful image and a thought-provoking observation. No greeting - dive straight into the subject.',
        closingStyle: 'End with broader implications or a reflective thought that lingers',
        cameraDirections: ['Zoom in', 'Pan left', 'Pan right', 'Static shot'],
        musicMood: 'ambient orchestral, contemplative',
        specialInstructions: 'Use specific details and numbers. Reference real places, people, or events. Include "moments of pause" where visuals speak alone.'
      },
      thriller: {
        name: 'Thriller/Mystery',
        description: 'Suspenseful, revelation-driven, keeps viewers on edge',
        narrativeStyle: 'mysterious, building tension with each scene. Use short punchy sentences. Strategic pauses. Rhetorical questions that haunt.',
        visualApproach: 'Dramatic shadows, noir lighting, tight framing that creates claustrophobia. Dutch angles for unease. Slow reveals.',
        emotionalArc: 'intrigue â†’ tension â†’ escalation â†’ twist â†’ resolution',
        openingStyle: 'Start mid-mystery. Something is wrong. A question that demands answers.',
        closingStyle: 'The revelation. But leave one thread hanging - a final question.',
        cameraDirections: ['Push in', 'Zoom in', 'Static shot', 'Tracking shot'],
        musicMood: 'tense, pulsing, minimal',
        specialInstructions: 'Build tension through information withholding. Each scene reveals a piece but raises new questions. Use "But then..." and "What they didn\'t know was..." transitions.'
      },
      inspirational: {
        name: 'Inspirational',
        description: 'Uplifting, motivational, emotionally resonant',
        narrativeStyle: 'warm, encouraging, building towards triumph. Use "you" directly - speak to the viewer. Share the struggle before the victory.',
        visualApproach: 'Golden hour lighting, upward camera angles suggesting aspiration, wide open spaces, people in silhouette against bright backgrounds.',
        emotionalArc: 'challenge â†’ struggle â†’ breakthrough â†’ triumph â†’ call to action',
        openingStyle: 'Start with a relatable struggle or universal dream. "You\'ve felt this..." or "Imagine if..."',
        closingStyle: 'End with empowerment. The viewer should feel capable of anything.',
        cameraDirections: ['Tilt up', 'Pull out', 'Push in', 'Pedestal up'],
        musicMood: 'building orchestral, uplifting crescendo',
        specialInstructions: 'Include a specific transformation story. Use contrast between "before" and "after" states. The protagonist overcomes through their own agency.'
      },
      story: {
        name: 'Story/Narrative',
        description: 'Character-driven, arc-based storytelling',
        narrativeStyle: 'storyteller voice with character moments. Use dialogue snippets. Describe actions, not just concepts. "Show don\'t tell."',
        visualApproach: 'Cinematic compositions like a movie. Character close-ups for emotion, wide shots for context. Visual metaphors that echo themes.',
        emotionalArc: 'setup â†’ inciting incident â†’ rising action â†’ climax â†’ resolution',
        openingStyle: 'In media res - start in the middle of action. "The day everything changed started like any other..."',
        closingStyle: 'The lesson learned. Circle back to the opening with new meaning.',
        cameraDirections: ['Tracking shot', 'Push in', 'Pull out', 'Pan left', 'Pan right'],
        musicMood: 'emotional score, thematic',
        specialInstructions: 'Create a protagonist (can be the viewer, a person, or even a concept). Include a clear antagonist or obstacle. Use the three-act structure within your scene count.'
      },
      cinematic: {
        name: 'Cinematic',
        description: 'High production value, film-quality storytelling',
        narrativeStyle: 'sparse, powerful narration. Let visuals carry emotion. Every word is deliberate. Poetic but not pretentious.',
        visualApproach: 'Movie-quality compositions. Anamorphic style, dramatic lighting contrasts, motivated camera movement. Think Nolan, Villeneuve, Fincher.',
        emotionalArc: 'immersion â†’ building atmosphere â†’ emotional peak â†’ contemplation',
        openingStyle: 'A striking visual with minimal or no narration for the first 3 seconds. Sound design matters.',
        closingStyle: 'Land on a powerful final image. The last frame should be iconic.',
        cameraDirections: ['Push in', 'Pull out', 'Tracking shot', 'Tilt up', 'Zoom in'],
        musicMood: 'cinematic score, hans zimmer style',
        specialInstructions: 'Plan for 30% more visual-only moments than other modes. Use aspect ratio to full effect. Include at least one visual metaphor. Color grade suggestions in visual descriptions (teal/orange, desaturated, high contrast).'
      }
    };

    const productionSettings = productionModes[productionMode] || productionModes.standard;

    // ============================================================
    // CINEMATIC PRODUCTION ARCHITECTURE
    // This transforms output from "presentation with voiceover" to
    // "actual film/TV production" with proper scene types and audio
    // ============================================================

    // ============================================================
    // NARRATIVE ARCHITECTURE SYSTEM
    // The foundation for diverse, Hollywood-quality storytelling
    // All downstream generation (concepts, scripts, visuals) references this
    // ============================================================

    /**
     * NARRATIVE STRUCTURES - Different story patterns beyond Hero's Journey
     * Each concept should be assigned ONE of these to ensure diversity
     */
    const NARRATIVE_STRUCTURES = {
      'heros_journey': {
        name: "Hero's Journey",
        description: 'Single protagonist transforms through trials',
        protagonistCount: 1,
        keyBeats: ['ordinary_world', 'call_to_adventure', 'refusal', 'mentor', 'crossing_threshold', 'tests', 'ordeal', 'reward', 'return'],
        examples: ['Lord of the Rings', 'Star Wars', 'The Lion King'],
        bestFor: ['epic', 'adventure', 'fantasy', 'coming_of_age'],
        sceneDistribution: { dialogue: 30, action: 25, emotional: 20, montage: 10, revelation: 15 }
      },
      'ensemble': {
        name: 'Ensemble',
        description: 'Group of 3-6 characters with equal narrative weight, interconnected storylines',
        protagonistCount: '3-6',
        keyBeats: ['introduce_each', 'establish_connections', 'individual_conflicts', 'convergence_point', 'group_challenge', 'resolution_per_character'],
        examples: ["Ocean's Eleven", 'Guardians of the Galaxy', 'The Breakfast Club', 'Avengers'],
        bestFor: ['heist', 'team_mission', 'workplace', 'found_family'],
        sceneDistribution: { dialogue: 45, action: 20, emotional: 20, montage: 5, revelation: 10 }
      },
      'dual_protagonist': {
        name: 'Dual Protagonist',
        description: 'Two leads with different/conflicting goals, parallel journeys',
        protagonistCount: 2,
        keyBeats: ['introduce_both', 'establish_conflict', 'parallel_struggles', 'intersection', 'confrontation_or_union', 'dual_resolution'],
        examples: ['Heat', 'The Prestige', 'Marriage Story', 'Challengers'],
        bestFor: ['rivalry', 'romance', 'cat_and_mouse', 'opposing_ideologies'],
        sceneDistribution: { dialogue: 40, action: 15, emotional: 30, montage: 5, revelation: 10 }
      },
      'mystery_procedural': {
        name: 'Mystery/Procedural',
        description: 'Uncovering truth about an event, clue by clue',
        protagonistCount: '1-2',
        keyBeats: ['inciting_crime', 'investigation_begins', 'false_leads', 'key_discovery', 'suspect_confrontation', 'truth_revealed'],
        examples: ['Knives Out', 'Se7en', 'Only Murders in the Building', 'Sherlock'],
        bestFor: ['detective', 'thriller', 'crime', 'whodunit'],
        sceneDistribution: { dialogue: 40, action: 10, emotional: 15, montage: 10, revelation: 25 }
      },
      'tragedy': {
        name: 'Tragedy',
        description: 'Protagonist undone by fatal flaw, descent from grace',
        protagonistCount: 1,
        keyBeats: ['greatness_established', 'flaw_introduced', 'temptation', 'wrong_choice', 'consequences_build', 'downfall', 'recognition_too_late'],
        examples: ['Breaking Bad', 'Scarface', 'Macbeth', 'The Godfather'],
        bestFor: ['crime_drama', 'power_corruption', 'cautionary_tale'],
        sceneDistribution: { dialogue: 35, action: 20, emotional: 25, montage: 5, revelation: 15 }
      },
      'rebirth': {
        name: 'Rebirth',
        description: 'Dark/flawed character transforms to redemption',
        protagonistCount: 1,
        keyBeats: ['darkness_shown', 'catalyst_for_change', 'resistance', 'gradual_awakening', 'sacrifice_or_action', 'new_person_emerges'],
        examples: ['A Christmas Carol', 'Groundhog Day', 'Schindlers List', 'Iron Man'],
        bestFor: ['redemption', 'holiday', 'second_chance', 'transformation'],
        sceneDistribution: { dialogue: 35, action: 15, emotional: 35, montage: 5, revelation: 10 }
      },
      'comedy_of_errors': {
        name: 'Comedy of Errors',
        description: 'Misunderstandings and mishaps lead to chaotic but happy resolution',
        protagonistCount: '2-4',
        keyBeats: ['setup_situation', 'initial_misunderstanding', 'complications_multiply', 'peak_chaos', 'truth_emerges', 'happy_resolution'],
        examples: ['Bridesmaids', 'The Hangover', 'Superbad', 'Some Like It Hot'],
        bestFor: ['romantic_comedy', 'buddy_comedy', 'farce', 'situational'],
        sceneDistribution: { dialogue: 50, action: 15, emotional: 15, montage: 10, revelation: 10 }
      },
      'anthology_connected': {
        name: 'Anthology/Hyperlink',
        description: 'Multiple separate stories connected by theme, location, or event',
        protagonistCount: '4-8',
        keyBeats: ['introduce_story_a', 'introduce_story_b', 'introduce_story_c', 'develop_parallels', 'connection_revealed', 'thematic_resolution'],
        examples: ['Love Actually', 'Crash', 'Magnolia', 'Cloud Atlas'],
        bestFor: ['thematic_exploration', 'city_portrait', 'time_spanning', 'interconnected'],
        sceneDistribution: { dialogue: 40, action: 10, emotional: 30, montage: 10, revelation: 10 }
      },
      'siege_survival': {
        name: 'Siege/Survival',
        description: 'Group trapped, must survive against external threat',
        protagonistCount: '3-8',
        keyBeats: ['normalcy', 'threat_arrives', 'trapped', 'initial_defense', 'losses', 'internal_conflict', 'final_stand', 'aftermath'],
        examples: ['Die Hard', 'Alien', 'The Thing', '10 Cloverfield Lane'],
        bestFor: ['horror', 'action', 'thriller', 'disaster'],
        sceneDistribution: { dialogue: 25, action: 35, emotional: 20, montage: 5, revelation: 15 }
      },
      'quest_journey': {
        name: 'Quest/Journey',
        description: 'Travel toward goal, episodic obstacles, companions gathered',
        protagonistCount: '1-4',
        keyBeats: ['quest_given', 'journey_begins', 'first_obstacle', 'ally_gained', 'setback', 'dark_moment', 'final_approach', 'goal_achieved'],
        examples: ['Finding Nemo', 'Mad Max Fury Road', 'The Wizard of Oz', 'Road Trip'],
        bestFor: ['adventure', 'road_movie', 'coming_of_age', 'fantasy'],
        sceneDistribution: { dialogue: 30, action: 30, emotional: 20, montage: 10, revelation: 10 }
      },
      'nonlinear_puzzle': {
        name: 'Non-Linear/Puzzle',
        description: 'Story told out of order, audience pieces together truth',
        protagonistCount: '1-3',
        keyBeats: ['fragment_present', 'fragment_past', 'connection_hints', 'revelation_piece', 'timeline_converges', 'full_picture'],
        examples: ['Pulp Fiction', 'Memento', 'Arrival', '21 Grams'],
        bestFor: ['psychological', 'mystery', 'art_house', 'thriller'],
        sceneDistribution: { dialogue: 35, action: 15, emotional: 25, montage: 5, revelation: 20 }
      }
    };

    /**
     * STORY ENGINES - What mechanism drives ongoing story/scenes forward
     * Determines HOW scenes generate conflict and progression
     */
    const STORY_ENGINES = {
      'case_of_week': {
        name: 'Case/Problem of the Week',
        description: 'Each segment presents new case/problem to solve',
        examples: ['House', 'CSI', 'Black Mirror'],
        sceneGenerator: 'Present problem â†’ Investigate â†’ Complication â†’ Solve',
        bestPairedWith: ['mystery_procedural', 'anthology_connected']
      },
      'relationship_dynamics': {
        name: 'Relationship Dynamics',
        description: 'Character interactions and evolving relationships drive plot',
        examples: ['Friends', 'The Office', 'Marriage Story'],
        sceneGenerator: 'Character A wants X â†’ Character B conflicts â†’ Negotiation/Growth',
        bestPairedWith: ['ensemble', 'dual_protagonist', 'comedy_of_errors']
      },
      'escalating_threat': {
        name: 'Escalating External Threat',
        description: 'Outside force grows stronger, raising stakes continuously',
        examples: ['Breaking Bad', 'Game of Thrones', 'The Walking Dead'],
        sceneGenerator: 'Threat appears â†’ Characters respond â†’ Threat escalates â†’ Bigger response needed',
        bestPairedWith: ['siege_survival', 'heros_journey', 'tragedy']
      },
      'mystery_layers': {
        name: 'Mystery Unraveling',
        description: 'Each scene peels back layer of central mystery',
        examples: ['Lost', 'Westworld', 'Dark'],
        sceneGenerator: 'Question posed â†’ Partial answer â†’ Bigger question revealed',
        bestPairedWith: ['mystery_procedural', 'nonlinear_puzzle']
      },
      'transformation_arc': {
        name: 'Character Transformation',
        description: 'Character change is the engine - each scene shows evolution',
        examples: ['Breaking Bad', 'Mad Men', 'Fleabag'],
        sceneGenerator: 'Character at state A â†’ Challenge â†’ Character moves toward state B',
        bestPairedWith: ['tragedy', 'rebirth', 'heros_journey']
      },
      'quest_progress': {
        name: 'Quest/Goal Progress',
        description: 'Movement toward clear objective drives each scene',
        examples: ['Lord of the Rings', 'Inception', 'Oceans Eleven'],
        sceneGenerator: 'Objective clear â†’ Obstacle appears â†’ Overcome/Adapt â†’ Closer to goal',
        bestPairedWith: ['quest_journey', 'ensemble', 'heros_journey']
      },
      'survival_pressure': {
        name: 'Survival Pressure',
        description: 'Constant threat keeps characters (and audience) on edge',
        examples: ['The Martian', 'Gravity', 'A Quiet Place'],
        sceneGenerator: 'Safety threatened â†’ Immediate response â†’ Brief respite â†’ New threat',
        bestPairedWith: ['siege_survival', 'quest_journey']
      },
      'social_dynamics': {
        name: 'Social/Power Dynamics',
        description: 'Shifting alliances, status, and power relationships',
        examples: ['Succession', 'Game of Thrones', 'Mean Girls'],
        sceneGenerator: 'Power balance â†’ Challenge to hierarchy â†’ Shift â†’ New balance',
        bestPairedWith: ['ensemble', 'tragedy', 'dual_protagonist']
      }
    };

    /**
     * CHARACTER_ARCHETYPES - Rich variety beyond "reluctant hero"
     * Each story should use MULTIPLE archetypes for depth
     */
    const CHARACTER_ARCHETYPES = {
      // PRIMARY PROTAGONISTS
      'reluctant_hero': { role: 'protagonist', description: 'Ordinary person thrust into extraordinary circumstances', flaw: 'self-doubt', arc: 'discovers inner strength' },
      'chosen_one': { role: 'protagonist', description: 'Destined for greatness, must accept responsibility', flaw: 'denial of destiny', arc: 'embraces purpose' },
      'anti_hero': { role: 'protagonist', description: 'Morally gray, does wrong things for understandable reasons', flaw: 'moral compromise', arc: 'finds line they wont cross or crosses it' },
      'tragic_hero': { role: 'protagonist', description: 'Great person with fatal flaw leading to downfall', flaw: 'hubris/obsession', arc: 'falls from grace' },
      'everyman': { role: 'protagonist', description: 'Relatable ordinary person, audience surrogate', flaw: 'passivity', arc: 'takes agency' },
      'wounded_healer': { role: 'protagonist', description: 'Damaged person who helps others heal', flaw: 'cant help self', arc: 'heals self through helping others' },

      // SUPPORTING CHARACTERS
      'mentor': { role: 'supporting', description: 'Wise guide who prepares hero', flaw: 'hidden past', arc: 'often sacrifices for hero' },
      'trickster': { role: 'supporting', description: 'Comic relief who speaks uncomfortable truths', flaw: 'unreliable', arc: 'proves loyal when it matters' },
      'loyal_friend': { role: 'supporting', description: 'Steadfast companion through all trials', flaw: 'blind loyalty', arc: 'must choose between loyalties' },
      'love_interest': { role: 'supporting', description: 'Romantic connection that humanizes protagonist', flaw: 'own agenda', arc: 'becomes partner not prize' },
      'rival': { role: 'supporting', description: 'Competitor who pushes protagonist to excel', flaw: 'jealousy', arc: 'becomes ally or doubles down' },
      'innocent': { role: 'supporting', description: 'Pure character who represents whats worth fighting for', flaw: 'naivety', arc: 'loss of innocence or protected' },

      // ANTAGONISTS
      'mastermind': { role: 'antagonist', description: 'Intelligent planner, always steps ahead', flaw: 'arrogance', arc: 'undone by underestimating others' },
      'force_of_nature': { role: 'antagonist', description: 'Unstoppable, almost elemental threat', flaw: 'single-minded', arc: 'finally met with equal force' },
      'fallen_hero': { role: 'antagonist', description: 'Once good, now corrupted by tragedy or power', flaw: 'twisted virtue', arc: 'redeemed or destroyed by former self' },
      'mirror_villain': { role: 'antagonist', description: 'What protagonist could become if they fail', flaw: 'same as hero', arc: 'shows hero the path not to take' },
      'system': { role: 'antagonist', description: 'Institution, society, or system as antagonist', flaw: 'inflexibility', arc: 'reformed or destroyed' },
      'inner_demon': { role: 'antagonist', description: 'Protagonists own nature/past/addiction as enemy', flaw: 'self-destructive', arc: 'conquered or succumbed to' },

      // ENSEMBLE-SPECIFIC
      'leader': { role: 'ensemble', description: 'Takes charge, makes hard decisions', flaw: 'burden of command', arc: 'learns to trust others' },
      'specialist': { role: 'ensemble', description: 'Expert in crucial skill', flaw: 'limited perspective', arc: 'grows beyond specialty' },
      'heart': { role: 'ensemble', description: 'Emotional center, keeps group human', flaw: 'too empathetic', arc: 'learns necessary hardness' },
      'wildcard': { role: 'ensemble', description: 'Unpredictable element, chaos agent', flaw: 'unreliable', arc: 'commits when it counts' },
      'skeptic': { role: 'ensemble', description: 'Questions everything, voice of caution', flaw: 'cynicism', arc: 'becomes believer' },
      'newbie': { role: 'ensemble', description: 'New to the group, audience surrogate', flaw: 'inexperience', arc: 'earns place' }
    };

    /**
     * PRODUCTION_BIBLE_TEMPLATE - Unified structure all generation references
     * Created during concept phase, used by script, visuals, audio
     */
    const PRODUCTION_BIBLE_TEMPLATE = {
      // Core Identity
      logline: '', // One sentence that sells the story
      premise: '', // What if question that drives everything
      theme: '', // What its REALLY about beneath the surface
      tone: '', // How it FEELS (dark comedy, hopeful drama, etc)

      // Narrative Architecture
      narrativeStructure: '', // Key from NARRATIVE_STRUCTURES
      storyEngine: '', // Key from STORY_ENGINES
      actStructure: { // How the story breaks into acts
        act1: { purpose: 'setup', percentOfRuntime: 25 },
        act2a: { purpose: 'rising_action', percentOfRuntime: 25 },
        act2b: { purpose: 'complications', percentOfRuntime: 25 },
        act3: { purpose: 'resolution', percentOfRuntime: 25 }
      },

      // Character Bible
      characters: [
        // Each character: { name, archetype, role, flaw, arc, relationships, visualDescription, voiceDescription }
      ],
      characterRelationships: [], // Array of { char1, char2, relationshipType, tension }

      // World Bible
      worldRules: {
        setting: '', // Where and when
        uniqueMechanic: '', // What makes this world special
        limitations: [], // What CAN'T happen in this world
        visualLanguage: '' // How this world should LOOK
      },

      // Visual Bible
      visualStyle: {
        colorPalette: '', // Dominant colors and why
        lightingApproach: '', // High key, low key, natural, etc
        cameraPhilosophy: '', // Handheld vs steady, close vs wide, etc
        referenceFilms: [] // Visual touchstones
      },

      // Audio Bible
      audioStyle: {
        musicGenre: '', // Score style
        dialogueApproach: '', // Naturalistic, stylized, minimal
        signatureSounds: [] // Recurring audio motifs
      }
    };

    // =========================================================================
    // LOCATION BIBLE SYSTEM - Cinematic Location Consistency
    // Ensures locations look consistent across scenes with proper state tracking
    // =========================================================================

    /**
     * LOCATION_ARCHETYPES - Common cinematic location types with base visual DNA
     * These provide starting points for rich location descriptions
     */
    const LOCATION_ARCHETYPES = {
      // INTERIOR LOCATIONS
      'dojo_traditional': {
        category: 'interior',
        baseElements: [
          'Weathered wooden beam construction',
          'Paper shoji screens filtering light',
          'Worn tatami mats with training patterns',
          'Ancestral weapons mounted on walls',
          'Central open training area',
          'Faded calligraphy scrolls on pillars'
        ],
        colorPalette: 'Warm wood tones, cream paper, aged brass, deep shadows',
        atmosphere: 'Sacred discipline, echoes of generations, timeless tradition',
        lightingSuggestions: {
          day: 'Soft diffused light through paper screens, dust motes visible',
          night: 'Warm lantern glow, deep shadows in corners, meditation ambiance',
          dramatic: 'Single overhead source, dramatic chiaroscuro, tension'
        }
      },
      'corporate_modern': {
        category: 'interior',
        baseElements: [
          'Floor-to-ceiling glass windows',
          'Sleek minimalist furniture',
          'Polished concrete or marble floors',
          'Ambient LED lighting strips',
          'Large digital displays',
          'Open floor plan with private offices'
        ],
        colorPalette: 'Cool grays, chrome accents, blue-white lighting, occasional warm wood',
        atmosphere: 'Power, control, cold efficiency, wealth',
        lightingSuggestions: {
          day: 'Bright natural light, city views, clean and clinical',
          night: 'Dramatic city lights through windows, desk lamps, moody blue',
          dramatic: 'Silhouettes against windows, harsh overhead, noir feeling'
        }
      },
      'abandoned_industrial': {
        category: 'interior',
        baseElements: [
          'Rusted metal beams and catwalks',
          'Broken skylights with vegetation growth',
          'Abandoned machinery and equipment',
          'Graffiti on crumbling walls',
          'Pools of standing water',
          'Debris and scattered objects'
        ],
        colorPalette: 'Rust oranges, concrete gray, toxic greens, faded industrial colors',
        atmosphere: 'Decay, danger, forgotten purpose, urban exploration',
        lightingSuggestions: {
          day: 'Shafts of light through broken skylights, dust particles, god rays',
          night: 'Moonlight through gaps, flashlight beams, emergency lighting',
          dramatic: 'Single light source, deep shadows, horror tension'
        }
      },
      'cozy_home': {
        category: 'interior',
        baseElements: [
          'Comfortable lived-in furniture',
          'Personal photos and memorabilia',
          'Warm textiles and rugs',
          'Kitchen visible with cooking elements',
          'Books and personal items scattered naturally',
          'Plants and natural elements'
        ],
        colorPalette: 'Warm earth tones, soft fabrics, golden lighting, personal colors',
        atmosphere: 'Safety, memory, family, emotional anchor',
        lightingSuggestions: {
          day: 'Warm sunlight through curtains, domestic comfort',
          night: 'Lamp light pools, TV glow, intimate warmth',
          dramatic: 'Single lamp, shadows of memory, emotional weight'
        }
      },
      'high_tech_lab': {
        category: 'interior',
        baseElements: [
          'Banks of monitors and screens',
          'Holographic or projected displays',
          'Clean white surfaces',
          'Complex equipment and machinery',
          'Server racks with blinking lights',
          'Sterile controlled environment'
        ],
        colorPalette: 'Clinical white, blue accent lighting, green data streams, chrome',
        atmosphere: 'Innovation, danger of technology, sterile progress',
        lightingSuggestions: {
          day: 'Bright clinical lighting, no shadows, clean',
          night: 'Screen glow, blue ambient, focused work lights',
          dramatic: 'Emergency red lighting, system alerts, tension'
        }
      },

      // EXTERIOR LOCATIONS
      'urban_street': {
        category: 'exterior',
        baseElements: [
          'Mixed architecture buildings',
          'Street lights and signage',
          'Vehicles parked or moving',
          'Sidewalks with pedestrians',
          'Urban infrastructure (fire hydrants, mailboxes)',
          'Visible sky between buildings'
        ],
        colorPalette: 'Concrete gray, neon accents at night, varied building colors',
        atmosphere: 'Urban energy, anonymity, life in motion',
        lightingSuggestions: {
          day: 'Direct sunlight with building shadows, harsh urban light',
          night: 'Neon signs, street lamps, car headlights, wet reflections',
          dramatic: 'Rain-slicked streets, isolated pools of light, noir'
        }
      },
      'forest_ancient': {
        category: 'exterior',
        baseElements: [
          'Towering ancient trees with thick canopy',
          'Filtered dappled light through leaves',
          'Moss-covered fallen logs',
          'Natural paths through undergrowth',
          'Visible wildlife or signs of it',
          'Mist or fog in low areas'
        ],
        colorPalette: 'Deep greens, brown earth, golden light shafts, silver mist',
        atmosphere: 'Mystery, ancient wisdom, nature untamed, spiritual',
        lightingSuggestions: {
          day: 'God rays through canopy, dappled light patterns, ethereal',
          night: 'Moonlight filtering, bioluminescence possible, mysterious',
          dramatic: 'Storm light, ominous shadows, nature threatening'
        }
      },
      'rooftop_urban': {
        category: 'exterior',
        baseElements: [
          'HVAC units and utility structures',
          'Low walls or railings at edge',
          'City skyline visible',
          'Water towers or antenna structures',
          'Gravel or membrane roofing surface',
          'Access door/stairwell structure'
        ],
        colorPalette: 'Gray industrial, city lights colors at night, sky gradient',
        atmosphere: 'Isolation above the world, confrontation space, transition',
        lightingSuggestions: {
          day: 'Bright open sky, harsh shadows from structures',
          night: 'City glow from below, dramatic sky, isolated',
          dramatic: 'Backlit by city, silhouettes, wind-swept'
        }
      },
      'beach_coastal': {
        category: 'exterior',
        baseElements: [
          'Sandy shore meeting water',
          'Wave patterns and foam',
          'Driftwood and natural debris',
          'Horizon line where sea meets sky',
          'Possible cliffs or vegetation at edge',
          'Shells and stones scattered'
        ],
        colorPalette: 'Sand beige, ocean blue-green, white foam, sky gradients',
        atmosphere: 'Freedom, contemplation, edge of world, emotional release',
        lightingSuggestions: {
          day: 'Bright reflections, harsh sun, squinting light',
          night: 'Moonlight on water, silvery tones, romantic',
          dramatic: 'Storm approaching, dark water, ominous sky, golden hour'
        }
      }
    };

    /**
     * LOCATION_STATE_MODIFIERS - How locations change between scenes
     */
    const LOCATION_STATE_MODIFIERS = {
      timeOfDay: {
        'dawn': {
          lighting: 'Soft pink and orange light emerging, long shadows, quiet awakening',
          mood: 'Hope, new beginning, quiet before the world wakes',
          colorShift: 'Warm pink-orange cast, soft contrast'
        },
        'morning': {
          lighting: 'Bright clear light, shorter shadows, fresh illumination',
          mood: 'Energy, possibility, clarity, productive',
          colorShift: 'Clean neutral tones, slight warmth'
        },
        'midday': {
          lighting: 'Harsh overhead sun, minimal shadows, bright exposure',
          mood: 'Intensity, nowhere to hide, confrontation',
          colorShift: 'High contrast, washed highlights'
        },
        'afternoon': {
          lighting: 'Warm angled light, lengthening shadows, golden tones',
          mood: 'Warmth, comfort, passing time, reflection',
          colorShift: 'Golden warm cast, rich colors'
        },
        'golden_hour': {
          lighting: 'Deep orange-gold light, long dramatic shadows, magical quality',
          mood: 'Romance, beauty, fleeting perfection, emotional peak',
          colorShift: 'Strong orange-gold, deep shadows, cinematic'
        },
        'dusk': {
          lighting: 'Fading light, blue hour beginning, mixed warm and cool',
          mood: 'Transition, melancholy, ending, uncertain',
          colorShift: 'Purple and blue tones emerging, warm pockets'
        },
        'night': {
          lighting: 'Artificial sources dominate, pools of light in darkness',
          mood: 'Mystery, danger, intimacy, secrets',
          colorShift: 'Cool blue overall, warm practical lights'
        },
        'deep_night': {
          lighting: 'Minimal light, moonlight or starlight only, near darkness',
          mood: 'Isolation, fear, contemplation, stillness',
          colorShift: 'Desaturated, silver-blue, high contrast'
        }
      },
      weather: {
        'clear': {
          atmosphere: 'Crisp visibility, clean air, normal conditions',
          particles: 'None or minimal dust motes',
          surfaces: 'Dry, normal textures'
        },
        'overcast': {
          atmosphere: 'Soft diffused light, gray sky, muted mood',
          particles: 'None',
          surfaces: 'Flat lighting on all surfaces'
        },
        'rain_light': {
          atmosphere: 'Gentle rain visible, wet surfaces, reflections',
          particles: 'Rain droplets, splashes',
          surfaces: 'Wet reflective surfaces, water streams'
        },
        'rain_heavy': {
          atmosphere: 'Intense downpour, limited visibility, dramatic',
          particles: 'Heavy rain sheets, mist, splashing',
          surfaces: 'Flooding, intense reflections, waterfall effects'
        },
        'fog': {
          atmosphere: 'Limited visibility, mysterious, sound muffled',
          particles: 'Thick atmospheric haze, wisps',
          surfaces: 'Moisture condensation, soft edges'
        },
        'snow': {
          atmosphere: 'White falling particles, muffled sounds, cold',
          particles: 'Snowflakes drifting, accumulation',
          surfaces: 'Snow coverage, frost, ice formations'
        },
        'storm': {
          atmosphere: 'Dark clouds, lightning flashes, wind visible, dramatic',
          particles: 'Wind-blown debris, rain, leaves',
          surfaces: 'Wet, damaged, wind-affected'
        }
      },
      condition: {
        'pristine': {
          description: 'Perfect original state, well-maintained, untouched',
          visualChanges: 'Clean surfaces, everything in place, no damage'
        },
        'lived_in': {
          description: 'Normal use wear, personal touches, comfortable',
          visualChanges: 'Minor clutter, some wear patterns, personal items visible'
        },
        'neglected': {
          description: 'Lack of maintenance, dust accumulation, fading',
          visualChanges: 'Dust layers, cobwebs, faded colors, minor disrepair'
        },
        'damaged_light': {
          description: 'Minor damage from conflict or accident',
          visualChanges: 'Broken items, scattered debris, scorch marks, cracks'
        },
        'damaged_heavy': {
          description: 'Significant structural damage, aftermath of major event',
          visualChanges: 'Collapsed sections, major debris, fire damage, destruction'
        },
        'destroyed': {
          description: 'Near-total destruction, barely recognizable',
          visualChanges: 'Ruins, rubble, smoke, complete devastation'
        },
        'repaired': {
          description: 'Fixed after damage, signs of restoration',
          visualChanges: 'New materials mixed with old, visible repair work, healing'
        },
        'transformed': {
          description: 'Location has changed purpose or been significantly altered',
          visualChanges: 'New elements added, repurposed space, evolution visible'
        }
      }
    };

    /**
     * SHOT_COMPOSITION_TEMPLATES - Cinematic framing templates for scene types
     * These define camera positioning, depth of field, and visual rhythm
     */
    const SHOT_COMPOSITION_TEMPLATES = {
      'hero_entrance': {
        description: 'Character enters frame with impact',
        composition: 'Low angle, character silhouette against sky/light, slow push in',
        dof: 'Deep focus transitioning to shallow on subject',
        movement: 'Slow dolly forward or crane up',
        timing: 'Hold wide for 2 beats, then reveal'
      },
      'confrontation': {
        description: 'Two opposing forces face off',
        composition: 'Over-the-shoulder alternating, tight close-ups, eye-level',
        dof: 'Shallow on active speaker, rack focus on reaction',
        movement: 'Static with subtle push on tension moments',
        timing: 'Quick cuts building pace, then hold on decisive moment'
      },
      'emotional_peak': {
        description: 'Character experiences breakthrough or breakdown',
        composition: 'Extreme close-up on eyes, then pull back to reveal',
        dof: 'Ultra-shallow, background completely dissolved',
        movement: 'Subtle orbit or slow zoom out',
        timing: 'Extended hold, breathing room'
      },
      'action_sequence': {
        description: 'High energy physical action',
        composition: 'Wide establishing, then tight coverage, Dutch angles for chaos',
        dof: 'Deep focus for spatial awareness',
        movement: 'Handheld energy, whip pans, tracking shots',
        timing: 'Rapid cuts, 1-2 second average shot length'
      },
      'revelation': {
        description: 'Secret or truth is exposed',
        composition: 'Start tight on reactor, then reveal the cause',
        dof: 'Rack focus from face to revealed object/person',
        movement: 'Slow push or pull depending on emotional valence',
        timing: 'Build anticipation, then snap reveal'
      },
      'intimate_dialogue': {
        description: 'Two characters share a personal moment',
        composition: 'Two-shot favoring space between, soft lighting',
        dof: 'Medium shallow, both subjects in focus',
        movement: 'Static or gentle float',
        timing: 'Longer takes, let performances breathe'
      },
      'establishing_world': {
        description: 'Introduce location or setting',
        composition: 'Epic wide, then guided tour of details',
        dof: 'Deep focus showcasing environment',
        movement: 'Crane, drone, or slow pan revealing scope',
        timing: 'Allow time to absorb visual information'
      },
      'montage_beat': {
        description: 'Single moment in a sequence',
        composition: 'Centered subject, clean background, iconic framing',
        dof: 'Medium depth, subject isolated',
        movement: 'Static or simple push/pull',
        timing: '2-4 seconds per beat, rhythm driven'
      },
      'tension_build': {
        description: 'Something bad is about to happen',
        composition: 'Wide with empty space suggesting threat, or claustrophobic tight',
        dof: 'Deep focus creating paranoia of unseen threat',
        movement: 'Slow creeping dolly, or locked-off stillness',
        timing: 'Extended takes, uncomfortable holds'
      },
      'resolution': {
        description: 'Story threads come together',
        composition: 'Balanced frame, subjects in harmony, natural light',
        dof: 'Medium depth, world in soft focus behind',
        movement: 'Gentle pullback or ascending crane',
        timing: 'Measured pace, satisfying resolution beats'
      }
    };

    /**
     * VISUAL_MOTIFS - Recurring symbols and imagery for thematic consistency
     */
    const VISUAL_MOTIFS = {
      'light_shadow': {
        meaning: 'Truth vs deception, hope vs despair, revelation',
        manifestations: [
          'Character stepping from shadow into light',
          'Shadows falling across faces during moral conflict',
          'Light sources representing knowledge or safety',
          'Darkness encroaching during threat'
        ]
      },
      'reflections': {
        meaning: 'Self-examination, duality, hidden truth, alternate reality',
        manifestations: [
          'Character seeing reflection in mirror/water/glass',
          'Distorted reflections during identity crisis',
          'Broken mirrors for shattered self',
          'Perfect reflections for harmony/acceptance'
        ]
      },
      'doorways_thresholds': {
        meaning: 'Transition, choice, point of no return, new chapter',
        manifestations: [
          'Character pausing at threshold',
          'Looking back before crossing',
          'Doors closing behind (finality)',
          'Multiple doors (choice/destiny)'
        ]
      },
      'water': {
        meaning: 'Emotion, purification, danger, the subconscious',
        manifestations: [
          'Rain during emotional release',
          'Still water for peace/reflection',
          'Turbulent water for inner turmoil',
          'Submersion as transformation/rebirth'
        ]
      },
      'fire': {
        meaning: 'Passion, destruction, renewal, danger, warmth',
        manifestations: [
          'Flames during intense emotion',
          'Candles for intimacy/hope',
          'Conflagration as climax/cleansing',
          'Embers as fading hope or smoldering threat'
        ]
      },
      'clocks_time': {
        meaning: 'Mortality, urgency, memory, fate',
        manifestations: [
          'Clock prominently ticking',
          'Time-lapse sequences',
          'Broken/stopped clocks for frozen moment',
          'Watches as personal time markers'
        ]
      },
      'color_coding': {
        meaning: 'Character/faction identification, emotional state',
        manifestations: [
          'Character associated with specific color palette',
          'Color bleeding/shifting during transformation',
          'Complementary colors for opposing forces',
          'Desaturation for despair, vibrancy for hope'
        ]
      },
      'nature_elements': {
        meaning: 'Natural order, chaos, connection to primal forces',
        manifestations: [
          'Wind stirring at emotional moments',
          'Trees as life/growth/permanence',
          'Storms reflecting inner turmoil',
          'Seasons marking passage of time'
        ]
      }
    };

    /**
     * PROP_BIBLE_ARCHETYPES - Important recurring objects with significance
     */
    const PROP_BIBLE_ARCHETYPES = {
      'personal_talisman': {
        description: 'Object of personal significance carried by character',
        examples: ['Locket with photo', 'Lucky coin', 'Ring', 'Watch from deceased'],
        visualTreatment: 'Close-up hero shots, warm lighting, careful handling',
        storyFunction: 'Grounds character, provides emotional anchor'
      },
      'weapon_signature': {
        description: 'Character-defining weapon or tool',
        examples: ['Katana with unique hilt', 'Custom gun', 'Staff with markings'],
        visualTreatment: 'Distinctive silhouette, detailed texture, threatening angles',
        storyFunction: 'Extension of character, visual shorthand for capability'
      },
      'macguffin': {
        description: 'Object that drives plot but has symbolic rather than intrinsic value',
        examples: ['Briefcase', 'Data drive', 'Ancient artifact', 'Key'],
        visualTreatment: 'Mysterious lighting, partial reveals, multiple perspectives',
        storyFunction: 'Motivates action, represents abstract goal'
      },
      'transformation_object': {
        description: 'Object that changes state or appearance as story progresses',
        examples: ['Photograph aging', 'Plant growing/dying', 'Building changing'],
        visualTreatment: 'Match framing across scenes to emphasize change',
        storyFunction: 'Visual metaphor for character/story arc'
      },
      'environment_signature': {
        description: 'Distinctive element that identifies a location',
        examples: ['Neon sign', 'Statue', 'Tree', 'Building facade'],
        visualTreatment: 'Consistent angle, establishing shot anchor',
        storyFunction: 'Instant location recognition, continuity'
      },
      'communication_device': {
        description: 'Object that enables connection between characters',
        examples: ['Phone', 'Radio', 'Letter', 'Photograph exchange'],
        visualTreatment: 'Intimate framing during use, POV of screen/page',
        storyFunction: 'Bridges distance, reveals relationship'
      }
    };

    /**
     * LIGHTING_SETUPS - Named lighting configurations for consistency
     */
    const LIGHTING_SETUPS = {
      'rembrandt': {
        description: 'Classic dramatic lighting with triangle on shadow side of face',
        keyPosition: '45 degrees from subject, elevated',
        fillRatio: 'Key:Fill 4:1 or higher',
        mood: 'Dramatic, artistic, mysterious',
        colorTemp: 'Warm key, neutral fill'
      },
      'butterfly': {
        description: 'Glamorous Hollywood lighting with shadow under nose',
        keyPosition: 'Directly in front, elevated',
        fillRatio: 'Key:Fill 2:1',
        mood: 'Glamorous, beautiful, classic Hollywood',
        colorTemp: 'Warm overall'
      },
      'split': {
        description: 'Half the face lit, half in shadow',
        keyPosition: '90 degrees from subject',
        fillRatio: 'Key:Fill 8:1 or no fill',
        mood: 'Duality, conflict, mystery, danger',
        colorTemp: 'Cool for menace, warm for complexity'
      },
      'silhouette': {
        description: 'Subject completely backlit, no face detail',
        keyPosition: 'Behind subject',
        fillRatio: 'No fill',
        mood: 'Mystery, power, anonymity, epic',
        colorTemp: 'Based on background light source'
      },
      'practical_naturalism': {
        description: 'Motivated by in-scene light sources',
        keyPosition: 'From visible lamps, windows, screens',
        fillRatio: 'Varies with environment',
        mood: 'Realistic, grounded, authentic',
        colorTemp: 'Mixed based on practicals'
      },
      'high_key': {
        description: 'Bright, even lighting with minimal shadows',
        keyPosition: 'Multiple sources, even coverage',
        fillRatio: 'Key:Fill 1:1',
        mood: 'Happy, safe, comedic, dreamlike',
        colorTemp: 'Bright neutral or slight warmth'
      },
      'chiaroscuro': {
        description: 'Extreme contrast between light and dark',
        keyPosition: 'Single hard source',
        fillRatio: 'Key:Fill 16:1 or higher',
        mood: 'Film noir, danger, psychological depth',
        colorTemp: 'Cool with isolated warm spots'
      },
      'motivated_moonlight': {
        description: 'Night exterior with moon as implied key',
        keyPosition: 'High angle, off camera',
        fillRatio: 'Key:Fill 8:1',
        mood: 'Romantic, mysterious, nocturnal',
        colorTemp: 'Blue key, warm practicals'
      }
    };

    /**
     * COLOR_GRADING_PRESETS - Named color grades for visual consistency
     */
    const COLOR_GRADING_PRESETS = {
      'teal_orange': {
        description: 'Complementary blockbuster look',
        shadows: 'Teal/cyan push',
        midtones: 'Slight desaturation except skin',
        highlights: 'Orange/gold warmth',
        contrast: 'Crushed blacks, punchy mids',
        saturation: 'Selective - skin warm, environment cool',
        use: 'Action, sci-fi, modern blockbuster'
      },
      'bleach_bypass': {
        description: 'Desaturated, high contrast gritty look',
        shadows: 'Deep true black',
        midtones: 'Desaturated, silver-gray',
        highlights: 'Blown, harsh',
        contrast: 'Extreme',
        saturation: '40-60% reduction',
        use: 'War, thriller, gritty drama'
      },
      'golden_hour': {
        description: 'Warm romantic magic hour look',
        shadows: 'Warm brown-orange',
        midtones: 'Rich warm tones',
        highlights: 'Golden bloom',
        contrast: 'Soft, lifted blacks',
        saturation: 'Enhanced warm colors',
        use: 'Romance, coming-of-age, nostalgia'
      },
      'noir': {
        description: 'Classic black and white or near-monochrome',
        shadows: 'Deep black, detail preserved',
        midtones: 'Silver gray, sharp',
        highlights: 'Clean white, occasional bloom',
        contrast: 'High with smooth falloff',
        saturation: 'Monochrome or heavily desaturated',
        use: 'Crime, mystery, psychological thriller'
      },
      'vintage_warmth': {
        description: 'Film emulation with lifted blacks',
        shadows: 'Lifted, green-brown tint',
        midtones: 'Faded, warm',
        highlights: 'Soft rolloff, cream tint',
        contrast: 'Reduced, gentle',
        saturation: 'Muted, vintage palette',
        use: 'Period piece, memory sequence, indie'
      },
      'cyberpunk': {
        description: 'Neon-drenched futuristic look',
        shadows: 'Deep purple-blue',
        midtones: 'Magenta-teal split',
        highlights: 'Hot pink or cyan bloom',
        contrast: 'High with crushed mids',
        saturation: 'Hypersaturated neons, desaturated else',
        use: 'Sci-fi, dystopia, club scenes'
      },
      'day_for_night': {
        description: 'Simulated night shot during day',
        shadows: 'Crushed to near-black',
        midtones: 'Heavy blue push',
        highlights: 'Selectively preserved as "moonlight"',
        contrast: 'Very high',
        saturation: 'Heavy desaturation',
        use: 'Night exterior scenes, dream'
      },
      'documentary_natural': {
        description: 'Neutral, truthful representation',
        shadows: 'Natural, detail preserved',
        midtones: 'Neutral, accurate',
        highlights: 'Natural rolloff',
        contrast: 'Moderate, realistic',
        saturation: 'Natural, slight enhancement',
        use: 'Documentary, interview, realism'
      }
    };

    /**
     * LOCATION_BIBLE_GENERATOR - Creates and manages location consistency
     */
    const LOCATION_BIBLE_GENERATOR = {
      /**
       * Extract unique locations from script scenes
       */
      extractLocationsFromScript: (scenes) => {
        const locationMap = new Map();

        for (const scene of scenes) {
          // Try to identify location from scene data
          const locationName = scene.location || scene.setting ||
            LOCATION_BIBLE_GENERATOR.inferLocationFromVisual(scene.visual || scene.visualPrompt || '');

          if (locationName && locationName !== 'unknown') {
            const normalizedName = locationName.toLowerCase().trim();

            if (!locationMap.has(normalizedName)) {
              locationMap.set(normalizedName, {
                id: `loc_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`,
                name: locationName,
                normalizedName,
                scenes: [],
                firstAppearance: scene.sceneId || scene.id,
                baseDescription: null,
                archetype: null,
                stateHistory: []
              });
            }

            const loc = locationMap.get(normalizedName);
            loc.scenes.push({
              sceneId: scene.sceneId || scene.id,
              visualContext: scene.visual || scene.visualPrompt || '',
              inferredTimeOfDay: LOCATION_BIBLE_GENERATOR.inferTimeOfDay(scene),
              inferredWeather: LOCATION_BIBLE_GENERATOR.inferWeather(scene),
              inferredCondition: LOCATION_BIBLE_GENERATOR.inferCondition(scene)
            });
          }
        }

        return Array.from(locationMap.values());
      },

      /**
       * Infer location name from visual description
       */
      inferLocationFromVisual: (visual) => {
        if (!visual) return 'unknown';
        const v = visual.toLowerCase();

        // Check for common location keywords
        const locationPatterns = [
          { pattern: /\b(dojo|training hall|martial arts)\b/i, name: 'The Dojo' },
          { pattern: /\b(office|corporate|boardroom|conference)\b/i, name: 'Corporate Office' },
          { pattern: /\b(warehouse|factory|industrial|abandoned)\b/i, name: 'Abandoned Warehouse' },
          { pattern: /\b(forest|woods|trees|jungle)\b/i, name: 'The Forest' },
          { pattern: /\b(street|alley|urban|city)\b/i, name: 'City Streets' },
          { pattern: /\b(rooftop|roof|skyline)\b/i, name: 'Rooftop' },
          { pattern: /\b(beach|shore|coast|ocean)\b/i, name: 'The Beach' },
          { pattern: /\b(lab|laboratory|research|science)\b/i, name: 'The Laboratory' },
          { pattern: /\b(home|house|apartment|living room|bedroom)\b/i, name: 'Home' },
          { pattern: /\b(hospital|medical|clinic)\b/i, name: 'Hospital' },
          { pattern: /\b(bar|club|restaurant|cafe)\b/i, name: 'The Bar' },
          { pattern: /\b(castle|fortress|palace|throne)\b/i, name: 'The Castle' },
          { pattern: /\b(cave|cavern|underground)\b/i, name: 'The Cave' },
          { pattern: /\b(ship|boat|vessel|deck)\b/i, name: 'The Ship' },
          { pattern: /\b(mountain|peak|summit|cliff)\b/i, name: 'The Mountain' }
        ];

        for (const { pattern, name } of locationPatterns) {
          if (pattern.test(v)) return name;
        }

        return 'unknown';
      },

      /**
       * Infer time of day from scene context
       */
      inferTimeOfDay: (scene) => {
        const text = `${scene.visual || ''} ${scene.visualPrompt || ''} ${scene.narration || ''}`.toLowerCase();

        if (/\b(dawn|sunrise|first light|early morning)\b/.test(text)) return 'dawn';
        if (/\b(morning|bright day|fresh day)\b/.test(text)) return 'morning';
        if (/\b(noon|midday|harsh sun|overhead sun)\b/.test(text)) return 'midday';
        if (/\b(afternoon|warm light|late day)\b/.test(text)) return 'afternoon';
        if (/\b(golden hour|sunset|orange light|evening sun)\b/.test(text)) return 'golden_hour';
        if (/\b(dusk|twilight|fading light)\b/.test(text)) return 'dusk';
        if (/\b(night|darkness|moonlight|stars|neon)\b/.test(text)) return 'night';
        if (/\b(midnight|deep night|dead of night)\b/.test(text)) return 'deep_night';

        return 'day'; // default
      },

      /**
       * Infer weather from scene context
       */
      inferWeather: (scene) => {
        const text = `${scene.visual || ''} ${scene.visualPrompt || ''} ${scene.narration || ''}`.toLowerCase();

        if (/\b(storm|thunder|lightning)\b/.test(text)) return 'storm';
        if (/\b(heavy rain|downpour|torrential)\b/.test(text)) return 'rain_heavy';
        if (/\b(rain|drizzle|wet|raining)\b/.test(text)) return 'rain_light';
        if (/\b(fog|mist|haze|foggy)\b/.test(text)) return 'fog';
        if (/\b(snow|blizzard|frost|winter)\b/.test(text)) return 'snow';
        if (/\b(overcast|cloudy|gray sky)\b/.test(text)) return 'overcast';

        return 'clear'; // default
      },

      /**
       * Infer condition from scene context
       */
      inferCondition: (scene) => {
        const text = `${scene.visual || ''} ${scene.visualPrompt || ''} ${scene.narration || ''}`.toLowerCase();

        if (/\b(destroyed|ruins|rubble|devastat)\b/.test(text)) return 'destroyed';
        if (/\b(heavily damaged|major damage|collapse|fire damage)\b/.test(text)) return 'damaged_heavy';
        if (/\b(damaged|broken|debris|aftermath|battle.?damage)\b/.test(text)) return 'damaged_light';
        if (/\b(repaired|restored|fixed|rebuilt)\b/.test(text)) return 'repaired';
        if (/\b(abandoned|neglected|dusty|cobweb)\b/.test(text)) return 'neglected';
        if (/\b(pristine|perfect|immaculate|spotless)\b/.test(text)) return 'pristine';

        return 'lived_in'; // default
      },

      /**
       * Match location to archetype for base description
       */
      matchArchetype: (locationName) => {
        const name = locationName.toLowerCase();

        if (/dojo|training|martial/.test(name)) return 'dojo_traditional';
        if (/corporate|office|tower|boardroom/.test(name)) return 'corporate_modern';
        if (/warehouse|factory|abandoned|industrial/.test(name)) return 'abandoned_industrial';
        if (/home|house|apartment/.test(name)) return 'cozy_home';
        if (/lab|laboratory|research/.test(name)) return 'high_tech_lab';
        if (/street|alley|urban|city/.test(name)) return 'urban_street';
        if (/forest|woods|jungle/.test(name)) return 'forest_ancient';
        if (/rooftop|roof/.test(name)) return 'rooftop_urban';
        if (/beach|coast|shore/.test(name)) return 'beach_coastal';

        return null; // No matching archetype
      },

      /**
       * Generate rich location description from archetype and context
       */
      generateLocationDescription: (location, archetype, customDetails = '') => {
        if (!archetype || !LOCATION_ARCHETYPES[archetype]) {
          // Generate generic description if no archetype
          return {
            baseDescription: customDetails || `${location.name} - a significant location in the story`,
            keyElements: [],
            colorPalette: 'Cinematic color palette appropriate to the setting',
            atmosphere: 'Atmosphere matching the story tone'
          };
        }

        const arch = LOCATION_ARCHETYPES[archetype];
        return {
          baseDescription: `${location.name}: ${arch.baseElements.slice(0, 4).join(', ')}`,
          keyElements: arch.baseElements,
          colorPalette: arch.colorPalette,
          atmosphere: arch.atmosphere,
          category: arch.category,
          lightingSuggestions: arch.lightingSuggestions,
          customDetails: customDetails
        };
      },

      /**
       * Build location state for a specific scene
       */
      buildLocationState: (location, sceneId) => {
        // Find this scene in the location's history
        const sceneData = location.scenes?.find(s => s.sceneId === sceneId);

        if (!sceneData) {
          return {
            timeOfDay: 'day',
            weather: 'clear',
            condition: 'lived_in'
          };
        }

        return {
          timeOfDay: sceneData.inferredTimeOfDay || 'day',
          weather: sceneData.inferredWeather || 'clear',
          condition: sceneData.inferredCondition || 'lived_in'
        };
      },

      /**
       * Generate complete location prompt component for a scene
       */
      generateLocationPromptComponent: (location, sceneId) => {
        if (!location) return '';

        const archetype = location.archetype || LOCATION_BIBLE_GENERATOR.matchArchetype(location.name);
        const arch = archetype ? LOCATION_ARCHETYPES[archetype] : null;
        const state = LOCATION_BIBLE_GENERATOR.buildLocationState(location, sceneId);

        const parts = [];

        // Base location description
        if (location.baseDescription?.baseDescription) {
          parts.push(`LOCATION: ${location.baseDescription.baseDescription}`);
        } else if (arch) {
          parts.push(`LOCATION: ${location.name} - ${arch.baseElements.slice(0, 3).join(', ')}`);
        } else {
          parts.push(`LOCATION: ${location.name}`);
        }

        // Key visual elements from archetype
        if (arch?.baseElements) {
          parts.push(`KEY ELEMENTS: ${arch.baseElements.slice(0, 4).join('; ')}`);
        }

        // Time of day state
        const timeState = LOCATION_STATE_MODIFIERS.timeOfDay[state.timeOfDay];
        if (timeState) {
          parts.push(`TIME: ${state.timeOfDay} - ${timeState.lighting}`);
        }

        // Weather state
        const weatherState = LOCATION_STATE_MODIFIERS.weather[state.weather];
        if (weatherState && state.weather !== 'clear') {
          parts.push(`WEATHER: ${state.weather} - ${weatherState.atmosphere}`);
          if (weatherState.particles !== 'None') {
            parts.push(`PARTICLES: ${weatherState.particles}`);
          }
        }

        // Condition state
        const conditionState = LOCATION_STATE_MODIFIERS.condition[state.condition];
        if (conditionState && state.condition !== 'lived_in') {
          parts.push(`CONDITION: ${conditionState.description} - ${conditionState.visualChanges}`);
        }

        // Color palette
        if (location.baseDescription?.colorPalette || arch?.colorPalette) {
          parts.push(`PALETTE: ${location.baseDescription?.colorPalette || arch.colorPalette}`);
        }

        return parts.join('\n');
      }
    };

    /**
     * SCENE TYPES - How each scene functions in a film
     * Different from production MODE - these are the building blocks
     */
    const SCENE_TYPES = {
      'opening_narration': {
        description: 'Narrator sets the stage (like Galadriel in LOTR, or Pixar openings)',
        audioType: 'voiceover',
        example: 'The world was young then... before the darkness came.',
        typicalPosition: 'first 1-2 scenes',
        visualStyle: 'Epic establishing shots, slow camera movements'
      },
      'dialogue': {
        description: 'Characters speak to each other - the heart of drama',
        audioType: 'character_dialogue',
        example: '{ speaker: "Maya", line: "We have to go. Now." }, { speaker: "Kai", line: "Not without her." }',
        typicalPosition: 'throughout, especially mid-story',
        visualStyle: 'Shot/reverse-shot, close-ups on speakers, reaction shots'
      },
      'action': {
        description: 'Physical action, chase, fight, movement - minimal/no dialogue',
        audioType: 'sfx_music',
        example: 'No spoken words - just sound effects and intense music',
        typicalPosition: 'climax moments, transitions',
        visualStyle: 'Dynamic camera, quick cuts, wide for scale then tight for impact'
      },
      'emotional': {
        description: 'Character internal moment, reflection, realization',
        audioType: 'internal_monologue_or_music',
        example: 'Internal: "What have I done?" or just emotional score',
        typicalPosition: 'after conflict, before resolution',
        visualStyle: 'Close-ups, slow motion possible, soft lighting'
      },
      'montage': {
        description: 'Time passage, training sequence, preparation',
        audioType: 'music_driven',
        example: 'Energetic music carrying visuals, no dialogue',
        typicalPosition: 'transitions between acts',
        visualStyle: 'Quick cuts, varied angles, progression visible'
      },
      'revelation': {
        description: 'Plot twist, discovery, shocking truth revealed',
        audioType: 'minimal_then_reaction',
        example: 'Silence... then "It was you all along."',
        typicalPosition: 'end of act 2, climax',
        visualStyle: 'Slow push-in, dramatic lighting shift'
      },
      'closing_narration': {
        description: 'Narrator wraps up, reflects on meaning',
        audioType: 'voiceover',
        example: 'And so the journey ended... but another was just beginning.',
        typicalPosition: 'final scene',
        visualStyle: 'Pull back to wide, sunset/sunrise, cyclical imagery'
      }
    };

    /**
     * CINEMATIC TRANSITIONS - Purpose-driven, not random
     */
    const CINEMATIC_TRANSITIONS = {
      'cut': { purpose: 'Direct continuation, same energy', when: 'Normal scene flow' },
      'smash_cut': { purpose: 'Shock, contrast, comedic timing', when: 'Sudden tonal shift' },
      'fade_to_black': { purpose: 'End of chapter, time passage, death', when: 'Major story beat ending' },
      'fade_from_black': { purpose: 'New beginning, awakening', when: 'After time jump or blackout' },
      'dissolve': { purpose: 'Time passing, memory, dreamlike transition', when: 'Flashback, passage of time' },
      'match_cut': { purpose: 'Thematic connection between different scenes', when: 'Showing parallel or contrast' },
      'wipe': { purpose: 'Scene change with energy, different location', when: 'Adventure, action genre' },
      'j_cut': { purpose: 'Audio from next scene starts before visual', when: 'Building anticipation' },
      'l_cut': { purpose: 'Audio from previous scene continues over new visual', when: 'Emotional continuity' }
    };

    /**
     * AUDIO LAYER TYPES - What we HEAR in each scene
     */
    const AUDIO_LAYER_TYPES = {
      'voiceover': {
        description: 'Narrator speaking over visuals',
        when: 'Opening, closing, transitional moments, documentary sections',
        ttsRequired: true,
        speakerType: 'narrator'
      },
      'dialogue': {
        description: 'Characters speaking to each other',
        when: 'Drama scenes, conflict, character development',
        ttsRequired: true,
        speakerType: 'character',
        format: '{ character: "Name", line: "What they say", emotion: "how they say it" }'
      },
      'internal_monologue': {
        description: 'Character thinking to themselves (audience hears thoughts)',
        when: 'Emotional moments, decision points, reflection',
        ttsRequired: true,
        speakerType: 'character_internal'
      },
      'action_sfx': {
        description: 'No speech - sound effects and music only',
        when: 'Action sequences, visual storytelling moments',
        ttsRequired: false,
        speakerType: 'none'
      },
      'music_only': {
        description: 'Emotional score carries the scene',
        when: 'Montages, powerful visual moments, endings',
        ttsRequired: false,
        speakerType: 'none'
      }
    };

    /**
     * SFX CATEGORIES - Sound effects to suggest
     */
    const SFX_CATEGORIES = {
      'ambient': ['wind', 'rain', 'city_noise', 'forest_sounds', 'ocean_waves', 'crowd_murmur'],
      'action': ['explosion', 'gunshot', 'sword_clash', 'punch_impact', 'glass_breaking', 'car_crash'],
      'tension': ['heartbeat', 'clock_ticking', 'door_creak', 'footsteps_approaching', 'breathing_heavy'],
      'emotional': ['gentle_wind', 'music_box', 'rain_on_window', 'fire_crackling', 'distant_thunder'],
      'tech': ['computer_beep', 'door_hiss', 'engine_hum', 'electricity_crackle', 'alarm_blaring'],
      'nature': ['bird_song', 'wolf_howl', 'thunder', 'waterfall', 'leaves_rustling']
    };

    /**
     * MUSIC MOOD PROGRESSION - How music should evolve
     */
    const MUSIC_MOODS = {
      'tension_building': 'Low strings, minimal percussion, building dread',
      'action_intense': 'Fast percussion, brass hits, driving rhythm',
      'emotional_sad': 'Piano, soft strings, minor key, slow tempo',
      'emotional_hopeful': 'Swelling strings, major key resolution, building warmth',
      'triumphant': 'Full orchestra, brass fanfare, timpani, major key',
      'mysterious': 'Ambient pads, unusual instruments, dissonant hints',
      'peaceful': 'Gentle acoustic, soft pads, nature sounds blend',
      'epic_scale': 'Full orchestra with choir, massive reverb, Hans Zimmer style',
      'intimate': 'Solo instrument, minimal arrangement, close and personal',
      'suspense': 'Staccato strings, held breaths, sudden stings'
    };

    // === HOLLYWOOD-STYLE SCENE ARCHITECTURE ===
    // Scenes are narrative units (30-60s) composed of multiple shots (6-10s each)
    // This mirrors professional film production where:
    // - Scene = narrative beat (30-60 seconds)
    // - Shot = individual video clip (6 or 10 seconds from Minimax)

    const clipDuration = videoModel.duration === '10s' ? 10 : 6;
    const isExtendedClip = clipDuration === 10;

    // === HOLLYWOOD SCENE CONFIGURATION ===
    // Scene duration based on production mode and content format
    const hollywoodSceneConfig = {
      'short-form': {
        // Social media content - shorter scenes
        minSceneDuration: 15,
        maxSceneDuration: 30,
        targetSceneDuration: 20,
        minShotsPerScene: 2,
        maxShotsPerScene: 5
      },
      'medium-form': {
        // YouTube style - balanced scenes
        minSceneDuration: 25,
        maxSceneDuration: 45,
        targetSceneDuration: 35,
        minShotsPerScene: 3,
        maxShotsPerScene: 7
      },
      'long-form': {
        // Documentary/Film style - cinematic scenes
        minSceneDuration: 35,
        maxSceneDuration: 60,
        targetSceneDuration: 45,
        minShotsPerScene: 4,
        maxShotsPerScene: 10
      },
      'episodic': {
        // TV series style
        minSceneDuration: 30,
        maxSceneDuration: 50,
        targetSceneDuration: 40,
        minShotsPerScene: 4,
        maxShotsPerScene: 8
      }
    };

    const sceneConfig = hollywoodSceneConfig[contentFormat] || hollywoodSceneConfig['medium-form'];

    // === SMART TIMING CALCULATION ===
    // Pacing determines narration coverage and shot rhythm
    const pacingConfig = {
      fast: {
        narrationRatio: 0.90,
        sceneDurationMultiplier: 0.8,  // Shorter scenes for fast pacing
        wordsPerSecond: 2.8,
        shotsPerSceneMultiplier: 1.2   // More shots = more cuts = faster feel
      },
      medium: {
        narrationRatio: 0.85,
        sceneDurationMultiplier: 1.0,
        wordsPerSecond: 2.5,
        shotsPerSceneMultiplier: 1.0
      },
      slow: {
        narrationRatio: 0.70,          // More visual breathing room
        sceneDurationMultiplier: 1.2,  // Longer scenes for contemplative pacing
        wordsPerSecond: 2.2,
        shotsPerSceneMultiplier: 0.8   // Fewer shots = longer takes = cinematic feel
      }
    };

    const pacingSettings = pacingConfig[pacing] || pacingConfig.medium;

    // Cinematic mode gets extra visual breathing room
    if (productionMode === 'cinematic') {
      pacingSettings.narrationRatio = Math.min(pacingSettings.narrationRatio, 0.70);
      pacingSettings.sceneDurationMultiplier = 1.3;
    }

    // === HOLLYWOOD MATH: CALCULATE SCENE STRUCTURE ===
    // Step 1: Calculate target scene duration (30-60s range)
    const targetSceneDuration = Math.round(
      sceneConfig.targetSceneDuration * pacingSettings.sceneDurationMultiplier
    );

    // Clamp to valid range
    const sceneDuration = Math.max(
      sceneConfig.minSceneDuration,
      Math.min(sceneConfig.maxSceneDuration, targetSceneDuration)
    );

    // Step 2: Calculate number of scenes for total video duration
    const sceneCount = Math.max(2, Math.min(12, Math.round(targetDuration / sceneDuration)));

    // Step 3: Recalculate actual scene duration to fit exactly
    const actualSceneDuration = Math.round(targetDuration / sceneCount);

    // Step 4: Calculate shots per scene based on clip duration (Minimax constraint)
    // Formula: shotsPerScene = ceil(sceneDuration / clipDuration)
    const baseShotsPerScene = Math.ceil(actualSceneDuration / clipDuration);
    const shotsPerScene = Math.round(baseShotsPerScene * pacingSettings.shotsPerSceneMultiplier);

    // Clamp shots to valid range
    const actualShotsPerScene = Math.max(
      sceneConfig.minShotsPerScene,
      Math.min(sceneConfig.maxShotsPerScene, shotsPerScene)
    );

    // Step 5: Calculate per-shot duration to match scene duration exactly
    const shotDuration = Math.round((actualSceneDuration / actualShotsPerScene) * 10) / 10;

    // Visual duration for script (the whole scene)
    const visualDuration = actualSceneDuration;

    // Calculate narration duration per scene (voiceover portion)
    const narrationDuration = Math.round(visualDuration * pacingSettings.narrationRatio);

    // Calculate approximate word count per scene - ENHANCED with content depth multiplier
    const baseWordsPerScene = Math.round(narrationDuration * pacingSettings.wordsPerSecond);
    const wordsPerScene = Math.round(baseWordsPerScene * depthSettings.wordsMultiplier);

    // Log Hollywood scene math
    console.log(`[creationWizardGenerateScript] Hollywood Scene Math:
      Total Duration: ${targetDuration}s
      Content Format: ${contentFormat}
      Scene Duration: ${actualSceneDuration}s (target: ${sceneDuration}s)
      Number of Scenes: ${sceneCount}
      Shots per Scene: ${actualShotsPerScene} (${clipDuration}s clips)
      Shot Duration: ${shotDuration}s
      Narration per Scene: ${narrationDuration}s (~${wordsPerScene} words)`);

    // Build content depth requirements for the prompt
    const contentRequirements = [];
    if (depthSettings.includeStats) {
      contentRequirements.push('Include relevant statistics, numbers, or data points');
    }
    if (depthSettings.includeExamples) {
      contentRequirements.push('Include real-world examples or case studies');
    }
    if (depthSettings.includeStory) {
      contentRequirements.push('Weave in storytelling elements - problems, solutions, transformations');
    }

    // Build camera direction guidance for production mode
    const cameraGuidance = productionSettings.cameraDirections.length > 0
      ? `\nSuggested camera movements for this style: ${productionSettings.cameraDirections.join(', ')}`
      : '';

    // Build genre-specific guidance if genre is selected
    let genreGuidance = '';
    if (genreSettings) {
      genreGuidance = `

=== GENRE REFERENCE: ${genreSettings.name.toUpperCase()} ===
Study these successful productions for reference: ${genreSettings.references.join(', ')}

WHAT MAKES THIS GENRE WORK:
${genreSettings.whatMakesItWork}

NARRATIVE VOICE FOR THIS GENRE:
${genreSettings.narrativeVoice}

SIGNATURE TECHNIQUES TO USE:
${genreSettings.signatureTechniques.map((t, i) => `${i + 1}. ${t}`).join('\n')}

HOOK STYLES FOR THIS GENRE:
${genreSettings.hookStyles.map((h, i) => `${i + 1}. ${h}`).join('\n')}

VISUAL GRAMMAR:
${genreSettings.visualGrammar}

EMOTIONAL BEATS: ${genreSettings.emotionalBeats.join(' â†’ ')}

âš ï¸ AVOID AT ALL COSTS:
${genreSettings.avoidAtAllCosts.map(a => `- ${a}`).join('\n')}`;
    }

    // Build format-specific guidance
    const formatGuidance = `
CONTENT FORMAT: ${formatSettings.name}
- Platforms: ${formatSettings.platforms.join(', ')}
- Adaptation: ${formatSettings.adaptations}
- Scene Count: ${formatSettings.sceneCount}`;

    // Anti-generic rules to ensure premium quality
    const antiGenericGuidance = `

ðŸš« ANTI-GENERIC RULES (NEVER DO THESE):
${ANTI_GENERIC_RULES.slice(0, 6).map(r => `- ${r}`).join('\n')}`;

    // ============================================================
    // PHASE 5: DEEP STORY ARCHITECTURE
    // Inject enriched concept data for complex, character-driven stories
    // ============================================================
    let deepStoryArchitecture = '';

    if (conceptEnrichment) {
      console.log('[creationWizardGenerateScript] Applying Deep Story Architecture from concept enrichment');

      // Extract character intelligence settings
      const charIntel = conceptEnrichment.characterIntelligence || {};
      const requestedCharacterCount = charIntel.suggestedCount || 0;
      const narrationMode = charIntel.narrationMode || 'voiceover';

      // Extract selected concept data
      const selectedConcept = conceptEnrichment.selectedConcept || null;

      // Check for pre-defined characters - prefer selectedConcept.characters, then improvedData.characters
      const conceptCharacters = selectedConcept?.characters || [];
      const improvedCharacters = conceptEnrichment.characters || [];
      const allCharacters = conceptCharacters.length > 0 ? conceptCharacters : improvedCharacters;
      const hasPreDefinedCharacters = allCharacters.length > 0;

      // Build character bible section
      let characterBible = '';

      if (hasPreDefinedCharacters) {
        // Use pre-defined characters - handles both selectedConcept format and improvedData format
        const isConceptFormat = allCharacters[0]?.name !== undefined;

        characterBible = `

ðŸŽ­ CHARACTER BIBLE - REQUIRED CHARACTERS
These characters MUST appear in your script with consistency:

${allCharacters.map((char, idx) => {
          if (isConceptFormat) {
            // Format from selectedConcept (has name, archetype, role, flaw, arc)
            return `CHARACTER ${idx + 1}: ${char.name || 'Unnamed'}
- Archetype: ${char.archetype || 'undefined'}
- Role: ${char.role || 'protagonist'}
- Fatal Flaw: ${char.flaw || 'to be developed'}
- Character Arc: ${char.arc || 'transformation through the story'}
- MUST appear in at least ${Math.max(2, Math.ceil(sceneCount / allCharacters.length))} scenes
- Maintain EXACT visual consistency in every appearance`;
          } else {
            // Format from improvedData (has archetype, uniqueTwist, visualDescription)
            return `CHARACTER ${idx + 1}: ${char.archetype}
- Unique Twist: ${char.uniqueTwist}
- Visual Description: ${char.visualDescription}
- MUST appear in at least ${Math.max(2, Math.ceil(sceneCount / 2))} scenes
- Maintain EXACT visual consistency in every appearance
- Show character development/arc across scenes`;
          }
        }).join('\n\n')}

CHARACTER DISTRIBUTION REQUIREMENTS:
- Each character MUST appear in multiple scenes (not just one)
- Characters should interact or their stories should interweave
- Show character growth/change between their first and last appearance
- Reference character visual descriptions EXACTLY in every visualPrompt they appear in`;
      } else if (requestedCharacterCount > 0) {
        // Generate character requirements based on requested count
        characterBible = `

ðŸŽ­ CHARACTER GENERATION REQUIRED
You MUST create EXACTLY ${requestedCharacterCount} distinct, memorable characters for this story.

CHARACTER CREATION REQUIREMENTS:
${Array.from({ length: requestedCharacterCount }, (_, i) => `
CHARACTER ${i + 1}:
- Create a unique archetype (e.g., The Reluctant Hero, The Wise Mentor, The Hidden Villain)
- Give them a distinctive visual appearance (specific clothing, features, mannerisms)
- Define their role in the story and their motivation
- They MUST appear in at least ${Math.max(2, Math.ceil(sceneCount / requestedCharacterCount))} scenes`).join('\n')}

MANDATORY CHARACTER RULES:
- Each character MUST have a UNIQUE, DETAILED visual description
- Use EXACT SAME description every time a character appears (visual consistency)
- Characters must interact with each other - no isolated character arcs
- Show character development/growth across their appearances
- Include diverse character types - not all heroes or all villains
- Each character needs clear motivations that drive their actions

NARRATION MODE: ${narrationMode.toUpperCase()}
${narrationMode === 'dialogue' ? '- Include character dialogue in narration - characters should speak to each other' : ''}
${narrationMode === 'voiceover' ? '- Use third-person narration describing character actions and thoughts' : ''}
${narrationMode === 'narrator' ? '- Use an omniscient narrator who knows all characters\' thoughts and feelings' : ''}`;
      }

      // Build selected concept section - OPTIMIZED to avoid duplicating data in other sections
      // Characters, relationships, and world are handled in their own dedicated sections
      let selectedConceptSection = '';
      if (selectedConcept) {
        selectedConceptSection = `

ðŸ“‹ CORE STORY CONCEPT
Title: "${selectedConcept.title}"
Logline: ${selectedConcept.logline}
${selectedConcept.theme ? `Theme: ${selectedConcept.theme}` : ''}
${selectedConcept.narrativeStructure ? `Structure: ${selectedConcept.narrativeStructure.replace(/_/g, ' ').toUpperCase()}` : ''}
${selectedConcept.storyEngine ? `Engine: ${selectedConcept.storyEngine.replace(/_/g, ' ')}` : ''}
${selectedConcept.uniqueElements && selectedConcept.uniqueElements.length > 0 ? `Unique Elements: ${selectedConcept.uniqueElements.join(', ')}` : ''}
${selectedConcept.mood ? `Mood: ${selectedConcept.mood}` : ''} ${selectedConcept.tone ? `Tone: ${selectedConcept.tone}` : ''}`;
      }

      // Build world-building section
      let worldBuildingSection = '';
      if (conceptEnrichment.worldBuilding) {
        worldBuildingSection = `

ðŸŒ WORLD-BUILDING - CONSISTENT UNIVERSE
This story takes place in a specific, consistent world:

SETTING: ${conceptEnrichment.worldBuilding.setting}
WORLD RULES: ${conceptEnrichment.worldBuilding.rules}
ATMOSPHERE: ${conceptEnrichment.worldBuilding.atmosphere}

WORLD CONSISTENCY REQUIREMENTS:
- Every scene MUST feel like it exists in this same world
- Environmental details should reference the world rules
- The atmosphere should pervade every visual description
- Background elements should reinforce the world's unique nature`;
      }

      // Build visual signature section
      let visualSignatureSection = '';
      if (conceptEnrichment.visualSignature) {
        visualSignatureSection = `

ðŸŽ¬ VISUAL SIGNATURE - CONSISTENT STYLE
Every scene MUST adhere to this visual signature:
"${conceptEnrichment.visualSignature}"

This visual style MUST be:
- Referenced in EVERY visualPrompt
- The unifying aesthetic thread across all scenes
- Adapted to each scene's needs while maintaining core signature elements`;
      }

      // Build key elements section
      let keyElementsSection = '';
      if (conceptEnrichment.keyElements && conceptEnrichment.keyElements.length > 0) {
        keyElementsSection = `

ðŸ”‘ KEY STORY ELEMENTS - MUST INCLUDE
These elements MUST appear throughout the story:
${conceptEnrichment.keyElements.map((elem, idx) => `${idx + 1}. ${elem}`).join('\n')}

Weave these elements naturally across multiple scenes - don't dump them all in one scene.`;
      }

      // Build genre fusion guidance
      let genreFusionSection = '';
      if (conceptEnrichment.genreFusion) {
        genreFusionSection = `

ðŸŽ¯ GENRE FUSION
${conceptEnrichment.genreFusion}

Balance these genre elements throughout the narrative structure.`;
      }

      // Build things to avoid section
      let avoidSection = '';
      if (conceptEnrichment.thingsToAvoid && conceptEnrichment.thingsToAvoid.length > 0) {
        avoidSection = `

âš ï¸ CRITICAL - THINGS TO AVOID (ORIGINALITY REQUIREMENTS)
${conceptEnrichment.thingsToAvoid.map(avoid => `- DO NOT: ${avoid}`).join('\n')}`;
      }

      // Build style references section
      let styleReferencesSection = '';
      if (conceptEnrichment.extractedStyles && conceptEnrichment.extractedStyles.length > 0) {
        styleReferencesSection = `

ðŸŽ¨ VISUAL STYLE INSPIRATIONS (Reference, don't copy)
${conceptEnrichment.extractedStyles.map(style => `- ${style}`).join('\n')}

Use these as VISUAL STYLE inspiration only - create original content, characters, and story.`;
      }

      // Build hook line section
      let hookLineSection = '';
      if (conceptEnrichment.hookLine) {
        hookLineSection = `

ðŸŽ£ HOOK LINE - USE THIS AS OPENING INSPIRATION
"${conceptEnrichment.hookLine}"

Your opening scene/hook should capture this energy and promise.`;
      }

      // Build NARRATIVE ARCHITECTURE section (NEW - uses production bible data)
      let narrativeArchitectureSection = '';
      if (conceptEnrichment.narrativeArchitecture) {
        const narArch = conceptEnrichment.narrativeArchitecture;

        // Get structure details from our constants
        const structureKey = narArch.structure || 'heros_journey';
        const structureInfo = NARRATIVE_STRUCTURES[structureKey] || NARRATIVE_STRUCTURES['heros_journey'];
        const storyEngineKey = narArch.storyEngine || 'transformation_arc';
        const storyEngineInfo = STORY_ENGINES[storyEngineKey] || STORY_ENGINES['transformation_arc'];

        narrativeArchitectureSection = `

ðŸ—ï¸ NARRATIVE ARCHITECTURE - STORY STRUCTURE
This story uses: ${structureInfo.name.toUpperCase()} structure
${structureInfo.description}
Examples: ${structureInfo.examples.join(', ')}

PROTAGONIST COUNT: ${narArch.protagonistCount || structureInfo.protagonistCount}
${narArch.structureReason ? `WHY THIS STRUCTURE: ${narArch.structureReason}` : ''}

KEY BEATS TO HIT (in order):
${(narArch.keyBeats || structureInfo.keyBeats).map((beat, idx) => `${idx + 1}. ${beat.replace(/_/g, ' ').toUpperCase()}`).join('\n')}

SCENE TYPE DISTRIBUTION (use as guide):
- Dialogue scenes: ~${structureInfo.sceneDistribution.dialogue}%
- Action scenes: ~${structureInfo.sceneDistribution.action}%
- Emotional scenes: ~${structureInfo.sceneDistribution.emotional}%
- Revelation scenes: ~${structureInfo.sceneDistribution.revelation}%
- Montage: ~${structureInfo.sceneDistribution.montage}%

âš¡ STORY ENGINE: ${storyEngineInfo.name}
${storyEngineInfo.description}
Scene generation pattern: ${storyEngineInfo.sceneGenerator}
`;
      }

      // Build character relationships section (NEW - explicit relationships)
      let characterRelationshipsSection = '';
      if (conceptEnrichment.characterRelationships && conceptEnrichment.characterRelationships.length > 0) {
        characterRelationshipsSection = `

ðŸ”— CHARACTER RELATIONSHIPS - MANDATORY DYNAMICS
These relationships MUST drive scenes and create tension:

${conceptEnrichment.characterRelationships.map((rel, idx) => `RELATIONSHIP ${idx + 1}: ${rel.char1} â†” ${rel.char2}
- Type: ${rel.relationshipType}
- Tension Source: ${rel.tension}
- Evolution: ${rel.evolution || 'Show this relationship changing across scenes'}`).join('\n\n')}

Include at least ONE scene that explicitly explores each relationship.`;
      }

      // Build theme section (NEW - thematic depth)
      let themeSection = '';
      if (conceptEnrichment.theme || conceptEnrichment.premise) {
        themeSection = `

ðŸŽ­ THEMATIC SPINE - WHAT IT'S REALLY ABOUT
${conceptEnrichment.theme ? `THEME: "${conceptEnrichment.theme}"` : ''}
${conceptEnrichment.premise ? `PREMISE (The "What If"): "${conceptEnrichment.premise}"` : ''}

Every scene should subtly reinforce this theme. The climax should be the ultimate expression of this thematic question.`;
      }

      // Build visual/audio style section (NEW - production bible)
      let productionStyleSection = '';
      if (conceptEnrichment.visualStyle || conceptEnrichment.audioStyle) {
        const vs = conceptEnrichment.visualStyle || {};
        const as = conceptEnrichment.audioStyle || {};
        productionStyleSection = `

ðŸŽ¨ PRODUCTION STYLE BIBLE
${vs.colorPalette ? `COLOR PALETTE: ${vs.colorPalette}` : ''}
${vs.lightingApproach ? `LIGHTING: ${vs.lightingApproach}` : ''}
${vs.cameraPhilosophy ? `CAMERA PHILOSOPHY: ${vs.cameraPhilosophy}` : ''}
${vs.referenceFilms && vs.referenceFilms.length > 0 ? `VISUAL REFERENCES: ${vs.referenceFilms.join(', ')}` : ''}

${as.musicGenre ? `MUSIC STYLE: ${as.musicGenre}` : ''}
${as.dialogueApproach ? `DIALOGUE APPROACH: ${as.dialogueApproach}` : ''}
${as.signatureSounds && as.signatureSounds.length > 0 ? `SIGNATURE SOUNDS: ${as.signatureSounds.join(', ')}` : ''}`;
      }

      // Combine all sections
      deepStoryArchitecture = `

============================================================
ðŸŽ¬ DEEP STORY ARCHITECTURE - PREMIUM PRODUCTION MODE ðŸŽ¬
============================================================
This content has been enriched with AI-powered concept development.
You MUST use this information to create a COMPLEX, MULTI-LAYERED story.
${selectedConceptSection}

ENHANCED CONCEPT VISION:
${conceptEnrichment.improvedConcept || topic}
${conceptEnrichment.suggestedMood ? `\nOVERALL MOOD: ${conceptEnrichment.suggestedMood}` : ''}
${conceptEnrichment.suggestedTone ? `NARRATIVE TONE: ${conceptEnrichment.suggestedTone}` : ''}
${narrativeArchitectureSection}
${characterBible}
${characterRelationshipsSection}
${worldBuildingSection}
${visualSignatureSection}
${productionStyleSection}
${themeSection}
${keyElementsSection}
${genreFusionSection}
${styleReferencesSection}
${avoidSection}
${hookLineSection}

QUALITY: Multi-layered narrative, character depth with growth arcs, visual consistency, complete emotional journey.
${requestedCharacterCount > 0 ? `âš ï¸ MUST include EXACTLY ${requestedCharacterCount} distinct characters.` : ''}
AVOID: Generic content, disconnected scenes, one-time characters, cookie-cutter stories.
`;
    }

    // Build the prompt for GPT-4o
    const systemPrompt = `You are a HOLLYWOOD-LEVEL PRODUCTION DIRECTOR creating premium video content.
You have studied the greatest productions ever made and apply their techniques with precision.
Your content NEVER feels like a basic presentation or generic corporate video.

PRODUCTION MODE: ${productionSettings.name.toUpperCase()}
${productionSettings.description}

PRODUCTION STYLE GUIDE:
- Narrative Voice: ${productionSettings.narrativeStyle}
- Visual Approach: ${productionSettings.visualApproach}
- Emotional Arc: ${productionSettings.emotionalArc}
- Opening: ${productionSettings.openingStyle}
- Closing: ${productionSettings.closingStyle}
- Music Mood: ${productionSettings.musicMood}
${productionSettings.specialInstructions ? `- Special: ${productionSettings.specialInstructions}` : ''}
${cameraGuidance}
${genreGuidance}
${formatGuidance}
${antiGenericGuidance}
${deepStoryArchitecture}

You create scripts for ${platform || 'social media'} in ${aspectRatio || '16:9'} format, specializing in ${niche || 'general'} content.

CONTENT DEPTH: ${contentDepth.toUpperCase()} - ${depthSettings.description}
You must provide ${depthSettings.requirements}.

VIDEO TECHNOLOGY CONSTRAINT:
Each scene will be generated as a ${clipDuration}-second AI video clip using Minimax Hailuo.
Visual descriptions MUST include camera movement directions in [brackets] at the start.
Example: "[Push in, Zoom in] A figure emerges from shadows..."

CRITICAL TIMING RULE: You MUST write narrations with the EXACT word count specified. This determines video pacing.
Count your words carefully. Too few words = awkward pauses. Too many words = rushed audio.
Always return valid JSON exactly matching the requested structure.`;

    // Build production-specific narrative structure
    const narrativeStructure = productionMode === 'standard' ? `
- Scene 1 (HOOK): Surprising fact, provocative question, or bold statement that demands attention
- Scenes 2-3 (SETUP): Establish the problem, context, or "why this matters"
- Scenes 4-${sceneCount - 2} (BODY): Main content with clear explanations
- Scene ${sceneCount - 1} (CLIMAX): Key insight, transformation, or "aha moment"
- Scene ${sceneCount} (CTA): Natural conclusion with clear call-to-action` : productionMode === 'documentary' ? `
- Scene 1 (OPENING): A powerful, unexpected observation. No greeting. Dive straight in.
- Scene 2 (CONTEXT): Pull back to show the bigger picture. Why does this matter?
- Scenes 3-${sceneCount - 2} (EXPLORATION): Each scene reveals a new facet. Build wonder and understanding.
- Scene ${sceneCount - 1} (REVELATION): The key insight that changes how we see the subject
- Scene ${sceneCount} (REFLECTION): Broader implications. Leave viewers thinking.` : productionMode === 'thriller' ? `
- Scene 1 (HOOK): Something is wrong. A mystery that demands answers.
- Scene 2 (SETUP): Establish the stakes. What's at risk?
- Scenes 3-${sceneCount - 2} (ESCALATION): Each scene raises the tension. "But then..." "What they didn't know..."
- Scene ${sceneCount - 1} (TWIST): The revelation that changes everything
- Scene ${sceneCount} (RESOLUTION): But leave one thread hanging. A final question.` : productionMode === 'inspirational' ? `
- Scene 1 (THE STRUGGLE): Start with a relatable challenge. "You've felt this..."
- Scene 2 (THE BARRIER): What makes this hard? Why do people fail?
- Scenes 3-${sceneCount - 2} (THE JOURNEY): Show the transformation happening. Small wins building.
- Scene ${sceneCount - 1} (THE BREAKTHROUGH): The moment of triumph. Victory earned.
- Scene ${sceneCount} (EMPOWERMENT): You can do this too. The call to action.` : productionMode === 'story' ? `
- Scene 1 (IN MEDIA RES): Start mid-action. "The day everything changed..."
- Scene 2 (INCITING INCIDENT): What disrupts the status quo?
- Scenes 3-${sceneCount - 2} (RISING ACTION): Obstacles, attempts, escalating stakes
- Scene ${sceneCount - 1} (CLIMAX): The decisive moment. Everything hangs in the balance.
- Scene ${sceneCount} (RESOLUTION): The lesson. Circle back to the opening with new meaning.` : `
- Scene 1 (STRIKING IMAGE): Open with a powerful visual. Minimal or no narration for 3 seconds.
- Scene 2 (ATMOSPHERE): Establish the world. Slow, deliberate pacing.
- Scenes 3-${sceneCount - 2} (IMMERSION): Each scene deepens the emotional experience. Let visuals breathe.
- Scene ${sceneCount - 1} (EMOTIONAL PEAK): The most powerful moment. Maximum impact.
- Scene ${sceneCount} (ICONIC CLOSE): Land on an unforgettable final image.`;

    const userPrompt = `Create a ${targetDuration}-second ${productionSettings.name.toUpperCase()} style video script about: "${topic}"

=== PRODUCTION MODE: ${productionSettings.name.toUpperCase()} ===
${productionSettings.description}
Emotional Arc: ${productionSettings.emotionalArc}

=== CONFIGURATION ===
Platform: ${platform || 'YouTube'}
Aspect Ratio: ${aspectRatio || '16:9'}
TOTAL VIDEO DURATION: ${targetDuration} seconds
Niche: ${niche || 'general'}${subniche ? ` > ${subniche}` : ''}
Visual Style: ${style || 'modern'}
Tone: ${tone}
Pacing: ${pacing} (${pacing === 'fast' ? 'energetic, quick cuts' : pacing === 'slow' ? 'contemplative, visual breathing room' : 'balanced pacing'})
Content Depth: ${contentDepth.toUpperCase()}

=== HOLLYWOOD-STYLE SCENE ARCHITECTURE ===
Each SCENE is a complete narrative unit (~${actualSceneDuration} seconds)
Each scene will be DECOMPOSED into ${actualShotsPerScene} SHOTS (${clipDuration}s video clips each)

SCENE = Narrative beat (${actualSceneDuration}s) - what you're writing
SHOT = Individual video clip (${clipDuration}s) - generated later from your scene

=== AI VIDEO TECHNOLOGY ===
Shots are generated using Minimax Hailuo (${videoModel.resolution || '1080p'})
Each ${clipDuration}-second shot will be a separate AI video clip
The scene's visual description will be decomposed into ${actualShotsPerScene} distinct shots

IMPORTANT: Visual descriptions MUST be rich enough to support ${actualShotsPerScene} different camera angles/moments
Include multiple visual elements, actions, and details that can be captured in separate shots.
Start with camera movement in [brackets]: Push in, Pull out, Pan left, Pan right, Tilt up, Tilt down, Zoom in, Zoom out, Tracking shot, Static shot
${productionSettings.cameraDirections.length > 0 ? `Recommended for ${productionSettings.name}: ${productionSettings.cameraDirections.join(', ')}` : ''}

=== TIMING BREAKDOWN ===
Total Video Duration: ${targetDuration} seconds
Number of Scenes: ${sceneCount}
Duration per Scene: ${actualSceneDuration} seconds
Shots per Scene: ${actualShotsPerScene} (each ${clipDuration}s)
Total Shots: ~${sceneCount * actualShotsPerScene} video clips
Narration per Scene: ${narrationDuration} seconds (~${wordsPerScene} words)
Visual-Only Time: ${visualDuration - narrationDuration} seconds per scene

=== CONTENT REQUIREMENTS ===
${contentRequirements.length > 0 ? contentRequirements.map((req, i) => `${i + 1}. ${req}`).join('\n') : '- Standard content depth'}
${productionSettings.specialInstructions ? `\nPRODUCTION SPECIAL: ${productionSettings.specialInstructions}` : ''}

=== ${productionSettings.name.toUpperCase()} NARRATIVE STRUCTURE ===${narrativeStructure}

=== CINEMATIC PRODUCTION REQUIREMENTS ===
1. OPENING: ${productionSettings.openingStyle}
2. Create EXACTLY ${sceneCount} scenes following the emotional arc: ${productionSettings.emotionalArc}
3. SCENE TYPE DISTRIBUTION (this is CINEMA, not presentation):
   - Opening scene: Use "opening_narration" type for world-setting
   - Middle scenes: Mix "dialogue", "action", "emotional" scenes - NOT all voiceover!
   - Climax: Use "action" or "revelation" type
   - Final scene: Use "closing_narration" or "emotional" for resolution
   - MINIMUM 40% of scenes MUST be "dialogue" type with characters talking

4. Each scene needs:
   - sceneType: Choose appropriate type (dialogue/action/emotional/etc.)
   - audioLayer: Structured audio matching the scene type
     * dialogue scenes: { type: "dialogue", dialogue: [{character, line, emotion}], sfx, musicMood }
     * voiceover scenes: { type: "voiceover", voiceover: "text", sfx, musicMood }
     * action scenes: { type: "action_sfx", sfx: [...], musicMood: "action_intense" }
   - visualPrompt: Detailed cinematography with [Camera Movement]
   - cameraIntent: WHY this camera move serves the story
   - transition: { type, purpose } - purposeful transitions
   - duration: ${visualDuration}
   - sceneAction: CRITICAL FOR VIDEO GENERATION (see below)

=== CRITICAL: sceneAction IS THE STORY ENGINE ===
The sceneAction field drives the entire video animation system. Each scene will be divided into ${actualShotsPerScene} video shots of ${clipDuration} seconds each. The sceneAction describes the COMPLETE ACTION SEQUENCE that unfolds across all shots.

POOR sceneAction (DO NOT DO THIS):
"Kai confronts the Shadow Warrior"
- Too brief, no detail, nothing to animate

EXCELLENT sceneAction (DO THIS):
"Kai stands at the edge of the rooftop, surveying Neo-Shaolin's sprawling neon skyline below. Wind catches his coat as he turns to face Ryu and Li Mei, his expression shifting from contemplation to resolve. He speaks, gesturing toward their destination in the distance. Ryu steps forward with a confident grin, cracking his knuckles in anticipation. Li Mei moves to Kai's side, placing her hand on his shoulder in support. The three exchange determined looks, then turn as one toward the rooftop's edge. Together they break into a run and leap from the building, coats billowing dramatically as they descend toward the glowing streets below."

This sceneAction will be divided into shots:
- Shot 1 (10s): Kai surveys city, turns to companions, begins speaking
- Shot 2 (10s): Ryu and Li Mei respond, they gather and prepare
- Shot 3 (10s): They turn, run, and leap from the building

REQUIREMENTS for sceneAction:
1. Describe ALL character movements, gestures, and expression changes
2. Include physical interactions between characters
3. Show environmental responses (wind, light changes, etc.)
4. Provide enough detail for ${actualShotsPerScene} distinct 10-second video clips
5. End with a clear "landing moment" that can be captured as a freeze frame

=== CRITICAL: HOLLYWOOD NARRATIVE STRUCTURE BY SCENE POSITION ===
Each scene's content MUST match its position in the story arc. Do NOT write generic content - write POSITION-AWARE content.

ðŸ“ SCENE 1 - THE OPENING (World-First Structure) - CRITICAL FOR CINEMATIC QUALITY:
The opening scene MUST follow this exact structure - this is how Hollywood films begin:

âš ï¸ WORLD-FIRST RULE (NON-NEGOTIABLE):
Hollywood films ALWAYS establish the WORLD before the CHARACTER. Think:
- Blade Runner: Los Angeles cityscape â†’ then finds Deckard
- Lord of the Rings: The Shire â†’ then finds Frodo
- Avatar: Pandora's forests â†’ then finds Jake
- The Matrix: Green code â†’ digital world â†’ then finds Neo

OPENING sceneAction STRUCTURE (designed for 3-4 shots):
- Shot 1 (0-10s): WORLD ESTABLISHMENT - Show ONLY the ENVIRONMENT. NO character visible at all.
  Camera: Drone/Crane/Sweeping movement across the world
  "The camera soars over the ancient peaks of the Jade Mountains at dawn, mist cascading through valleys below, ancient temples clinging to impossible cliffsides..."

- Shot 2 (10-20s): WORLD DETAIL - Continue exploring the world. Character may appear as TINY distant figure (less than 5% of frame).
  Camera: Still wide, descending toward a specific location
  "Descending through prayer flags and cloud layers, the camera finds a hidden monastery courtyard. Smoke rises from incense burners. In the far corner, barely visible, a small figure moves in meditation..."

- Shot 3 (20-30s): CHARACTER REVEAL - The MOMENT we meet the protagonist. This is their introduction to the audience.
  Camera: Arrives at character, medium-wide to medium shot
  "The camera lands on Mei - her eyes closed, her breath visible in the cold air, her movements precise as she flows through ancient forms. This is our hero."

- Shot 4+ (30s+): CHARACTER CONTEXT - Establish what drives them, plant the story hook.
  Camera: Medium shot, character with world context
  "Mei's eyes snap open. She gazes toward a distant tower glowing with forbidden energy. Her hand moves unconsciously to an amulet at her chest. Something calls to her..."

OPENING visualPrompt STRUCTURE (MANDATORY FORMAT):
- MUST begin with environment description (at least 40% of the prompt)
- Camera movement MUST be sweeping/crane/drone (establishing movement)
- Character appears LATER in the description, not at the start
- First 1-2 sentences: Pure environment and atmosphere
- Middle sentences: Camera finds/lands on character
- Final sentences: Character purpose and hint of journey

OPENING CHOREOGRAPHY (sceneArc MUST be "steady_build"):
- intensityProgression: [0.2, 0.4, 0.6, 0.5] - gradual introduction, not explosive
- Beat 1: Pure world (no character)
- Beat 2: World with tiny figure (if any)
- Beat 3: Character reveal moment
- Beat 4: Character context and story hook

âŒ WRONG OPENING (ABSOLUTELY DO NOT WRITE THIS):
"Kai stands at the heart of the nexus, channeling ancient energy..."
- WRONG: Character is prominent from the start
- WRONG: Character is already doing something significant
- WRONG: No world establishment
- This is MID-STORY content forced into an opening

âŒ ALSO WRONG:
"A lone warrior named Kai surveys the battlefield..."
- WRONG: Character is immediately the focus
- WRONG: No environment established first
- WRONG: Sounds like a generic character introduction

âœ… CORRECT OPENING (STUDY THIS EXAMPLE):
"[Sweeping crane shot] Dawn breaks over the mist-shrouded peaks of the Jade Mountains, golden light cascading across ancient temples that dot the cliffsides like scattered jewels. Prayer flags flutter in the morning breeze, their colors vibrant against the grey stone. The camera descends through layers of cloud, revealing a hidden monastery courtyard where incense smoke curls upward into the cool air.

Amid the swirling fog of the courtyard, a solitary figure moves in graceful meditation - this is Mei, a young woman in simple training robes, her breath forming delicate clouds as she flows through ancient martial forms. Her eyes are closed, movements precise yet fluid, completely at one with her world.

Her eyes snap open, fixed on something beyond the mountain range. Her hand moves to the jade pendant at her throat. In the distant valley, an unnatural storm gathers around a forbidden tower. Mei takes a steadying breath - her journey is about to begin."

Notice how:
âœ“ First paragraph: 100% environment (no character)
âœ“ Second paragraph: Character revealed within the world
âœ“ Third paragraph: Story hook established
âœ“ Camera flows from wide to close, world to character

ðŸ“ SCENES 2-3 - ESTABLISHMENT & CONFLICT INTRODUCTION:
These scenes establish characters and introduce the central conflict.

ESTABLISHMENT sceneAction PATTERNS:
- Introduce supporting characters through INTERACTION, not description
- Show character relationships through dialogue and body language
- Plant seeds of conflict - a disagreement, a warning, a mysterious event
- End with a clear "call to action" or disruption of status quo

Example for Scene 2:
"In the monastery's candlelit library, Mei kneels before Master Chen, his ancient face etched with concern. He speaks slowly, each word weighted with meaning. Mei's expression shifts from confusion to disbelief as he reveals a hidden scroll. She reaches for it hesitantly, her fingers trembling. As she unrolls the brittle parchment, her eyes widen - the camera pushes in on her face as realization dawns. She looks up at her master, a question forming on her lips, but his grave nod confirms her fears."

ðŸ“ MIDDLE SCENES - RISING ACTION:
These scenes BUILD TENSION and COMPLICATE the journey.

RISING ACTION sceneAction PATTERNS:
- Each scene should ESCALATE stakes from the previous
- Introduce obstacles, setbacks, or new information
- Show character growth through challenges faced
- Create mini-arcs within each scene: goal â†’ obstacle â†’ adaptation

Progression principles:
- Scene after scene should feel like climbing stairs, not walking flat
- Each scene ends with MORE tension/stakes than it began
- Victories should be partial; setbacks should reveal new paths

ðŸ“ CLIMAX SCENE (Scene ${sceneCount - 1} or ${sceneCount - 2}):
The peak of action/revelation - maximum tension.

CLIMAX sceneAction REQUIREMENTS:
- FASTEST pacing of any scene - quick cuts, intense action OR dramatic revelation
- All story threads should converge here
- Character faces their greatest challenge/makes crucial choice
- NO new information - payoff what was set up earlier
- End on the pivotal moment before resolution

Example Climax:
"Kai and the Shadow Lord circle each other in the shattered throne room, debris floating in the anti-gravity field. Lightning crackles between them as they clash - Kai is thrown back but rolls to his feet. He sees Mei struggling in energy bonds across the chamber. Their eyes meet. In that moment, Kai makes his choice - he hurls his weapon not at the Shadow Lord, but at the power conduit. The world erupts in blinding light..."

ðŸ“ FINAL SCENE - RESOLUTION:
The closing must provide emotional satisfaction and closure.

RESOLUTION sceneAction STRUCTURE:
- SLOW DOWN the pacing - let the audience breathe
- Show the NEW STATUS QUO - how the world/characters have changed
- Provide emotional payoff for character arcs
- End with an IMAGE that encapsulates the journey's meaning
- Optional: hint at future (for series) but resolve the main arc

Example Resolution:
"Sunlight streams through the restored temple windows as villagers gather in celebration. Mei moves through the crowd, accepting grateful touches and tearful embraces. She finds Kai standing apart, gazing at the horizon. They share a wordless moment - everything that needed saying already said. Mei places the restored artifact on its ancient pedestal. As its light fills the chamber, she turns back to Kai with the smallest smile. Together, they walk toward the temple doors, silhouettes framed against the bright new day. The massive doors slowly close behind them."

=== SCENE-TO-SCENE FLOW REQUIREMENTS ===
1. Each scene's OPENING SHOT should connect to the previous scene's FINAL SHOT
2. Emotional energy should flow logically: calmâ†’tensionâ†’peakâ†’resolution
3. Character knowledge should accumulate - they learn/change each scene
4. Location changes should be motivated by story needs, not random
5. Time progression should be clear (same day? next morning? weeks later?)

5. AUDIO TIMING:
   - Voiceover scenes: ~${wordsPerScene} words
   - Dialogue scenes: 2-4 lines of dialogue per ${clipDuration}-second clip
   - Action scenes: SFX list, NO spoken words

6. CLOSING: ${productionSettings.closingStyle}
7. Visual descriptions: ${productionSettings.visualApproach}

${additionalInstructions ? `=== ADDITIONAL INSTRUCTIONS ===\n${additionalInstructions}\n` : ''}
${conceptEnrichment ? `
=== CRITICAL: DEEP STORY ARCHITECTURE ACTIVE ===
Refer to the DEEP STORY ARCHITECTURE section in the system prompt.
You MUST incorporate all characters, world-building, and visual signature elements.
This is NOT optional - failure to use this enrichment data will result in generic content.
` : ''}

=== SCENE CHOREOGRAPHY (for video animation) ===
Each scene includes a "choreography" object to guide AI video generation with 4-beat timing:

- sceneArc: The emotional shape ("build_to_climax" | "tension_release" | "steady_build" | "peak_then_calm")
- intensityProgression: Array of 4 intensity values [beat1, beat2, beat3, beat4] from 0.0-1.0
- blocking: String describing character positions and key movements
- keyBeats: Array of 4 strings describing what happens in each beat:
  * Beat 1 (0-2s): Establish the scene
  * Beat 2 (2-5s): Develop the action
  * Beat 3 (5-8s): Peak moment
  * Beat 4 (8-10s): Resolve/transition
- eyeContact: String describing key gaze moments between characters
- physicsNote: String with body language hints (weight, breathing, tension)
- environmentSync: String describing how environment reacts to action

Example choreography:
{
  "sceneArc": "build_to_climax",
  "intensityProgression": [0.4, 0.6, 0.85, 0.7],
  "blocking": "Kai center-left facing Mei at center-right, 4ft apart. Kai steps forward to 2ft by beat 3.",
  "keyBeats": [
    "Beat 1 (0-2s): Mei reveals artifact, Kai's eyes widen in recognition",
    "Beat 2 (2-5s): Kai reaches hesitantly toward artifact, internal conflict visible",
    "Beat 3 (5-8s): Kai grasps artifact, both feel its power surge through them",
    "Beat 4 (8-10s): Eyes meet over glowing artifact, alliance sealed"
  ],
  "eyeContact": "Kai: artifact â†’ Mei's hands â†’ artifact â†’ Mei's eyes",
  "physicsNote": "Kai: weight back (hesitant) â†’ forward (committed). Visible breath held at beat 3.",
  "environmentSync": "Artifact glow: soft â†’ pulsing â†’ flare at contact â†’ warm steady"
}

=== HOLLYWOOD CINEMATOGRAPHY REQUIREMENTS FOR visualPrompt ===
Each visualPrompt MUST be a RICH, DETAILED cinematographic description with ALL these elements:

1. SUBJECT DESCRIPTION (Required):
   - Specific appearance: age, build, distinguishing features
   - Clothing: specific garments, textures, condition
   - Pose/body language: specific position and posture
   - Expression: emotional state visible on face
   - Action: what they're actively doing

2. ENVIRONMENT IN THREE LAYERS (Required):
   - FOREGROUND: Elements closest to camera (objects, textures, frame elements)
   - MIDGROUND: Main action area where subject exists
   - BACKGROUND: Distant elements providing depth and context

3. ATMOSPHERE & PARTICLES (Required):
   - Time of day and specific lighting conditions
   - Atmospheric effects: dust motes, smoke wisps, embers, rain, mist, god rays
   - Weather and environmental mood

4. LIGHTING DESIGN (Required):
   - Key light: main source, direction, color temperature (warm/cool/Kelvin)
   - Shadow quality: hard, soft, dramatic, chiaroscuro
   - Practical lights: visible light sources in scene (lamps, screens, fire)
   - Overall contrast level

5. CAMERA LANGUAGE (Required):
   - Shot type: extreme-wide, wide, medium, close-up, extreme-close-up
   - Camera angle: eye-level, low-angle (power), high-angle (vulnerability), dutch (tension)
   - Movement in [brackets]: push-in, tracking, handheld, crane, static
   - Subject placement: center, rule-of-thirds left/right, leading space

6. STYLE & TEXTURE (Required):
   - Film look: cinematic reference if applicable
   - Depth of field: shallow with bokeh, or deep focus
   - Color palette: dominant and accent colors
   - Quality markers: 8K, photorealistic, detailed textures

POOR EXAMPLE (DO NOT DO THIS):
"[Push in] A workshop with an inventor working on something."

EXCELLENT EXAMPLE (DO THIS):
"[Slow push-in, low-angle] A weathered inventor in his 60s with wild gray hair and oil-stained leather apron hunches intensely over a glowing brass mechanism, his calloused hands carefully adjusting delicate gears, determined expression lit by sparks. Foreground: scattered brass gears and burnt-out bulbs on worn wooden table. Midground: cluttered workbench with smoking apparatus, copper tubing, half-assembled devices. Background: brick walls covered in yellowed blueprints and shelves of failed prototypes. Late night atmosphere, single overhead work lamp casting harsh amber tungsten light (3200K), deep dramatic shadows across his face, dust motes and tiny sparks floating in the warm glow. Steam rising from cooling metal. Shallow depth of field with bokeh on background lights. Steampunk industrial aesthetic, Ridley Scott visual style."

=== CRITICAL: CINEMATIC SCENE ARCHITECTURE ===
You are NOT creating a presentation with voiceover. You are creating a FILM/TV PRODUCTION.
Each scene must be structured like actual cinema - with proper scene types and audio layers.

ðŸŽ¬ SCENE TYPES - Choose the RIGHT type for each scene:
- "opening_narration": Narrator sets the stage (LOTR opening, Pixar intros) - use for scene 1
- "dialogue": Characters speak to each other - THE HEART OF DRAMA
- "action": Physical action, chase, fight - minimal/no dialogue, SFX + music
- "emotional": Character reflection, internal moment - close-ups, soft lighting
- "montage": Time passage, preparation - quick cuts, music-driven
- "revelation": Plot twist, discovery - dramatic pause then reaction
- "closing_narration": Narrator reflects on meaning - final scene wrap-up

ðŸŽ§ AUDIO LAYER - What we HEAR depends on scene type:

1. For "dialogue" scenes - audioLayer.type = "dialogue":
   audioLayer: {
     type: "dialogue",
     dialogue: [
       { character: "Maya", line: "We have to leave. Now.", emotion: "urgent" },
       { character: "Kai", line: "Not without the artifact.", emotion: "determined" }
     ],
     sfx: ["wind_howling", "distant_thunder"],
     musicMood: "tension_building"
   }

2. For "opening_narration" or "closing_narration" - audioLayer.type = "voiceover":
   audioLayer: {
     type: "voiceover",
     voiceover: "The world was young then... before the darkness came.",
     sfx: ["gentle_wind"],
     musicMood: "epic_scale"
   }

3. For "action" scenes - audioLayer.type = "action_sfx":
   audioLayer: {
     type: "action_sfx",
     voiceover: null,
     dialogue: null,
     sfx: ["explosion", "sword_clash", "footsteps_running"],
     musicMood: "action_intense"
   }

4. For "emotional" scenes - audioLayer.type = "internal_monologue" or "music_only":
   audioLayer: {
     type: "internal_monologue",
     internalMonologue: { character: "Maya", thought: "What have I done?" },
     sfx: ["rain_on_window"],
     musicMood: "emotional_sad"
   }

ðŸŽ¬ TRANSITION PURPOSE - Every transition must serve the story:
- "cut": Normal flow, same energy
- "smash_cut": Shock, sudden contrast
- "fade_to_black": Chapter end, time passage, death
- "dissolve": Memory, dream, time passing
- "match_cut": Thematic connection between scenes
- "j_cut": Next scene's audio starts before visual (builds anticipation)
- "l_cut": Previous audio continues over new visual (emotional continuity)

ðŸ“¹ visualPrompt - What we SEE (unchanged - still detailed cinematography):
   - Start with [Camera Movement] in brackets
   - Include ALL cinematography elements (lighting, composition, atmosphere)
   - Minimum 150 words per scene
   - When characters appear, use their EXACT visual descriptions

âš ï¸ KEY DIFFERENCES FROM PRESENTATION:
- Presentation: Every scene has narrator talking
- Cinema: Scenes flow between dialogue, action, emotion, narration
- Presentation: Same energy throughout
- Cinema: Ebb and flow - tension, release, intimacy, spectacle
- Presentation: Narrator describes what we see
- Cinema: Audio ADDS meaning, doesn't describe visuals

SCENE DISTRIBUTION FOR FILM QUALITY:
${productionMode === 'cinematic' || productionMode === 'story' ? `
- Scene 1: opening_narration (set the world)
- Scenes 2-3: dialogue + action (establish conflict)
- Scenes 4-${sceneCount - 2}: Mix of dialogue, action, emotional (rising action)
- Scene ${sceneCount - 1}: revelation or climax action
- Scene ${sceneCount}: closing_narration or emotional resolution
` : `
- Opening: Can use voiceover to hook
- Middle: Mix scene types based on content needs
- Climax: Action or revelation
- Closing: Resolution with optional closing narration
`}

MUSIC MOOD OPTIONS (choose one per scene):
tension_building, action_intense, emotional_sad, emotional_hopeful,
triumphant, mysterious, peaceful, epic_scale, intimate, suspense

=== OUTPUT FORMAT ===
Return ONLY valid JSON:
{
  "title": "Compelling ${productionSettings.name} style title (50-70 chars)",
  "hook": "The attention-grabbing opening - can be narration or impactful dialogue",
  "characters": [
    {
      "name": "Character's name",
      "archetype": "The archetype (e.g., Reluctant Hero, Wise Mentor, Hidden Villain)",
      "visualDescription": "DETAILED visual description - exact appearance, clothing, features, distinguishing marks",
      "voiceDescription": "How they sound - accent, pitch, speech patterns, emotional range",
      "role": "Their role in the story",
      "motivation": "What drives this character",
      "firstAppearance": 1,
      "sceneAppearances": [1, 3, 5]
    }
  ],
  "scenes": [
    {
      "id": 1,
      "sceneType": "opening_narration|dialogue|action|emotional|montage|revelation|closing_narration",
      "location": {
        "name": "The specific location name (e.g., 'The Dojo', 'Corporate Tower', 'City Streets')",
        "timeOfDay": "dawn|morning|midday|afternoon|golden_hour|dusk|night|deep_night",
        "weather": "clear|overcast|rain_light|rain_heavy|fog|snow|storm",
        "condition": "pristine|lived_in|neglected|damaged_light|damaged_heavy|destroyed|repaired",
        "conditionDetails": "Specific details about damage or changes if not pristine/lived_in"
      },
      "visualPrompt": "[Camera Movement] Detailed cinematography for AI video generation",
      "visual": "[Camera Movement] Same as visualPrompt for backwards compatibility",
      "sceneAction": "DETAILED ACTION SEQUENCE: The complete story action for this ${actualSceneDuration}-second scene. Describe ALL physical actions, character movements, interactions, and reactions in sequence. This will be divided into ${actualShotsPerScene} shots of ${clipDuration} seconds each. Example for a 30s scene with 3 shots: 'Kai stands on the rooftop surveying the city, his coat rippling in the wind. He turns to face Ryu and Li Mei, speaking with growing conviction as he outlines their mission. Ryu steps forward with a confident nod, cracking his knuckles. Li Mei places her hand on Kai\\'s shoulder reassuringly. Together they turn toward the rooftop edge. In unison, the three heroes leap from the building, coats billowing as they descend toward the neon-lit streets below.' Include: character movements, gestures, expressions changing, physical interactions, environmental responses.",
      "audioLayer": {
        "type": "voiceover|dialogue|internal_monologue|action_sfx|music_only",
        "voiceover": "Narrator text if type=voiceover, otherwise null",
        "dialogue": [
          { "character": "Name", "line": "What they say", "emotion": "how they feel" }
        ],
        "internalMonologue": { "character": "Name", "thought": "Their internal thought" },
        "sfx": ["sound_effect_1", "sound_effect_2"],
        "musicMood": "tension_building|action_intense|emotional_sad|etc",
        "musicTransition": "continue|fade_out|crescendo|sudden_stop"
      },
      "narration": "BACKWARDS COMPATIBLE: voiceover text or null (populated from audioLayer.voiceover)",
      "duration": ${visualDuration},
      "cameraMovement": ["Push in"],
      "cameraIntent": "Why this camera move - what emotion/story it serves",
      "transition": {
        "type": "cut|smash_cut|fade_to_black|dissolve|match_cut|j_cut|l_cut",
        "purpose": "Why this transition - what story beat it serves"
      },
      "charactersInScene": ["Character names from characters array"],
      "sceneRole": "setup|conflict|rising_action|climax|resolution",

      "choreography": {
        "sceneArc": "build_to_climax|tension_release|steady_build|peak_then_calm",
        "intensityProgression": [0.4, 0.6, 0.85, 0.7],
        "blocking": "Character positions and movements (e.g., 'Kai center-left facing Mei, moves to center')",
        "keyBeats": [
          "Beat 1 (0-2s): Establish - what happens",
          "Beat 2 (2-5s): Develop - what happens",
          "Beat 3 (5-8s): Peak - what happens",
          "Beat 4 (8-10s): Resolve - what happens"
        ],
        "eyeContact": "Key gaze moments (e.g., 'Kai looks at artifact then to Mei')",
        "physicsNote": "Body language hints (e.g., 'weight forward, breath held')",
        "environmentSync": "Environment reactions (e.g., 'artifact glows brighter at contact')"
      }
    }
  ],
  "cta": "Call-to-action woven naturally into story (not presentation-style)",
  "totalDuration": ${targetDuration},
  "timing": {
    "sceneCount": ${sceneCount},
    "avgSceneDuration": ${actualSceneDuration},
    "pacing": "${pacing}",
    "clipDuration": ${clipDuration},
    "shotsPerScene": ${actualShotsPerScene},
    "shotDuration": ${shotDuration},
    "totalShots": ${sceneCount * actualShotsPerScene},
    "contentFormat": "${contentFormat}",
    "architecture": "hollywood"
  },
  "productionMode": "${productionMode}",
  "audioDesign": {
    "overallMusicStyle": "The musical theme/genre for the whole piece",
    "musicProgression": "How music evolves: calmâ†’tenseâ†’epicâ†’resolution",
    "keyAudioMoments": [
      { "sceneId": 1, "moment": "Music swells as hero makes decision" }
    ],
    "silentMoments": ["Scene IDs where silence is used dramatically"],
    "sfxHighlights": ["Key sound effects that drive story"]
  },
  "metadata": {
    "targetAudience": "Who this video is for",
    "keyMessage": "The main takeaway",
    "emotionalArc": "${productionSettings.emotionalArc}",
    "productionStyle": "Film genre this most resembles"
  },
  "storyArchitecture": {
    "centralTheme": "The core theme threading through all scenes",
    "characterArcs": ["Brief description of each character's journey/growth"],
    "visualSignatureUsed": "How the visual signature was applied",
    "iconicMoments": ["Scene IDs that contain memorable/iconic visual moments"],
    "storyComplexity": "simple|layered|complex",
    "sceneTypeDistribution": {
      "dialogue": 0,
      "action": 0,
      "narration": 0,
      "emotional": 0
    }
  },
  "locations": [
    {
      "name": "Location name (e.g., 'The Dojo', 'Corporate Tower')",
      "description": "Detailed visual description of this location's architecture, key elements, atmosphere",
      "keyElements": ["Key visual element 1", "Key visual element 2", "Key visual element 3"],
      "colorPalette": "The dominant colors and textures of this location",
      "atmosphere": "The mood and feeling of this place",
      "scenesUsed": [1, 4, 7]
    }
  ],
  "styleBible": {
    "visualStyle": "Overall visual approach (e.g., 'ultra-cinematic noir thriller')",
    "colorGrade": "Color grading approach (e.g., 'desaturated teal shadows, amber highlights')",
    "lighting": "Lighting philosophy (e.g., 'harsh single-source, dramatic rim lights')",
    "atmosphere": "Environmental elements (e.g., 'smoke, rain reflections, urban grit')",
    "cameraLanguage": "Camera philosophy (e.g., 'handheld tension, slow push-ins for reveals')"
  }
}

CRITICAL REQUIREMENTS:
1. The "characters" array MUST contain ALL characters with complete visual AND voice descriptions
2. Each scene's "audioLayer" MUST match its "sceneType" - dialogue scenes need dialogue array, action needs sfx
3. The "visualPrompt" must describe the SCENE ACTION - what is happening, where, how it looks cinematically
   - DO NOT just describe a character standing there
   - MUST include: the action taking place, character interactions, location details, cinematography
   - Example: "Kai blocks The Shadow Warrior's strike in the rain-soaked dojo, their weapons locked, determination in his eyes"
4. "narration" field must be populated from audioLayer for backwards compatibility
5. At least 40% of scenes should be "dialogue" type for character-driven stories
6. transition.purpose MUST explain WHY that transition was chosen

LOCATION CONSISTENCY RULES:
7. The "locations" array MUST define EVERY unique location used in scenes
8. When the story returns to a location, the base visual elements MUST stay the same
   - Same architecture, same key elements, same atmosphere
   - ONLY the state changes: timeOfDay, weather, condition
9. Location condition changes must be LOGICAL:
   - If battle happened at location in scene 3, show damage in scene 7
   - If time passed, show appropriate changes
10. Each scene's "location" field must reference a location from the "locations" array
11. The "styleBible" must be consistent across ALL scenes - this is the visual DNA of the entire production`;


    // Log enrichment data for debugging
    if (conceptEnrichment) {
      console.log('[creationWizardGenerateScript] Deep Story Architecture active:', {
        hasImprovedConcept: !!conceptEnrichment.improvedConcept,
        hasSelectedConcept: !!conceptEnrichment.selectedConcept,
        preDefinedCharacters: conceptEnrichment.characters?.length || 0,
        requestedCharacterCount: conceptEnrichment.characterIntelligence?.suggestedCount || 0,
        narrationMode: conceptEnrichment.characterIntelligence?.narrationMode || 'voiceover',
        hasWorldBuilding: !!conceptEnrichment.worldBuilding,
        hasVisualSignature: !!conceptEnrichment.visualSignature,
        keyElements: conceptEnrichment.keyElements?.length || 0
      });
    }

    // Call GPT-4o for script generation
    // Note: max_tokens increased to 6500 for cinematic production with:
    // - Full audioLayer structure per scene
    // - Character dialogue arrays
    // - audioDesign metadata
    // - Detailed transition purposes

    // Log prompt sizes for debugging timeout issues
    const systemPromptLength = systemPrompt.length;
    const userPromptLength = userPrompt.length;
    console.log(`[creationWizardGenerateScript] Prompt sizes - System: ${systemPromptLength} chars, User: ${userPromptLength} chars, Total: ${systemPromptLength + userPromptLength} chars`);
    console.log(`[creationWizardGenerateScript] Starting GPT-4o call at ${new Date().toISOString()}`);

    const startTime = Date.now();
    const completion = await openai.chat.completions.create({
      model: 'gpt-4o',
      messages: [
        { role: 'system', content: systemPrompt },
        { role: 'user', content: userPrompt }
      ],
      response_format: { type: 'json_object' }, // CRITICAL: Force valid JSON output
      temperature: conceptEnrichment ? 0.85 : 0.8, // Slightly higher creativity for enriched concepts
      max_tokens: 6500 // Cinematic production requires more tokens for rich scene structure
    });

    const elapsedTime = Date.now() - startTime;
    console.log(`[creationWizardGenerateScript] GPT-4o completed in ${elapsedTime}ms (${(elapsedTime / 1000).toFixed(1)}s)`);

    // Parse the response
    const responseText = completion.choices[0].message.content.trim();
    const cleanJson = responseText.replace(/```json\n?/g, '').replace(/```\n?/g, '').trim();

    let script;
    try {
      script = JSON.parse(cleanJson);
    } catch (parseError) {
      console.error('[creationWizardGenerateScript] JSON parse error:', parseError);
      console.error('[creationWizardGenerateScript] Raw response:', responseText);
      throw new functions.https.HttpsError('internal', 'Failed to parse script response');
    }

    // Validate and normalize the script structure
    if (!script.scenes || !Array.isArray(script.scenes) || script.scenes.length === 0) {
      throw new functions.https.HttpsError('internal', 'Invalid script structure - no scenes generated');
    }

    // Calculate default durations if not provided
    const defaultVisualDuration = Math.round(targetDuration / script.scenes.length);
    const defaultNarrationDuration = Math.round(defaultVisualDuration * 0.7);

    // Ensure each scene has required fields including timing and camera movements
    script.scenes = script.scenes.map((scene, index) => {
      // Handle narration - can be null for cinematic/music-only scenes
      const hasNarration = scene.narration !== null && scene.narration !== undefined && scene.narration.trim() !== '';
      const narrationText = hasNarration ? scene.narration.trim() : null;

      // Count words in narration to estimate actual duration (0 if no narration)
      const wordCount = hasNarration ? narrationText.split(/\s+/).filter(w => w.length > 0).length : 0;
      const estimatedNarrationDuration = hasNarration ? Math.ceil(wordCount / 2.5) : 0; // ~2.5 words per second

      // Get visual prompt (new field) or fall back to visual (legacy) or narration (very old)
      const visualPrompt = scene.visualPrompt || scene.visual || '';

      // Extract camera movements from visual description if not provided
      let cameraMovements = scene.cameraMovement || [];
      if (cameraMovements.length === 0 && visualPrompt) {
        // Try to extract [Camera Movement] from visual description
        const bracketMatch = visualPrompt.match(/^\[([^\]]+)\]/);
        if (bracketMatch) {
          cameraMovements = bracketMatch[1].split(',').map(m => m.trim()).slice(0, 3);
        }
      }

      return {
        id: scene.id || index + 1,
        // NEW: Separate visual prompt for AI video generation (what we SEE)
        visualPrompt: visualPrompt,
        // NEW: Narration for voiceover (what we HEAR) - can be null
        narration: narrationText,
        // Flag to indicate if this scene has voiceover
        hasNarration: hasNarration,
        // When narration starts within the scene (in seconds)
        narrationStartTime: scene.narrationStartTime || 0.5,
        // Legacy field for backwards compatibility
        visual: visualPrompt,
        // Scene mood for audio/visual matching
        mood: scene.mood || 'neutral',
        // Visual duration is how long the scene appears on screen
        visualDuration: scene.visualDuration || scene.duration || defaultVisualDuration,
        // Narration duration is how long the voiceover is (estimated from word count)
        narrationDuration: estimatedNarrationDuration || 0,
        // Keep legacy duration field for backwards compatibility
        duration: scene.visualDuration || scene.duration || defaultVisualDuration,
        wordCount: wordCount,
        // Camera movements for Minimax AI video generation
        cameraMovement: cameraMovements,
        transition: scene.transition || 'cut',
        status: 'pending' // For tracking storyboard/animation progress
      };
    });

    // Calculate actual total duration (visual duration, not narration)
    script.totalDuration = script.scenes.reduce((sum, scene) => sum + scene.visualDuration, 0);
    script.totalNarrationDuration = script.scenes.reduce((sum, scene) => sum + scene.narrationDuration, 0);

    // Add generation metadata including timing info
    script.generatedAt = new Date().toISOString();
    script.generationConfig = {
      platform,
      aspectRatio,
      targetDuration,
      niche,
      subniche,
      style,
      tone,
      pacing,
      contentDepth,
      productionMode,
      // Phase 3A: Genre Reference
      genre: genreKey,
      genreName: genreSettings?.name || null,
      contentFormat: formatKey,
      videoModel
    };
    script.timing = script.timing || {
      sceneCount: script.scenes.length,
      visualDurationPerScene: defaultVisualDuration,
      narrationDurationPerScene: defaultNarrationDuration,
      pacing: pacing
    };

    // If projectId provided, update the project with the script
    if (projectId) {
      await db.collection('creationProjects').doc(projectId).update({
        script: {
          ...script,
          status: 'generated'
        },
        'project.status': 'script_ready',
        updatedAt: admin.firestore.FieldValue.serverTimestamp()
      });
    }

    // Log usage for analytics
    await db.collection('apiUsage').add({
      userId: uid,
      type: 'script_generation',
      model: 'gpt-4o',
      inputTokens: completion.usage?.prompt_tokens || 0,
      outputTokens: completion.usage?.completion_tokens || 0,
      projectId: projectId || null,
      timestamp: admin.firestore.FieldValue.serverTimestamp()
    });

    return {
      success: true,
      script,
      usage: {
        promptTokens: completion.usage?.prompt_tokens || 0,
        completionTokens: completion.usage?.completion_tokens || 0,
        totalTokens: completion.usage?.total_tokens || 0
      }
    };

  } catch (error) {
    console.error('[creationWizardGenerateScript] Error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to generate script'));
  }
});

/**
 * creationWizardExtractCharacters - Analyzes script and extracts character descriptions
 *
 * This enables the Scene Memory System's Character Bible to be auto-populated
 * based on the script content, ensuring visual consistency across scenes.
 */
exports.creationWizardExtractCharacters = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const { script, genre, productionMode, styleBible } = data;

  if (!script || !script.scenes || !Array.isArray(script.scenes)) {
    throw new functions.https.HttpsError('invalid-argument', 'Valid script with scenes required');
  }

  try {
    // Combine all scene content for analysis
    const sceneContent = script.scenes.map((scene, idx) =>
      `Scene ${idx + 1}:\nNarration: ${scene.narration || 'No narration'}\nVisual: ${scene.visual || scene.visualPrompt || 'No visual description'}`
    ).join('\n\n');

    const systemPrompt = `You are an expert at analyzing video scripts and identifying characters for visual consistency.
Your task is to extract all characters that appear in the script and create detailed visual descriptions for AI image generation.

CRITICAL RULES:
1. Focus on characters that APPEAR VISUALLY in the video (not just mentioned in narration)
2. Create SPECIFIC, CONSISTENT descriptions that can be used across all scenes
3. Include: age, gender, ethnicity, build, hair, eyes, distinctive features, clothing
4. Make descriptions CONCRETE, not vague (e.g., "short dark brown hair" not "dark hair")
5. Consider the genre and style to match character descriptions appropriately
6. If the script is abstract/conceptual with no human characters, return empty array
7. Maximum 5 characters (focus on main/recurring characters)

GENRE CONSIDERATIONS:
- Corporate/Business: Professional attire, polished appearance
- Action/Adventure: Practical clothing, battle-ready look
- Fantasy: Period-appropriate or magical attire
- Sci-Fi: Futuristic clothing, tech accessories
- Documentary: Natural, authentic appearance
- Lifestyle: Contemporary casual or stylish clothing

Return valid JSON only, no markdown formatting.`;

    const userPrompt = `Analyze this script and extract character descriptions for visual consistency.

=== SCRIPT CONTENT ===
Title: ${script.title || 'Untitled'}
Genre: ${genre || 'General'}
Production Mode: ${productionMode || 'standard'}

${sceneContent}

${styleBible?.enabled ? `
=== STYLE BIBLE (match characters to this style) ===
Style: ${styleBible.style || 'cinematic'}
Color Grade: ${styleBible.colorGrade || 'natural'}
Atmosphere: ${styleBible.atmosphere || 'standard'}
` : ''}

=== REQUIRED OUTPUT FORMAT ===
{
  "characters": [
    {
      "name": "Character Name or Role (e.g., 'The Protagonist', 'Sarah', 'The CEO')",
      "description": "Detailed physical description for AI image generation: age, gender, ethnicity, build, hair color and style, eye color, distinctive features, specific clothing and accessories. Be very specific and concrete.",
      "role": "Main/Supporting/Background",
      "appearsInScenes": [1, 2, 5],
      "traits": ["confident", "mysterious", "professional"]
    }
  ],
  "hasHumanCharacters": true,
  "suggestedStyleNote": "Optional note about character style recommendations"
}

If there are no human characters or the video is purely abstract/conceptual/nature footage, return:
{
  "characters": [],
  "hasHumanCharacters": false,
  "suggestedStyleNote": "This script focuses on [type of content] without human characters."
}`;

    const completion = await openai.chat.completions.create({
      model: 'gpt-4o-mini',
      messages: [
        { role: 'system', content: systemPrompt },
        { role: 'user', content: userPrompt }
      ],
      response_format: { type: 'json_object' }, // Force valid JSON output
      temperature: 0.7,
      max_tokens: 2000
    });

    const responseText = completion.choices[0].message.content;
    console.log('[extractCharacters] Raw response:', responseText.substring(0, 500));

    // Parse the response
    let result;
    try {
      result = JSON.parse(responseText.trim());
    } catch (parseError) {
      console.error('[extractCharacters] Parse error:', parseError);
      // Return empty result on parse error
      result = {
        characters: [],
        hasHumanCharacters: false,
        suggestedStyleNote: 'Could not analyze script for characters.'
      };
    }

    // Normalize and validate the result
    const characters = (result.characters || []).map((char, idx) => ({
      id: `char-${Date.now()}-${idx}`,
      name: char.name || `Character ${idx + 1}`,
      description: char.description || '',
      role: char.role || 'Supporting',
      appearsInScenes: char.appearsInScenes || [],
      traits: char.traits || [],
      referenceImageUrl: null,
      referenceImageBase64: null,
      referenceImageMimeType: null,
      referenceImageStatus: 'none',
      appliedToScenes: char.appearsInScenes || []
    }));

    console.log(`[extractCharacters] Extracted ${characters.length} characters`);

    return {
      success: true,
      characters,
      hasHumanCharacters: result.hasHumanCharacters !== false,
      suggestedStyleNote: result.suggestedStyleNote || null,
      usage: {
        promptTokens: completion.usage?.prompt_tokens || 0,
        completionTokens: completion.usage?.completion_tokens || 0
      }
    };

  } catch (error) {
    console.error('[extractCharacters] Error:', error);
    // Return empty result instead of throwing to not block the flow
    return {
      success: false,
      characters: [],
      hasHumanCharacters: false,
      error: error.message
    };
  }
});

/**
 * creationWizardRegenerateScene - Regenerates a single scene's script
 */
exports.creationWizardRegenerateScene = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const { projectId, sceneId, currentScript, instructions } = data;

  if (!currentScript || !sceneId) {
    throw new functions.https.HttpsError('invalid-argument', 'Scene ID and current script required');
  }

  const scene = currentScript.scenes?.find(s => s.id === sceneId);
  if (!scene) {
    throw new functions.https.HttpsError('not-found', 'Scene not found');
  }

  try {
    const systemPrompt = `You are an expert video scriptwriter. You're improving a single scene in an existing script.
Maintain consistency with the overall video topic and style.
Return only valid JSON matching the exact structure provided.`;

    const userPrompt = `Regenerate this scene with improvements:

=== CURRENT SCENE ===
Scene ${sceneId} of ${currentScript.scenes.length}
Narration: "${scene.narration}"
Visual: "${scene.visual}"
Duration: ${scene.duration} seconds

=== VIDEO CONTEXT ===
Title: ${currentScript.title}
Topic: ${currentScript.generationConfig?.topic || 'General'}
Style: ${currentScript.generationConfig?.style || 'modern'}

${instructions ? `=== SPECIFIC INSTRUCTIONS ===\n${instructions}\n` : ''}

=== REQUIREMENTS ===
- Keep similar duration (~${scene.duration} seconds)
- Make narration more engaging/clear
- Make visual description more detailed for AI image generation
- Maintain consistency with surrounding scenes

Return ONLY valid JSON:
{
  "narration": "Improved voiceover text",
  "visual": "Detailed visual description for AI image generation",
  "duration": ${scene.duration},
  "transition": "${scene.transition || 'cut'}"
}`;

    const completion = await openai.chat.completions.create({
      model: 'gpt-4o',
      messages: [
        { role: 'system', content: systemPrompt },
        { role: 'user', content: userPrompt }
      ],
      response_format: { type: 'json_object' }, // Force valid JSON output
      temperature: 0.8,
      max_tokens: 500
    });

    const responseText = completion.choices[0].message.content.trim();
    const newScene = JSON.parse(responseText);

    // Merge with existing scene data
    const updatedScene = {
      ...scene,
      narration: newScene.narration || scene.narration,
      visual: newScene.visual || scene.visual,
      duration: newScene.duration || scene.duration,
      transition: newScene.transition || scene.transition,
      regeneratedAt: new Date().toISOString()
    };

    return {
      success: true,
      scene: updatedScene
    };

  } catch (error) {
    console.error('[creationWizardRegenerateScene] Error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to regenerate scene'));
  }
});

// ============================================================
// PHASE 3B: VISUAL INTELLIGENCE SYSTEM
// Cinematic compositions, lighting, and color grading
// ============================================================

/**
 * VISUAL INTELLIGENCE - Comprehensive visual enhancement system
 * Maps genres, moods, and production modes to cinematic visual styles
 */
const VISUAL_INTELLIGENCE = {
  // === SHOT COMPOSITION LIBRARY ===
  compositions: {
    'wide-establishing': {
      name: 'Wide Establishing Shot',
      prompt: 'wide angle establishing shot, expansive view, full environment visible, cinematic scope',
      useFor: ['opening', 'context', 'scale'],
      genres: ['documentary-nature', 'documentary-historical', 'cinematic']
    },
    'medium-shot': {
      name: 'Medium Shot',
      prompt: 'medium shot, waist-up framing, balanced composition, subject clearly visible',
      useFor: ['dialogue', 'action', 'explanation'],
      genres: ['educational-tutorial', 'business-testimonial', 'social-storytime']
    },
    'close-up': {
      name: 'Close-Up',
      prompt: 'close-up shot, face or detail filling frame, intimate framing, emotional connection',
      useFor: ['emotion', 'detail', 'emphasis'],
      genres: ['entertainment-drama', 'inspirational', 'documentary-true-crime']
    },
    'extreme-close-up': {
      name: 'Extreme Close-Up',
      prompt: 'extreme close-up, macro detail, texture visible, dramatic intimacy',
      useFor: ['tension', 'detail', 'reveal'],
      genres: ['entertainment-horror', 'documentary-nature', 'educational-science']
    },
    'over-the-shoulder': {
      name: 'Over the Shoulder',
      prompt: 'over-the-shoulder shot, perspective framing, depth layering, conversational angle',
      useFor: ['dialogue', 'pov', 'interaction'],
      genres: ['entertainment-drama', 'business-testimonial', 'documentary-social']
    },
    'birds-eye': {
      name: 'Bird\'s Eye View',
      prompt: 'birds eye view, top-down perspective, god-like vantage point, pattern visible from above',
      useFor: ['scale', 'overview', 'drama'],
      genres: ['documentary-nature', 'cinematic', 'documentary-historical']
    },
    'low-angle': {
      name: 'Low Angle',
      prompt: 'low angle shot, looking up at subject, powerful imposing presence, heroic framing',
      useFor: ['power', 'inspiration', 'drama'],
      genres: ['inspirational', 'business-brand', 'entertainment-drama']
    },
    'dutch-angle': {
      name: 'Dutch Angle',
      prompt: 'dutch angle, tilted frame, disorienting composition, psychological tension',
      useFor: ['unease', 'tension', 'chaos'],
      genres: ['entertainment-horror', 'thriller', 'entertainment-drama']
    },
    'symmetrical': {
      name: 'Symmetrical',
      prompt: 'perfectly symmetrical composition, centered subject, balanced framing, Wes Anderson style',
      useFor: ['order', 'beauty', 'emphasis'],
      genres: ['cinematic', 'business-product', 'educational-explainer']
    },
    'rule-of-thirds': {
      name: 'Rule of Thirds',
      prompt: 'rule of thirds composition, subject off-center, balanced negative space, professional framing',
      useFor: ['standard', 'dynamic', 'professional'],
      genres: ['all']
    },
    'leading-lines': {
      name: 'Leading Lines',
      prompt: 'leading lines composition, visual path guiding eye to subject, depth perspective, dynamic composition',
      useFor: ['journey', 'direction', 'depth'],
      genres: ['documentary-historical', 'inspirational', 'business-brand']
    },
    'frame-within-frame': {
      name: 'Frame Within Frame',
      prompt: 'frame within frame composition, subject framed by environment, layered depth, cinematic framing',
      useFor: ['focus', 'isolation', 'art'],
      genres: ['cinematic', 'entertainment-drama', 'documentary-social']
    }
  },

  // === LIGHTING STYLES ===
  lighting: {
    'natural': {
      name: 'Natural Light',
      prompt: 'natural lighting, soft daylight, realistic illumination, organic light sources',
      mood: 'authentic, documentary, honest',
      genres: ['documentary-nature', 'documentary-social', 'educational-tutorial']
    },
    'golden-hour': {
      name: 'Golden Hour',
      prompt: 'golden hour lighting, warm orange sunset glow, magical hour, romantic light',
      mood: 'warm, hopeful, beautiful',
      genres: ['inspirational', 'documentary-nature', 'business-brand']
    },
    'blue-hour': {
      name: 'Blue Hour',
      prompt: 'blue hour lighting, cool twilight tones, contemplative atmosphere, pre-dawn or post-sunset',
      mood: 'melancholy, thoughtful, peaceful',
      genres: ['documentary-historical', 'entertainment-drama', 'cinematic']
    },
    'high-key': {
      name: 'High Key',
      prompt: 'high key lighting, bright even illumination, minimal shadows, clean professional look',
      mood: 'clean, optimistic, professional',
      genres: ['business-product', 'educational-explainer', 'business-testimonial']
    },
    'low-key': {
      name: 'Low Key / Noir',
      prompt: 'low key lighting, dramatic shadows, high contrast, film noir style, chiaroscuro',
      mood: 'mysterious, dramatic, intense',
      genres: ['entertainment-horror', 'thriller', 'documentary-true-crime', 'entertainment-drama']
    },
    'rembrandt': {
      name: 'Rembrandt Lighting',
      prompt: 'Rembrandt lighting, triangle of light on cheek, classic portrait lighting, dramatic yet natural',
      mood: 'classic, artistic, dignified',
      genres: ['documentary-historical', 'business-testimonial', 'cinematic']
    },
    'neon': {
      name: 'Neon / Cyberpunk',
      prompt: 'neon lighting, vibrant colored lights, cyberpunk aesthetic, electric glow, urban night',
      mood: 'futuristic, edgy, energetic',
      genres: ['educational-science', 'social-viral', 'entertainment-comedy']
    },
    'silhouette': {
      name: 'Silhouette',
      prompt: 'silhouette lighting, backlit subject, dramatic outline, mysterious figure against light',
      mood: 'dramatic, mysterious, iconic',
      genres: ['inspirational', 'cinematic', 'documentary-social']
    },
    'soft-diffused': {
      name: 'Soft Diffused',
      prompt: 'soft diffused lighting, gentle shadows, flattering illumination, dreamy atmosphere',
      mood: 'gentle, approachable, comforting',
      genres: ['educational-tutorial', 'business-testimonial', 'inspirational']
    },
    'harsh-dramatic': {
      name: 'Harsh Dramatic',
      prompt: 'harsh dramatic lighting, strong shadows, high contrast, intense spotlight effect',
      mood: 'intense, confrontational, powerful',
      genres: ['entertainment-horror', 'thriller', 'business-brand']
    }
  },

  // === COLOR PALETTES ===
  colorPalettes: {
    'teal-orange': {
      name: 'Teal & Orange',
      prompt: 'teal and orange color grading, complementary colors, cinematic blockbuster look, Hollywood color palette',
      mood: 'cinematic, dynamic, polished',
      genres: ['cinematic', 'entertainment-drama', 'business-brand']
    },
    'desaturated': {
      name: 'Desaturated',
      prompt: 'desaturated color palette, muted tones, subtle colors, understated elegance',
      mood: 'serious, sophisticated, documentary',
      genres: ['documentary-historical', 'documentary-true-crime', 'entertainment-drama']
    },
    'high-saturation': {
      name: 'High Saturation',
      prompt: 'high saturation colors, vibrant vivid palette, eye-catching hues, bold color choices',
      mood: 'energetic, fun, attention-grabbing',
      genres: ['social-viral', 'entertainment-comedy', 'educational-explainer']
    },
    'warm-tones': {
      name: 'Warm Tones',
      prompt: 'warm color palette, orange red yellow tones, cozy inviting atmosphere, sunset colors',
      mood: 'nostalgic, comforting, friendly',
      genres: ['inspirational', 'documentary-historical', 'business-testimonial']
    },
    'cool-tones': {
      name: 'Cool Tones',
      prompt: 'cool color palette, blue green teal tones, calm professional atmosphere, modern feel',
      mood: 'professional, calm, trustworthy',
      genres: ['educational-science', 'business-product', 'documentary-social']
    },
    'monochromatic': {
      name: 'Monochromatic',
      prompt: 'monochromatic color scheme, single color variations, sophisticated unified look, artistic palette',
      mood: 'artistic, focused, elegant',
      genres: ['cinematic', 'business-brand', 'documentary-historical']
    },
    'noir-bw': {
      name: 'Noir Black & White',
      prompt: 'black and white, film noir aesthetic, high contrast monochrome, classic cinema look',
      mood: 'timeless, dramatic, artistic',
      genres: ['documentary-historical', 'entertainment-drama', 'documentary-true-crime']
    },
    'pastel': {
      name: 'Pastel',
      prompt: 'pastel color palette, soft muted colors, gentle aesthetic, light and airy',
      mood: 'gentle, approachable, modern',
      genres: ['educational-tutorial', 'business-testimonial', 'inspirational']
    },
    'earth-tones': {
      name: 'Earth Tones',
      prompt: 'earth tone palette, natural browns greens, organic colors, grounded aesthetic',
      mood: 'natural, authentic, grounded',
      genres: ['documentary-nature', 'documentary-social', 'inspirational']
    },
    'neon-pop': {
      name: 'Neon Pop',
      prompt: 'neon pop colors, electric bright palette, bold contrasting hues, high energy colors',
      mood: 'exciting, youthful, bold',
      genres: ['social-viral', 'entertainment-comedy', 'educational-science']
    }
  },

  // === MOOD ATMOSPHERES ===
  moods: {
    'epic': {
      prompt: 'epic cinematic atmosphere, grand scale, awe-inspiring, majestic feel',
      lighting: 'golden-hour',
      composition: 'wide-establishing',
      colorPalette: 'teal-orange'
    },
    'intimate': {
      prompt: 'intimate personal atmosphere, close connection, emotional depth, private moment',
      lighting: 'soft-diffused',
      composition: 'close-up',
      colorPalette: 'warm-tones'
    },
    'mysterious': {
      prompt: 'mysterious atmospheric, enigmatic mood, hidden depths, intriguing shadows',
      lighting: 'low-key',
      composition: 'frame-within-frame',
      colorPalette: 'desaturated'
    },
    'energetic': {
      prompt: 'energetic dynamic atmosphere, high energy, exciting motion, vibrant action',
      lighting: 'neon',
      composition: 'dutch-angle',
      colorPalette: 'high-saturation'
    },
    'contemplative': {
      prompt: 'contemplative peaceful atmosphere, thoughtful mood, serene reflection, quiet beauty',
      lighting: 'blue-hour',
      composition: 'symmetrical',
      colorPalette: 'cool-tones'
    },
    'tense': {
      prompt: 'tense suspenseful atmosphere, building dread, uncomfortable anticipation, edge of seat',
      lighting: 'harsh-dramatic',
      composition: 'dutch-angle',
      colorPalette: 'desaturated'
    },
    'hopeful': {
      prompt: 'hopeful optimistic atmosphere, rising possibility, dawn of change, inspirational mood',
      lighting: 'golden-hour',
      composition: 'low-angle',
      colorPalette: 'warm-tones'
    },
    'professional': {
      prompt: 'professional polished atmosphere, clean competent, trustworthy quality, business appropriate',
      lighting: 'high-key',
      composition: 'rule-of-thirds',
      colorPalette: 'cool-tones'
    },
    'nostalgic': {
      prompt: 'nostalgic wistful atmosphere, remembering past, bittersweet memories, time gone by',
      lighting: 'golden-hour',
      composition: 'medium-shot',
      colorPalette: 'warm-tones'
    },
    'dark': {
      prompt: 'dark ominous atmosphere, foreboding shadows, dangerous undertones, threat lurking',
      lighting: 'low-key',
      composition: 'extreme-close-up',
      colorPalette: 'noir-bw'
    }
  },

  // === GENRE TO VISUAL MAPPING ===
  genreVisuals: {
    'documentary-nature': {
      defaultComposition: 'wide-establishing',
      defaultLighting: 'natural',
      defaultPalette: 'earth-tones',
      promptModifiers: 'nature documentary style, National Geographic quality, wildlife photography aesthetic, BBC Earth cinematography',
      negativeModifiers: 'artificial, studio, urban, man-made'
    },
    'documentary-true-crime': {
      defaultComposition: 'close-up',
      defaultLighting: 'low-key',
      defaultPalette: 'desaturated',
      promptModifiers: 'true crime documentary style, investigative mood, evidence aesthetic, Making a Murderer cinematography',
      negativeModifiers: 'bright, cheerful, colorful, happy'
    },
    'documentary-social': {
      defaultComposition: 'medium-shot',
      defaultLighting: 'natural',
      defaultPalette: 'desaturated',
      promptModifiers: 'social documentary style, real life aesthetic, authentic journalism, The Social Dilemma cinematography',
      negativeModifiers: 'staged, fake, advertisement, commercial'
    },
    'documentary-historical': {
      defaultComposition: 'rule-of-thirds',
      defaultLighting: 'rembrandt',
      defaultPalette: 'warm-tones',
      promptModifiers: 'historical documentary style, Ken Burns aesthetic, archival quality, epic history cinematography',
      negativeModifiers: 'modern, futuristic, contemporary technology'
    },
    'educational-explainer': {
      defaultComposition: 'symmetrical',
      defaultLighting: 'high-key',
      defaultPalette: 'high-saturation',
      promptModifiers: 'explainer video style, Kurzgesagt aesthetic, clear visual metaphor, educational illustration',
      negativeModifiers: 'confusing, cluttered, dark, unclear'
    },
    'educational-tutorial': {
      defaultComposition: 'medium-shot',
      defaultLighting: 'soft-diffused',
      defaultPalette: 'pastel',
      promptModifiers: 'tutorial style, hands-on demonstration, clear instructional view, maker aesthetic',
      negativeModifiers: 'confusing angle, unclear, messy, unprofessional'
    },
    'educational-science': {
      defaultComposition: 'extreme-close-up',
      defaultLighting: 'high-key',
      defaultPalette: 'cool-tones',
      promptModifiers: 'science visualization, Veritasium aesthetic, experimental setup, discovery moment',
      negativeModifiers: 'abstract, unclear, non-scientific, magical'
    },
    'entertainment-comedy': {
      defaultComposition: 'medium-shot',
      defaultLighting: 'high-key',
      defaultPalette: 'high-saturation',
      promptModifiers: 'comedy style, sitcom lighting, expressive framing, comedic timing visual',
      negativeModifiers: 'dark, serious, scary, dramatic'
    },
    'entertainment-drama': {
      defaultComposition: 'close-up',
      defaultLighting: 'rembrandt',
      defaultPalette: 'teal-orange',
      promptModifiers: 'dramatic cinematography, Breaking Bad aesthetic, emotional depth, prestige TV quality',
      negativeModifiers: 'flat, boring, amateur, sitcom-like'
    },
    'entertainment-horror': {
      defaultComposition: 'dutch-angle',
      defaultLighting: 'low-key',
      defaultPalette: 'desaturated',
      promptModifiers: 'horror cinematography, unsettling framing, dread atmosphere, psychological terror aesthetic',
      negativeModifiers: 'bright, cheerful, safe, comforting'
    },
    'business-brand': {
      defaultComposition: 'low-angle',
      defaultLighting: 'golden-hour',
      defaultPalette: 'teal-orange',
      promptModifiers: 'brand commercial style, Nike ad aesthetic, aspirational imagery, Apple commercial quality',
      negativeModifiers: 'cheap, amateur, stock photo, generic'
    },
    'business-product': {
      defaultComposition: 'symmetrical',
      defaultLighting: 'high-key',
      defaultPalette: 'cool-tones',
      promptModifiers: 'product photography, Apple keynote aesthetic, clean showcase, premium product visualization',
      negativeModifiers: 'messy background, poor lighting, unprofessional, cluttered'
    },
    'business-testimonial': {
      defaultComposition: 'medium-shot',
      defaultLighting: 'soft-diffused',
      defaultPalette: 'warm-tones',
      promptModifiers: 'testimonial interview style, authentic documentary feel, real person in real environment',
      negativeModifiers: 'staged, fake, studio backdrop, corporate sterile'
    },
    'social-viral': {
      defaultComposition: 'close-up',
      defaultLighting: 'neon',
      defaultPalette: 'neon-pop',
      promptModifiers: 'viral content style, high impact visual, scroll-stopping image, TikTok aesthetic',
      negativeModifiers: 'boring, generic, slow, subtle'
    },
    'social-storytime': {
      defaultComposition: 'medium-shot',
      defaultLighting: 'soft-diffused',
      defaultPalette: 'warm-tones',
      promptModifiers: 'storytime aesthetic, engaging visual, personality-driven, relatable imagery',
      negativeModifiers: 'corporate, impersonal, sterile, distant'
    },
    'series-docuseries': {
      defaultComposition: 'wide-establishing',
      defaultLighting: 'natural',
      defaultPalette: 'teal-orange',
      promptModifiers: 'docuseries cinematography, Drive to Survive aesthetic, episodic visual language, Netflix documentary quality',
      negativeModifiers: 'amateur, inconsistent, low budget, single-shot'
    },
    'cinematic': {
      defaultComposition: 'leading-lines',
      defaultLighting: 'golden-hour',
      defaultPalette: 'teal-orange',
      promptModifiers: 'cinematic film quality, Christopher Nolan aesthetic, IMAX visual, blockbuster cinematography, anamorphic lens feel',
      negativeModifiers: 'TV quality, amateur, flat, documentary'
    },
    'standard': {
      defaultComposition: 'rule-of-thirds',
      defaultLighting: 'natural',
      defaultPalette: 'cool-tones',
      promptModifiers: 'professional quality, clean composition, well-lit, broadcast quality',
      negativeModifiers: 'amateur, blurry, poorly lit, distorted'
    }
  }
};

/**
 * buildVisualPrompt - Constructs an enhanced visual prompt using Visual Intelligence
 * Now supports Phase 4: Scene Memory System with 4-layer architecture
 *
 * @param {string} basePrompt - The original visual description from the script (Layer 3: Scene Content)
 * @param {object} options - { genre, productionMode, mood, visualSettings, sceneMemory }
 * @returns {object} - { enhancedPrompt, negativePrompt, visualMetadata }
 *
 * 4-Layer Prompt Architecture:
 * [STYLE BIBLE] + [CHARACTER BIBLE] + [SCENE CONTENT] + [TECHNICAL SPECS]
 */
function buildVisualPrompt(basePrompt, options = {}) {
  const {
    genre = null,
    productionMode = 'standard',
    mood = null,
    visualSettings = {},
    style = 'cinematic',
    // Phase 4: Scene Memory System
    sceneMemory = null
  } = options;

  const promptParts = [];

  // ===== PHASE 4: SCENE MEMORY 4-LAYER ARCHITECTURE =====

  // Layer 1: Style Bible (if enabled) - SAME for every scene
  if (sceneMemory?.styleBible?.enabled) {
    const bible = sceneMemory.styleBible;
    if (bible.style) promptParts.push(`Style: ${bible.style}`);
    if (bible.colorGrade) promptParts.push(`Color Grade: ${bible.colorGrade}`);
    if (bible.lighting) promptParts.push(`Lighting: ${bible.lighting}`);
    if (bible.atmosphere) promptParts.push(`Atmosphere: ${bible.atmosphere}`);
    if (bible.camera) promptParts.push(`Camera: ${bible.camera}`);
  }

  // Layer 2: Character Bible (if enabled) - Characters assigned to this scene
  if (sceneMemory?.characterDescriptions && sceneMemory.characterDescriptions.length > 0) {
    // Character descriptions are pre-filtered by the frontend for this specific scene
    for (const charDesc of sceneMemory.characterDescriptions) {
      if (charDesc && charDesc.trim()) {
        promptParts.push(charDesc);
      }
    }
  }

  // Layer 3: Scene Content (the original visual prompt - UNIQUE per scene)
  if (basePrompt) {
    promptParts.push(basePrompt);
  }

  // === ORIGINAL VISUAL INTELLIGENCE (Only if Scene Memory Style Bible is NOT enabled) ===
  // This maintains backward compatibility with existing projects
  if (!sceneMemory?.styleBible?.enabled) {
    // Get genre visuals or fall back to production mode or standard
    const genreVisual = VISUAL_INTELLIGENCE.genreVisuals[genre] ||
                        VISUAL_INTELLIGENCE.genreVisuals[productionMode] ||
                        VISUAL_INTELLIGENCE.genreVisuals['standard'];

    // Get specific visual components (allow override from visualSettings)
    const compositionKey = visualSettings.composition || genreVisual.defaultComposition;
    const lightingKey = visualSettings.lighting || genreVisual.defaultLighting;
    const paletteKey = visualSettings.colorPalette || genreVisual.defaultPalette;

    const composition = VISUAL_INTELLIGENCE.compositions[compositionKey] || VISUAL_INTELLIGENCE.compositions['rule-of-thirds'];
    const lighting = VISUAL_INTELLIGENCE.lighting[lightingKey] || VISUAL_INTELLIGENCE.lighting['natural'];
    const palette = VISUAL_INTELLIGENCE.colorPalettes[paletteKey] || VISUAL_INTELLIGENCE.colorPalettes['cool-tones'];

    // Get mood if specified
    const moodSettings = mood ? VISUAL_INTELLIGENCE.moods[mood] : null;

    // Add Visual Intelligence components
    promptParts.push(`Composition: ${composition.prompt}`);
    promptParts.push(`Lighting: ${lighting.prompt}`);
    promptParts.push(`Color: ${palette.prompt}`);
    if (genreVisual.promptModifiers) promptParts.push(genreVisual.promptModifiers);
    if (moodSettings?.prompt) promptParts.push(moodSettings.prompt);
  }

  // Layer 4: Technical Specs
  if (sceneMemory?.technicalSpecs?.enabled && sceneMemory.technicalSpecs.positive) {
    promptParts.push(sceneMemory.technicalSpecs.positive);
  } else {
    // Default technical specs for backward compatibility
    promptParts.push('high quality, detailed, professional, 8K resolution, sharp focus');
  }

  const enhancedPrompt = promptParts.filter(Boolean).join('. ');

  // Build negative prompt
  let negativePrompt = '';
  if (sceneMemory?.technicalSpecs?.enabled && sceneMemory.technicalSpecs.negative) {
    negativePrompt = sceneMemory.technicalSpecs.negative;
  } else {
    // Default negative prompt
    const negativeParts = [
      'blurry, low quality, ugly, distorted, watermark, nsfw, text, words, letters, logo, signature, amateur'
    ];

    if (!sceneMemory?.styleBible?.enabled) {
      const genreVisual = VISUAL_INTELLIGENCE.genreVisuals[genre] ||
                          VISUAL_INTELLIGENCE.genreVisuals[productionMode] ||
                          VISUAL_INTELLIGENCE.genreVisuals['standard'];
      if (genreVisual.negativeModifiers) {
        negativeParts.push(genreVisual.negativeModifiers);
      }
    }

    negativePrompt = negativeParts.filter(Boolean).join(', ');
  }

  // Get composition/lighting/palette keys for metadata
  const genreVisual = VISUAL_INTELLIGENCE.genreVisuals[genre] ||
                      VISUAL_INTELLIGENCE.genreVisuals[productionMode] ||
                      VISUAL_INTELLIGENCE.genreVisuals['standard'];
  const compositionKey = visualSettings.composition || genreVisual.defaultComposition;
  const lightingKey = visualSettings.lighting || genreVisual.defaultLighting;
  const paletteKey = visualSettings.colorPalette || genreVisual.defaultPalette;

  // Return structured result
  return {
    enhancedPrompt,
    negativePrompt,
    visualMetadata: {
      genre,
      productionMode,
      composition: compositionKey,
      lighting: lightingKey,
      colorPalette: paletteKey,
      mood,
      // Phase 4: Scene Memory info
      sceneMemoryEnabled: {
        styleBible: !!sceneMemory?.styleBible?.enabled,
        characterBible: !!(sceneMemory?.characterDescriptions?.length > 0),
        technicalSpecs: sceneMemory?.technicalSpecs?.enabled !== false
      }
    }
  };
}

/**
 * creationWizardGenerateSceneImage - Generate a single scene image using RunPod
 *
 * Uses existing RunPod HiDream integration to generate storyboard images
 * Enhanced with Phase 3B Visual Intelligence and Phase 4 Scene Memory System
 */
exports.creationWizardGenerateSceneImage = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const {
    projectId,
    sceneId,
    prompt,
    style,
    aspectRatio,
    settings,
    // Phase 3B: Visual Intelligence parameters
    genre = null,
    productionMode = 'standard',
    mood = null,
    visualSettings = {},
    // Phase 4: Scene Memory System parameters
    sceneMemory = null
  } = data;

  if (!prompt || prompt.trim().length < 10) {
    throw new functions.https.HttpsError('invalid-argument', 'Image prompt is required (min 10 chars)');
  }

  const runpodKey = functions.config().runpod?.key;
  if (!runpodKey) {
    throw new functions.https.HttpsError('failed-precondition', 'RunPod API key not configured');
  }

  try {
    // Phase 3B + Phase 4: Use Visual Intelligence + Scene Memory to build enhanced prompt
    const visualResult = buildVisualPrompt(prompt, {
      genre,
      productionMode,
      mood,
      visualSettings,
      style,
      sceneMemory  // Phase 4: Scene Memory data
    });

    const enhancedPrompt = visualResult.enhancedPrompt;
    const negativePrompt = visualResult.negativePrompt;

    // Determine dimensions based on aspect ratio
    const dimensions = {
      '16:9': { width: 1280, height: 720 },
      '9:16': { width: 720, height: 1280 },
      '1:1': { width: 1024, height: 1024 },
      '4:5': { width: 864, height: 1080 }
    };
    const { width, height } = dimensions[aspectRatio] || dimensions['16:9'];

    // Generate unique filename
    const seed = Math.floor(Math.random() * 999999999999);
    const fileName = `creation-projects/${projectId || uid}/storyboard/scene_${sceneId}_${Date.now()}_${seed}.png`;

    // Create signed URL for upload
    const bucket = admin.storage().bucket();
    const file = bucket.file(fileName);
    const [uploadUrl] = await file.getSignedUrl({
      version: 'v4',
      action: 'write',
      expires: Date.now() + 30 * 60 * 1000,
      contentType: 'application/octet-stream',
    });

    // Build RunPod input
    const runpodInput = {
      positive_prompt: enhancedPrompt,
      negative_prompt: negativePrompt,
      width,
      height,
      batch_size: 1,
      shift: 3.0,
      seed,
      steps: settings?.steps || 30,
      cfg: settings?.cfg || 5,
      sampler_name: "euler",
      scheduler: "simple",
      denoise: 1,
      image_upload_url: uploadUrl
    };

    // Call RunPod API
    const runpodEndpoint = 'https://api.runpod.ai/v2/rgq0go2nkcfx4h/run';
    const runpodResponse = await axios.post(runpodEndpoint, {
      input: runpodInput
    }, {
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${runpodKey}`
      },
      timeout: 30000
    });

    const jobId = runpodResponse.data.id;
    const status = runpodResponse.data.status;

    // Generate public URL
    const encodedFileName = encodeURIComponent(fileName);
    const publicUrl = `https://firebasestorage.googleapis.com/v0/b/${bucket.name}/o/${encodedFileName}?alt=media`;

    // Log usage
    await db.collection('apiUsage').add({
      userId: uid,
      type: 'storyboard_image',
      model: 'runpod-hidream',
      projectId: projectId || null,
      sceneId,
      timestamp: admin.firestore.FieldValue.serverTimestamp()
    });

    return {
      success: true,
      jobId,
      status,
      sceneId,
      imageUrl: publicUrl,
      fileName,
      prompt: enhancedPrompt,
      // Phase 3B: Include visual metadata
      visualMetadata: visualResult.visualMetadata,
      checkEndpoint: `https://api.runpod.ai/v2/rgq0go2nkcfx4h/status/${jobId}`
    };

  } catch (error) {
    console.error('[creationWizardGenerateSceneImage] Error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to generate scene image'));
  }
});

/**
 * creationWizardGetVisualStyles - Get available Visual Intelligence options
 * Phase 3B: Exposes compositions, lighting, color palettes for frontend UI
 */
exports.creationWizardGetVisualStyles = functions.https.onCall(async (data, context) => {
  await verifyAuth(context);

  // Build simplified options for frontend
  const compositions = Object.entries(VISUAL_INTELLIGENCE.compositions).map(([id, comp]) => ({
    id,
    name: comp.name,
    description: comp.useFor.join(', '),
    genres: comp.genres
  }));

  const lighting = Object.entries(VISUAL_INTELLIGENCE.lighting).map(([id, light]) => ({
    id,
    name: light.name,
    mood: light.mood,
    genres: light.genres
  }));

  const colorPalettes = Object.entries(VISUAL_INTELLIGENCE.colorPalettes).map(([id, palette]) => ({
    id,
    name: palette.name,
    mood: palette.mood,
    genres: palette.genres
  }));

  const moods = Object.entries(VISUAL_INTELLIGENCE.moods).map(([id, mood]) => ({
    id,
    name: id.charAt(0).toUpperCase() + id.slice(1),
    lighting: mood.lighting,
    composition: mood.composition,
    colorPalette: mood.colorPalette
  }));

  return {
    success: true,
    compositions,
    lighting,
    colorPalettes,
    moods,
    genreDefaults: VISUAL_INTELLIGENCE.genreVisuals
  };
});

// ============================================================
// PHASE 3C: MULTI-FORMAT EXPORT INTELLIGENCE
// Platform optimization, aspect ratios, and series consistency
// ============================================================

const EXPORT_INTELLIGENCE = {
  // Platform Profiles - comprehensive specs for each platform
  platforms: {
    'youtube-shorts': {
      name: 'YouTube Shorts',
      icon: 'ðŸŽ¬',
      aspectRatio: '9:16',
      maxDuration: 60,
      optimalDuration: { min: 30, max: 58 },
      resolution: { width: 1080, height: 1920 },
      maxFileSize: '256MB',
      codec: 'h264',
      bitrate: { video: '8M', audio: '192k' },
      fps: 30,
      hookTiming: { start: 0, critical: 3 },
      pacing: 'fast',
      features: ['vertical', 'mobile-first', 'loop-friendly'],
      titleStyle: 'bold-overlay',
      captionStyle: 'large-centered',
      musicVolume: 0.7,
      algorithm: {
        retentionFocus: true,
        loopBonus: true,
        hashtagLimit: 3
      }
    },
    'youtube-standard': {
      name: 'YouTube Standard',
      icon: 'ðŸ“º',
      aspectRatio: '16:9',
      maxDuration: 720, // 12 hours technically, but 12 min optimal
      optimalDuration: { min: 480, max: 900 }, // 8-15 minutes
      resolution: { width: 1920, height: 1080 },
      maxFileSize: '256GB',
      codec: 'h264',
      bitrate: { video: '16M', audio: '320k' },
      fps: 30,
      hookTiming: { start: 0, critical: 30 },
      pacing: 'balanced',
      features: ['chapters', 'end-screens', 'cards', 'descriptions'],
      titleStyle: 'thumbnail-bait',
      captionStyle: 'standard-bottom',
      musicVolume: 0.3,
      algorithm: {
        watchTimeKey: true,
        ctrImportant: true,
        retentionCurve: 'gradual'
      }
    },
    'youtube-longform': {
      name: 'YouTube Long-form',
      icon: 'ðŸŽ¥',
      aspectRatio: '16:9',
      maxDuration: 7200, // 2 hours
      optimalDuration: { min: 900, max: 2700 }, // 15-45 minutes
      resolution: { width: 1920, height: 1080 },
      maxFileSize: '256GB',
      codec: 'h264',
      bitrate: { video: '20M', audio: '320k' },
      fps: 30,
      hookTiming: { start: 0, critical: 60 },
      pacing: 'contemplative',
      features: ['chapters', 'timestamps', 'deep-dive', 'sponsorship-segments'],
      titleStyle: 'curiosity-gap',
      captionStyle: 'standard-bottom',
      musicVolume: 0.2,
      algorithm: {
        sessionTimeBonus: true,
        subscriberConversion: true
      }
    },
    'youtube-premiere': {
      name: 'YouTube Premiere',
      icon: 'ðŸŒŸ',
      aspectRatio: '16:9',
      maxDuration: 7200,
      optimalDuration: { min: 600, max: 3600 },
      resolution: { width: 3840, height: 2160 }, // 4K
      maxFileSize: '256GB',
      codec: 'h265',
      bitrate: { video: '45M', audio: '320k' },
      fps: 60,
      hookTiming: { start: 0, critical: 30 },
      pacing: 'cinematic',
      features: ['live-chat', 'countdown', '4k', 'hdr'],
      titleStyle: 'event-style',
      captionStyle: 'cinematic-subtitles',
      musicVolume: 0.4
    },
    'tiktok-quick': {
      name: 'TikTok Quick',
      icon: 'âš¡',
      aspectRatio: '9:16',
      maxDuration: 15,
      optimalDuration: { min: 7, max: 15 },
      resolution: { width: 1080, height: 1920 },
      maxFileSize: '287MB',
      codec: 'h264',
      bitrate: { video: '6M', audio: '192k' },
      fps: 30,
      hookTiming: { start: 0, critical: 1 },
      pacing: 'rapid-fire',
      features: ['trending-sounds', 'duet-friendly', 'stitch-friendly'],
      titleStyle: 'text-overlay',
      captionStyle: 'tiktok-style',
      musicVolume: 0.8,
      algorithm: {
        completionRate: true,
        shareBoost: true,
        soundTrending: true
      }
    },
    'tiktok-standard': {
      name: 'TikTok Standard',
      icon: 'ðŸŽµ',
      aspectRatio: '9:16',
      maxDuration: 60,
      optimalDuration: { min: 21, max: 34 },
      resolution: { width: 1080, height: 1920 },
      maxFileSize: '287MB',
      codec: 'h264',
      bitrate: { video: '6M', audio: '192k' },
      fps: 30,
      hookTiming: { start: 0, critical: 2 },
      pacing: 'fast',
      features: ['trending-sounds', 'effects', 'text-to-speech'],
      titleStyle: 'text-overlay',
      captionStyle: 'tiktok-style',
      musicVolume: 0.75
    },
    'tiktok-extended': {
      name: 'TikTok Extended',
      icon: 'ðŸ“±',
      aspectRatio: '9:16',
      maxDuration: 180, // 3 minutes
      optimalDuration: { min: 60, max: 120 },
      resolution: { width: 1080, height: 1920 },
      maxFileSize: '287MB',
      codec: 'h264',
      bitrate: { video: '6M', audio: '192k' },
      fps: 30,
      hookTiming: { start: 0, critical: 3 },
      pacing: 'balanced',
      features: ['series', 'storytelling', 'educational'],
      titleStyle: 'text-overlay',
      captionStyle: 'tiktok-style',
      musicVolume: 0.6
    },
    'instagram-reels': {
      name: 'Instagram Reels',
      icon: 'ðŸ“¸',
      aspectRatio: '9:16',
      maxDuration: 90,
      optimalDuration: { min: 15, max: 30 },
      resolution: { width: 1080, height: 1920 },
      maxFileSize: '4GB',
      codec: 'h264',
      bitrate: { video: '8M', audio: '192k' },
      fps: 30,
      hookTiming: { start: 0, critical: 2 },
      pacing: 'fast',
      features: ['audio-sync', 'effects', 'collab'],
      titleStyle: 'aesthetic-overlay',
      captionStyle: 'instagram-style',
      musicVolume: 0.7,
      algorithm: {
        saveBoost: true,
        shareBoost: true,
        aestheticPriority: true
      }
    },
    'instagram-stories': {
      name: 'Instagram Stories',
      icon: 'ðŸ“–',
      aspectRatio: '9:16',
      maxDuration: 60,
      optimalDuration: { min: 5, max: 15 },
      resolution: { width: 1080, height: 1920 },
      maxFileSize: '4GB',
      codec: 'h264',
      bitrate: { video: '6M', audio: '128k' },
      fps: 30,
      hookTiming: { start: 0, critical: 1 },
      pacing: 'rapid-fire',
      features: ['swipe-up', 'polls', 'questions', 'stickers'],
      titleStyle: 'sticker-overlay',
      captionStyle: 'story-text',
      musicVolume: 0.5
    },
    'instagram-feed': {
      name: 'Instagram Feed Video',
      icon: 'ðŸ–¼ï¸',
      aspectRatio: '4:5',
      maxDuration: 60,
      optimalDuration: { min: 15, max: 45 },
      resolution: { width: 1080, height: 1350 },
      maxFileSize: '4GB',
      codec: 'h264',
      bitrate: { video: '8M', audio: '192k' },
      fps: 30,
      hookTiming: { start: 0, critical: 3 },
      pacing: 'balanced',
      features: ['carousel-friendly', 'thumbnail-preview'],
      titleStyle: 'minimal',
      captionStyle: 'standard-bottom',
      musicVolume: 0.4
    },
    'facebook-reels': {
      name: 'Facebook Reels',
      icon: 'ðŸ“˜',
      aspectRatio: '9:16',
      maxDuration: 90,
      optimalDuration: { min: 15, max: 60 },
      resolution: { width: 1080, height: 1920 },
      maxFileSize: '4GB',
      codec: 'h264',
      bitrate: { video: '8M', audio: '192k' },
      fps: 30,
      hookTiming: { start: 0, critical: 3 },
      pacing: 'balanced',
      features: ['cross-post-instagram', 'monetization'],
      titleStyle: 'text-overlay',
      captionStyle: 'facebook-style',
      musicVolume: 0.6
    },
    'facebook-watch': {
      name: 'Facebook Watch',
      icon: 'ðŸ“º',
      aspectRatio: '16:9',
      maxDuration: 14400, // 4 hours
      optimalDuration: { min: 180, max: 600 }, // 3-10 minutes
      resolution: { width: 1920, height: 1080 },
      maxFileSize: '10GB',
      codec: 'h264',
      bitrate: { video: '12M', audio: '256k' },
      fps: 30,
      hookTiming: { start: 0, critical: 10 },
      pacing: 'balanced',
      features: ['ad-breaks', 'series', 'episodes'],
      titleStyle: 'tv-style',
      captionStyle: 'standard-bottom',
      musicVolume: 0.3
    },
    'linkedin-video': {
      name: 'LinkedIn Video',
      icon: 'ðŸ’¼',
      aspectRatio: '16:9',
      maxDuration: 600, // 10 minutes
      optimalDuration: { min: 30, max: 120 },
      resolution: { width: 1920, height: 1080 },
      maxFileSize: '5GB',
      codec: 'h264',
      bitrate: { video: '10M', audio: '256k' },
      fps: 30,
      hookTiming: { start: 0, critical: 5 },
      pacing: 'professional',
      features: ['native-captions', 'professional-tone'],
      titleStyle: 'professional',
      captionStyle: 'accessible-captions',
      musicVolume: 0.15,
      algorithm: {
        dwellTimeKey: true,
        commentBoost: true
      }
    },
    'twitter-video': {
      name: 'Twitter/X Video',
      icon: 'ðŸ¦',
      aspectRatio: '16:9',
      maxDuration: 140, // 2:20
      optimalDuration: { min: 15, max: 60 },
      resolution: { width: 1920, height: 1080 },
      maxFileSize: '512MB',
      codec: 'h264',
      bitrate: { video: '8M', audio: '192k' },
      fps: 30,
      hookTiming: { start: 0, critical: 3 },
      pacing: 'fast',
      features: ['auto-loop', 'muted-default'],
      titleStyle: 'minimal',
      captionStyle: 'burned-in',
      musicVolume: 0.4
    },
    'snapchat-spotlight': {
      name: 'Snapchat Spotlight',
      icon: 'ðŸ‘»',
      aspectRatio: '9:16',
      maxDuration: 60,
      optimalDuration: { min: 10, max: 30 },
      resolution: { width: 1080, height: 1920 },
      maxFileSize: '1GB',
      codec: 'h264',
      bitrate: { video: '6M', audio: '128k' },
      fps: 30,
      hookTiming: { start: 0, critical: 1 },
      pacing: 'rapid-fire',
      features: ['ar-lenses', 'sounds'],
      titleStyle: 'snap-style',
      captionStyle: 'snap-text',
      musicVolume: 0.8
    },
    'pinterest-idea': {
      name: 'Pinterest Idea Pin',
      icon: 'ðŸ“Œ',
      aspectRatio: '9:16',
      maxDuration: 60,
      optimalDuration: { min: 15, max: 45 },
      resolution: { width: 1080, height: 1920 },
      maxFileSize: '2GB',
      codec: 'h264',
      bitrate: { video: '8M', audio: '192k' },
      fps: 30,
      hookTiming: { start: 0, critical: 2 },
      pacing: 'balanced',
      features: ['multi-page', 'tutorial-friendly', 'save-focused'],
      titleStyle: 'instructional',
      captionStyle: 'step-by-step',
      musicVolume: 0.4
    },
    'netflix-episode': {
      name: 'Netflix-Style Episode',
      icon: 'ðŸŽ¬',
      aspectRatio: '16:9',
      maxDuration: 3600, // 60 minutes
      optimalDuration: { min: 1320, max: 2700 }, // 22-45 minutes
      resolution: { width: 3840, height: 2160 }, // 4K
      maxFileSize: '100GB',
      codec: 'h265',
      bitrate: { video: '45M', audio: '640k' },
      fps: 24, // Cinematic
      hookTiming: { start: 0, critical: 180 }, // 3 minute cold open
      pacing: 'cinematic',
      features: ['hdr', 'dolby-atmos', 'chapters', 'skip-intro', 'credits'],
      titleStyle: 'cinematic-title-card',
      captionStyle: 'netflix-subtitles',
      musicVolume: 0.35,
      structure: {
        coldOpen: { duration: 180, required: true },
        titleSequence: { duration: 60, required: true },
        actBreaks: 4,
        credits: { duration: 90, required: true }
      }
    },
    'netflix-movie': {
      name: 'Netflix-Style Movie',
      icon: 'ðŸŽ¥',
      aspectRatio: '2.39:1', // Cinemascope
      maxDuration: 10800, // 3 hours
      optimalDuration: { min: 5400, max: 9000 }, // 90-150 minutes
      resolution: { width: 4096, height: 1716 }, // 4K Cinemascope
      maxFileSize: '200GB',
      codec: 'h265',
      bitrate: { video: '80M', audio: '768k' },
      fps: 24,
      hookTiming: { start: 0, critical: 600 }, // 10 minute setup
      pacing: 'cinematic',
      features: ['hdr10+', 'dolby-vision', 'dolby-atmos', '4k-dcp'],
      titleStyle: 'film-title-sequence',
      captionStyle: 'film-subtitles',
      musicVolume: 0.4,
      structure: {
        actOne: { percentage: 25 },
        actTwo: { percentage: 50 },
        actThree: { percentage: 25 }
      }
    },
    'broadcast-tv': {
      name: 'Broadcast TV',
      icon: 'ðŸ“¡',
      aspectRatio: '16:9',
      maxDuration: 2640, // 44 minutes (1 hour slot minus ads)
      optimalDuration: { min: 1260, max: 2640 }, // 21-44 minutes
      resolution: { width: 1920, height: 1080 },
      maxFileSize: '50GB',
      codec: 'prores',
      bitrate: { video: '220M', audio: '1536k' },
      fps: 30, // 29.97 drop frame
      hookTiming: { start: 0, critical: 120 },
      pacing: 'balanced',
      features: ['ad-breaks', 'act-structure', 'cold-open', 'broadcast-safe'],
      titleStyle: 'broadcast-bumper',
      captionStyle: 'cea-608',
      musicVolume: 0.3,
      structure: {
        coldOpen: { duration: 120, required: false },
        actBreaks: [420, 840, 1260, 1680], // Break points in seconds
        bumpers: { duration: 5, required: true }
      }
    },
    'documentary-feature': {
      name: 'Documentary Feature',
      icon: 'ðŸŽžï¸',
      aspectRatio: '16:9',
      maxDuration: 7200, // 2 hours
      optimalDuration: { min: 4800, max: 6600 }, // 80-110 minutes
      resolution: { width: 3840, height: 2160 },
      maxFileSize: '150GB',
      codec: 'h265',
      bitrate: { video: '50M', audio: '512k' },
      fps: 24,
      hookTiming: { start: 0, critical: 300 },
      pacing: 'contemplative',
      features: ['archival-footage', 'interviews', 'b-roll-heavy', 'voice-over'],
      titleStyle: 'documentary-title',
      captionStyle: 'documentary-subtitles',
      musicVolume: 0.25
    }
  },

  // Aspect Ratio Specifications
  aspectRatios: {
    '16:9': {
      name: 'Landscape (16:9)',
      ratio: 16/9,
      width: 1920,
      height: 1080,
      use: ['youtube', 'facebook-watch', 'linkedin', 'twitter', 'broadcast'],
      reframeFrom: {
        '9:16': 'center-crop-or-letterbox',
        '1:1': 'pillarbox',
        '4:5': 'pillarbox',
        '2.39:1': 'letterbox'
      }
    },
    '9:16': {
      name: 'Portrait (9:16)',
      ratio: 9/16,
      width: 1080,
      height: 1920,
      use: ['tiktok', 'instagram-reels', 'youtube-shorts', 'snapchat', 'pinterest'],
      reframeFrom: {
        '16:9': 'center-crop-or-pillarbox',
        '1:1': 'extend-vertical',
        '4:5': 'extend-top-bottom',
        '2.39:1': 'pillarbox-heavy'
      }
    },
    '1:1': {
      name: 'Square (1:1)',
      ratio: 1,
      width: 1080,
      height: 1080,
      use: ['instagram-feed', 'facebook-feed', 'twitter-square'],
      reframeFrom: {
        '16:9': 'center-crop',
        '9:16': 'center-crop',
        '4:5': 'crop-top-bottom',
        '2.39:1': 'center-crop-heavy'
      }
    },
    '4:5': {
      name: 'Portrait Feed (4:5)',
      ratio: 4/5,
      width: 1080,
      height: 1350,
      use: ['instagram-feed', 'facebook-feed'],
      reframeFrom: {
        '16:9': 'center-crop',
        '9:16': 'crop-top-bottom',
        '1:1': 'extend-bottom',
        '2.39:1': 'center-crop'
      }
    },
    '4:3': {
      name: 'Classic (4:3)',
      ratio: 4/3,
      width: 1440,
      height: 1080,
      use: ['retro-style', 'archive-footage'],
      reframeFrom: {
        '16:9': 'crop-sides',
        '9:16': 'letterbox-heavy',
        '1:1': 'extend-sides'
      }
    },
    '2.39:1': {
      name: 'Cinemascope (2.39:1)',
      ratio: 2.39,
      width: 2560,
      height: 1072,
      use: ['film', 'netflix-movie', 'cinematic'],
      reframeFrom: {
        '16:9': 'extend-width-or-crop-height',
        '9:16': 'not-recommended',
        '1:1': 'not-recommended'
      }
    },
    '2.35:1': {
      name: 'Widescreen (2.35:1)',
      ratio: 2.35,
      width: 2540,
      height: 1080,
      use: ['film', 'theatrical'],
      reframeFrom: {
        '16:9': 'letterbox-or-crop'
      }
    },
    '21:9': {
      name: 'Ultra-Wide (21:9)',
      ratio: 21/9,
      width: 2560,
      height: 1080,
      use: ['gaming', 'ultra-wide-monitors'],
      reframeFrom: {
        '16:9': 'extend-sides'
      }
    }
  },

  // Duration Presets by Content Type
  durationPresets: {
    'micro-content': {
      name: 'Micro Content',
      icon: 'âš¡',
      duration: { min: 5, max: 15 },
      platforms: ['tiktok-quick', 'instagram-stories', 'snapchat-spotlight'],
      structure: {
        hook: { duration: 1, required: true },
        content: { duration: 10, required: true },
        cta: { duration: 2, required: false }
      },
      pacing: 'rapid-fire',
      sceneCount: { min: 3, max: 8 }
    },
    'short-form': {
      name: 'Short-Form',
      icon: 'ðŸŽ¬',
      duration: { min: 15, max: 60 },
      platforms: ['youtube-shorts', 'tiktok-standard', 'instagram-reels'],
      structure: {
        hook: { duration: 3, required: true },
        setup: { duration: 10, required: true },
        content: { duration: 35, required: true },
        payoff: { duration: 10, required: true },
        cta: { duration: 2, required: false }
      },
      pacing: 'fast',
      sceneCount: { min: 6, max: 15 }
    },
    'medium-form': {
      name: 'Medium-Form',
      icon: 'ðŸ“º',
      duration: { min: 60, max: 300 },
      platforms: ['tiktok-extended', 'twitter-video', 'linkedin-video'],
      structure: {
        hook: { duration: 10, required: true },
        intro: { duration: 20, required: true },
        mainContent: { duration: 200, required: true },
        conclusion: { duration: 30, required: true },
        cta: { duration: 10, required: true }
      },
      pacing: 'balanced',
      sceneCount: { min: 10, max: 30 }
    },
    'standard-youtube': {
      name: 'Standard YouTube',
      icon: 'â–¶ï¸',
      duration: { min: 480, max: 900 },
      platforms: ['youtube-standard'],
      structure: {
        hook: { duration: 30, required: true },
        intro: { duration: 30, required: true },
        chapterOne: { duration: 180, required: true },
        chapterTwo: { duration: 180, required: true },
        chapterThree: { duration: 120, required: true },
        conclusion: { duration: 60, required: true },
        cta: { duration: 30, required: true }
      },
      pacing: 'balanced',
      sceneCount: { min: 20, max: 50 },
      features: ['chapters', 'timestamps', 'pattern-interrupt']
    },
    'deep-dive': {
      name: 'Deep Dive',
      icon: 'ðŸ”¬',
      duration: { min: 900, max: 2700 },
      platforms: ['youtube-longform'],
      structure: {
        hook: { duration: 60, required: true },
        overview: { duration: 120, required: true },
        deepDive: { duration: 1800, required: true },
        examples: { duration: 300, required: true },
        conclusion: { duration: 180, required: true },
        cta: { duration: 60, required: true }
      },
      pacing: 'contemplative',
      sceneCount: { min: 40, max: 100 },
      features: ['chapters', 'timestamps', 'sponsor-segments']
    },
    'tv-episode': {
      name: 'TV Episode',
      icon: 'ðŸ“¡',
      duration: { min: 1200, max: 2700 },
      platforms: ['netflix-episode', 'broadcast-tv'],
      structure: {
        coldOpen: { duration: 180, required: true },
        titleSequence: { duration: 60, required: true },
        actOne: { duration: 600, required: true },
        actBreak: { duration: 5, required: false },
        actTwo: { duration: 600, required: true },
        actBreak2: { duration: 5, required: false },
        actThree: { duration: 480, required: true },
        resolution: { duration: 180, required: true },
        teaser: { duration: 60, required: false },
        credits: { duration: 90, required: true }
      },
      pacing: 'cinematic',
      sceneCount: { min: 30, max: 60 }
    },
    'feature-film': {
      name: 'Feature Film',
      icon: 'ðŸŽ¥',
      duration: { min: 5400, max: 10800 },
      platforms: ['netflix-movie'],
      structure: {
        openingSequence: { duration: 300, required: true },
        actOne: { percentage: 25, required: true },
        firstPlotPoint: { duration: 60, required: true },
        actTwo: { percentage: 50, required: true },
        midpoint: { duration: 60, required: true },
        actThree: { percentage: 25, required: true },
        climax: { duration: 300, required: true },
        resolution: { duration: 180, required: true },
        credits: { duration: 300, required: true }
      },
      pacing: 'cinematic',
      sceneCount: { min: 60, max: 150 }
    },
    'documentary': {
      name: 'Documentary',
      icon: 'ðŸŽžï¸',
      duration: { min: 2400, max: 7200 },
      platforms: ['documentary-feature', 'youtube-longform'],
      structure: {
        opening: { duration: 300, required: true },
        thesis: { duration: 180, required: true },
        exploration: { duration: 3600, required: true },
        counterpoint: { duration: 600, required: false },
        conclusion: { duration: 300, required: true },
        callToAction: { duration: 120, required: false },
        credits: { duration: 180, required: true }
      },
      pacing: 'contemplative',
      sceneCount: { min: 40, max: 120 }
    }
  },

  // Series/Episode Consistency Templates
  seriesTemplates: {
    'youtube-series': {
      name: 'YouTube Series',
      episodeCount: { min: 5, max: 100 },
      consistency: {
        intro: { duration: 10, style: 'branded', required: true },
        outro: { duration: 20, style: 'subscribe-cta', required: true },
        thumbnailStyle: 'consistent-branding',
        titleFormat: '[Series Name] - Episode {n}: {title}',
        chaptersConsistent: true
      },
      branding: {
        logoPlacement: 'corner-watermark',
        colorScheme: 'from-brand',
        fontFamily: 'consistent',
        musicTheme: 'signature-intro'
      },
      scheduling: {
        releasePattern: 'weekly',
        suggestedDays: ['tuesday', 'thursday', 'saturday']
      }
    },
    'netflix-series': {
      name: 'Streaming Series',
      episodeCount: { min: 6, max: 13 },
      consistency: {
        titleSequence: { duration: 60, style: 'cinematic', required: true },
        recap: { duration: 60, style: 'previouslyOn', required: false },
        teaser: { duration: 60, style: 'nextOn', required: false },
        credits: { duration: 90, style: 'scrolling', required: true }
      },
      branding: {
        showLogo: 'title-card',
        episodeTitleCard: true,
        creditStyle: 'network-style'
      },
      narrative: {
        arcType: 'serialized',
        cliffhangers: true,
        characterDevelopment: 'progressive'
      }
    },
    'docuseries': {
      name: 'Documentary Series',
      episodeCount: { min: 3, max: 10 },
      consistency: {
        intro: { duration: 90, style: 'thematic', required: true },
        interviewStyle: 'consistent-framing',
        graphicsPackage: 'unified',
        archivalTreatment: 'consistent-grade'
      },
      branding: {
        titleTreatment: 'documentary-style',
        lowerThirds: 'consistent-design',
        transitionStyle: 'thematic'
      },
      narrative: {
        arcType: 'episodic-with-throughline',
        interviewSubjects: 'recurring'
      }
    },
    'tiktok-series': {
      name: 'TikTok Series',
      episodeCount: { min: 3, max: 20 },
      consistency: {
        hook: { style: 'series-branded', required: true },
        partIndicator: { style: 'Part {n}', required: true },
        cliffhanger: { required: true }
      },
      branding: {
        soundSignature: 'consistent',
        captionStyle: 'consistent',
        visualSignature: 'recognizable-opening'
      },
      scheduling: {
        releasePattern: 'daily-or-bidaily',
        cliffhangerStrategy: 'engagement-bait'
      }
    },
    'educational-course': {
      name: 'Educational Course',
      episodeCount: { min: 5, max: 50 },
      consistency: {
        intro: { duration: 15, style: 'lesson-number', required: true },
        objectives: { duration: 30, style: 'learning-goals', required: true },
        summary: { duration: 30, style: 'key-takeaways', required: true },
        nextLesson: { duration: 10, style: 'preview', required: true }
      },
      branding: {
        courseTitle: 'persistent-header',
        progressIndicator: true,
        chapterMarkers: true
      },
      structure: {
        moduleGrouping: true,
        prerequisites: true,
        quizPoints: 'end-of-lesson'
      }
    }
  },

  // Smart Reframe Rules
  reframeRules: {
    'subject-tracking': {
      description: 'Keep main subject in frame during reframe',
      priority: 'high',
      faceDetection: true,
      safeZone: 0.8
    },
    'text-safe': {
      description: 'Ensure text/graphics remain visible',
      priority: 'high',
      textDetection: true,
      marginBuffer: 0.1
    },
    'action-safe': {
      description: 'Keep action within safe zones',
      priority: 'medium',
      motionTracking: true,
      safeZone: 0.9
    },
    'thirds-recompose': {
      description: 'Recompose using rule of thirds',
      priority: 'medium',
      gridAlignment: true
    },
    'headroom-adjust': {
      description: 'Maintain appropriate headroom for subjects',
      priority: 'medium',
      headroomRatio: 0.15
    }
  },

  // Export Quality Presets
  qualityPresets: {
    'web-optimized': {
      name: 'Web Optimized',
      resolution: '1080p',
      bitrate: 'adaptive',
      codec: 'h264',
      profile: 'high',
      preset: 'medium',
      twoPass: false
    },
    'high-quality': {
      name: 'High Quality',
      resolution: '1080p',
      bitrate: 'high',
      codec: 'h264',
      profile: 'high',
      preset: 'slow',
      twoPass: true
    },
    '4k-master': {
      name: '4K Master',
      resolution: '2160p',
      bitrate: 'very-high',
      codec: 'h265',
      profile: 'main10',
      preset: 'slow',
      twoPass: true,
      hdr: 'optional'
    },
    'broadcast-master': {
      name: 'Broadcast Master',
      resolution: '1080i',
      bitrate: 'broadcast-standard',
      codec: 'prores',
      profile: '422-hq',
      colorSpace: 'rec709',
      audioChannels: 8
    },
    'archive-master': {
      name: 'Archive Master',
      resolution: 'native',
      bitrate: 'lossless',
      codec: 'prores',
      profile: '4444',
      colorSpace: 'native',
      preserveMetadata: true
    }
  }
};

/**
 * Get recommended export settings based on content and platform
 */
function getExportRecommendations(options = {}) {
  const {
    platform,
    contentType,
    duration,
    hasDialogue,
    genre,
    targetAudience,
    seriesInfo
  } = options;

  const platformProfile = EXPORT_INTELLIGENCE.platforms[platform];
  const recommendations = {
    platform: platformProfile,
    adjustments: [],
    warnings: []
  };

  // Duration recommendations
  if (duration && platformProfile) {
    const { optimalDuration } = platformProfile;
    if (duration < optimalDuration.min) {
      recommendations.adjustments.push({
        type: 'duration',
        suggestion: `Consider extending to at least ${optimalDuration.min}s for better ${platform} performance`,
        priority: 'medium'
      });
    } else if (duration > optimalDuration.max) {
      recommendations.adjustments.push({
        type: 'duration',
        suggestion: `Consider trimming to under ${optimalDuration.max}s or splitting into parts`,
        priority: 'high'
      });
    }
  }

  // Hook timing check
  if (platformProfile?.hookTiming) {
    recommendations.hookAdvice = {
      criticalWindow: platformProfile.hookTiming.critical,
      message: `Your hook must land within the first ${platformProfile.hookTiming.critical} seconds for ${platformProfile.name}`
    };
  }

  // Pacing recommendation
  if (platformProfile?.pacing) {
    recommendations.pacingAdvice = {
      style: platformProfile.pacing,
      message: getPacingAdvice(platformProfile.pacing)
    };
  }

  // Caption requirements
  if (platformProfile) {
    recommendations.captionAdvice = {
      style: platformProfile.captionStyle,
      required: ['tiktok', 'instagram', 'facebook', 'linkedin'].some(p => platform.includes(p)),
      message: getCaptionAdvice(platform)
    };
  }

  // Series consistency check
  if (seriesInfo) {
    const seriesTemplate = EXPORT_INTELLIGENCE.seriesTemplates[seriesInfo.template];
    if (seriesTemplate) {
      recommendations.seriesConsistency = {
        template: seriesTemplate,
        checks: getSeriesConsistencyChecks(seriesTemplate, seriesInfo)
      };
    }
  }

  return recommendations;
}

function getPacingAdvice(pacing) {
  const advice = {
    'rapid-fire': 'Use quick cuts (0.5-2s), high energy, constant motion. No dead space.',
    'fast': 'Maintain momentum with 2-4s cuts. Quick transitions. Energetic music.',
    'balanced': 'Mix of pacing. 3-6s average cuts. Allow breathing room at key moments.',
    'contemplative': 'Longer shots (5-15s). Let scenes breathe. Atmospheric pacing.',
    'cinematic': 'Film-style pacing. Motivated cuts only. Let emotion build.',
    'professional': 'Clear, measured pacing. Emphasis on clarity over energy.'
  };
  return advice[pacing] || 'Standard pacing recommended';
}

function getCaptionAdvice(platform) {
  if (platform.includes('tiktok') || platform.includes('instagram')) {
    return 'Large, centered captions essential. 85%+ of viewers watch without sound.';
  } else if (platform.includes('linkedin')) {
    return 'Professional captions required. Accessibility-focused formatting.';
  } else if (platform.includes('youtube')) {
    return 'Optional but recommended. Use YouTube auto-captions or upload SRT.';
  } else if (platform.includes('netflix')) {
    return 'Multiple language subtitle tracks required. Follow Netflix Timed Text Style Guide.';
  }
  return 'Captions recommended for accessibility';
}

function getSeriesConsistencyChecks(template, seriesInfo) {
  const checks = [];

  if (template.consistency.intro) {
    checks.push({
      element: 'intro',
      required: template.consistency.intro.required,
      expectedDuration: template.consistency.intro.duration,
      status: 'pending'
    });
  }

  if (template.consistency.outro) {
    checks.push({
      element: 'outro',
      required: template.consistency.outro.required,
      expectedDuration: template.consistency.outro.duration,
      status: 'pending'
    });
  }

  if (template.branding) {
    checks.push({
      element: 'branding',
      required: true,
      elements: Object.keys(template.branding),
      status: 'pending'
    });
  }

  return checks;
}

/**
 * Generate multi-platform export package
 */
function generateExportPackage(options = {}) {
  const { primaryPlatform, additionalPlatforms = [], content, seriesInfo } = options;

  const exportPackage = {
    primary: null,
    variants: [],
    assets: [],
    metadata: {}
  };

  // Primary platform export
  const primaryProfile = EXPORT_INTELLIGENCE.platforms[primaryPlatform];
  if (primaryProfile) {
    exportPackage.primary = {
      platform: primaryPlatform,
      profile: primaryProfile,
      recommendations: getExportRecommendations({
        platform: primaryPlatform,
        ...content,
        seriesInfo
      })
    };
  }

  // Generate variants for additional platforms
  for (const platform of additionalPlatforms) {
    const profile = EXPORT_INTELLIGENCE.platforms[platform];
    if (profile) {
      const variant = {
        platform,
        profile,
        recommendations: getExportRecommendations({ platform, ...content }),
        reframeNeeded: profile.aspectRatio !== primaryProfile?.aspectRatio,
        durationAdjustment: getDurationAdjustment(content.duration, profile)
      };

      if (variant.reframeNeeded) {
        variant.reframeStrategy = getReframeStrategy(
          primaryProfile?.aspectRatio,
          profile.aspectRatio
        );
      }

      exportPackage.variants.push(variant);
    }
  }

  // Generate required assets list
  exportPackage.assets = generateAssetsList(exportPackage);

  return exportPackage;
}

function getDurationAdjustment(currentDuration, targetProfile) {
  if (!currentDuration || !targetProfile) return null;

  const { optimalDuration, maxDuration } = targetProfile;

  if (currentDuration > maxDuration) {
    return {
      action: 'split',
      message: `Content exceeds ${targetProfile.name} maximum. Split into ${Math.ceil(currentDuration / maxDuration)} parts.`,
      partCount: Math.ceil(currentDuration / maxDuration)
    };
  } else if (currentDuration > optimalDuration.max) {
    return {
      action: 'trim',
      message: `Consider trimming from ${currentDuration}s to ${optimalDuration.max}s for optimal ${targetProfile.name} performance.`,
      trimAmount: currentDuration - optimalDuration.max
    };
  } else if (currentDuration < optimalDuration.min) {
    return {
      action: 'extend-or-micro',
      message: `Content is ${currentDuration}s. Either extend to ${optimalDuration.min}s or use a micro-content format.`
    };
  }

  return { action: 'none', message: 'Duration is optimal for this platform' };
}

function getReframeStrategy(sourceAspect, targetAspect) {
  const sourceRatio = EXPORT_INTELLIGENCE.aspectRatios[sourceAspect];
  const targetRatio = EXPORT_INTELLIGENCE.aspectRatios[targetAspect];

  if (!sourceRatio || !targetRatio) {
    return { strategy: 'manual', message: 'Manual reframing required' };
  }

  const reframeInfo = sourceRatio.reframeFrom?.[targetAspect];

  return {
    strategy: reframeInfo || 'smart-crop',
    source: sourceAspect,
    target: targetAspect,
    rules: EXPORT_INTELLIGENCE.reframeRules,
    message: `Reframe from ${sourceAspect} to ${targetAspect} using ${reframeInfo || 'smart crop'}`
  };
}

function generateAssetsList(exportPackage) {
  const assets = [];

  // Thumbnail for each platform
  if (exportPackage.primary) {
    assets.push({
      type: 'thumbnail',
      platform: exportPackage.primary.platform,
      specs: getThumbnailSpecs(exportPackage.primary.platform)
    });
  }

  for (const variant of exportPackage.variants) {
    assets.push({
      type: 'thumbnail',
      platform: variant.platform,
      specs: getThumbnailSpecs(variant.platform)
    });
  }

  // Caption files
  assets.push({
    type: 'captions',
    formats: ['srt', 'vtt'],
    languages: ['en']
  });

  return assets;
}

function getThumbnailSpecs(platform) {
  const specs = {
    'youtube-shorts': { width: 1080, height: 1920, format: 'jpg' },
    'youtube-standard': { width: 1280, height: 720, format: 'jpg' },
    'youtube-longform': { width: 1280, height: 720, format: 'jpg' },
    'tiktok-quick': { width: 1080, height: 1920, format: 'jpg' },
    'tiktok-standard': { width: 1080, height: 1920, format: 'jpg' },
    'instagram-reels': { width: 1080, height: 1920, format: 'jpg' },
    'instagram-feed': { width: 1080, height: 1080, format: 'jpg' },
    'netflix-episode': { width: 1920, height: 1080, format: 'jpg' },
    'netflix-movie': { width: 1920, height: 1080, format: 'jpg' }
  };

  return specs[platform] || { width: 1920, height: 1080, format: 'jpg' };
}

/**
 * Cloud function to get export intelligence data
 */
exports.creationWizardGetExportProfiles = functions.https.onCall(async (data, context) => {
  await verifyAuth(context);

  // Format platforms for frontend
  const platforms = Object.entries(EXPORT_INTELLIGENCE.platforms).map(([id, platform]) => ({
    id,
    name: platform.name,
    icon: platform.icon,
    aspectRatio: platform.aspectRatio,
    maxDuration: platform.maxDuration,
    optimalDuration: platform.optimalDuration,
    pacing: platform.pacing,
    features: platform.features,
    hookTiming: platform.hookTiming
  }));

  // Format aspect ratios
  const aspectRatios = Object.entries(EXPORT_INTELLIGENCE.aspectRatios).map(([id, ratio]) => ({
    id,
    name: ratio.name,
    ratio: ratio.ratio,
    width: ratio.width,
    height: ratio.height,
    use: ratio.use
  }));

  // Format duration presets
  const durationPresets = Object.entries(EXPORT_INTELLIGENCE.durationPresets).map(([id, preset]) => ({
    id,
    name: preset.name,
    icon: preset.icon,
    duration: preset.duration,
    platforms: preset.platforms,
    pacing: preset.pacing,
    sceneCount: preset.sceneCount
  }));

  // Format series templates
  const seriesTemplates = Object.entries(EXPORT_INTELLIGENCE.seriesTemplates).map(([id, template]) => ({
    id,
    name: template.name,
    episodeCount: template.episodeCount,
    consistency: template.consistency,
    branding: template.branding
  }));

  // Format quality presets
  const qualityPresets = Object.entries(EXPORT_INTELLIGENCE.qualityPresets).map(([id, preset]) => ({
    id,
    name: preset.name,
    resolution: preset.resolution,
    codec: preset.codec
  }));

  return {
    success: true,
    platforms,
    aspectRatios,
    durationPresets,
    seriesTemplates,
    qualityPresets
  };
});

/**
 * Cloud function to get export recommendations
 */
exports.creationWizardGetExportRecommendations = functions.https.onCall(async (data, context) => {
  await verifyAuth(context);

  const { platform, duration, contentType, genre, hasDialogue, seriesInfo } = data;

  const recommendations = getExportRecommendations({
    platform,
    duration,
    contentType,
    genre,
    hasDialogue,
    seriesInfo
  });

  return {
    success: true,
    recommendations
  };
});

/**
 * Cloud function to generate multi-platform export package
 */
exports.creationWizardGenerateExportPackage = functions.https.onCall(async (data, context) => {
  await verifyAuth(context);

  const { primaryPlatform, additionalPlatforms, content, seriesInfo } = data;

  const exportPackage = generateExportPackage({
    primaryPlatform,
    additionalPlatforms,
    content,
    seriesInfo
  });

  return {
    success: true,
    exportPackage
  };
});

// ============================================================
// PHASE 3D: AUDIO INTELLIGENCE SYSTEM
// Music scoring, sound design, and dynamic mixing
// ============================================================

const AUDIO_INTELLIGENCE = {
  // Music Scoring Engine - Genre and mood-based music selection
  musicScoring: {
    // Genre-to-music style mapping
    genreMusic: {
      'documentary-nature': {
        primaryStyle: 'ambient-orchestral',
        secondaryStyle: 'world-music',
        tempo: { min: 60, max: 90 },
        energy: 'low-medium',
        instruments: ['strings', 'piano', 'flute', 'ambient-pads'],
        avoid: ['heavy-drums', 'electronic-beats', 'distortion'],
        reference: 'Planet Earth, Our Planet soundtracks'
      },
      'documentary-historical': {
        primaryStyle: 'cinematic-orchestral',
        secondaryStyle: 'period-appropriate',
        tempo: { min: 70, max: 110 },
        energy: 'medium',
        instruments: ['full-orchestra', 'piano', 'choir'],
        avoid: ['modern-synths', 'trap-beats'],
        reference: 'Ken Burns documentaries, The Crown'
      },
      'documentary-crime': {
        primaryStyle: 'dark-ambient',
        secondaryStyle: 'tension-underscore',
        tempo: { min: 50, max: 80 },
        energy: 'low-tension',
        instruments: ['low-strings', 'synth-drones', 'prepared-piano'],
        avoid: ['upbeat', 'major-key', 'cheerful'],
        reference: 'Making a Murderer, The Jinx'
      },
      'documentary-social': {
        primaryStyle: 'indie-acoustic',
        secondaryStyle: 'emotional-piano',
        tempo: { min: 70, max: 100 },
        energy: 'medium-emotional',
        instruments: ['acoustic-guitar', 'piano', 'strings', 'ambient'],
        avoid: ['aggressive', 'heavy'],
        reference: '13th, Won\'t You Be My Neighbor'
      },
      'educational-explainer': {
        primaryStyle: 'upbeat-corporate',
        secondaryStyle: 'tech-minimal',
        tempo: { min: 100, max: 130 },
        energy: 'medium-high',
        instruments: ['synth', 'light-percussion', 'piano', 'ukulele'],
        avoid: ['dark', 'heavy', 'distracting'],
        reference: 'Kurzgesagt, TED-Ed'
      },
      'educational-tutorial': {
        primaryStyle: 'lo-fi-chill',
        secondaryStyle: 'ambient-focus',
        tempo: { min: 70, max: 95 },
        energy: 'low-background',
        instruments: ['lo-fi-keys', 'soft-drums', 'ambient-pads'],
        avoid: ['lyrics', 'dramatic', 'distracting'],
        reference: 'Focus music, study beats'
      },
      'entertainment-comedy': {
        primaryStyle: 'playful-quirky',
        secondaryStyle: 'upbeat-fun',
        tempo: { min: 110, max: 140 },
        energy: 'high',
        instruments: ['pizzicato-strings', 'woodwinds', 'xylophone', 'bouncy-synth'],
        avoid: ['serious', 'dark', 'slow'],
        reference: 'The Office, Parks and Rec'
      },
      'entertainment-drama': {
        primaryStyle: 'emotional-orchestral',
        secondaryStyle: 'piano-driven',
        tempo: { min: 60, max: 100 },
        energy: 'medium-emotional',
        instruments: ['strings', 'piano', 'cello', 'subtle-choir'],
        avoid: ['upbeat', 'quirky', 'electronic'],
        reference: 'This Is Us, Breaking Bad'
      },
      'entertainment-action': {
        primaryStyle: 'epic-hybrid',
        secondaryStyle: 'electronic-orchestral',
        tempo: { min: 120, max: 160 },
        energy: 'high-intense',
        instruments: ['brass', 'percussion', 'synth-bass', 'electric-guitar'],
        avoid: ['soft', 'ambient', 'slow'],
        reference: 'Mission Impossible, John Wick'
      },
      'entertainment-horror': {
        primaryStyle: 'dark-atmospheric',
        secondaryStyle: 'dissonant-tension',
        tempo: { min: 40, max: 80 },
        energy: 'tension-building',
        instruments: ['low-drones', 'dissonant-strings', 'prepared-piano', 'reversed-sounds'],
        avoid: ['major-key', 'upbeat', 'cheerful'],
        reference: 'Hereditary, The Conjuring'
      },
      'entertainment-romance': {
        primaryStyle: 'romantic-orchestral',
        secondaryStyle: 'indie-love',
        tempo: { min: 60, max: 100 },
        energy: 'soft-emotional',
        instruments: ['strings', 'piano', 'acoustic-guitar', 'soft-vocals'],
        avoid: ['aggressive', 'dark', 'electronic'],
        reference: 'The Notebook, La La Land'
      },
      'marketing-product': {
        primaryStyle: 'modern-corporate',
        secondaryStyle: 'upbeat-inspiring',
        tempo: { min: 100, max: 130 },
        energy: 'medium-high',
        instruments: ['synth', 'claps', 'piano', 'light-strings'],
        avoid: ['dark', 'slow', 'complex'],
        reference: 'Apple keynotes, premium brand ads'
      },
      'marketing-brand': {
        primaryStyle: 'emotional-uplifting',
        secondaryStyle: 'cinematic-inspiring',
        tempo: { min: 80, max: 120 },
        energy: 'building-to-high',
        instruments: ['piano', 'strings', 'drums-build', 'choir'],
        avoid: ['generic', 'stock-sounding'],
        reference: 'Nike, Google Year in Search'
      },
      'marketing-social': {
        primaryStyle: 'trending-pop',
        secondaryStyle: 'viral-hooks',
        tempo: { min: 100, max: 140 },
        energy: 'high-catchy',
        instruments: ['synth-bass', 'trap-hats', 'catchy-melody'],
        avoid: ['slow', 'complex', 'dated'],
        reference: 'TikTok trending sounds'
      },
      'cinematic': {
        primaryStyle: 'epic-orchestral',
        secondaryStyle: 'hans-zimmer-style',
        tempo: { min: 60, max: 140 },
        energy: 'dynamic-range',
        instruments: ['full-orchestra', 'choir', 'percussion', 'brass', 'synth-hybrid'],
        avoid: ['thin', 'cheap', 'stock-sounding'],
        reference: 'Inception, Interstellar, Gladiator'
      },
      'vlog-lifestyle': {
        primaryStyle: 'indie-chill',
        secondaryStyle: 'acoustic-happy',
        tempo: { min: 90, max: 120 },
        energy: 'medium-positive',
        instruments: ['acoustic-guitar', 'ukulele', 'light-percussion', 'whistling'],
        avoid: ['heavy', 'dark', 'complex'],
        reference: 'Casey Neistat style vlogs'
      },
      'gaming': {
        primaryStyle: 'electronic-energetic',
        secondaryStyle: 'chiptune-hybrid',
        tempo: { min: 120, max: 160 },
        energy: 'high-constant',
        instruments: ['synth', 'electronic-drums', 'bass', '8-bit-elements'],
        avoid: ['slow', 'acoustic', 'orchestral'],
        reference: 'Twitch streams, gaming montages'
      },
      'fitness': {
        primaryStyle: 'edm-workout',
        secondaryStyle: 'hip-hop-energy',
        tempo: { min: 120, max: 150 },
        energy: 'high-driving',
        instruments: ['heavy-bass', 'synth', 'claps', 'build-ups'],
        avoid: ['slow', 'ambient', 'soft'],
        reference: 'Workout playlists, Nike Training'
      }
    },

    // Mood-to-music mapping (links to Visual Intelligence moods)
    moodMusic: {
      'epic': {
        style: 'epic-orchestral-hybrid',
        tempo: { min: 90, max: 140 },
        energy: 'high-building',
        dynamics: 'wide-range',
        elements: ['brass-fanfares', 'timpani', 'choir', 'synth-layers'],
        buildPattern: 'gradual-crescendo'
      },
      'intimate': {
        style: 'minimal-emotional',
        tempo: { min: 60, max: 85 },
        energy: 'low-gentle',
        dynamics: 'soft-consistent',
        elements: ['solo-piano', 'soft-strings', 'ambient-pads'],
        buildPattern: 'subtle-swells'
      },
      'mysterious': {
        style: 'dark-ambient-tension',
        tempo: { min: 50, max: 80 },
        energy: 'low-unsettling',
        dynamics: 'quiet-with-stings',
        elements: ['drones', 'reversed-sounds', 'sparse-piano', 'low-strings'],
        buildPattern: 'tension-hold'
      },
      'energetic': {
        style: 'upbeat-driving',
        tempo: { min: 120, max: 150 },
        energy: 'high-constant',
        dynamics: 'loud-punchy',
        elements: ['driving-drums', 'synth-bass', 'brass-stabs'],
        buildPattern: 'verse-chorus'
      },
      'contemplative': {
        style: 'ambient-reflective',
        tempo: { min: 55, max: 75 },
        energy: 'very-low',
        dynamics: 'soft-flowing',
        elements: ['ambient-textures', 'soft-piano', 'gentle-pads'],
        buildPattern: 'static-evolving'
      },
      'tense': {
        style: 'suspense-thriller',
        tempo: { min: 70, max: 100 },
        energy: 'medium-anxious',
        dynamics: 'building-pressure',
        elements: ['staccato-strings', 'ticking', 'low-pulse', 'dissonance'],
        buildPattern: 'escalating'
      },
      'hopeful': {
        style: 'uplifting-inspiring',
        tempo: { min: 80, max: 110 },
        energy: 'medium-rising',
        dynamics: 'building-to-bright',
        elements: ['major-keys', 'strings-swell', 'piano', 'light-percussion'],
        buildPattern: 'dawn-chorus'
      },
      'professional': {
        style: 'corporate-clean',
        tempo: { min: 90, max: 115 },
        energy: 'medium-steady',
        dynamics: 'consistent-polished',
        elements: ['clean-synth', 'light-percussion', 'piano-accents'],
        buildPattern: 'structured'
      },
      'nostalgic': {
        style: 'vintage-warm',
        tempo: { min: 70, max: 100 },
        energy: 'medium-wistful',
        dynamics: 'warm-soft',
        elements: ['vinyl-texture', 'retro-keys', 'warm-strings', 'music-box'],
        buildPattern: 'memory-waves'
      },
      'dark': {
        style: 'ominous-heavy',
        tempo: { min: 50, max: 90 },
        energy: 'low-heavy',
        dynamics: 'rumbling-powerful',
        elements: ['sub-bass', 'distorted-elements', 'industrial', 'dark-choir'],
        buildPattern: 'doom-build'
      }
    },

    // Tempo categories
    tempoCategories: {
      'very-slow': { bpm: { min: 40, max: 60 }, use: ['ambient', 'horror', 'meditation'] },
      'slow': { bpm: { min: 60, max: 80 }, use: ['emotional', 'documentary', 'romance'] },
      'medium-slow': { bpm: { min: 80, max: 100 }, use: ['narrative', 'corporate', 'drama'] },
      'medium': { bpm: { min: 100, max: 120 }, use: ['explainer', 'marketing', 'lifestyle'] },
      'medium-fast': { bpm: { min: 120, max: 140 }, use: ['action', 'sports', 'gaming'] },
      'fast': { bpm: { min: 140, max: 160 }, use: ['workout', 'edm', 'high-energy'] },
      'very-fast': { bpm: { min: 160, max: 200 }, use: ['extreme-sports', 'drum-and-bass'] }
    },

    // Energy levels
    energyLevels: {
      'minimal': { volume: 0.15, presence: 'background', dynamics: 'flat' },
      'low': { volume: 0.25, presence: 'underscore', dynamics: 'subtle' },
      'medium-low': { volume: 0.35, presence: 'supporting', dynamics: 'gentle' },
      'medium': { volume: 0.45, presence: 'balanced', dynamics: 'moderate' },
      'medium-high': { volume: 0.55, presence: 'prominent', dynamics: 'active' },
      'high': { volume: 0.65, presence: 'driving', dynamics: 'punchy' },
      'intense': { volume: 0.75, presence: 'dominant', dynamics: 'powerful' }
    }
  },

  // Sound Design Library
  soundDesign: {
    // Ambient backgrounds per setting
    ambience: {
      'nature-forest': {
        layers: ['birds-distant', 'leaves-rustle', 'wind-gentle', 'creek-subtle'],
        volume: 0.2,
        stereoWidth: 'wide'
      },
      'nature-ocean': {
        layers: ['waves-crashing', 'seagulls-distant', 'wind-coastal'],
        volume: 0.25,
        stereoWidth: 'wide'
      },
      'nature-rain': {
        layers: ['rain-steady', 'thunder-distant', 'drips'],
        volume: 0.3,
        stereoWidth: 'full'
      },
      'nature-night': {
        layers: ['crickets', 'owl-distant', 'wind-night'],
        volume: 0.15,
        stereoWidth: 'wide'
      },
      'urban-city': {
        layers: ['traffic-distant', 'people-murmur', 'city-hum'],
        volume: 0.2,
        stereoWidth: 'medium'
      },
      'urban-cafe': {
        layers: ['coffee-shop-ambience', 'cups-clinking', 'murmur-conversations'],
        volume: 0.15,
        stereoWidth: 'medium'
      },
      'urban-office': {
        layers: ['office-hum', 'keyboard-distant', 'hvac'],
        volume: 0.1,
        stereoWidth: 'narrow'
      },
      'indoor-room': {
        layers: ['room-tone', 'clock-ticking', 'hvac-subtle'],
        volume: 0.08,
        stereoWidth: 'narrow'
      },
      'space-scifi': {
        layers: ['ship-hum', 'electronic-beeps', 'air-systems'],
        volume: 0.15,
        stereoWidth: 'surround'
      },
      'historical-medieval': {
        layers: ['fire-crackling', 'distant-crowd', 'wind-stone'],
        volume: 0.2,
        stereoWidth: 'wide'
      }
    },

    // Transition sounds
    transitions: {
      'whoosh-soft': { duration: 0.5, energy: 'low', use: ['cuts', 'gentle-transitions'] },
      'whoosh-medium': { duration: 0.4, energy: 'medium', use: ['standard-transitions'] },
      'whoosh-hard': { duration: 0.3, energy: 'high', use: ['fast-cuts', 'action'] },
      'swoosh-magical': { duration: 0.6, energy: 'medium', use: ['fantasy', 'reveal'] },
      'glitch': { duration: 0.2, energy: 'high', use: ['tech', 'modern', 'error'] },
      'tape-stop': { duration: 0.4, energy: 'medium', use: ['retro', 'comedy', 'pause'] },
      'reverse-cymbal': { duration: 1.5, energy: 'building', use: ['builds', 'transitions'] },
      'sub-drop': { duration: 0.8, energy: 'impact', use: ['reveals', 'statements'] },
      'paper-flip': { duration: 0.3, energy: 'low', use: ['text', 'pages', 'chapters'] },
      'camera-shutter': { duration: 0.2, energy: 'low', use: ['photo-reveals', 'memories'] }
    },

    // Impact and hit sounds
    impacts: {
      'hit-deep': { power: 'high', frequency: 'low', use: ['major-reveals', 'statements'] },
      'hit-punchy': { power: 'medium', frequency: 'mid', use: ['text-hits', 'beats'] },
      'hit-bright': { power: 'medium', frequency: 'high', use: ['accents', 'highlights'] },
      'boom-cinematic': { power: 'very-high', frequency: 'sub', use: ['trailers', 'epic'] },
      'slam': { power: 'high', frequency: 'mid-low', use: ['action', 'emphasis'] },
      'snap': { power: 'low', frequency: 'high', use: ['quick-cuts', 'beats'] },
      'thud': { power: 'medium', frequency: 'low', use: ['grounded', 'solid'] }
    },

    // Risers and stingers
    risersStingers: {
      'riser-tension': { duration: 3, type: 'build', mood: 'anxious' },
      'riser-epic': { duration: 4, type: 'build', mood: 'exciting' },
      'riser-horror': { duration: 5, type: 'build', mood: 'dread' },
      'stinger-reveal': { duration: 1, type: 'accent', mood: 'dramatic' },
      'stinger-comedy': { duration: 0.5, type: 'accent', mood: 'funny' },
      'stinger-horror': { duration: 1.5, type: 'accent', mood: 'scary' },
      'drone-tension': { duration: 'loop', type: 'sustain', mood: 'uneasy' },
      'swell-emotional': { duration: 3, type: 'build-release', mood: 'moving' }
    },

    // UI and notification sounds
    uiSounds: {
      'notification-positive': { duration: 0.3, mood: 'success' },
      'notification-negative': { duration: 0.4, mood: 'error' },
      'click-soft': { duration: 0.1, mood: 'neutral' },
      'pop-playful': { duration: 0.15, mood: 'fun' },
      'ding': { duration: 0.2, mood: 'attention' },
      'typing': { duration: 0.05, mood: 'activity' }
    }
  },

  // Dynamic Mixing System
  dynamicMixing: {
    // Voice-to-music balance presets
    voiceMixPresets: {
      'dialogue-focus': {
        voiceVolume: 1.0,
        musicVolume: 0.15,
        musicDucking: 0.6,
        duckingAttack: 100,
        duckingRelease: 500
      },
      'narration-standard': {
        voiceVolume: 1.0,
        musicVolume: 0.25,
        musicDucking: 0.5,
        duckingAttack: 150,
        duckingRelease: 600
      },
      'voice-with-music': {
        voiceVolume: 0.95,
        musicVolume: 0.35,
        musicDucking: 0.4,
        duckingAttack: 200,
        duckingRelease: 800
      },
      'music-emphasis': {
        voiceVolume: 0.85,
        musicVolume: 0.5,
        musicDucking: 0.3,
        duckingAttack: 250,
        duckingRelease: 1000
      },
      'music-only': {
        voiceVolume: 0,
        musicVolume: 0.7,
        musicDucking: 0,
        duckingAttack: 0,
        duckingRelease: 0
      }
    },

    // Scene energy to audio mapping
    sceneEnergyMapping: {
      'very-low': { musicEnergy: 'minimal', sfxPresence: 'subtle', ambienceLevel: 'prominent' },
      'low': { musicEnergy: 'low', sfxPresence: 'occasional', ambienceLevel: 'present' },
      'medium': { musicEnergy: 'medium', sfxPresence: 'balanced', ambienceLevel: 'supporting' },
      'high': { musicEnergy: 'high', sfxPresence: 'active', ambienceLevel: 'minimal' },
      'climax': { musicEnergy: 'intense', sfxPresence: 'impactful', ambienceLevel: 'none' }
    },

    // Fade presets
    fadePresets: {
      'quick-fade': { in: 200, out: 200 },
      'standard-fade': { in: 500, out: 500 },
      'slow-fade': { in: 1000, out: 1000 },
      'cinematic-fade': { in: 2000, out: 2000 },
      'crossfade-short': { duration: 500, curve: 'equal-power' },
      'crossfade-long': { duration: 1500, curve: 'equal-power' }
    },

    // Master output presets
    masterPresets: {
      'broadcast': {
        targetLoudness: -24,
        truePeak: -2,
        dynamicRange: 'controlled'
      },
      'streaming': {
        targetLoudness: -14,
        truePeak: -1,
        dynamicRange: 'moderate'
      },
      'social-media': {
        targetLoudness: -14,
        truePeak: -1,
        dynamicRange: 'punchy'
      },
      'cinematic': {
        targetLoudness: -27,
        truePeak: -3,
        dynamicRange: 'wide'
      }
    }
  },

  // Voice Enhancement Profiles
  voiceEnhancement: {
    profiles: {
      'narrator-warm': {
        eq: { lowCut: 80, lowShelf: { freq: 200, gain: 2 }, presence: { freq: 3000, gain: 3 }, air: { freq: 12000, gain: 2 } },
        compression: { threshold: -18, ratio: 3, attack: 10, release: 100 },
        deEss: { frequency: 6000, threshold: -20 },
        reverb: { type: 'room', mix: 0.05 }
      },
      'narrator-authoritative': {
        eq: { lowCut: 60, lowShelf: { freq: 150, gain: 3 }, presence: { freq: 2500, gain: 4 }, air: { freq: 10000, gain: 1 } },
        compression: { threshold: -15, ratio: 4, attack: 5, release: 80 },
        deEss: { frequency: 5500, threshold: -18 },
        reverb: { type: 'none', mix: 0 }
      },
      'narrator-intimate': {
        eq: { lowCut: 100, lowShelf: { freq: 250, gain: 1 }, presence: { freq: 4000, gain: 2 }, air: { freq: 14000, gain: 3 } },
        compression: { threshold: -20, ratio: 2.5, attack: 15, release: 150 },
        deEss: { frequency: 6500, threshold: -22 },
        reverb: { type: 'small-room', mix: 0.08 }
      },
      'narrator-energetic': {
        eq: { lowCut: 100, lowShelf: { freq: 180, gain: 1 }, presence: { freq: 3500, gain: 5 }, air: { freq: 11000, gain: 2 } },
        compression: { threshold: -12, ratio: 5, attack: 3, release: 50 },
        deEss: { frequency: 5000, threshold: -16 },
        reverb: { type: 'none', mix: 0 }
      },
      'documentary': {
        eq: { lowCut: 80, lowShelf: { freq: 200, gain: 1 }, presence: { freq: 3000, gain: 2 }, air: { freq: 12000, gain: 2 } },
        compression: { threshold: -16, ratio: 3, attack: 10, release: 100 },
        deEss: { frequency: 6000, threshold: -20 },
        reverb: { type: 'natural', mix: 0.03 }
      },
      'podcast': {
        eq: { lowCut: 90, lowShelf: { freq: 180, gain: 2 }, presence: { freq: 2800, gain: 3 }, air: { freq: 10000, gain: 1 } },
        compression: { threshold: -18, ratio: 3.5, attack: 8, release: 120 },
        deEss: { frequency: 5500, threshold: -18 },
        reverb: { type: 'none', mix: 0 }
      },
      'cinematic-trailer': {
        eq: { lowCut: 60, lowShelf: { freq: 120, gain: 4 }, presence: { freq: 2000, gain: 3 }, air: { freq: 8000, gain: 2 } },
        compression: { threshold: -10, ratio: 6, attack: 2, release: 40 },
        deEss: { frequency: 5000, threshold: -15 },
        reverb: { type: 'large-hall', mix: 0.15 }
      }
    },

    // Voice style to profile mapping
    styleToProfile: {
      'professional': 'narrator-authoritative',
      'friendly': 'narrator-warm',
      'casual': 'narrator-intimate',
      'excited': 'narrator-energetic',
      'documentary': 'documentary',
      'storytelling': 'narrator-warm',
      'dramatic': 'cinematic-trailer'
    }
  },

  // Audio Mood Presets (linked to Visual Intelligence)
  audioMoodPresets: {
    'epic': {
      music: { style: 'epic-orchestral-hybrid', energy: 'high', tempo: 'variable' },
      sfx: { impacts: true, risers: true, whooshes: 'hard' },
      voice: { profile: 'cinematic-trailer', reverb: 'large' },
      mixing: { musicVolume: 0.5, voiceDucking: 0.4, masterPreset: 'cinematic' }
    },
    'intimate': {
      music: { style: 'minimal-piano', energy: 'low', tempo: 'slow' },
      sfx: { impacts: false, risers: false, whooshes: 'soft' },
      voice: { profile: 'narrator-intimate', reverb: 'small-room' },
      mixing: { musicVolume: 0.2, voiceDucking: 0.6, masterPreset: 'streaming' }
    },
    'mysterious': {
      music: { style: 'dark-ambient', energy: 'low', tempo: 'very-slow' },
      sfx: { impacts: 'occasional', risers: 'tension', whooshes: 'swoosh-magical' },
      voice: { profile: 'narrator-warm', reverb: 'none' },
      mixing: { musicVolume: 0.3, voiceDucking: 0.5, masterPreset: 'streaming' }
    },
    'energetic': {
      music: { style: 'upbeat-driving', energy: 'high', tempo: 'fast' },
      sfx: { impacts: true, risers: false, whooshes: 'hard' },
      voice: { profile: 'narrator-energetic', reverb: 'none' },
      mixing: { musicVolume: 0.45, voiceDucking: 0.35, masterPreset: 'social-media' }
    },
    'contemplative': {
      music: { style: 'ambient-reflective', energy: 'minimal', tempo: 'very-slow' },
      sfx: { impacts: false, risers: false, whooshes: false },
      voice: { profile: 'narrator-warm', reverb: 'natural' },
      mixing: { musicVolume: 0.15, voiceDucking: 0.65, masterPreset: 'streaming' }
    },
    'tense': {
      music: { style: 'suspense-underscore', energy: 'building', tempo: 'medium' },
      sfx: { impacts: 'stingers', risers: 'tension', whooshes: false },
      voice: { profile: 'narrator-authoritative', reverb: 'none' },
      mixing: { musicVolume: 0.35, voiceDucking: 0.45, masterPreset: 'streaming' }
    },
    'hopeful': {
      music: { style: 'uplifting-inspiring', energy: 'building', tempo: 'medium' },
      sfx: { impacts: 'soft', risers: 'swell-emotional', whooshes: 'soft' },
      voice: { profile: 'narrator-warm', reverb: 'small-room' },
      mixing: { musicVolume: 0.35, voiceDucking: 0.5, masterPreset: 'streaming' }
    },
    'professional': {
      music: { style: 'corporate-clean', energy: 'medium', tempo: 'medium' },
      sfx: { impacts: 'subtle', risers: false, whooshes: 'soft' },
      voice: { profile: 'narrator-authoritative', reverb: 'none' },
      mixing: { musicVolume: 0.25, voiceDucking: 0.55, masterPreset: 'streaming' }
    },
    'nostalgic': {
      music: { style: 'vintage-warm', energy: 'medium-low', tempo: 'medium-slow' },
      sfx: { impacts: false, risers: false, whooshes: 'tape-stop' },
      voice: { profile: 'narrator-warm', reverb: 'small-room' },
      mixing: { musicVolume: 0.3, voiceDucking: 0.5, masterPreset: 'streaming' }
    },
    'dark': {
      music: { style: 'ominous-heavy', energy: 'low-heavy', tempo: 'slow' },
      sfx: { impacts: 'deep', risers: 'horror', whooshes: false },
      voice: { profile: 'narrator-authoritative', reverb: 'large-hall' },
      mixing: { musicVolume: 0.4, voiceDucking: 0.4, masterPreset: 'cinematic' }
    }
  },

  // Genre to complete audio treatment
  genreAudioTreatment: {
    'documentary-nature': {
      musicProfile: 'documentary-nature',
      defaultAmbience: 'nature-forest',
      voiceProfile: 'documentary',
      transitionSound: 'whoosh-soft',
      moodDefault: 'contemplative'
    },
    'documentary-crime': {
      musicProfile: 'documentary-crime',
      defaultAmbience: 'indoor-room',
      voiceProfile: 'narrator-authoritative',
      transitionSound: 'whoosh-medium',
      moodDefault: 'tense'
    },
    'educational-explainer': {
      musicProfile: 'educational-explainer',
      defaultAmbience: null,
      voiceProfile: 'narrator-energetic',
      transitionSound: 'whoosh-medium',
      moodDefault: 'professional'
    },
    'entertainment-action': {
      musicProfile: 'entertainment-action',
      defaultAmbience: null,
      voiceProfile: 'cinematic-trailer',
      transitionSound: 'whoosh-hard',
      moodDefault: 'epic'
    },
    'entertainment-horror': {
      musicProfile: 'entertainment-horror',
      defaultAmbience: 'nature-night',
      voiceProfile: 'narrator-warm',
      transitionSound: 'swoosh-magical',
      moodDefault: 'dark'
    },
    'marketing-brand': {
      musicProfile: 'marketing-brand',
      defaultAmbience: null,
      voiceProfile: 'narrator-warm',
      transitionSound: 'whoosh-medium',
      moodDefault: 'hopeful'
    },
    'cinematic': {
      musicProfile: 'cinematic',
      defaultAmbience: null,
      voiceProfile: 'cinematic-trailer',
      transitionSound: 'whoosh-hard',
      moodDefault: 'epic'
    }
  }
};

/**
 * Get audio recommendations based on genre and mood
 */
function getAudioRecommendations(options = {}) {
  const { genre, mood, productionMode, sceneEnergy, hasVoiceover } = options;

  const recommendations = {
    music: null,
    sfx: null,
    voice: null,
    mixing: null,
    ambience: null
  };

  // Get genre-based music
  const genreMusic = AUDIO_INTELLIGENCE.musicScoring.genreMusic[genre];
  if (genreMusic) {
    recommendations.music = {
      style: genreMusic.primaryStyle,
      alternativeStyle: genreMusic.secondaryStyle,
      tempo: genreMusic.tempo,
      energy: genreMusic.energy,
      instruments: genreMusic.instruments,
      avoid: genreMusic.avoid,
      reference: genreMusic.reference
    };
  }

  // Get mood-based audio preset
  const moodPreset = AUDIO_INTELLIGENCE.audioMoodPresets[mood];
  if (moodPreset) {
    recommendations.sfx = moodPreset.sfx;
    recommendations.mixing = moodPreset.mixing;

    // Override music style with mood if more specific
    if (moodPreset.music) {
      recommendations.music = {
        ...recommendations.music,
        ...moodPreset.music
      };
    }
  }

  // Get voice profile
  if (hasVoiceover) {
    const genreTreatment = AUDIO_INTELLIGENCE.genreAudioTreatment[genre];
    if (genreTreatment) {
      recommendations.voice = AUDIO_INTELLIGENCE.voiceEnhancement.profiles[genreTreatment.voiceProfile];
    } else if (moodPreset?.voice) {
      recommendations.voice = AUDIO_INTELLIGENCE.voiceEnhancement.profiles[moodPreset.voice.profile];
    }
  }

  // Get ambience
  const genreTreatment = AUDIO_INTELLIGENCE.genreAudioTreatment[genre];
  if (genreTreatment?.defaultAmbience) {
    recommendations.ambience = AUDIO_INTELLIGENCE.soundDesign.ambience[genreTreatment.defaultAmbience];
  }

  // Adjust for scene energy
  if (sceneEnergy) {
    const energyMapping = AUDIO_INTELLIGENCE.dynamicMixing.sceneEnergyMapping[sceneEnergy];
    if (energyMapping) {
      recommendations.energyAdjustments = energyMapping;
    }
  }

  return recommendations;
}

/**
 * Build complete audio treatment for a scene
 */
function buildSceneAudioTreatment(options = {}) {
  const {
    genre,
    mood,
    sceneType,
    hasVoiceover,
    sceneEnergy = 'medium',
    isTransition = false,
    previousMood = null
  } = options;

  const treatment = {
    music: {},
    sfx: [],
    voice: {},
    mixing: {},
    transitions: {}
  };

  // Get base recommendations
  const recommendations = getAudioRecommendations({ genre, mood, sceneEnergy, hasVoiceover });

  // Music settings
  if (recommendations.music) {
    const energyLevel = AUDIO_INTELLIGENCE.musicScoring.energyLevels[sceneEnergy] ||
                        AUDIO_INTELLIGENCE.musicScoring.energyLevels['medium'];

    treatment.music = {
      ...recommendations.music,
      volume: energyLevel.volume,
      presence: energyLevel.presence
    };
  }

  // SFX for transitions
  if (isTransition) {
    const genreTreatment = AUDIO_INTELLIGENCE.genreAudioTreatment[genre];
    const transitionSound = genreTreatment?.transitionSound || 'whoosh-soft';
    treatment.sfx.push({
      type: 'transition',
      sound: AUDIO_INTELLIGENCE.soundDesign.transitions[transitionSound],
      timing: 'on-cut'
    });
  }

  // Add mood-based SFX
  if (recommendations.sfx) {
    if (recommendations.sfx.risers) {
      treatment.sfx.push({
        type: 'riser',
        sound: AUDIO_INTELLIGENCE.soundDesign.risersStingers[recommendations.sfx.risers] ||
               AUDIO_INTELLIGENCE.soundDesign.risersStingers['riser-tension'],
        timing: 'before-climax'
      });
    }
    if (recommendations.sfx.impacts) {
      treatment.sfx.push({
        type: 'impact',
        sound: AUDIO_INTELLIGENCE.soundDesign.impacts['hit-punchy'],
        timing: 'on-emphasis'
      });
    }
  }

  // Voice settings
  if (hasVoiceover && recommendations.voice) {
    treatment.voice = {
      enhancement: recommendations.voice,
      mixPreset: AUDIO_INTELLIGENCE.dynamicMixing.voiceMixPresets['narration-standard']
    };
  }

  // Mixing settings
  treatment.mixing = recommendations.mixing ||
                     AUDIO_INTELLIGENCE.dynamicMixing.voiceMixPresets['narration-standard'];

  // Ambience
  if (recommendations.ambience) {
    treatment.ambience = recommendations.ambience;
  }

  // Handle mood transitions
  if (previousMood && previousMood !== mood) {
    treatment.transitions.moodChange = {
      from: previousMood,
      to: mood,
      crossfade: AUDIO_INTELLIGENCE.dynamicMixing.fadePresets['crossfade-long']
    };
  }

  return treatment;
}

/**
 * Cloud function to get audio intelligence data
 */
exports.creationWizardGetAudioProfiles = functions.https.onCall(async (data, context) => {
  await verifyAuth(context);

  // Format genre music profiles
  const genreMusic = Object.entries(AUDIO_INTELLIGENCE.musicScoring.genreMusic).map(([id, profile]) => ({
    id,
    primaryStyle: profile.primaryStyle,
    secondaryStyle: profile.secondaryStyle,
    tempo: profile.tempo,
    energy: profile.energy,
    instruments: profile.instruments,
    reference: profile.reference
  }));

  // Format mood music profiles
  const moodMusic = Object.entries(AUDIO_INTELLIGENCE.musicScoring.moodMusic).map(([id, profile]) => ({
    id,
    style: profile.style,
    tempo: profile.tempo,
    energy: profile.energy,
    elements: profile.elements
  }));

  // Format sound design categories
  const ambience = Object.entries(AUDIO_INTELLIGENCE.soundDesign.ambience).map(([id, amb]) => ({
    id,
    layers: amb.layers,
    volume: amb.volume
  }));

  const transitions = Object.entries(AUDIO_INTELLIGENCE.soundDesign.transitions).map(([id, trans]) => ({
    id,
    duration: trans.duration,
    energy: trans.energy,
    use: trans.use
  }));

  // Format voice profiles
  const voiceProfiles = Object.entries(AUDIO_INTELLIGENCE.voiceEnhancement.profiles).map(([id, profile]) => ({
    id,
    eq: profile.eq,
    compression: profile.compression
  }));

  // Format audio mood presets
  const audioMoods = Object.entries(AUDIO_INTELLIGENCE.audioMoodPresets).map(([id, preset]) => ({
    id,
    music: preset.music,
    sfx: preset.sfx,
    mixing: preset.mixing
  }));

  return {
    success: true,
    genreMusic,
    moodMusic,
    ambience,
    transitions,
    voiceProfiles,
    audioMoods,
    tempoCategories: AUDIO_INTELLIGENCE.musicScoring.tempoCategories,
    energyLevels: AUDIO_INTELLIGENCE.musicScoring.energyLevels,
    mixingPresets: AUDIO_INTELLIGENCE.dynamicMixing.voiceMixPresets
  };
});

/**
 * Cloud function to get audio recommendations
 */
exports.creationWizardGetAudioRecommendations = functions.https.onCall(async (data, context) => {
  await verifyAuth(context);

  const { genre, mood, productionMode, sceneEnergy, hasVoiceover } = data;

  const recommendations = getAudioRecommendations({
    genre,
    mood,
    productionMode,
    sceneEnergy,
    hasVoiceover
  });

  return {
    success: true,
    recommendations
  };
});

/**
 * Cloud function to build scene audio treatment
 */
exports.creationWizardBuildAudioTreatment = functions.https.onCall(async (data, context) => {
  await verifyAuth(context);

  const { genre, mood, sceneType, hasVoiceover, sceneEnergy, isTransition, previousMood } = data;

  const treatment = buildSceneAudioTreatment({
    genre,
    mood,
    sceneType,
    hasVoiceover,
    sceneEnergy,
    isTransition,
    previousMood
  });

  return {
    success: true,
    treatment
  };
});

// ============================================================
// PHASE 3E: SMART ASSEMBLY ENGINE
// Intelligent editing with transitions, pacing, and beat sync
// ============================================================

const ASSEMBLY_INTELLIGENCE = {
  // Transition Intelligence - Genre and context-aware transitions
  transitions: {
    // Transition types with properties
    types: {
      'cut': {
        name: 'Hard Cut',
        duration: 0,
        energy: 'high',
        use: ['fast-paced', 'action', 'dialogue', 'emphasis'],
        audioSync: false,
        description: 'Instant switch between shots'
      },
      'dissolve': {
        name: 'Cross Dissolve',
        duration: { min: 0.5, max: 2 },
        energy: 'low',
        use: ['time-passage', 'emotional', 'dreamlike', 'memory'],
        audioSync: true,
        description: 'Gradual blend between shots'
      },
      'fade-black': {
        name: 'Fade to Black',
        duration: { min: 1, max: 3 },
        energy: 'very-low',
        use: ['scene-end', 'chapter-break', 'dramatic-pause', 'death'],
        audioSync: true,
        description: 'Fade out to black, then fade in'
      },
      'fade-white': {
        name: 'Fade to White',
        duration: { min: 1, max: 2 },
        energy: 'transcendent',
        use: ['flashback', 'revelation', 'spiritual', 'memory'],
        audioSync: true,
        description: 'Fade out to white, bright transition'
      },
      'wipe-left': {
        name: 'Wipe Left',
        duration: { min: 0.3, max: 1 },
        energy: 'medium',
        use: ['scene-change', 'location-change', 'retro-style'],
        audioSync: true,
        description: 'New scene wipes in from right'
      },
      'wipe-right': {
        name: 'Wipe Right',
        duration: { min: 0.3, max: 1 },
        energy: 'medium',
        use: ['scene-change', 'location-change', 'retro-style'],
        audioSync: true,
        description: 'New scene wipes in from left'
      },
      'push': {
        name: 'Push',
        duration: { min: 0.3, max: 0.8 },
        energy: 'medium-high',
        use: ['slide-show', 'list-items', 'fast-montage'],
        audioSync: true,
        description: 'New scene pushes old scene out'
      },
      'zoom-in': {
        name: 'Zoom In',
        duration: { min: 0.5, max: 1.5 },
        energy: 'high',
        use: ['focus', 'emphasis', 'reveal', 'dramatic'],
        audioSync: true,
        description: 'Zoom into transition point'
      },
      'zoom-out': {
        name: 'Zoom Out',
        duration: { min: 0.5, max: 1.5 },
        energy: 'medium',
        use: ['reveal', 'context', 'establishing'],
        audioSync: true,
        description: 'Zoom out to reveal new scene'
      },
      'glitch': {
        name: 'Glitch',
        duration: { min: 0.2, max: 0.5 },
        energy: 'very-high',
        use: ['tech', 'error', 'dramatic', 'modern'],
        audioSync: true,
        description: 'Digital glitch effect'
      },
      'flash': {
        name: 'Flash',
        duration: { min: 0.1, max: 0.3 },
        energy: 'very-high',
        use: ['impact', 'photo', 'memory', 'emphasis'],
        audioSync: true,
        description: 'Quick white flash between scenes'
      },
      'blur': {
        name: 'Blur Transition',
        duration: { min: 0.5, max: 1.5 },
        energy: 'low',
        use: ['dreamlike', 'memory', 'confusion', 'passage'],
        audioSync: true,
        description: 'Blur out then in'
      },
      'whip-pan': {
        name: 'Whip Pan',
        duration: { min: 0.2, max: 0.5 },
        energy: 'very-high',
        use: ['action', 'fast-paced', 'comedy', 'vlog'],
        audioSync: true,
        description: 'Fast motion blur pan'
      },
      'morph': {
        name: 'Morph',
        duration: { min: 1, max: 3 },
        energy: 'medium',
        use: ['transformation', 'comparison', 'before-after'],
        audioSync: true,
        description: 'Shape morphing between scenes'
      },
      'slide-up': {
        name: 'Slide Up',
        duration: { min: 0.3, max: 0.8 },
        energy: 'medium',
        use: ['vertical-content', 'social-media', 'lists'],
        audioSync: true,
        description: 'New scene slides up'
      },
      'slide-down': {
        name: 'Slide Down',
        duration: { min: 0.3, max: 0.8 },
        energy: 'medium',
        use: ['vertical-content', 'social-media', 'reveals'],
        audioSync: true,
        description: 'New scene slides down'
      }
    },

    // Genre to transition style mapping
    genreTransitions: {
      'documentary-nature': {
        primary: ['dissolve', 'fade-black'],
        secondary: ['cut'],
        avoid: ['glitch', 'whip-pan', 'flash'],
        defaultDuration: 1.5,
        rhythm: 'slow'
      },
      'documentary-historical': {
        primary: ['dissolve', 'fade-black', 'fade-white'],
        secondary: ['cut', 'wipe-left'],
        avoid: ['glitch', 'whip-pan'],
        defaultDuration: 1.5,
        rhythm: 'measured'
      },
      'documentary-crime': {
        primary: ['cut', 'fade-black'],
        secondary: ['dissolve', 'flash'],
        avoid: ['wipe-left', 'push', 'slide-up'],
        defaultDuration: 1,
        rhythm: 'tense'
      },
      'educational-explainer': {
        primary: ['cut', 'push', 'slide-up'],
        secondary: ['dissolve', 'wipe-left'],
        avoid: ['fade-black', 'blur', 'morph'],
        defaultDuration: 0.5,
        rhythm: 'snappy'
      },
      'educational-tutorial': {
        primary: ['cut', 'dissolve'],
        secondary: ['fade-black', 'push'],
        avoid: ['glitch', 'whip-pan', 'flash'],
        defaultDuration: 0.8,
        rhythm: 'steady'
      },
      'entertainment-comedy': {
        primary: ['cut', 'whip-pan', 'push'],
        secondary: ['flash', 'glitch', 'zoom-in'],
        avoid: ['fade-black', 'dissolve', 'blur'],
        defaultDuration: 0.3,
        rhythm: 'punchy'
      },
      'entertainment-drama': {
        primary: ['dissolve', 'fade-black', 'cut'],
        secondary: ['fade-white', 'blur'],
        avoid: ['glitch', 'whip-pan', 'push'],
        defaultDuration: 1.5,
        rhythm: 'emotional'
      },
      'entertainment-action': {
        primary: ['cut', 'whip-pan', 'flash'],
        secondary: ['glitch', 'zoom-in', 'push'],
        avoid: ['dissolve', 'fade-black', 'blur'],
        defaultDuration: 0.3,
        rhythm: 'rapid'
      },
      'entertainment-horror': {
        primary: ['cut', 'fade-black', 'flash'],
        secondary: ['glitch', 'blur'],
        avoid: ['dissolve', 'wipe-left', 'push'],
        defaultDuration: 0.5,
        rhythm: 'jarring'
      },
      'entertainment-romance': {
        primary: ['dissolve', 'fade-white', 'blur'],
        secondary: ['cut', 'fade-black'],
        avoid: ['glitch', 'whip-pan', 'flash'],
        defaultDuration: 1.5,
        rhythm: 'dreamy'
      },
      'marketing-product': {
        primary: ['cut', 'push', 'zoom-in'],
        secondary: ['dissolve', 'slide-up'],
        avoid: ['fade-black', 'blur', 'morph'],
        defaultDuration: 0.4,
        rhythm: 'dynamic'
      },
      'marketing-brand': {
        primary: ['dissolve', 'cut', 'fade-white'],
        secondary: ['zoom-in', 'push'],
        avoid: ['glitch', 'whip-pan'],
        defaultDuration: 1,
        rhythm: 'emotional'
      },
      'marketing-social': {
        primary: ['cut', 'whip-pan', 'push'],
        secondary: ['glitch', 'flash', 'slide-up'],
        avoid: ['dissolve', 'fade-black', 'blur'],
        defaultDuration: 0.3,
        rhythm: 'viral'
      },
      'cinematic': {
        primary: ['cut', 'dissolve', 'fade-black'],
        secondary: ['fade-white', 'zoom-in'],
        avoid: ['wipe-left', 'push', 'glitch'],
        defaultDuration: 1,
        rhythm: 'cinematic'
      },
      'vlog-lifestyle': {
        primary: ['cut', 'whip-pan', 'push'],
        secondary: ['dissolve', 'zoom-in'],
        avoid: ['fade-black', 'morph'],
        defaultDuration: 0.4,
        rhythm: 'casual'
      },
      'gaming': {
        primary: ['cut', 'glitch', 'flash'],
        secondary: ['whip-pan', 'zoom-in'],
        avoid: ['dissolve', 'fade-black', 'blur'],
        defaultDuration: 0.3,
        rhythm: 'intense'
      },
      'fitness': {
        primary: ['cut', 'whip-pan', 'flash'],
        secondary: ['push', 'zoom-in'],
        avoid: ['dissolve', 'fade-black', 'blur'],
        defaultDuration: 0.25,
        rhythm: 'energetic'
      }
    },

    // Context-based transition rules
    contextRules: {
      'scene-start': { prefer: ['cut', 'fade-black'], avoid: ['dissolve'] },
      'scene-end': { prefer: ['fade-black', 'dissolve'], avoid: ['cut', 'push'] },
      'same-location': { prefer: ['cut', 'dissolve'], avoid: ['fade-black', 'wipe-left'] },
      'location-change': { prefer: ['dissolve', 'fade-black', 'wipe-left'], avoid: ['cut'] },
      'time-skip': { prefer: ['dissolve', 'fade-black', 'fade-white'], avoid: ['cut', 'push'] },
      'flashback': { prefer: ['fade-white', 'blur', 'dissolve'], avoid: ['cut', 'glitch'] },
      'montage': { prefer: ['cut', 'dissolve', 'push'], avoid: ['fade-black'] },
      'emphasis': { prefer: ['cut', 'flash', 'zoom-in'], avoid: ['dissolve', 'fade-black'] },
      'reveal': { prefer: ['zoom-out', 'fade-white', 'dissolve'], avoid: ['cut'] },
      'climax': { prefer: ['cut', 'flash'], avoid: ['dissolve', 'fade-black'] }
    }
  },

  // Pacing Algorithm - Scene duration and rhythm control
  pacing: {
    // Pacing profiles
    profiles: {
      'rapid-fire': {
        name: 'Rapid Fire',
        avgSceneDuration: { min: 1, max: 3 },
        cutFrequency: 'very-high',
        breathingRoom: 0,
        energyCurve: 'constant-high',
        use: ['tiktok', 'action', 'comedy', 'gaming']
      },
      'fast': {
        name: 'Fast',
        avgSceneDuration: { min: 2, max: 5 },
        cutFrequency: 'high',
        breathingRoom: 0.1,
        energyCurve: 'building',
        use: ['youtube-shorts', 'marketing', 'explainer']
      },
      'dynamic': {
        name: 'Dynamic',
        avgSceneDuration: { min: 3, max: 8 },
        cutFrequency: 'medium-high',
        breathingRoom: 0.15,
        energyCurve: 'varied',
        use: ['vlog', 'product', 'brand']
      },
      'balanced': {
        name: 'Balanced',
        avgSceneDuration: { min: 5, max: 12 },
        cutFrequency: 'medium',
        breathingRoom: 0.2,
        energyCurve: 'wave',
        use: ['youtube-standard', 'documentary', 'tutorial']
      },
      'contemplative': {
        name: 'Contemplative',
        avgSceneDuration: { min: 8, max: 20 },
        cutFrequency: 'low',
        breathingRoom: 0.3,
        energyCurve: 'gradual',
        use: ['nature-doc', 'art-film', 'meditation']
      },
      'cinematic': {
        name: 'Cinematic',
        avgSceneDuration: { min: 5, max: 30 },
        cutFrequency: 'motivated',
        breathingRoom: 0.25,
        energyCurve: 'dramatic',
        use: ['film', 'drama', 'epic']
      },
      'episodic': {
        name: 'Episodic TV',
        avgSceneDuration: { min: 4, max: 15 },
        cutFrequency: 'structured',
        breathingRoom: 0.2,
        energyCurve: 'act-based',
        use: ['tv-episode', 'series', 'streaming']
      }
    },

    // Energy curves - how energy flows through the video
    energyCurves: {
      'constant-high': {
        pattern: [0.8, 0.85, 0.9, 0.85, 0.9, 0.95, 0.9, 0.85],
        description: 'Maintains high energy throughout'
      },
      'building': {
        pattern: [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],
        description: 'Gradual build to climax'
      },
      'varied': {
        pattern: [0.5, 0.7, 0.4, 0.8, 0.5, 0.9, 0.6, 0.8],
        description: 'Dynamic ups and downs'
      },
      'wave': {
        pattern: [0.4, 0.6, 0.8, 0.6, 0.4, 0.6, 0.8, 0.7],
        description: 'Rolling wave pattern'
      },
      'gradual': {
        pattern: [0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65],
        description: 'Slow steady build'
      },
      'dramatic': {
        pattern: [0.5, 0.4, 0.6, 0.5, 0.7, 0.6, 0.9, 1.0],
        description: 'Tension and release'
      },
      'act-based': {
        pattern: [0.5, 0.6, 0.7, 0.5, 0.6, 0.8, 0.7, 0.9],
        description: 'Three-act structure'
      }
    },

    // Scene duration modifiers based on content
    durationModifiers: {
      'dialogue-heavy': 1.5,
      'action-sequence': 0.6,
      'establishing-shot': 0.8,
      'emotional-moment': 1.3,
      'information-dense': 1.2,
      'visual-spectacle': 0.9,
      'transition-scene': 0.7,
      'climax': 1.1,
      'denouement': 1.4
    }
  },

  // Beat Synchronization - Aligning cuts to music
  beatSync: {
    // Beat sync modes
    modes: {
      'off': {
        name: 'Off',
        description: 'No beat synchronization',
        cutOnBeat: false
      },
      'subtle': {
        name: 'Subtle',
        description: 'Occasional cuts on strong beats',
        cutOnBeat: true,
        beatStrength: 'strong-only',
        percentage: 0.3
      },
      'moderate': {
        name: 'Moderate',
        description: 'Regular cuts aligned to beats',
        cutOnBeat: true,
        beatStrength: 'medium-strong',
        percentage: 0.5
      },
      'aggressive': {
        name: 'Aggressive',
        description: 'Most cuts on beats',
        cutOnBeat: true,
        beatStrength: 'all',
        percentage: 0.8
      },
      'music-video': {
        name: 'Music Video',
        description: 'Nearly all cuts on beats',
        cutOnBeat: true,
        beatStrength: 'all',
        percentage: 0.95
      }
    },

    // Beat markers
    beatMarkers: {
      'downbeat': { strength: 1.0, prefer: ['cut', 'flash', 'zoom-in'] },
      'backbeat': { strength: 0.8, prefer: ['cut', 'push'] },
      'offbeat': { strength: 0.4, prefer: ['dissolve'] },
      'measure-start': { strength: 1.0, prefer: ['cut', 'fade-black', 'scene-change'] },
      'phrase-end': { strength: 0.9, prefer: ['dissolve', 'fade-black'] },
      'drop': { strength: 1.0, prefer: ['flash', 'glitch', 'impact'] },
      'build-peak': { strength: 0.9, prefer: ['cut', 'zoom-in'] }
    },

    // Tempo to cut frequency mapping
    tempoMapping: {
      'very-slow': { bpm: [40, 60], cutsPerMinute: [4, 8] },
      'slow': { bpm: [60, 80], cutsPerMinute: [6, 12] },
      'medium-slow': { bpm: [80, 100], cutsPerMinute: [8, 15] },
      'medium': { bpm: [100, 120], cutsPerMinute: [10, 20] },
      'medium-fast': { bpm: [120, 140], cutsPerMinute: [15, 30] },
      'fast': { bpm: [140, 160], cutsPerMinute: [20, 40] },
      'very-fast': { bpm: [160, 200], cutsPerMinute: [30, 60] }
    }
  },

  // Pattern Interrupts - Strategic engagement hooks
  patternInterrupts: {
    // Types of pattern interrupts
    types: {
      'visual-change': {
        name: 'Visual Change',
        description: 'Sudden visual style shift',
        elements: ['color-grade-shift', 'aspect-ratio-change', 'zoom-level'],
        frequency: 'every-45-90s',
        impact: 'medium'
      },
      'audio-shift': {
        name: 'Audio Shift',
        description: 'Music or sound change',
        elements: ['music-change', 'silence', 'sfx-hit'],
        frequency: 'every-30-60s',
        impact: 'medium'
      },
      'pacing-change': {
        name: 'Pacing Change',
        description: 'Speed up or slow down',
        elements: ['faster-cuts', 'slower-shots', 'freeze-frame'],
        frequency: 'every-60-120s',
        impact: 'high'
      },
      'direct-address': {
        name: 'Direct Address',
        description: 'Speaker looks at camera',
        elements: ['eye-contact', 'question', 'call-to-action'],
        frequency: 'every-90-180s',
        impact: 'very-high'
      },
      'b-roll-burst': {
        name: 'B-Roll Burst',
        description: 'Rapid B-roll sequence',
        elements: ['quick-cuts', 'montage', 'visual-variety'],
        frequency: 'every-30-60s',
        impact: 'medium'
      },
      'text-graphic': {
        name: 'Text/Graphic',
        description: 'On-screen text or graphic',
        elements: ['title-card', 'statistic', 'quote'],
        frequency: 'every-45-90s',
        impact: 'medium'
      },
      'perspective-shift': {
        name: 'Perspective Shift',
        description: 'Camera angle change',
        elements: ['angle-change', 'pov-shot', 'drone-shot'],
        frequency: 'every-60-120s',
        impact: 'medium'
      },
      'humor-break': {
        name: 'Humor Break',
        description: 'Light moment or joke',
        elements: ['funny-aside', 'blooper', 'self-deprecation'],
        frequency: 'every-120-180s',
        impact: 'high'
      }
    },

    // Platform-specific interrupt strategies
    platformStrategies: {
      'tiktok': {
        maxAttentionSpan: 8,
        interruptFrequency: 'very-high',
        preferredTypes: ['visual-change', 'audio-shift', 'text-graphic'],
        hookWindow: 1
      },
      'youtube-shorts': {
        maxAttentionSpan: 15,
        interruptFrequency: 'high',
        preferredTypes: ['visual-change', 'pacing-change', 'b-roll-burst'],
        hookWindow: 3
      },
      'instagram-reels': {
        maxAttentionSpan: 10,
        interruptFrequency: 'high',
        preferredTypes: ['visual-change', 'audio-shift', 'text-graphic'],
        hookWindow: 2
      },
      'youtube-standard': {
        maxAttentionSpan: 60,
        interruptFrequency: 'medium',
        preferredTypes: ['pacing-change', 'direct-address', 'b-roll-burst'],
        hookWindow: 30
      },
      'youtube-longform': {
        maxAttentionSpan: 120,
        interruptFrequency: 'low',
        preferredTypes: ['pacing-change', 'perspective-shift', 'humor-break'],
        hookWindow: 60
      },
      'netflix-episode': {
        maxAttentionSpan: 300,
        interruptFrequency: 'very-low',
        preferredTypes: ['pacing-change', 'perspective-shift'],
        hookWindow: 180
      }
    }
  },

  // B-Roll Intelligence - Smart supplementary footage placement
  bRoll: {
    // B-roll placement strategies
    strategies: {
      'illustrative': {
        name: 'Illustrative',
        description: 'Shows what narrator describes',
        timing: 'on-keyword',
        duration: { min: 2, max: 5 },
        overlap: 0.5
      },
      'atmospheric': {
        name: 'Atmospheric',
        description: 'Sets mood and tone',
        timing: 'continuous',
        duration: { min: 3, max: 8 },
        overlap: 0.3
      },
      'transitional': {
        name: 'Transitional',
        description: 'Bridges between scenes',
        timing: 'between-scenes',
        duration: { min: 1, max: 3 },
        overlap: 0
      },
      'emphasis': {
        name: 'Emphasis',
        description: 'Reinforces key points',
        timing: 'on-emphasis',
        duration: { min: 1.5, max: 4 },
        overlap: 0.4
      },
      'variety': {
        name: 'Variety',
        description: 'Breaks visual monotony',
        timing: 'periodic',
        duration: { min: 2, max: 4 },
        overlap: 0.5
      }
    },

    // B-roll density by genre
    genreDensity: {
      'documentary-nature': { density: 0.7, strategy: 'atmospheric' },
      'documentary-crime': { density: 0.5, strategy: 'illustrative' },
      'educational-explainer': { density: 0.6, strategy: 'illustrative' },
      'educational-tutorial': { density: 0.3, strategy: 'illustrative' },
      'entertainment-action': { density: 0.8, strategy: 'variety' },
      'marketing-product': { density: 0.7, strategy: 'emphasis' },
      'marketing-brand': { density: 0.6, strategy: 'atmospheric' },
      'vlog-lifestyle': { density: 0.5, strategy: 'variety' },
      'cinematic': { density: 0.4, strategy: 'atmospheric' }
    },

    // Keywords that trigger B-roll
    triggerKeywords: {
      'location': ['city', 'beach', 'mountain', 'office', 'home', 'street', 'park'],
      'action': ['running', 'walking', 'driving', 'flying', 'building', 'creating'],
      'emotion': ['happy', 'sad', 'excited', 'worried', 'peaceful', 'angry'],
      'time': ['morning', 'evening', 'night', 'sunset', 'sunrise', 'season'],
      'abstract': ['growth', 'success', 'failure', 'change', 'innovation', 'future']
    }
  },

  // Assembly Presets - Complete assembly configurations
  presets: {
    'documentary-standard': {
      name: 'Documentary Standard',
      transitions: 'documentary-nature',
      pacing: 'balanced',
      beatSync: 'subtle',
      patternInterrupts: 'youtube-standard',
      bRoll: { density: 0.5, strategy: 'illustrative' }
    },
    'explainer-fast': {
      name: 'Fast Explainer',
      transitions: 'educational-explainer',
      pacing: 'fast',
      beatSync: 'moderate',
      patternInterrupts: 'youtube-shorts',
      bRoll: { density: 0.6, strategy: 'illustrative' }
    },
    'social-viral': {
      name: 'Social Viral',
      transitions: 'marketing-social',
      pacing: 'rapid-fire',
      beatSync: 'aggressive',
      patternInterrupts: 'tiktok',
      bRoll: { density: 0.7, strategy: 'variety' }
    },
    'cinematic-epic': {
      name: 'Cinematic Epic',
      transitions: 'cinematic',
      pacing: 'cinematic',
      beatSync: 'subtle',
      patternInterrupts: 'netflix-episode',
      bRoll: { density: 0.4, strategy: 'atmospheric' }
    },
    'brand-emotional': {
      name: 'Brand Emotional',
      transitions: 'marketing-brand',
      pacing: 'dynamic',
      beatSync: 'moderate',
      patternInterrupts: 'youtube-standard',
      bRoll: { density: 0.6, strategy: 'atmospheric' }
    },
    'action-intense': {
      name: 'Action Intense',
      transitions: 'entertainment-action',
      pacing: 'rapid-fire',
      beatSync: 'aggressive',
      patternInterrupts: 'youtube-shorts',
      bRoll: { density: 0.8, strategy: 'variety' }
    },
    'tutorial-clear': {
      name: 'Tutorial Clear',
      transitions: 'educational-tutorial',
      pacing: 'balanced',
      beatSync: 'off',
      patternInterrupts: 'youtube-standard',
      bRoll: { density: 0.3, strategy: 'illustrative' }
    },
    'drama-emotional': {
      name: 'Drama Emotional',
      transitions: 'entertainment-drama',
      pacing: 'contemplative',
      beatSync: 'subtle',
      patternInterrupts: 'netflix-episode',
      bRoll: { density: 0.4, strategy: 'atmospheric' }
    }
  }
};

// =============================================================================
// PHASE 3F: NARRATIVE STRUCTURE INTELLIGENCE
// Complete story structure system for professional narrative pacing
// =============================================================================

const NARRATIVE_STRUCTURE = {
  // Story Arc Templates - Classic narrative frameworks
  storyArcs: {
    'three-act': {
      name: 'Three-Act Structure',
      description: 'Classic Hollywood narrative structure',
      acts: [
        { name: 'Setup', percentage: 25, purpose: 'Establish world, characters, conflict', energyRange: [0.3, 0.5] },
        { name: 'Confrontation', percentage: 50, purpose: 'Rising action, complications, midpoint', energyRange: [0.5, 0.9] },
        { name: 'Resolution', percentage: 25, purpose: 'Climax, falling action, resolution', energyRange: [0.8, 0.4] }
      ],
      beats: ['hook', 'inciting-incident', 'plot-point-1', 'midpoint', 'plot-point-2', 'climax', 'resolution'],
      bestFor: ['film', 'drama', 'documentary', 'brand-story']
    },
    'five-act': {
      name: 'Five-Act Structure',
      description: 'Shakespearean dramatic structure',
      acts: [
        { name: 'Exposition', percentage: 15, purpose: 'Introduction and setup', energyRange: [0.3, 0.4] },
        { name: 'Rising Action', percentage: 25, purpose: 'Complications develop', energyRange: [0.4, 0.7] },
        { name: 'Climax', percentage: 20, purpose: 'Peak tension and turning point', energyRange: [0.8, 1.0] },
        { name: 'Falling Action', percentage: 25, purpose: 'Consequences unfold', energyRange: [0.7, 0.5] },
        { name: 'Denouement', percentage: 15, purpose: 'Resolution and closure', energyRange: [0.4, 0.3] }
      ],
      beats: ['opening', 'complication', 'rising-stakes', 'crisis', 'climax', 'reversal', 'resolution'],
      bestFor: ['drama', 'thriller', 'epic', 'series']
    },
    'heros-journey': {
      name: "Hero's Journey",
      description: "Campbell's monomyth structure",
      acts: [
        { name: 'Ordinary World', percentage: 8, purpose: 'Establish normal life', energyRange: [0.3, 0.4] },
        { name: 'Call to Adventure', percentage: 7, purpose: 'Disruption occurs', energyRange: [0.4, 0.6] },
        { name: 'Refusal of Call', percentage: 5, purpose: 'Initial hesitation', energyRange: [0.4, 0.5] },
        { name: 'Meeting the Mentor', percentage: 5, purpose: 'Guidance received', energyRange: [0.5, 0.6] },
        { name: 'Crossing Threshold', percentage: 10, purpose: 'Enter special world', energyRange: [0.6, 0.7] },
        { name: 'Tests & Allies', percentage: 15, purpose: 'Face challenges', energyRange: [0.6, 0.8] },
        { name: 'Approach', percentage: 10, purpose: 'Prepare for ordeal', energyRange: [0.7, 0.8] },
        { name: 'Ordeal', percentage: 10, purpose: 'Central crisis', energyRange: [0.9, 1.0] },
        { name: 'Reward', percentage: 8, purpose: 'Seize the prize', energyRange: [0.8, 0.7] },
        { name: 'Road Back', percentage: 7, purpose: 'Return begins', energyRange: [0.6, 0.7] },
        { name: 'Resurrection', percentage: 10, purpose: 'Final test', energyRange: [0.8, 0.9] },
        { name: 'Return with Elixir', percentage: 5, purpose: 'Transformation complete', energyRange: [0.5, 0.4] }
      ],
      beats: ['ordinary', 'call', 'refusal', 'mentor', 'threshold', 'tests', 'cave', 'ordeal', 'reward', 'road-back', 'resurrection', 'return'],
      bestFor: ['adventure', 'fantasy', 'inspirational', 'transformation']
    },
    'dan-harmon-circle': {
      name: 'Story Circle',
      description: "Dan Harmon's simplified hero's journey",
      acts: [
        { name: 'Comfort Zone', percentage: 12, purpose: 'Character in comfort', energyRange: [0.3, 0.4] },
        { name: 'Want Something', percentage: 12, purpose: 'Desire established', energyRange: [0.4, 0.5] },
        { name: 'Enter Unfamiliar', percentage: 12, purpose: 'Leave comfort zone', energyRange: [0.5, 0.7] },
        { name: 'Adapt to It', percentage: 14, purpose: 'Learn and struggle', energyRange: [0.6, 0.8] },
        { name: 'Get What Wanted', percentage: 12, purpose: 'Achieve goal', energyRange: [0.8, 0.9] },
        { name: 'Pay the Price', percentage: 14, purpose: 'Face consequences', energyRange: [0.7, 0.8] },
        { name: 'Return to Familiar', percentage: 12, purpose: 'Go back changed', energyRange: [0.5, 0.4] },
        { name: 'Having Changed', percentage: 12, purpose: 'Show transformation', energyRange: [0.4, 0.5] }
      ],
      beats: ['you', 'need', 'go', 'search', 'find', 'take', 'return', 'change'],
      bestFor: ['tv-series', 'web-series', 'youtube', 'short-film']
    },
    'freytags-pyramid': {
      name: "Freytag's Pyramid",
      description: 'Classic dramatic tension structure',
      acts: [
        { name: 'Exposition', percentage: 15, purpose: 'Background information', energyRange: [0.2, 0.4] },
        { name: 'Rising Action', percentage: 30, purpose: 'Building tension', energyRange: [0.4, 0.8] },
        { name: 'Climax', percentage: 10, purpose: 'Peak of tension', energyRange: [0.9, 1.0] },
        { name: 'Falling Action', percentage: 30, purpose: 'Tension release', energyRange: [0.7, 0.4] },
        { name: 'Catastrophe', percentage: 15, purpose: 'Final resolution', energyRange: [0.4, 0.3] }
      ],
      beats: ['introduction', 'complication', 'climax', 'reversal', 'catastrophe'],
      bestFor: ['tragedy', 'drama', 'literary']
    },
    'kishotenketsu': {
      name: 'Kishotenketsu',
      description: 'Four-act structure without conflict (East Asian)',
      acts: [
        { name: 'Ki (Introduction)', percentage: 25, purpose: 'Introduce elements', energyRange: [0.3, 0.5] },
        { name: 'Sho (Development)', percentage: 25, purpose: 'Develop elements', energyRange: [0.5, 0.6] },
        { name: 'Ten (Twist)', percentage: 25, purpose: 'Unexpected turn', energyRange: [0.7, 0.8] },
        { name: 'Ketsu (Conclusion)', percentage: 25, purpose: 'Reconcile elements', energyRange: [0.5, 0.4] }
      ],
      beats: ['introduction', 'development', 'twist', 'reconciliation'],
      bestFor: ['anime', 'slice-of-life', 'contemplative', 'artistic']
    },
    'inverted-pyramid': {
      name: 'Inverted Pyramid',
      description: 'News/journalistic structure - most important first',
      acts: [
        { name: 'Lead', percentage: 20, purpose: 'Most important info', energyRange: [0.8, 0.9] },
        { name: 'Body', percentage: 50, purpose: 'Supporting details', energyRange: [0.6, 0.5] },
        { name: 'Tail', percentage: 30, purpose: 'Background context', energyRange: [0.4, 0.3] }
      ],
      beats: ['hook', 'key-facts', 'details', 'context', 'background'],
      bestFor: ['news', 'documentary', 'explainer', 'educational']
    },
    'youtube-retention': {
      name: 'YouTube Retention Structure',
      description: 'Optimized for audience retention metrics',
      acts: [
        { name: 'Hook', percentage: 5, purpose: 'Grab attention instantly', energyRange: [0.9, 1.0] },
        { name: 'Promise', percentage: 5, purpose: 'State value proposition', energyRange: [0.7, 0.8] },
        { name: 'Setup', percentage: 10, purpose: 'Context and credibility', energyRange: [0.5, 0.6] },
        { name: 'Content Block 1', percentage: 20, purpose: 'First main point', energyRange: [0.6, 0.8] },
        { name: 'Pattern Break 1', percentage: 5, purpose: 'Re-engage attention', energyRange: [0.8, 0.9] },
        { name: 'Content Block 2', percentage: 20, purpose: 'Second main point', energyRange: [0.6, 0.8] },
        { name: 'Pattern Break 2', percentage: 5, purpose: 'Re-engage attention', energyRange: [0.8, 0.9] },
        { name: 'Content Block 3', percentage: 15, purpose: 'Third main point', energyRange: [0.7, 0.9] },
        { name: 'Climax/Reveal', percentage: 10, purpose: 'Biggest payoff', energyRange: [0.9, 1.0] },
        { name: 'CTA/Outro', percentage: 5, purpose: 'Call to action', energyRange: [0.6, 0.7] }
      ],
      beats: ['hook', 'promise', 'setup', 'content-1', 'break-1', 'content-2', 'break-2', 'content-3', 'climax', 'cta'],
      bestFor: ['youtube', 'educational', 'tutorial', 'listicle']
    },
    'tiktok-viral': {
      name: 'TikTok Viral Structure',
      description: 'Optimized for short-form viral content',
      acts: [
        { name: 'Hook', percentage: 10, purpose: 'Stop the scroll', energyRange: [1.0, 1.0] },
        { name: 'Setup', percentage: 15, purpose: 'Quick context', energyRange: [0.7, 0.8] },
        { name: 'Build', percentage: 25, purpose: 'Rising tension/curiosity', energyRange: [0.8, 0.9] },
        { name: 'Payoff', percentage: 30, purpose: 'The reveal/punchline', energyRange: [0.9, 1.0] },
        { name: 'Tag', percentage: 20, purpose: 'Reaction/loop point', energyRange: [0.8, 1.0] }
      ],
      beats: ['hook', 'setup', 'build', 'payoff', 'tag'],
      bestFor: ['tiktok', 'reels', 'shorts', 'viral']
    },
    'problem-solution': {
      name: 'Problem-Solution',
      description: 'Classic marketing/educational structure',
      acts: [
        { name: 'Problem', percentage: 30, purpose: 'Establish pain point', energyRange: [0.5, 0.7] },
        { name: 'Agitation', percentage: 20, purpose: 'Amplify the problem', energyRange: [0.7, 0.8] },
        { name: 'Solution', percentage: 35, purpose: 'Present the answer', energyRange: [0.6, 0.8] },
        { name: 'Proof/CTA', percentage: 15, purpose: 'Evidence and action', energyRange: [0.7, 0.9] }
      ],
      beats: ['problem', 'agitation', 'solution', 'proof', 'action'],
      bestFor: ['commercial', 'explainer', 'sales', 'tutorial']
    },
    'before-after-bridge': {
      name: 'Before-After-Bridge',
      description: 'Transformation-focused narrative',
      acts: [
        { name: 'Before', percentage: 30, purpose: 'Current painful state', energyRange: [0.4, 0.5] },
        { name: 'After', percentage: 30, purpose: 'Desired future state', energyRange: [0.7, 0.9] },
        { name: 'Bridge', percentage: 40, purpose: 'How to get there', energyRange: [0.6, 0.8] }
      ],
      beats: ['current-state', 'future-vision', 'transformation-path', 'action-steps'],
      bestFor: ['transformation', 'testimonial', 'coaching', 'inspirational']
    },
    'documentary-observational': {
      name: 'Documentary Observational',
      description: 'CinÃ©ma vÃ©ritÃ© style structure',
      acts: [
        { name: 'Immersion', percentage: 20, purpose: 'Enter the world', energyRange: [0.3, 0.5] },
        { name: 'Observation', percentage: 35, purpose: 'Witness events unfold', energyRange: [0.4, 0.7] },
        { name: 'Revelation', percentage: 25, purpose: 'Truth emerges', energyRange: [0.6, 0.8] },
        { name: 'Reflection', percentage: 20, purpose: 'Meaning and impact', energyRange: [0.5, 0.4] }
      ],
      beats: ['entry', 'observation', 'development', 'revelation', 'meaning'],
      bestFor: ['documentary', 'reality', 'nature', 'portrait']
    }
  },

  // Emotional Journey Mapping
  emotionalJourneys: {
    'triumph': {
      name: 'Triumph Arc',
      description: 'Low to high emotional journey',
      curve: [0.3, 0.4, 0.5, 0.6, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],
      mood: 'hopeful',
      endFeeling: 'inspired'
    },
    'tragedy': {
      name: 'Tragedy Arc',
      description: 'High to low emotional journey',
      curve: [0.7, 0.8, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2],
      mood: 'melancholic',
      endFeeling: 'reflective'
    },
    'redemption': {
      name: 'Redemption Arc',
      description: 'Fall then rise pattern',
      curve: [0.6, 0.5, 0.4, 0.3, 0.2, 0.3, 0.5, 0.7, 0.8, 0.9],
      mood: 'hopeful',
      endFeeling: 'uplifted'
    },
    'rags-to-riches': {
      name: 'Rags to Riches',
      description: 'Steady climb from bottom',
      curve: [0.2, 0.3, 0.4, 0.5, 0.6, 0.65, 0.7, 0.8, 0.9, 0.95],
      mood: 'inspirational',
      endFeeling: 'satisfied'
    },
    'riches-to-rags': {
      name: 'Riches to Rags',
      description: 'Decline from prosperity',
      curve: [0.9, 0.85, 0.8, 0.7, 0.6, 0.5, 0.4, 0.35, 0.3, 0.25],
      mood: 'cautionary',
      endFeeling: 'sobered'
    },
    'icarus': {
      name: 'Icarus Arc',
      description: 'Rise then fall pattern',
      curve: [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.7, 0.5, 0.3, 0.2],
      mood: 'cautionary',
      endFeeling: 'thoughtful'
    },
    'oedipus': {
      name: 'Oedipus Arc',
      description: 'Fall, rise, then fall',
      curve: [0.7, 0.5, 0.3, 0.4, 0.6, 0.8, 0.7, 0.5, 0.3, 0.2],
      mood: 'tragic',
      endFeeling: 'cathartic'
    },
    'cinderella': {
      name: 'Cinderella Arc',
      description: 'Rise, fall, rise pattern',
      curve: [0.3, 0.5, 0.7, 0.8, 0.5, 0.3, 0.5, 0.7, 0.9, 1.0],
      mood: 'magical',
      endFeeling: 'delighted'
    },
    'thriller': {
      name: 'Thriller Arc',
      description: 'Tension building with peaks',
      curve: [0.5, 0.6, 0.7, 0.6, 0.8, 0.7, 0.9, 0.8, 1.0, 0.6],
      mood: 'tense',
      endFeeling: 'relieved'
    },
    'mystery': {
      name: 'Mystery Arc',
      description: 'Curiosity building to revelation',
      curve: [0.4, 0.5, 0.6, 0.55, 0.65, 0.7, 0.75, 0.85, 1.0, 0.7],
      mood: 'curious',
      endFeeling: 'satisfied'
    },
    'comedy': {
      name: 'Comedy Arc',
      description: 'Ups and downs with happy ending',
      curve: [0.6, 0.7, 0.5, 0.8, 0.4, 0.9, 0.5, 0.7, 0.85, 0.95],
      mood: 'playful',
      endFeeling: 'joyful'
    },
    'horror': {
      name: 'Horror Arc',
      description: 'Dread building with shocks',
      curve: [0.5, 0.6, 0.7, 0.5, 0.8, 0.6, 0.9, 0.7, 1.0, 0.3],
      mood: 'dread',
      endFeeling: 'unsettled'
    },
    'meditative': {
      name: 'Meditative Arc',
      description: 'Calm and contemplative throughout',
      curve: [0.4, 0.45, 0.5, 0.55, 0.5, 0.55, 0.5, 0.45, 0.5, 0.45],
      mood: 'peaceful',
      endFeeling: 'centered'
    },
    'educational': {
      name: 'Educational Arc',
      description: 'Steady engagement with peaks',
      curve: [0.7, 0.6, 0.65, 0.7, 0.8, 0.65, 0.7, 0.8, 0.85, 0.9],
      mood: 'curious',
      endFeeling: 'enlightened'
    }
  },

  // Narrative Beat Types
  narrativeBeats: {
    'hook': {
      name: 'Hook',
      purpose: 'Capture immediate attention',
      position: 'opening',
      duration: { percentage: 3, min: 3, max: 15 },
      energy: 0.9,
      techniques: ['question', 'statement', 'visual-shock', 'mystery', 'promise'],
      transitionIn: 'none',
      transitionOut: 'cut'
    },
    'inciting-incident': {
      name: 'Inciting Incident',
      purpose: 'Disrupt the status quo',
      position: 'act-1',
      duration: { percentage: 5, min: 10, max: 60 },
      energy: 0.7,
      techniques: ['event', 'revelation', 'arrival', 'discovery'],
      transitionIn: 'dissolve',
      transitionOut: 'cut'
    },
    'plot-point': {
      name: 'Plot Point',
      purpose: 'Major story turning point',
      position: 'act-break',
      duration: { percentage: 8, min: 15, max: 90 },
      energy: 0.8,
      techniques: ['revelation', 'decision', 'confrontation', 'discovery'],
      transitionIn: 'cut',
      transitionOut: 'dissolve'
    },
    'midpoint': {
      name: 'Midpoint',
      purpose: 'Central pivot of the story',
      position: 'center',
      duration: { percentage: 10, min: 20, max: 120 },
      energy: 0.85,
      techniques: ['revelation', 'reversal', 'commitment', 'false-victory'],
      transitionIn: 'dissolve',
      transitionOut: 'fade'
    },
    'climax': {
      name: 'Climax',
      purpose: 'Peak emotional/narrative moment',
      position: 'act-3',
      duration: { percentage: 12, min: 30, max: 180 },
      energy: 1.0,
      techniques: ['confrontation', 'revelation', 'sacrifice', 'transformation'],
      transitionIn: 'cut',
      transitionOut: 'dissolve'
    },
    'resolution': {
      name: 'Resolution',
      purpose: 'Tie up loose ends',
      position: 'ending',
      duration: { percentage: 8, min: 15, max: 90 },
      energy: 0.5,
      techniques: ['new-normal', 'reflection', 'callback', 'future-glimpse'],
      transitionIn: 'dissolve',
      transitionOut: 'fade'
    },
    'pattern-break': {
      name: 'Pattern Break',
      purpose: 'Re-engage wandering attention',
      position: 'throughout',
      duration: { percentage: 2, min: 2, max: 10 },
      energy: 0.85,
      techniques: ['visual-change', 'tone-shift', 'direct-address', 'humor', 'surprise'],
      transitionIn: 'cut',
      transitionOut: 'cut'
    },
    'tension-peak': {
      name: 'Tension Peak',
      purpose: 'Moment of maximum suspense',
      position: 'pre-climax',
      duration: { percentage: 5, min: 10, max: 60 },
      energy: 0.95,
      techniques: ['pause', 'escalation', 'countdown', 'confrontation'],
      transitionIn: 'cut',
      transitionOut: 'cut'
    },
    'relief-moment': {
      name: 'Relief Moment',
      purpose: 'Release built-up tension',
      position: 'post-tension',
      duration: { percentage: 3, min: 5, max: 30 },
      energy: 0.4,
      techniques: ['humor', 'breath', 'beauty-shot', 'quiet-moment'],
      transitionIn: 'dissolve',
      transitionOut: 'dissolve'
    },
    'callback': {
      name: 'Callback',
      purpose: 'Reference earlier moment',
      position: 'resolution',
      duration: { percentage: 2, min: 3, max: 15 },
      energy: 0.6,
      techniques: ['visual-echo', 'dialogue-repeat', 'motif-return'],
      transitionIn: 'match-cut',
      transitionOut: 'dissolve'
    },
    'montage': {
      name: 'Montage',
      purpose: 'Compress time/show progression',
      position: 'transition',
      duration: { percentage: 10, min: 20, max: 120 },
      energy: 0.7,
      techniques: ['time-lapse', 'parallel-action', 'training', 'relationship-build'],
      transitionIn: 'dissolve',
      transitionOut: 'dissolve'
    },
    'revelation': {
      name: 'Revelation',
      purpose: 'Major information reveal',
      position: 'variable',
      duration: { percentage: 5, min: 10, max: 45 },
      energy: 0.85,
      techniques: ['twist', 'discovery', 'confession', 'flashback'],
      transitionIn: 'cut',
      transitionOut: 'beat-pause'
    },
    'cliffhanger': {
      name: 'Cliffhanger',
      purpose: 'Create anticipation for next segment',
      position: 'episode-end',
      duration: { percentage: 3, min: 5, max: 20 },
      energy: 0.9,
      techniques: ['question', 'danger', 'revelation-partial', 'arrival'],
      transitionIn: 'cut',
      transitionOut: 'fade-to-black'
    }
  },

  // Genre-Specific Narrative Patterns
  genrePatterns: {
    'horror': {
      arc: 'heros-journey',
      emotionalJourney: 'horror',
      requiredBeats: ['hook', 'inciting-incident', 'tension-peak', 'climax', 'relief-moment'],
      pacing: 'dynamic',
      tensionStyle: 'building-with-releases',
      scareIntervals: { min: 45, max: 90 },
      endingTypes: ['twist', 'ambiguous', 'survivor', 'cycle-continues']
    },
    'thriller': {
      arc: 'five-act',
      emotionalJourney: 'thriller',
      requiredBeats: ['hook', 'inciting-incident', 'plot-point', 'tension-peak', 'climax', 'resolution'],
      pacing: 'escalating',
      tensionStyle: 'ratcheting',
      twistPlacement: 0.75,
      endingTypes: ['reveal', 'confrontation', 'escape', 'sacrifice']
    },
    'drama': {
      arc: 'three-act',
      emotionalJourney: 'redemption',
      requiredBeats: ['hook', 'inciting-incident', 'midpoint', 'climax', 'resolution'],
      pacing: 'contemplative',
      tensionStyle: 'emotional-stakes',
      characterMoments: 0.4,
      endingTypes: ['transformation', 'acceptance', 'sacrifice', 'reconciliation']
    },
    'comedy': {
      arc: 'dan-harmon-circle',
      emotionalJourney: 'comedy',
      requiredBeats: ['hook', 'inciting-incident', 'midpoint', 'climax', 'resolution'],
      pacing: 'varied',
      tensionStyle: 'comic-escalation',
      jokeIntervals: { min: 15, max: 45 },
      endingTypes: ['happy', 'ironic', 'callback', 'absurd']
    },
    'documentary': {
      arc: 'inverted-pyramid',
      emotionalJourney: 'educational',
      requiredBeats: ['hook', 'revelation', 'midpoint', 'climax', 'resolution'],
      pacing: 'measured',
      tensionStyle: 'curiosity-building',
      interviewRatio: 0.4,
      endingTypes: ['reflection', 'call-to-action', 'update', 'question']
    },
    'action': {
      arc: 'heros-journey',
      emotionalJourney: 'triumph',
      requiredBeats: ['hook', 'inciting-incident', 'montage', 'tension-peak', 'climax', 'resolution'],
      pacing: 'fast',
      tensionStyle: 'physical-stakes',
      actionSequenceRatio: 0.4,
      endingTypes: ['victory', 'sacrifice', 'setup-sequel', 'hero-walk-away']
    },
    'romance': {
      arc: 'three-act',
      emotionalJourney: 'cinderella',
      requiredBeats: ['hook', 'inciting-incident', 'midpoint', 'revelation', 'climax', 'resolution'],
      pacing: 'balanced',
      tensionStyle: 'will-they-wont-they',
      romanticBeatInterval: 0.2,
      endingTypes: ['together', 'apart-but-growth', 'reunion', 'sacrifice']
    },
    'sci-fi': {
      arc: 'five-act',
      emotionalJourney: 'mystery',
      requiredBeats: ['hook', 'inciting-incident', 'revelation', 'midpoint', 'climax', 'resolution'],
      pacing: 'dynamic',
      tensionStyle: 'world-building-mystery',
      expositionStyle: 'show-dont-tell',
      endingTypes: ['resolution', 'open-question', 'transformation', 'cycle']
    },
    'fantasy': {
      arc: 'heros-journey',
      emotionalJourney: 'triumph',
      requiredBeats: ['hook', 'inciting-incident', 'montage', 'midpoint', 'climax', 'resolution'],
      pacing: 'epic',
      tensionStyle: 'quest-stakes',
      worldBuildingRatio: 0.25,
      endingTypes: ['victory', 'sacrifice', 'transformation', 'new-beginning']
    },
    'tutorial': {
      arc: 'problem-solution',
      emotionalJourney: 'educational',
      requiredBeats: ['hook', 'pattern-break', 'revelation', 'resolution'],
      pacing: 'instructional',
      tensionStyle: 'curiosity',
      stepInterval: 60,
      endingTypes: ['summary', 'call-to-action', 'next-steps', 'challenge']
    },
    'vlog': {
      arc: 'youtube-retention',
      emotionalJourney: 'comedy',
      requiredBeats: ['hook', 'pattern-break', 'climax', 'resolution'],
      pacing: 'conversational',
      tensionStyle: 'authenticity',
      personalMomentRatio: 0.5,
      endingTypes: ['cta', 'tease', 'callback', 'outro-bit']
    },
    'commercial': {
      arc: 'before-after-bridge',
      emotionalJourney: 'rags-to-riches',
      requiredBeats: ['hook', 'revelation', 'climax'],
      pacing: 'punchy',
      tensionStyle: 'desire-creation',
      brandMomentPlacement: 0.7,
      endingTypes: ['cta', 'brand-reveal', 'tagline', 'offer']
    },
    'music-video': {
      arc: 'kishotenketsu',
      emotionalJourney: 'triumph',
      requiredBeats: ['hook', 'midpoint', 'climax'],
      pacing: 'music-driven',
      tensionStyle: 'visual-narrative',
      performanceRatio: 0.6,
      endingTypes: ['fade', 'callback', 'visual-punctuation', 'loop-point']
    },
    'asmr': {
      arc: 'documentary-observational',
      emotionalJourney: 'meditative',
      requiredBeats: ['hook', 'resolution'],
      pacing: 'slow',
      tensionStyle: 'relaxation-deepening',
      triggerInterval: 30,
      endingTypes: ['fade', 'ambient', 'whispered-outro']
    }
  },

  // Tension Curve Patterns
  tensionCurves: {
    'steady-build': {
      name: 'Steady Build',
      description: 'Continuous tension increase',
      curve: [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 0.5],
      bestFor: ['thriller', 'horror', 'action']
    },
    'waves': {
      name: 'Tension Waves',
      description: 'Build and release cycles',
      curve: [0.3, 0.5, 0.7, 0.4, 0.6, 0.8, 0.5, 0.7, 0.9, 0.6],
      bestFor: ['drama', 'romance', 'adventure']
    },
    'flat-with-spikes': {
      name: 'Flat with Spikes',
      description: 'Calm baseline with sudden peaks',
      curve: [0.4, 0.4, 0.9, 0.4, 0.4, 0.4, 0.95, 0.4, 0.4, 1.0],
      bestFor: ['horror', 'mystery', 'thriller']
    },
    'escalating-steps': {
      name: 'Escalating Steps',
      description: 'Stepped increases in tension',
      curve: [0.3, 0.3, 0.5, 0.5, 0.5, 0.7, 0.7, 0.7, 0.9, 0.9],
      bestFor: ['action', 'heist', 'competition']
    },
    'slow-burn': {
      name: 'Slow Burn',
      description: 'Very gradual build to explosive climax',
      curve: [0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.55, 0.7, 0.9, 1.0],
      bestFor: ['psychological', 'drama', 'mystery']
    },
    'rollercoaster': {
      name: 'Rollercoaster',
      description: 'Extreme ups and downs',
      curve: [0.5, 0.9, 0.3, 0.8, 0.4, 0.95, 0.3, 0.85, 1.0, 0.5],
      bestFor: ['comedy', 'adventure', 'animation']
    },
    'inverted-u': {
      name: 'Inverted U',
      description: 'Build to middle peak then decline',
      curve: [0.3, 0.5, 0.7, 0.85, 0.95, 1.0, 0.85, 0.7, 0.5, 0.4],
      bestFor: ['tragedy', 'rise-and-fall', 'cautionary']
    },
    'double-peak': {
      name: 'Double Peak',
      description: 'Two major tension climaxes',
      curve: [0.3, 0.5, 0.8, 1.0, 0.5, 0.6, 0.8, 0.95, 1.0, 0.5],
      bestFor: ['epic', 'series-finale', 'action']
    }
  },

  // Scene-Level Structure
  sceneStructure: {
    'standard': {
      name: 'Standard Scene',
      components: ['setup', 'conflict', 'resolution'],
      timing: [0.2, 0.6, 0.2]
    },
    'action': {
      name: 'Action Scene',
      components: ['stakes-reminder', 'action', 'consequence'],
      timing: [0.1, 0.7, 0.2]
    },
    'dialogue': {
      name: 'Dialogue Scene',
      components: ['context', 'exchange', 'shift'],
      timing: [0.15, 0.7, 0.15]
    },
    'revelation': {
      name: 'Revelation Scene',
      components: ['setup', 'build', 'reveal', 'reaction'],
      timing: [0.2, 0.3, 0.2, 0.3]
    },
    'montage': {
      name: 'Montage Scene',
      components: ['establish', 'progression', 'culmination'],
      timing: [0.1, 0.7, 0.2]
    },
    'transition': {
      name: 'Transition Scene',
      components: ['exit-previous', 'bridge', 'enter-next'],
      timing: [0.3, 0.4, 0.3]
    }
  },

  // Narrative Presets combining all elements
  narrativePresets: {
    'youtube-standard': {
      name: 'YouTube Standard',
      arc: 'youtube-retention',
      emotionalJourney: 'educational',
      tensionCurve: 'waves',
      requiredBeats: ['hook', 'pattern-break', 'climax', 'resolution'],
      patternBreakInterval: 45,
      hookDuration: 5,
      endingStyle: 'cta'
    },
    'tiktok-viral': {
      name: 'TikTok Viral',
      arc: 'tiktok-viral',
      emotionalJourney: 'comedy',
      tensionCurve: 'steady-build',
      requiredBeats: ['hook', 'payoff'],
      hookDuration: 1,
      payoffPlacement: 0.8,
      endingStyle: 'loop'
    },
    'cinematic-short': {
      name: 'Cinematic Short Film',
      arc: 'three-act',
      emotionalJourney: 'redemption',
      tensionCurve: 'slow-burn',
      requiredBeats: ['hook', 'inciting-incident', 'midpoint', 'climax', 'resolution'],
      characterDevelopment: 0.3,
      endingStyle: 'emotional-resolution'
    },
    'documentary-feature': {
      name: 'Documentary Feature',
      arc: 'documentary-observational',
      emotionalJourney: 'mystery',
      tensionCurve: 'escalating-steps',
      requiredBeats: ['hook', 'revelation', 'midpoint', 'revelation', 'climax', 'resolution'],
      interviewRatio: 0.4,
      endingStyle: 'reflection'
    },
    'series-episode': {
      name: 'Series Episode',
      arc: 'dan-harmon-circle',
      emotionalJourney: 'cinderella',
      tensionCurve: 'double-peak',
      requiredBeats: ['hook', 'inciting-incident', 'midpoint', 'climax', 'cliffhanger'],
      serialElements: true,
      endingStyle: 'cliffhanger'
    },
    'commercial-spot': {
      name: 'Commercial Spot',
      arc: 'problem-solution',
      emotionalJourney: 'rags-to-riches',
      tensionCurve: 'steady-build',
      requiredBeats: ['hook', 'revelation', 'climax'],
      brandMoment: 0.8,
      endingStyle: 'cta'
    },
    'music-video-narrative': {
      name: 'Music Video (Narrative)',
      arc: 'kishotenketsu',
      emotionalJourney: 'triumph',
      tensionCurve: 'waves',
      requiredBeats: ['hook', 'midpoint', 'climax'],
      performanceRatio: 0.6,
      endingStyle: 'visual-punctuation'
    },
    'thriller-short': {
      name: 'Thriller Short',
      arc: 'five-act',
      emotionalJourney: 'thriller',
      tensionCurve: 'slow-burn',
      requiredBeats: ['hook', 'inciting-incident', 'tension-peak', 'revelation', 'climax'],
      twistPlacement: 0.75,
      endingStyle: 'twist'
    },
    'horror-short': {
      name: 'Horror Short',
      arc: 'freytags-pyramid',
      emotionalJourney: 'horror',
      tensionCurve: 'flat-with-spikes',
      requiredBeats: ['hook', 'tension-peak', 'climax', 'relief-moment'],
      scareIntervals: 45,
      endingStyle: 'ambiguous'
    },
    'inspirational': {
      name: 'Inspirational Story',
      arc: 'heros-journey',
      emotionalJourney: 'triumph',
      tensionCurve: 'escalating-steps',
      requiredBeats: ['hook', 'inciting-incident', 'montage', 'climax', 'resolution'],
      emotionalPeaks: 3,
      endingStyle: 'uplifting'
    }
  }
};

/**
 * Build narrative structure for a project
 */
function buildNarrativeStructure(options = {}) {
  const {
    genre,
    duration,
    platform,
    preset,
    sceneCount,
    mood,
    customArc
  } = options;

  // Get narrative preset or build from genre
  let narrativeConfig;
  if (preset && NARRATIVE_STRUCTURE.narrativePresets[preset]) {
    narrativeConfig = NARRATIVE_STRUCTURE.narrativePresets[preset];
  } else {
    // Build from genre pattern
    const genrePattern = NARRATIVE_STRUCTURE.genrePatterns[genre] ||
                         NARRATIVE_STRUCTURE.genrePatterns['drama'];
    narrativeConfig = {
      arc: genrePattern.arc,
      emotionalJourney: genrePattern.emotionalJourney,
      tensionCurve: 'waves',
      requiredBeats: genrePattern.requiredBeats,
      pacing: genrePattern.pacing
    };
  }

  // Get story arc
  const storyArc = customArc || NARRATIVE_STRUCTURE.storyArcs[narrativeConfig.arc] ||
                   NARRATIVE_STRUCTURE.storyArcs['three-act'];

  // Get emotional journey
  const emotionalJourney = NARRATIVE_STRUCTURE.emotionalJourneys[narrativeConfig.emotionalJourney] ||
                           NARRATIVE_STRUCTURE.emotionalJourneys['triumph'];

  // Get tension curve
  const tensionCurve = NARRATIVE_STRUCTURE.tensionCurves[narrativeConfig.tensionCurve] ||
                       NARRATIVE_STRUCTURE.tensionCurves['waves'];

  // Calculate act timings
  const acts = storyArc.acts.map((act, index) => {
    const startPercentage = storyArc.acts.slice(0, index)
      .reduce((sum, a) => sum + a.percentage, 0);
    const startTime = (startPercentage / 100) * duration;
    const actDuration = (act.percentage / 100) * duration;

    return {
      name: act.name,
      startTime,
      endTime: startTime + actDuration,
      duration: actDuration,
      purpose: act.purpose,
      energyRange: act.energyRange,
      percentage: act.percentage
    };
  });

  // Map required beats to timeline
  const beatTimings = narrativeConfig.requiredBeats.map(beatId => {
    const beat = NARRATIVE_STRUCTURE.narrativeBeats[beatId];
    if (!beat) return null;

    // Calculate position based on beat type
    let position;
    switch (beat.position) {
      case 'opening':
        position = 0;
        break;
      case 'act-1':
        position = 0.15;
        break;
      case 'center':
        position = 0.5;
        break;
      case 'act-break':
        position = 0.25;
        break;
      case 'pre-climax':
        position = 0.75;
        break;
      case 'act-3':
        position = 0.85;
        break;
      case 'ending':
        position = 0.92;
        break;
      default:
        position = 0.5;
    }

    const beatDuration = Math.min(
      beat.duration.max,
      Math.max(beat.duration.min, (beat.duration.percentage / 100) * duration)
    );

    return {
      id: beatId,
      name: beat.name,
      purpose: beat.purpose,
      startTime: position * duration,
      duration: beatDuration,
      energy: beat.energy,
      techniques: beat.techniques,
      transitionIn: beat.transitionIn,
      transitionOut: beat.transitionOut
    };
  }).filter(Boolean);

  // Generate emotional curve mapped to timeline
  const emotionalCurve = emotionalJourney.curve.map((value, index) => ({
    position: index / (emotionalJourney.curve.length - 1),
    time: (index / (emotionalJourney.curve.length - 1)) * duration,
    energy: value,
    mood: emotionalJourney.mood
  }));

  // Generate tension curve mapped to timeline
  const tensionPoints = tensionCurve.curve.map((value, index) => ({
    position: index / (tensionCurve.curve.length - 1),
    time: (index / (tensionCurve.curve.length - 1)) * duration,
    tension: value
  }));

  // Calculate scene assignments to acts
  const sceneAssignments = [];
  if (sceneCount > 0) {
    let sceneIndex = 0;
    acts.forEach(act => {
      const scenesInAct = Math.round((act.percentage / 100) * sceneCount);
      for (let i = 0; i < scenesInAct && sceneIndex < sceneCount; i++) {
        sceneAssignments.push({
          sceneIndex,
          act: act.name,
          suggestedEnergy: (act.energyRange[0] + act.energyRange[1]) / 2,
          purpose: act.purpose
        });
        sceneIndex++;
      }
    });
    // Assign remaining scenes to last act
    while (sceneIndex < sceneCount) {
      const lastAct = acts[acts.length - 1];
      sceneAssignments.push({
        sceneIndex,
        act: lastAct.name,
        suggestedEnergy: lastAct.energyRange[1],
        purpose: lastAct.purpose
      });
      sceneIndex++;
    }
  }

  return {
    structure: {
      arc: storyArc.name,
      arcId: narrativeConfig.arc,
      description: storyArc.description
    },
    acts,
    beats: beatTimings,
    emotionalJourney: {
      type: emotionalJourney.name,
      mood: emotionalJourney.mood,
      endFeeling: emotionalJourney.endFeeling,
      curve: emotionalCurve
    },
    tension: {
      type: tensionCurve.name,
      description: tensionCurve.description,
      curve: tensionPoints
    },
    sceneAssignments,
    recommendations: {
      hookDuration: narrativeConfig.hookDuration || 5,
      patternBreakInterval: narrativeConfig.patternBreakInterval || 60,
      endingStyle: narrativeConfig.endingStyle || 'resolution',
      pacing: narrativeConfig.pacing || 'balanced'
    }
  };
}

/**
 * Get narrative beat suggestions for a specific scene
 */
function getSceneBeatSuggestion(options = {}) {
  const { sceneIndex, totalScenes, genre, currentEnergy, duration } = options;

  const position = sceneIndex / (totalScenes - 1);
  const genrePattern = NARRATIVE_STRUCTURE.genrePatterns[genre] ||
                       NARRATIVE_STRUCTURE.genrePatterns['drama'];

  // Determine most likely beat based on position
  let suggestedBeat = 'standard';
  let suggestedEnergy = 0.5;
  let purpose = '';

  if (position === 0) {
    suggestedBeat = 'hook';
    suggestedEnergy = 0.9;
    purpose = 'Capture immediate attention';
  } else if (position < 0.15) {
    suggestedBeat = 'inciting-incident';
    suggestedEnergy = 0.7;
    purpose = 'Establish the central conflict';
  } else if (position >= 0.45 && position <= 0.55) {
    suggestedBeat = 'midpoint';
    suggestedEnergy = 0.85;
    purpose = 'Central turning point';
  } else if (position >= 0.8 && position < 0.9) {
    suggestedBeat = 'climax';
    suggestedEnergy = 1.0;
    purpose = 'Peak emotional moment';
  } else if (position >= 0.9) {
    suggestedBeat = 'resolution';
    suggestedEnergy = 0.5;
    purpose = 'Conclude and resolve';
  } else if (position % 0.25 < 0.05) {
    suggestedBeat = 'plot-point';
    suggestedEnergy = 0.8;
    purpose = 'Major story development';
  }

  const beat = NARRATIVE_STRUCTURE.narrativeBeats[suggestedBeat];

  return {
    beat: suggestedBeat,
    name: beat?.name || 'Scene',
    energy: suggestedEnergy,
    purpose,
    techniques: beat?.techniques || [],
    transitionIn: beat?.transitionIn || 'cut',
    transitionOut: beat?.transitionOut || 'cut',
    sceneDuration: calculateSceneDurationForBeat(suggestedBeat, duration)
  };
}

/**
 * Calculate appropriate scene duration for a narrative beat
 */
function calculateSceneDurationForBeat(beatType, totalDuration) {
  const beat = NARRATIVE_STRUCTURE.narrativeBeats[beatType];
  if (!beat) return 5;

  const targetDuration = (beat.duration.percentage / 100) * totalDuration;
  return Math.min(beat.duration.max, Math.max(beat.duration.min, targetDuration));
}

/**
 * Get emotional journey recommendation based on genre and mood
 */
function getEmotionalJourneyRecommendation(options = {}) {
  const { genre, mood, targetFeeling } = options;

  const genrePattern = NARRATIVE_STRUCTURE.genrePatterns[genre];
  let recommendedJourney = 'triumph';

  if (genrePattern) {
    recommendedJourney = genrePattern.emotionalJourney;
  } else if (mood) {
    // Match mood to journey
    const moodToJourney = {
      'epic': 'triumph',
      'dark': 'tragedy',
      'hopeful': 'redemption',
      'mysterious': 'mystery',
      'tense': 'thriller',
      'playful': 'comedy',
      'peaceful': 'meditative'
    };
    recommendedJourney = moodToJourney[mood] || 'triumph';
  }

  const journey = NARRATIVE_STRUCTURE.emotionalJourneys[recommendedJourney];

  return {
    type: recommendedJourney,
    name: journey.name,
    description: journey.description,
    mood: journey.mood,
    endFeeling: journey.endFeeling,
    curve: journey.curve,
    alternatives: Object.keys(NARRATIVE_STRUCTURE.emotionalJourneys)
      .filter(j => j !== recommendedJourney)
      .slice(0, 3)
  };
}

/**
 * Get recommended transition for scene change
 */
function getTransitionRecommendation(options = {}) {
  const { genre, fromScene, toScene, context, mood, tempo } = options;

  const genreTransitions = ASSEMBLY_INTELLIGENCE.transitions.genreTransitions[genre] ||
                           ASSEMBLY_INTELLIGENCE.transitions.genreTransitions['cinematic'];

  // Check context rules first
  let preferredTransitions = [];
  let avoidTransitions = [];

  if (context) {
    const contextRule = ASSEMBLY_INTELLIGENCE.transitions.contextRules[context];
    if (contextRule) {
      preferredTransitions = contextRule.prefer || [];
      avoidTransitions = contextRule.avoid || [];
    }
  }

  // Combine with genre preferences
  const genrePrimary = genreTransitions.primary || [];
  const genreSecondary = genreTransitions.secondary || [];
  const genreAvoid = genreTransitions.avoid || [];

  // Filter and prioritize
  const allPreferred = [...new Set([...preferredTransitions, ...genrePrimary])];
  const allAvoid = [...new Set([...avoidTransitions, ...genreAvoid])];

  // Select transition
  let selectedTransition = 'cut';
  for (const trans of allPreferred) {
    if (!allAvoid.includes(trans)) {
      selectedTransition = trans;
      break;
    }
  }

  // Get transition details
  const transitionType = ASSEMBLY_INTELLIGENCE.transitions.types[selectedTransition];
  let duration = genreTransitions.defaultDuration || 0.5;

  if (transitionType.duration && typeof transitionType.duration === 'object') {
    duration = (transitionType.duration.min + transitionType.duration.max) / 2;
  }

  return {
    type: selectedTransition,
    name: transitionType.name,
    duration,
    audioSync: transitionType.audioSync,
    alternatives: genreSecondary.filter(t => !allAvoid.includes(t))
  };
}

/**
 * Calculate scene pacing based on content and energy
 */
function calculateScenePacing(options = {}) {
  const { genre, platform, sceneCount, totalDuration, energyLevel, contentType } = options;

  // Get pacing profile
  let pacingProfile = ASSEMBLY_INTELLIGENCE.pacing.profiles['balanced'];

  // Determine profile based on platform/genre
  if (platform?.includes('tiktok') || platform?.includes('shorts')) {
    pacingProfile = ASSEMBLY_INTELLIGENCE.pacing.profiles['rapid-fire'];
  } else if (genre?.includes('action') || genre?.includes('gaming')) {
    pacingProfile = ASSEMBLY_INTELLIGENCE.pacing.profiles['fast'];
  } else if (genre?.includes('documentary') || genre?.includes('nature')) {
    pacingProfile = ASSEMBLY_INTELLIGENCE.pacing.profiles['contemplative'];
  } else if (genre?.includes('cinematic')) {
    pacingProfile = ASSEMBLY_INTELLIGENCE.pacing.profiles['cinematic'];
  }

  // Get energy curve
  const energyCurve = ASSEMBLY_INTELLIGENCE.pacing.energyCurves[pacingProfile.energyCurve];

  // Calculate scene durations
  const avgDuration = totalDuration / sceneCount;
  const sceneDurations = [];

  for (let i = 0; i < sceneCount; i++) {
    const curvePosition = i / (sceneCount - 1 || 1);
    const curveIndex = Math.floor(curvePosition * (energyCurve.pattern.length - 1));
    const energyFactor = energyCurve.pattern[curveIndex];

    // Higher energy = shorter scenes
    const durationMultiplier = 1.5 - (energyFactor * 0.8);
    let sceneDuration = avgDuration * durationMultiplier;

    // Apply content type modifier
    if (contentType) {
      const modifier = ASSEMBLY_INTELLIGENCE.pacing.durationModifiers[contentType] || 1;
      sceneDuration *= modifier;
    }

    // Clamp to profile limits
    sceneDuration = Math.max(pacingProfile.avgSceneDuration.min,
                            Math.min(pacingProfile.avgSceneDuration.max, sceneDuration));

    sceneDurations.push({
      index: i,
      duration: sceneDuration,
      energy: energyFactor
    });
  }

  return {
    profile: pacingProfile.name,
    energyCurve: pacingProfile.energyCurve,
    sceneDurations,
    breathingRoom: pacingProfile.breathingRoom
  };
}

/**
 * Generate pattern interrupt suggestions
 */
function getPatternInterruptSuggestions(options = {}) {
  const { platform, duration, sceneCount, genre } = options;

  const strategy = ASSEMBLY_INTELLIGENCE.patternInterrupts.platformStrategies[platform] ||
                   ASSEMBLY_INTELLIGENCE.patternInterrupts.platformStrategies['youtube-standard'];

  const interrupts = [];
  const interruptInterval = strategy.maxAttentionSpan;

  // Calculate interrupt positions
  for (let time = interruptInterval; time < duration; time += interruptInterval) {
    // Select interrupt type
    const typeIndex = interrupts.length % strategy.preferredTypes.length;
    const interruptTypeKey = strategy.preferredTypes[typeIndex];
    const interruptType = ASSEMBLY_INTELLIGENCE.patternInterrupts.types[interruptTypeKey];

    interrupts.push({
      time,
      type: interruptTypeKey,
      name: interruptType.name,
      description: interruptType.description,
      elements: interruptType.elements,
      impact: interruptType.impact
    });
  }

  return {
    strategy: platform,
    hookWindow: strategy.hookWindow,
    interrupts,
    frequency: strategy.interruptFrequency
  };
}

/**
 * Get B-roll placement suggestions
 */
function getBRollPlacements(options = {}) {
  const { genre, scenes, keywords } = options;

  const genreConfig = ASSEMBLY_INTELLIGENCE.bRoll.genreDensity[genre] ||
                      { density: 0.5, strategy: 'illustrative' };

  const strategy = ASSEMBLY_INTELLIGENCE.bRoll.strategies[genreConfig.strategy];
  const placements = [];

  // Determine B-roll placement for each scene
  scenes.forEach((scene, index) => {
    const shouldHaveBRoll = Math.random() < genreConfig.density;

    if (shouldHaveBRoll) {
      // Check for keyword triggers
      const sceneText = (scene.narration || scene.visual || '').toLowerCase();
      let triggerType = null;

      for (const [category, words] of Object.entries(ASSEMBLY_INTELLIGENCE.bRoll.triggerKeywords)) {
        for (const word of words) {
          if (sceneText.includes(word)) {
            triggerType = category;
            break;
          }
        }
        if (triggerType) break;
      }

      placements.push({
        sceneIndex: index,
        strategy: genreConfig.strategy,
        duration: (strategy.duration.min + strategy.duration.max) / 2,
        timing: strategy.timing,
        trigger: triggerType,
        overlap: strategy.overlap
      });
    }
  });

  return {
    density: genreConfig.density,
    strategy: genreConfig.strategy,
    placements
  };
}

/**
 * Build complete assembly plan for a video
 */
function buildAssemblyPlan(options = {}) {
  const {
    genre,
    platform,
    scenes,
    totalDuration,
    musicTempo,
    mood
  } = options;

  // Get preset or build custom
  let preset = null;
  if (genre?.includes('documentary')) {
    preset = ASSEMBLY_INTELLIGENCE.presets['documentary-standard'];
  } else if (genre?.includes('explainer')) {
    preset = ASSEMBLY_INTELLIGENCE.presets['explainer-fast'];
  } else if (platform?.includes('tiktok') || platform?.includes('shorts')) {
    preset = ASSEMBLY_INTELLIGENCE.presets['social-viral'];
  } else if (genre?.includes('cinematic')) {
    preset = ASSEMBLY_INTELLIGENCE.presets['cinematic-epic'];
  } else if (genre?.includes('brand')) {
    preset = ASSEMBLY_INTELLIGENCE.presets['brand-emotional'];
  } else {
    preset = ASSEMBLY_INTELLIGENCE.presets['documentary-standard'];
  }

  const plan = {
    preset: preset.name,
    transitions: [],
    pacing: null,
    patternInterrupts: [],
    bRollPlacements: []
  };

  // Generate transition plan
  for (let i = 0; i < scenes.length - 1; i++) {
    const transition = getTransitionRecommendation({
      genre,
      fromScene: scenes[i],
      toScene: scenes[i + 1],
      mood
    });
    plan.transitions.push({
      afterScene: i,
      ...transition
    });
  }

  // Calculate pacing
  plan.pacing = calculateScenePacing({
    genre,
    platform,
    sceneCount: scenes.length,
    totalDuration
  });

  // Generate pattern interrupts
  plan.patternInterrupts = getPatternInterruptSuggestions({
    platform,
    duration: totalDuration,
    sceneCount: scenes.length,
    genre
  }).interrupts;

  // Generate B-roll placements
  plan.bRollPlacements = getBRollPlacements({
    genre,
    scenes
  }).placements;

  return plan;
}

/**
 * Cloud function to get assembly intelligence profiles
 */
exports.creationWizardGetAssemblyProfiles = functions.https.onCall(async (data, context) => {
  await verifyAuth(context);

  // Format transitions
  const transitions = Object.entries(ASSEMBLY_INTELLIGENCE.transitions.types).map(([id, trans]) => ({
    id,
    name: trans.name,
    duration: trans.duration,
    energy: trans.energy,
    use: trans.use,
    description: trans.description
  }));

  // Format pacing profiles
  const pacingProfiles = Object.entries(ASSEMBLY_INTELLIGENCE.pacing.profiles).map(([id, profile]) => ({
    id,
    name: profile.name,
    avgSceneDuration: profile.avgSceneDuration,
    cutFrequency: profile.cutFrequency,
    energyCurve: profile.energyCurve,
    use: profile.use
  }));

  // Format beat sync modes
  const beatSyncModes = Object.entries(ASSEMBLY_INTELLIGENCE.beatSync.modes).map(([id, mode]) => ({
    id,
    name: mode.name,
    description: mode.description,
    percentage: mode.percentage
  }));

  // Format pattern interrupt types
  const patternInterrupts = Object.entries(ASSEMBLY_INTELLIGENCE.patternInterrupts.types).map(([id, type]) => ({
    id,
    name: type.name,
    description: type.description,
    elements: type.elements,
    impact: type.impact
  }));

  // Format presets
  const presets = Object.entries(ASSEMBLY_INTELLIGENCE.presets).map(([id, preset]) => ({
    id,
    name: preset.name,
    transitions: preset.transitions,
    pacing: preset.pacing,
    beatSync: preset.beatSync
  }));

  return {
    success: true,
    transitions,
    pacingProfiles,
    beatSyncModes,
    patternInterrupts,
    presets,
    genreTransitions: ASSEMBLY_INTELLIGENCE.transitions.genreTransitions,
    bRollStrategies: ASSEMBLY_INTELLIGENCE.bRoll.strategies
  };
});

/**
 * Cloud function to build assembly plan
 */
exports.creationWizardBuildAssemblyPlan = functions.https.onCall(async (data, context) => {
  await verifyAuth(context);

  const { genre, platform, scenes, totalDuration, musicTempo, mood } = data;

  const plan = buildAssemblyPlan({
    genre,
    platform,
    scenes,
    totalDuration,
    musicTempo,
    mood
  });

  return {
    success: true,
    plan
  };
});

/**
 * Cloud function to get transition recommendation
 */
exports.creationWizardGetTransitionRecommendation = functions.https.onCall(async (data, context) => {
  await verifyAuth(context);

  const { genre, fromScene, toScene, context: transContext, mood, tempo } = data;

  const recommendation = getTransitionRecommendation({
    genre,
    fromScene,
    toScene,
    context: transContext,
    mood,
    tempo
  });

  return {
    success: true,
    recommendation
  };
});

// =============================================================================
// PHASE 3F: NARRATIVE STRUCTURE CLOUD FUNCTIONS
// =============================================================================

/**
 * creationWizardGetNarrativeProfiles - Get all available narrative structures
 */
exports.creationWizardGetNarrativeProfiles = functions.https.onCall(async (data, context) => {
  await verifyAuth(context);

  // Get story arcs
  const storyArcs = Object.entries(NARRATIVE_STRUCTURE.storyArcs).map(([id, arc]) => ({
    id,
    name: arc.name,
    description: arc.description,
    actCount: arc.acts.length,
    bestFor: arc.bestFor
  }));

  // Get emotional journeys
  const emotionalJourneys = Object.entries(NARRATIVE_STRUCTURE.emotionalJourneys).map(([id, journey]) => ({
    id,
    name: journey.name,
    description: journey.description,
    mood: journey.mood,
    endFeeling: journey.endFeeling
  }));

  // Get tension curves
  const tensionCurves = Object.entries(NARRATIVE_STRUCTURE.tensionCurves).map(([id, curve]) => ({
    id,
    name: curve.name,
    description: curve.description,
    bestFor: curve.bestFor
  }));

  // Get narrative beats
  const narrativeBeats = Object.entries(NARRATIVE_STRUCTURE.narrativeBeats).map(([id, beat]) => ({
    id,
    name: beat.name,
    purpose: beat.purpose,
    position: beat.position,
    energy: beat.energy
  }));

  // Get presets
  const presets = Object.entries(NARRATIVE_STRUCTURE.narrativePresets).map(([id, preset]) => ({
    id,
    name: preset.name,
    arc: preset.arc,
    emotionalJourney: preset.emotionalJourney,
    endingStyle: preset.endingStyle
  }));

  // Get genre patterns
  const genrePatterns = Object.entries(NARRATIVE_STRUCTURE.genrePatterns).map(([id, pattern]) => ({
    id,
    arc: pattern.arc,
    emotionalJourney: pattern.emotionalJourney,
    pacing: pattern.pacing,
    requiredBeats: pattern.requiredBeats
  }));

  return {
    success: true,
    profiles: {
      storyArcs,
      emotionalJourneys,
      tensionCurves,
      narrativeBeats,
      presets,
      genrePatterns
    }
  };
});

/**
 * creationWizardBuildNarrativeStructure - Build complete narrative structure for a project
 */
exports.creationWizardBuildNarrativeStructure = functions.https.onCall(async (data, context) => {
  await verifyAuth(context);

  const { genre, duration, platform, preset, sceneCount, mood, customArc } = data;

  const structure = buildNarrativeStructure({
    genre,
    duration: duration || 120,
    platform,
    preset,
    sceneCount: sceneCount || 10,
    mood,
    customArc
  });

  return {
    success: true,
    structure
  };
});

/**
 * creationWizardGetSceneBeatSuggestion - Get narrative beat suggestion for a specific scene
 */
exports.creationWizardGetSceneBeatSuggestion = functions.https.onCall(async (data, context) => {
  await verifyAuth(context);

  const { sceneIndex, totalScenes, genre, currentEnergy, duration } = data;

  const suggestion = getSceneBeatSuggestion({
    sceneIndex: sceneIndex || 0,
    totalScenes: totalScenes || 10,
    genre: genre || 'drama',
    currentEnergy,
    duration: duration || 120
  });

  return {
    success: true,
    suggestion
  };
});

/**
 * creationWizardGetEmotionalJourney - Get emotional journey recommendation
 */
exports.creationWizardGetEmotionalJourney = functions.https.onCall(async (data, context) => {
  await verifyAuth(context);

  const { genre, mood, targetFeeling } = data;

  const recommendation = getEmotionalJourneyRecommendation({
    genre,
    mood,
    targetFeeling
  });

  return {
    success: true,
    recommendation
  };
});

/**
 * creationWizardAnalyzeNarrativePacing - Analyze and optimize narrative pacing for scenes
 */
exports.creationWizardAnalyzeNarrativePacing = functions.https.onCall(async (data, context) => {
  await verifyAuth(context);

  const { scenes, genre, preset, targetDuration } = data;

  if (!scenes || !Array.isArray(scenes)) {
    return { success: false, error: 'Scenes array is required' };
  }

  // Build narrative structure
  const structure = buildNarrativeStructure({
    genre: genre || 'drama',
    duration: targetDuration || scenes.reduce((sum, s) => sum + (s.duration || 5), 0),
    preset,
    sceneCount: scenes.length
  });

  // Analyze each scene against the structure
  const analysis = scenes.map((scene, index) => {
    const assignment = structure.sceneAssignments[index];
    const beatSuggestion = getSceneBeatSuggestion({
      sceneIndex: index,
      totalScenes: scenes.length,
      genre: genre || 'drama',
      duration: targetDuration
    });

    // Calculate how well scene aligns with structure
    const currentDuration = scene.duration || 5;
    const suggestedDuration = beatSuggestion.sceneDuration;
    const durationAlignment = 1 - Math.abs(currentDuration - suggestedDuration) / suggestedDuration;

    return {
      sceneIndex: index,
      sceneId: scene.id,
      currentDuration,
      act: assignment?.act,
      suggestedBeat: beatSuggestion.beat,
      beatName: beatSuggestion.name,
      suggestedEnergy: beatSuggestion.energy,
      suggestedDuration,
      purpose: beatSuggestion.purpose,
      techniques: beatSuggestion.techniques,
      transitionIn: beatSuggestion.transitionIn,
      transitionOut: beatSuggestion.transitionOut,
      alignment: {
        duration: Math.round(durationAlignment * 100),
        overall: Math.round(durationAlignment * 100)
      },
      suggestions: durationAlignment < 0.7 ? [
        `Consider adjusting duration to ~${Math.round(suggestedDuration)}s for better pacing`
      ] : []
    };
  });

  return {
    success: true,
    structure: {
      arc: structure.structure.arc,
      emotionalJourney: structure.emotionalJourney.type,
      acts: structure.acts.map(a => ({ name: a.name, percentage: a.percentage }))
    },
    analysis,
    overallRecommendations: {
      hookDuration: structure.recommendations.hookDuration,
      patternBreakInterval: structure.recommendations.patternBreakInterval,
      endingStyle: structure.recommendations.endingStyle,
      pacing: structure.recommendations.pacing
    }
  };
});

/**
 * creationWizardCheckImageStatus - Check the status of a RunPod image generation job
 */
exports.creationWizardCheckImageStatus = functions.https.onCall(async (data, context) => {
  await verifyAuth(context);
  const { jobId } = data;

  if (!jobId) {
    throw new functions.https.HttpsError('invalid-argument', 'Job ID is required');
  }

  const runpodKey = functions.config().runpod?.key;
  if (!runpodKey) {
    throw new functions.https.HttpsError('failed-precondition', 'RunPod API key not configured');
  }

  try {
    const statusResponse = await axios.get(
      `https://api.runpod.ai/v2/rgq0go2nkcfx4h/status/${jobId}`,
      {
        headers: { 'Authorization': `Bearer ${runpodKey}` },
        timeout: 10000
      }
    );

    return {
      success: true,
      jobId,
      status: statusResponse.data.status,
      output: statusResponse.data.output || null,
      error: statusResponse.data.error || null
    };

  } catch (error) {
    console.error('[creationWizardCheckImageStatus] Error:', error);
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to check image status'));
  }
});

/**
 * creationWizardGenerateInitialStoryboard - Generate first N scene images in batch
 *
 * Called when user enters Step 4, generates first 4 scenes automatically
 */
exports.creationWizardGenerateInitialStoryboard = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const { projectId, scenes, style, aspectRatio, maxScenes = 4 } = data;

  if (!scenes || !Array.isArray(scenes) || scenes.length === 0) {
    throw new functions.https.HttpsError('invalid-argument', 'Scenes array is required');
  }

  const runpodKey = functions.config().runpod?.key;
  if (!runpodKey) {
    throw new functions.https.HttpsError('failed-precondition', 'RunPod API key not configured');
  }

  try {
    // Only generate for first N scenes
    const scenesToGenerate = scenes.slice(0, maxScenes);
    const jobs = [];

    for (const scene of scenesToGenerate) {
      // Use the visual description from script as base prompt
      const prompt = scene.visual || scene.narration || 'A cinematic scene';

      // Enhance prompt
      const styleKeywords = {
        modern: 'modern, sleek, clean design',
        cinematic: 'cinematic, dramatic lighting, film quality',
        energetic: 'vibrant, dynamic, bold colors',
        documentary: 'realistic, natural lighting',
        retro: 'retro, vintage, nostalgic',
        futuristic: 'futuristic, sci-fi, high-tech',
        cartoon: 'cartoon style, animated, colorful',
        elegant: 'elegant, sophisticated, refined',
        nature: 'natural, organic, outdoor',
        dark: 'dark, moody, dramatic shadows'
      };
      const styleEnhancement = styleKeywords[style] || styleKeywords.cinematic;
      const enhancedPrompt = `${prompt}. Style: ${styleEnhancement}. High quality, 4K.`;

      // Dimensions
      const dimensions = {
        '16:9': { width: 1280, height: 720 },
        '9:16': { width: 720, height: 1280 },
        '1:1': { width: 1024, height: 1024 },
        '4:5': { width: 864, height: 1080 }
      };
      const { width, height } = dimensions[aspectRatio] || dimensions['16:9'];

      // Generate filename and signed URL
      const seed = Math.floor(Math.random() * 999999999999);
      const fileName = `creation-projects/${projectId || uid}/storyboard/scene_${scene.id}_${Date.now()}_${seed}.png`;

      const bucket = admin.storage().bucket();
      const file = bucket.file(fileName);
      const [uploadUrl] = await file.getSignedUrl({
        version: 'v4',
        action: 'write',
        expires: Date.now() + 30 * 60 * 1000,
        contentType: 'application/octet-stream',
      });

      // Call RunPod
      const runpodEndpoint = 'https://api.runpod.ai/v2/rgq0go2nkcfx4h/run';
      const runpodResponse = await axios.post(runpodEndpoint, {
        input: {
          positive_prompt: enhancedPrompt,
          negative_prompt: "blurry, low quality, ugly, distorted, watermark, nsfw, text, words, logo",
          width,
          height,
          batch_size: 1,
          shift: 3.0,
          seed,
          steps: 30,
          cfg: 5,
          sampler_name: "euler",
          scheduler: "simple",
          denoise: 1,
          image_upload_url: uploadUrl
        }
      }, {
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${runpodKey}`
        },
        timeout: 30000
      });

      const encodedFileName = encodeURIComponent(fileName);
      const publicUrl = `https://firebasestorage.googleapis.com/v0/b/${bucket.name}/o/${encodedFileName}?alt=media`;

      jobs.push({
        sceneId: scene.id,
        jobId: runpodResponse.data.id,
        status: runpodResponse.data.status,
        imageUrl: publicUrl,
        fileName,
        prompt: enhancedPrompt
      });
    }

    // Log batch usage
    await db.collection('apiUsage').add({
      userId: uid,
      type: 'storyboard_batch',
      model: 'runpod-hidream',
      projectId: projectId || null,
      sceneCount: jobs.length,
      timestamp: admin.firestore.FieldValue.serverTimestamp()
    });

    return {
      success: true,
      jobs,
      totalGenerated: jobs.length,
      totalScenes: scenes.length
    };

  } catch (error) {
    console.error('[creationWizardGenerateInitialStoryboard] Error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to generate storyboard'));
  }
});

/**
 * creationWizardUpdateStoryboard - Save storyboard data to project
 */
exports.creationWizardUpdateStoryboard = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const { projectId, storyboard } = data;

  if (!projectId) {
    throw new functions.https.HttpsError('invalid-argument', 'Project ID required');
  }

  try {
    const projectRef = db.collection('creationProjects').doc(projectId);
    const projectDoc = await projectRef.get();

    if (!projectDoc.exists) {
      throw new functions.https.HttpsError('not-found', 'Project not found');
    }

    if (projectDoc.data().userId !== uid) {
      throw new functions.https.HttpsError('permission-denied', 'Not authorized');
    }

    await projectRef.update({
      storyboard: {
        scenes: storyboard.scenes || [],
        status: storyboard.status || 'in_progress',
        updatedAt: new Date().toISOString()
      },
      updatedAt: admin.firestore.FieldValue.serverTimestamp()
    });

    return { success: true };

  } catch (error) {
    console.error('[creationWizardUpdateStoryboard] Error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to update storyboard'));
  }
});

// ============================================================================
// VIDEO CREATION WIZARD - PHASE 4: ANIMATION
// ============================================================================

/**
 * creationWizardGenerateVoiceover - Generate TTS audio using OpenAI
 *
 * Uses OpenAI's TTS API to generate high-quality voiceover from narration text
 */
exports.creationWizardGenerateVoiceover = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const { projectId, sceneId, text, voice = 'alloy', speed = 1.0 } = data;

  if (!text || text.trim().length < 3) {
    throw new functions.https.HttpsError('invalid-argument', 'Narration text is required');
  }

  // Available voices: alloy, echo, fable, onyx, nova, shimmer
  const validVoices = ['alloy', 'echo', 'fable', 'onyx', 'nova', 'shimmer'];
  const selectedVoice = validVoices.includes(voice) ? voice : 'alloy';

  try {
    // Generate audio using OpenAI TTS
    const mp3Response = await openai.audio.speech.create({
      model: 'tts-1',
      voice: selectedVoice,
      input: text,
      speed: Math.max(0.25, Math.min(4.0, speed))
    });

    // Convert response to buffer
    const audioBuffer = Buffer.from(await mp3Response.arrayBuffer());

    // Upload to Firebase Storage
    const fileName = `creation-projects/${projectId || uid}/voiceover/scene_${sceneId}_${Date.now()}.mp3`;
    const bucket = admin.storage().bucket();
    const file = bucket.file(fileName);

    await file.save(audioBuffer, {
      metadata: {
        contentType: 'audio/mpeg',
        metadata: {
          sceneId: String(sceneId),
          voice: selectedVoice,
          textLength: String(text.length)
        }
      }
    });

    // Make file public and get URL
    await file.makePublic();
    const publicUrl = `https://storage.googleapis.com/${bucket.name}/${fileName}`;

    // Estimate duration based on text length and speed
    // Average speaking rate is ~150 words per minute
    const wordCount = text.split(/\s+/).length;
    const estimatedDuration = Math.ceil((wordCount / 150) * 60 / speed);

    // Log usage
    await db.collection('apiUsage').add({
      userId: uid,
      type: 'voiceover_generation',
      model: 'openai-tts-1',
      projectId: projectId || null,
      sceneId,
      textLength: text.length,
      timestamp: admin.firestore.FieldValue.serverTimestamp()
    });

    return {
      success: true,
      sceneId,
      audioUrl: publicUrl,
      fileName,
      voice: selectedVoice,
      estimatedDuration,
      textLength: text.length
    };

  } catch (error) {
    console.error('[creationWizardGenerateVoiceover] Error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to generate voiceover'));
  }
});

/**
 * creationWizardAnimateScene - Animate image with voiceover using RunPod Multi-talk
 *
 * Creates a video clip from a still image and audio using the Multi-talk API
 */
exports.creationWizardAnimateScene = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const { projectId, sceneId, imageUrl, audioUrl, animationType = 'ken_burns', settings = {} } = data;

  if (!imageUrl) {
    throw new functions.https.HttpsError('invalid-argument', 'Image URL is required');
  }

  const runpodKey = functions.config().runpod?.key;
  if (!runpodKey) {
    throw new functions.https.HttpsError('failed-precondition', 'RunPod API key not configured');
  }

  try {
    // Multi-talk endpoint
    const runpodEndpoint = 'https://api.runpod.ai/v2/mekewddvpqb0b4/run';

    // Build animation input based on type
    const animationInput = {
      image_url: imageUrl,
      audio_url: audioUrl || null,
      animation_type: animationType, // 'talking_head', 'ken_burns', 'static'
    };

    // Add type-specific settings
    if (animationType === 'talking_head') {
      animationInput.lip_sync = settings.lipSync !== false;
      animationInput.head_motion = settings.headMotion !== false;
      animationInput.eye_blink = settings.eyeBlink !== false;
    } else if (animationType === 'ken_burns') {
      animationInput.zoom_direction = settings.zoomDirection || 'in'; // 'in', 'out', 'none'
      animationInput.pan_direction = settings.panDirection || 'left'; // 'left', 'right', 'up', 'down', 'none'
      animationInput.motion_intensity = settings.motionIntensity || 0.3; // 0.0 - 1.0
    }

    // Duration settings
    animationInput.duration = settings.duration || 15; // Max 15 seconds for Multi-talk

    // Generate output filename
    const outputFileName = `creation-projects/${projectId || uid}/animation/scene_${sceneId}_${Date.now()}.mp4`;
    const bucket = admin.storage().bucket();
    const outputFile = bucket.file(outputFileName);

    // Create signed URL for upload
    const [uploadUrl] = await outputFile.getSignedUrl({
      version: 'v4',
      action: 'write',
      expires: Date.now() + 60 * 60 * 1000, // 1 hour
      contentType: 'video/mp4',
    });

    // Add upload URL to input
    animationInput.output_url = uploadUrl;

    // Call RunPod API
    const runpodResponse = await axios.post(runpodEndpoint, {
      input: animationInput
    }, {
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${runpodKey}`
      },
      timeout: 30000
    });

    const jobId = runpodResponse.data.id;
    const status = runpodResponse.data.status;

    // Generate public URL for the output video
    const encodedFileName = encodeURIComponent(outputFileName);
    const publicUrl = `https://firebasestorage.googleapis.com/v0/b/${bucket.name}/o/${encodedFileName}?alt=media`;

    // Log usage
    await db.collection('apiUsage').add({
      userId: uid,
      type: 'scene_animation',
      model: 'runpod-multitalk',
      projectId: projectId || null,
      sceneId,
      animationType,
      timestamp: admin.firestore.FieldValue.serverTimestamp()
    });

    return {
      success: true,
      sceneId,
      jobId,
      status,
      videoUrl: publicUrl,
      fileName: outputFileName,
      animationType,
      checkEndpoint: `https://api.runpod.ai/v2/mekewddvpqb0b4/status/${jobId}`
    };

  } catch (error) {
    console.error('[creationWizardAnimateScene] Error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to start animation'));
  }
});

/**
 * creationWizardCheckAnimationStatus - Check animation job status
 */
exports.creationWizardCheckAnimationStatus = functions.https.onCall(async (data, context) => {
  await verifyAuth(context);
  const { jobId } = data;

  if (!jobId) {
    throw new functions.https.HttpsError('invalid-argument', 'Job ID is required');
  }

  const runpodKey = functions.config().runpod?.key;
  if (!runpodKey) {
    throw new functions.https.HttpsError('failed-precondition', 'RunPod API key not configured');
  }

  try {
    const statusResponse = await axios.get(
      `https://api.runpod.ai/v2/mekewddvpqb0b4/status/${jobId}`,
      {
        headers: { 'Authorization': `Bearer ${runpodKey}` },
        timeout: 10000
      }
    );

    return {
      success: true,
      jobId,
      status: statusResponse.data.status,
      output: statusResponse.data.output || null,
      error: statusResponse.data.error || null
    };

  } catch (error) {
    console.error('[creationWizardCheckAnimationStatus] Error:', error);
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to check animation status'));
  }
});

/**
 * creationWizardUpdateAnimation - Save animation data to project
 */
exports.creationWizardUpdateAnimation = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const { projectId, animation } = data;

  if (!projectId) {
    throw new functions.https.HttpsError('invalid-argument', 'Project ID required');
  }

  try {
    const projectRef = db.collection('creationProjects').doc(projectId);
    const projectDoc = await projectRef.get();

    if (!projectDoc.exists) {
      throw new functions.https.HttpsError('not-found', 'Project not found');
    }

    if (projectDoc.data().userId !== uid) {
      throw new functions.https.HttpsError('permission-denied', 'Not authorized');
    }

    await projectRef.update({
      animation: {
        scenes: animation.scenes || [],
        status: animation.status || 'in_progress',
        voiceSettings: animation.voiceSettings || { voice: 'alloy', speed: 1.0 },
        updatedAt: new Date().toISOString()
      },
      updatedAt: admin.firestore.FieldValue.serverTimestamp()
    });

    return { success: true };

  } catch (error) {
    console.error('[creationWizardUpdateAnimation] Error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to update animation'));
  }
});

// ==============================================
// CREATION WIZARD - MINIMAX VIDEO GENERATION
// ==============================================

/**
 * Minimax Video Model Configuration
 */
const MINIMAX_VIDEO_MODELS = {
  // IMPORTANT: Model IDs must match official Minimax API names
  // See: https://platform.minimax.io/docs/api-reference/video-generation-t2v
  'hailuo-2.3': {
    name: 'Hailuo 2.3 Quality',
    modelId: 'MiniMax-Hailuo-2.3',  // Supports 10s at 768p
    inputTypes: ['text', 'image'],
    durations: {
      '6s': ['768p', '1080p'],
      '10s': ['768p']  // 10s only at 768p
    },
    pricing: { '6s-768p': 0.28, '6s-1080p': 0.49, '10s-768p': 0.40 },
    features: ['camera_control', 'prompt_optimizer'],
    description: 'Standard quality video generation (6s and 10s)'
  },
  'hailuo-2.3-fast': {
    name: 'Hailuo 2.3 Fast',
    modelId: 'T2V-01-Fast',  // Fast mode - 6s only
    inputTypes: ['image'],
    durations: {
      '6s': ['768p', '1080p']
    },
    pricing: { '6s-768p': 0.14, '6s-1080p': 0.25 },
    features: ['camera_control', 'fast_generation'],
    description: 'Faster generation, image-to-video only (6s only)'
  },
  'hailuo-02': {
    name: 'Hailuo 02',
    modelId: 'MiniMax-Hailuo-02',  // Supports 10s at 768p
    inputTypes: ['text', 'image'],
    durations: {
      '6s': ['768p', '1080p'],
      '10s': ['768p']  // 10s only at 768p
    },
    pricing: { '6s-768p': 0.28, '6s-1080p': 0.49, '10s-768p': 0.40 },
    features: ['camera_control', 'last_frame_conditioning'],
    description: 'Advanced model with last frame conditioning'
  },
  // S2V-01: Subject-to-Video for CHARACTER CONSISTENCY
  'hailuo-s2v': {
    name: 'Hailuo S2V-01 (Character)',
    modelId: 'S2V-01',
    inputTypes: ['text', 'image', 'subject_reference'],
    durations: {
      '6s': ['768p', '1080p']
    },
    pricing: { '6s-768p': 0.50, '6s-1080p': 0.70 },
    features: ['camera_control', 'subject_reference', 'character_consistency'],
    description: 'Character-consistent video - maintains face/identity from reference image',
    requiresSubjectReference: true
  }
};

/**
 * Camera movement commands supported by Minimax
 */
const MINIMAX_CAMERA_MOVEMENTS = [
  'Truck left', 'Truck right',
  'Pan left', 'Pan right',
  'Push in', 'Pull out',
  'Pedestal up', 'Pedestal down',
  'Tilt up', 'Tilt down',
  'Zoom in', 'Zoom out',
  'Shake', 'Tracking shot', 'Static shot'
];

/**
 * NANOBANANA_PROMPT_BUILDER
 *
 * Builds optimized prompts for Google's NanoBanana (Gemini Image) models.
 * Based on research from:
 * - Google's official prompting guide
 * - Max Woolf's engineering analysis
 * - Community best practices
 *
 * Key principles:
 * 1. NARRATIVE over keywords - describe scenes like a cinematographer
 * 2. Use CAPS for critical requirements (proven to improve adherence)
 * 3. Include technical camera specs for photorealism
 * 4. Add "physicality constraints" to avoid AI illustration look
 * 5. Use composition buzzwords that trigger professional framing
 */
const NANOBANANA_PROMPT_BUILDER = {
  // Camera/lens specifications that improve photorealism
  cameraSpecs: {
    portrait: 'Shot with Canon EOS R5, 85mm f/1.4 lens, shallow depth of field',
    wide: 'Shot with Sony A7IV, 24mm wide-angle lens, deep focus',
    closeup: 'Shot with Canon 100mm macro lens, f/2.8, precise focus on subject',
    medium: 'Shot with 50mm prime lens, f/2.0, natural perspective',
    cinematic: 'Shot with ARRI Alexa, anamorphic lens, 2.39:1 cinematic aspect'
  },

  // Lighting setups that work well
  lightingSetups: {
    dramatic: 'dramatic chiaroscuro lighting with deep shadows and bright highlights',
    soft: 'soft diffused natural lighting, gentle shadows, even illumination',
    golden: 'warm golden hour sunlight casting long shadows, backlit with lens flare',
    studio: 'professional three-point studio lighting, clean separation from background',
    moody: 'low-key atmospheric lighting with volumetric haze, single source',
    neon: 'neon-lit urban night scene, cyan and magenta reflections on wet surfaces'
  },

  // Composition buzzwords proven to improve framing
  compositionBuzzwords: [
    'Pulitzer-prize-winning photograph',
    'Vanity Fair cover portrait',
    'National Geographic documentary still',
    'professional cinematography',
    'rule of thirds composition',
    'masterful negative space'
  ],

  // Physicality constraints to avoid AI illustration look
  realismConstraints: [
    'natural skin texture with visible pores and fine lines',
    'film grain texture',
    'real-world physics',
    'authentic fabric texture and wrinkles',
    'natural hair strands with flyaways',
    'subtle subsurface skin scattering'
  ],

  // Shot type framing requirements (using CAPS for emphasis)
  // CRITICAL: These rules ensure character is properly visible and not cut off
  framingRules: {
    wide: 'CRITICAL FRAMING: FULL BODY MUST be visible from head to feet. Character FACING CAMERA. Characters occupy 30-40% of frame height. Generous margins on ALL sides. Environment context clearly visible. DO NOT cut off any body parts.',
    medium: 'CRITICAL FRAMING: Waist-up framing with head FULLY VISIBLE and FACING CAMERA. 15% headroom above head. Shoulders and hands in frame. Character centered with proper negative space. Face MUST be clearly visible.',
    closeup: 'CRITICAL FRAMING: Face and upper chest fills frame. Character FACING CAMERA. Eyes positioned in upper third. Head MUST have 10% margin above. Sharp focus on facial features. Face identity clearly recognizable.',
    extreme_closeup: 'CRITICAL FRAMING: Face detail only. Character FACING CAMERA. Eyes prominent and sharp. Dramatic intentional tight framing. Skin texture visible.',
    establishing: 'CRITICAL FRAMING: If characters present, they MUST be clearly visible and FACING CAMERA. Wide establishing shot showing environment but characters as clear focal point.'
  },

  /**
   * Build a narrative prompt for shot image generation
   * NOW INTEGRATES WITH VISUAL_STYLE_DNA for comprehensive style control
   *
   * @param {Object} shot - Shot data with shotType, prompt, etc.
   * @param {Object} context - Scene context with environment, lighting, mood
   * @param {Object} characterBible - Character information
   * @param {Object} styleBible - Style/visual guidelines
   * @param {string} visualStyleMode - VISUAL_STYLE_DNA mode (photorealistic, cinematic, etc.)
   * @returns {string} Optimized narrative prompt
   */
  buildShotPrompt(shot, context = {}, characterBible = null, styleBible = null, visualStyleMode = 'photorealistic') {
    const shotType = (shot.shotType || 'medium').toLowerCase();
    const parts = [];

    // 0. GET VISUAL_STYLE_DNA ENHANCEMENT - This is the KEY to jaw-dropping results
    const hasHumanSubject = !!(characterBible && characterBible.length > 0);
    const styleEnhancement = VISUAL_STYLE_DNA.buildStyleEnhancement(visualStyleMode, hasHumanSubject);

    // 1. VISUAL_STYLE_DNA PREFIX - MUST come FIRST for maximum impact
    // This is what differentiates photorealistic from 3D rendered output
    parts.push(styleEnhancement.prefix);

    // 2. COMPOSITION BUZZWORD (proven to improve framing)
    const buzzword = this.compositionBuzzwords[Math.floor(Math.random() * 3)]; // Use top 3
    parts.push(buzzword + '.');

    // 3. SHOT TYPE with NARRATIVE description
    const shotTypeDesc = this._getShotTypeNarrative(shotType, shot, context);
    parts.push(shotTypeDesc);

    // 4. SUBJECT description (if character present)
    if (hasHumanSubject) {
      const charDesc = this._buildCharacterDescription(characterBible, shot);
      if (charDesc) parts.push(charDesc);

      // 4.5 SKIN RENDERING from VISUAL_STYLE_DNA (critical for realism)
      if (styleEnhancement.skinText) {
        parts.push(styleEnhancement.skinText + '.');
      }
    }

    // 5. ENVIRONMENT and ATMOSPHERE
    if (context.visualPrompt || context.environment) {
      const envDesc = this._buildEnvironmentDescription(context);
      parts.push(envDesc);
    }

    // 6. LIGHTING - Now enhanced with VISUAL_STYLE_DNA lighting
    const baseLighting = this._selectLighting(context, styleBible);
    const styleLighting = styleEnhancement.lighting || '';
    const combinedLighting = styleLighting ? `${baseLighting}, ${styleLighting}` : baseLighting;
    parts.push(`The scene is illuminated by ${combinedLighting}.`);

    // 7. MOOD and ATMOSPHERE
    if (context.mood || shot.mood) {
      parts.push(`The atmosphere feels ${context.mood || shot.mood}.`);
    }

    // 8. CAMERA SPECIFICATIONS - Enhanced with VISUAL_STYLE_DNA camera language
    const baseCameraSpec = this.cameraSpecs[this._mapShotTypeToCamera(shotType)];
    const styleCameraSpec = styleEnhancement.camera || '';
    const combinedCamera = styleCameraSpec || baseCameraSpec;
    parts.push(combinedCamera + '.');

    // 9. FRAMING RULES with CAPS emphasis
    const framingRule = this.framingRules[this._mapToFramingCategory(shotType)];
    parts.push(framingRule);

    // 10. REALISM CONSTRAINTS - Now style-specific
    if (visualStyleMode === 'photorealistic' || visualStyleMode === 'cinematic') {
      parts.push('Natural skin texture with visible pores, film grain, authentic fabric textures, real-world physics.');
    } else if (visualStyleMode === 'stylized_3d') {
      parts.push('Pixar-quality rendering, appealing stylized surfaces, expressive animation design.');
    } else if (visualStyleMode === 'anime') {
      parts.push('Clean anime linework, expressive anime eyes, dynamic anime composition.');
    }

    // 11. STYLE BIBLE elements
    if (styleBible && styleBible.enabled) {
      const styleElements = [];
      if (styleBible.colorGrade) styleElements.push(styleBible.colorGrade);
      if (styleBible.atmosphere) styleElements.push(styleBible.atmosphere);
      if (styleElements.length > 0) {
        parts.push(`Visual style: ${styleElements.join(', ')}.`);
      }
    }

    // 12. TECHNICAL QUALITY from VISUAL_STYLE_DNA
    parts.push(styleEnhancement.suffix + '.');

    // 13. NEGATIVE instructions - Now using VISUAL_STYLE_DNA negatives
    // Note: For Imagen, we include AVOID in the positive prompt since it doesn't support negative prompt parameter
    const negativeKeywords = styleEnhancement.negative.split(', ').slice(0, 20).join(', '); // Limit to avoid prompt bloat
    parts.push(`STRICTLY AVOID: ${negativeKeywords}, cropped heads, cut-off faces, characters facing away, back-of-head shots.`);

    // 14. FINAL CHARACTER EMPHASIS (critical for proper output)
    parts.push('FINAL REQUIREMENT: Character MUST be the hero of this shot - clearly visible, properly framed, and FACING THE CAMERA with face clearly recognizable. The character is the main subject, environment is secondary.');

    return parts.join(' ');
  },

  /**
   * Build narrative description based on shot type
   */
  _getShotTypeNarrative(shotType, shot, context) {
    const subject = shot.subject || context.subject || 'the subject';
    const action = shot.action || context.action || '';

    if (shotType.includes('wide') || shotType.includes('establishing')) {
      return `A sweeping wide shot establishes the scene. ${subject} is visible in full, standing confidently within the expansive environment. ${action}`;
    } else if (shotType.includes('closeup') || shotType.includes('close-up')) {
      return `An intimate close-up captures ${subject}'s expression with striking clarity. Every subtle emotion is visible in their eyes and features. ${action}`;
    } else if (shotType.includes('extreme')) {
      return `An intense extreme close-up focuses on ${subject}'s face, revealing every nuance of emotion. ${action}`;
    } else {
      return `A balanced medium shot frames ${subject} from the waist up, capturing both their expression and body language. ${action}`;
    }
  },

  /**
   * Build character description from Character Bible
   */
  _buildCharacterDescription(characterBible, shot) {
    if (!characterBible || characterBible.length === 0) return null;

    // Find matching character or use first
    const char = characterBible[0];
    const parts = [];

    if (char.name) parts.push(char.name);
    if (char.physicalDescription) {
      // Extract key visual elements
      const desc = char.physicalDescription;
      if (desc.length > 100) {
        parts.push(desc.substring(0, 100) + '...');
      } else {
        parts.push(desc);
      }
    }
    if (char.attire) parts.push(`wearing ${char.attire}`);

    return parts.length > 0 ? parts.join(', ') + '.' : null;
  },

  /**
   * Build environment description
   */
  _buildEnvironmentDescription(context) {
    const env = context.environment || context.visualPrompt || '';
    if (!env) return '';

    // Clean and enhance environment description
    let enhanced = env;

    // Add atmosphere if present
    if (context.atmosphere) {
      enhanced += `, ${context.atmosphere}`;
    }

    return `Set within ${enhanced}.`;
  },

  /**
   * Select appropriate lighting based on context and style
   */
  _selectLighting(context, styleBible) {
    // Use style bible lighting if available
    if (styleBible && styleBible.lighting) {
      return styleBible.lighting;
    }

    // Infer from mood/atmosphere
    const mood = (context.mood || '').toLowerCase();
    const visual = (context.visualPrompt || '').toLowerCase();

    if (mood.includes('tense') || mood.includes('dramatic') || mood.includes('dark')) {
      return this.lightingSetups.dramatic;
    } else if (mood.includes('warm') || mood.includes('hopeful') || visual.includes('sunset') || visual.includes('golden')) {
      return this.lightingSetups.golden;
    } else if (visual.includes('neon') || visual.includes('city') || visual.includes('night') || visual.includes('cyber')) {
      return this.lightingSetups.neon;
    } else if (visual.includes('studio') || visual.includes('interview') || visual.includes('portrait')) {
      return this.lightingSetups.studio;
    } else if (mood.includes('mysterious') || mood.includes('moody')) {
      return this.lightingSetups.moody;
    }

    return this.lightingSetups.soft;
  },

  /**
   * Map shot type to camera spec category
   */
  _mapShotTypeToCamera(shotType) {
    if (shotType.includes('wide') || shotType.includes('establishing')) return 'wide';
    if (shotType.includes('closeup') || shotType.includes('close-up')) return 'closeup';
    if (shotType.includes('extreme')) return 'closeup';
    if (shotType.includes('portrait') || shotType.includes('face')) return 'portrait';
    return 'medium';
  },

  /**
   * Map shot type to framing category
   */
  _mapToFramingCategory(shotType) {
    if (shotType.includes('wide') || shotType.includes('establishing')) return 'wide';
    if (shotType.includes('extreme')) return 'extreme_closeup';
    if (shotType.includes('closeup') || shotType.includes('close-up')) return 'closeup';
    return 'medium';
  },

  /**
   * Quick enhance for existing prompts - adds quality markers
   * Use when you have a basic prompt and want to improve it
   */
  enhanceExistingPrompt(prompt, shotType = 'medium') {
    const parts = [];

    // Add composition buzzword
    parts.push('Professional cinematography, Pulitzer-prize-winning quality.');

    // Add the original prompt
    parts.push(prompt);

    // Add framing rules
    const framingRule = this.framingRules[this._mapToFramingCategory(shotType)];
    parts.push(framingRule);

    // Add realism constraints
    parts.push('Natural skin texture with visible pores, film grain, authentic details. AVOID: blurry, distorted, cropped heads, AI artifacts.');

    return parts.join(' ');
  }
};

/**
 * MINIMAX_PROMPT_OPTIMIZER (Fix 4)
 * Optimizes video prompts for Minimax's AI video generation capabilities
 * Adds motion quality keywords and cinematic descriptors that Minimax responds well to
 */
const MINIMAX_PROMPT_OPTIMIZER = {
  // Quality keywords that improve Minimax output
  qualityKeywords: [
    'cinematic', 'photorealistic', 'high production value',
    'professional cinematography', 'smooth fluid motion'
  ],

  // Motion descriptors for realistic animation
  motionKeywords: [
    'natural movement', 'realistic physics', 'lifelike motion',
    'seamless continuous action', 'organic flow'
  ],

  /**
   * Enhance a video prompt for Minimax
   * @param {string} prompt - Original video prompt
   * @param {string} cameraMovement - Camera movement type
   * @param {boolean} isImageToVideo - Whether this is I2V (more subtle motion needed)
   * @returns {string} Enhanced prompt
   */
  enhance(prompt, cameraMovement = null, isImageToVideo = false) {
    if (!prompt || prompt.trim().length === 0) {
      return prompt;
    }

    let enhanced = prompt.trim();

    // For I2V, emphasize subtle, continuous motion from the starting frame
    if (isImageToVideo) {
      // Check if prompt already has quality keywords
      const hasQuality = /cinematic|photorealistic|realistic|smooth|fluid/i.test(enhanced);

      if (!hasQuality) {
        // Add quality prefix for I2V - emphasize continuity from first frame
        enhanced = `Cinematic, photorealistic video with smooth fluid motion. ${enhanced}`;
      }

      // Add motion continuity suffix if not present
      if (!/continuous|seamless|natural.*motion/i.test(enhanced)) {
        enhanced += '. Natural lifelike movement, seamless animation, professional cinematography quality.';
      }
    } else {
      // Text-to-video: can be more dynamic
      const hasQuality = /cinematic|photorealistic|realistic/i.test(enhanced);

      if (!hasQuality) {
        enhanced = `Cinematic, high production value, photorealistic. ${enhanced}`;
      }

      if (!/smooth|fluid|natural.*motion/i.test(enhanced)) {
        enhanced += '. Smooth fluid motion, realistic physics, professional quality animation.';
      }
    }

    return enhanced;
  },

  /**
   * Format camera movement for Minimax
   * @param {Array|string} movements - Camera movements
   * @returns {string} Formatted movement string for prompt prefix
   */
  formatCameraMovements(movements) {
    if (!movements) return '';

    const movementArray = Array.isArray(movements) ? movements : [movements];
    const valid = movementArray.filter(m => MINIMAX_CAMERA_MOVEMENTS.includes(m));

    if (valid.length === 0) return '';

    // Minimax uses bracket notation for camera movements
    return `[${valid.slice(0, 3).join(', ')}]`;
  }
};

/**
 * creationWizardGenerateMinimaxVideo - Generate video using Minimax API
 *
 * Supports both text-to-video and image-to-video generation
 */
exports.creationWizardGenerateMinimaxVideo = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const {
    projectId,
    sceneId,
    prompt,
    imageUrl = null,
    model = 'hailuo-2.3',
    duration = '10s',  // Default to 10s for richer action sequences
    resolution = '768p',
    cameraMovements = [],
    promptOptimizer = true
  } = data;

  if (!prompt || prompt.trim().length < 5) {
    throw new functions.https.HttpsError('invalid-argument', 'Video prompt is required (min 5 chars)');
  }

  // Validate model - auto-switch for models that don't support requested duration
  let effectiveModel = model;
  const is10sVideo = duration === '10s';

  // Only hailuo-2.3-fast doesn't support 10s - auto-switch to hailuo-2.3
  if (is10sVideo && model === 'hailuo-2.3-fast') {
    console.log(`[creationWizardGenerateMinimaxVideo] Auto-switching from ${model} to hailuo-2.3 for 10s duration`);
    effectiveModel = 'hailuo-2.3';
  }

  const modelConfig = MINIMAX_VIDEO_MODELS[effectiveModel];
  if (!modelConfig) {
    throw new functions.https.HttpsError('invalid-argument', `Invalid model: ${effectiveModel}`);
  }

  // Validate duration/resolution combination
  const allowedResolutions = modelConfig.durations[duration];
  if (!allowedResolutions) {
    throw new functions.https.HttpsError('invalid-argument', `Duration ${duration} not supported for model ${effectiveModel}`);
  }
  if (!allowedResolutions.includes(resolution)) {
    throw new functions.https.HttpsError('invalid-argument', `Resolution ${resolution} not supported for ${duration} duration. Allowed: ${allowedResolutions.join(', ')}`);
  }

  // Get Minimax API key - supports both env vars and Firebase config
  const minimaxKey = process.env.MINIMAX_API_KEY || functions.config().minimax?.key;
  if (!minimaxKey) {
    throw new functions.https.HttpsError('failed-precondition', 'Minimax API key not configured. Set MINIMAX_API_KEY in .env or use: firebase functions:config:set minimax.key="YOUR_KEY"');
  }

  try {
    // Determine if this is image-to-video
    const isImageToVideo = imageUrl && modelConfig.inputTypes.includes('image');

    // ENHANCED: Use MINIMAX_PROMPT_OPTIMIZER for better video quality (Fix 4)
    let enhancedPrompt = MINIMAX_PROMPT_OPTIMIZER.enhance(prompt, cameraMovements[0], isImageToVideo);

    // Add camera movements using optimizer's formatter
    const cameraPrefix = MINIMAX_PROMPT_OPTIMIZER.formatCameraMovements(cameraMovements);
    if (cameraPrefix) {
      enhancedPrompt = `${cameraPrefix} ${enhancedPrompt}`;
    }

    console.log(`[creationWizardGenerateMinimaxVideo] Enhanced prompt (${isImageToVideo ? 'I2V' : 'T2V'}): ${enhancedPrompt.substring(0, 100)}...`);

    // Determine API endpoint
    const apiEndpoint = 'https://api.minimax.io/v1/video_generation';

    // Build request payload
    const payload = {
      model: modelConfig.modelId,
      prompt: enhancedPrompt,
      prompt_optimizer: promptOptimizer
    };

    // Add image for I2V
    if (isImageToVideo) {
      payload.first_frame_image = imageUrl;
    }

    // Map duration string to seconds
    const durationSeconds = duration === '6s' ? 6 : 10;

    // Map resolution to Minimax format
    const resolutionMap = {
      '512p': '512',
      '768p': '768',
      '1080p': '1080'
    };

    // Add optional parameters if supported
    if (modelConfig.features.includes('fast_generation')) {
      payload.fast_pretreatment = true;
    }

    // CRITICAL: Add duration to payload (was missing, causing 5s default)
    payload.duration = durationSeconds;

    console.log(`[creationWizardGenerateMinimaxVideo] Payload with duration=${durationSeconds}:`, JSON.stringify(payload, null, 2));

    // Call Minimax API
    const minimaxResponse = await axios.post(apiEndpoint, payload, {
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${minimaxKey}`
      },
      timeout: 60000
    });

    const taskId = minimaxResponse.data.task_id;
    const baseResp = minimaxResponse.data.base_resp;

    if (baseResp && baseResp.status_code !== 0) {
      throw new Error(`Minimax API error: ${baseResp.status_msg || 'Unknown error'}`);
    }

    // Log usage
    await db.collection('apiUsage').add({
      userId: uid,
      type: 'minimax_video_generation',
      model: model,
      modelId: modelConfig.modelId,
      projectId: projectId || null,
      sceneId: sceneId || null,
      duration,
      resolution,
      isImageToVideo,
      estimatedCost: modelConfig.pricing[`${duration}-${resolution}`] || 0,
      timestamp: admin.firestore.FieldValue.serverTimestamp()
    });

    return {
      success: true,
      taskId,
      sceneId,
      model,
      duration,
      resolution,
      provider: 'minimax',
      status: 'processing'
    };

  } catch (error) {
    console.error('[creationWizardGenerateMinimaxVideo] Error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to start Minimax video generation'));
  }
});

/**
 * creationWizardCheckMinimaxVideoStatus - Check Minimax video generation status
 */
exports.creationWizardCheckMinimaxVideoStatus = functions.https.onCall(async (data, context) => {
  await verifyAuth(context);
  const { taskId } = data;

  if (!taskId) {
    throw new functions.https.HttpsError('invalid-argument', 'Task ID is required');
  }

  // Get Minimax API key - supports both env vars and Firebase config
  const minimaxKey = process.env.MINIMAX_API_KEY || functions.config().minimax?.key;
  if (!minimaxKey) {
    throw new functions.https.HttpsError('failed-precondition', 'Minimax API key not configured');
  }

  try {
    // Query task status
    const statusResponse = await axios.get(
      `https://api.minimax.io/v1/query/video_generation?task_id=${taskId}`,
      {
        headers: { 'Authorization': `Bearer ${minimaxKey}` },
        timeout: 15000
      }
    );

    const { status, file_id, base_resp } = statusResponse.data;

    if (base_resp && base_resp.status_code !== 0) {
      return {
        success: false,
        taskId,
        status: 'failed',
        error: base_resp.status_msg || 'Unknown error'
      };
    }

    // Map Minimax status to our status
    const statusMap = {
      'Queueing': 'queued',
      'Processing': 'processing',
      'Success': 'completed',
      'Fail': 'failed'
    };

    const result = {
      success: true,
      taskId,
      status: statusMap[status] || status.toLowerCase(),
      fileId: file_id || null
    };

    // If completed, get the video URL
    if (status === 'Success' && file_id) {
      try {
        const fileResponse = await axios.get(
          `https://api.minimax.io/v1/files/retrieve?file_id=${file_id}`,
          {
            headers: { 'Authorization': `Bearer ${minimaxKey}` },
            timeout: 15000
          }
        );

        if (fileResponse.data.file && fileResponse.data.file.download_url) {
          result.videoUrl = fileResponse.data.file.download_url;
        }
      } catch (fileError) {
        console.error('[creationWizardCheckMinimaxVideoStatus] Error fetching file:', fileError);
      }
    }

    return result;

  } catch (error) {
    console.error('[creationWizardCheckMinimaxVideoStatus] Error:', error);
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to check Minimax video status'));
  }
});

/**
 * creationWizardGetMinimaxModels - Get available Minimax video models
 */
exports.creationWizardGetMinimaxModels = functions.https.onCall(async (data, context) => {
  await verifyAuth(context);

  return {
    success: true,
    models: MINIMAX_VIDEO_MODELS,
    cameraMovements: MINIMAX_CAMERA_MOVEMENTS,
    recommendations: {
      quality: 'hailuo-2.3',
      speed: 'hailuo-2.3-fast',
      continuity: 'hailuo-02'
    }
  };
});

/**
 * creationWizardGetGenres - Get available genres and content formats
 * Phase 3A: Genre Reference Library for premium content creation
 */
exports.creationWizardGetGenres = functions.https.onCall(async (data, context) => {
  await verifyAuth(context);

  // Simplified genre data for the frontend
  const genres = {
    // Documentary Genres
    'documentary-nature': {
      id: 'documentary-nature',
      name: 'Nature Documentary',
      category: 'documentary',
      icon: 'ðŸŒ',
      description: 'Planet Earth style - epic scale, intimate moments',
      references: ['Planet Earth', 'Our Planet', 'Blue Planet']
    },
    'documentary-true-crime': {
      id: 'documentary-true-crime',
      name: 'True Crime',
      category: 'documentary',
      icon: 'ðŸ”',
      description: 'Mystery-driven, investigative storytelling',
      references: ['Making a Murderer', 'The Jinx', 'Tiger King']
    },
    'documentary-social': {
      id: 'documentary-social',
      name: 'Social Documentary',
      category: 'documentary',
      icon: 'ðŸ“¢',
      description: 'Issues-driven, awareness-building content',
      references: ['The Social Dilemma', 'Blackfish', '13th']
    },
    'documentary-historical': {
      id: 'documentary-historical',
      name: 'Historical Documentary',
      category: 'documentary',
      icon: 'ðŸ“œ',
      description: 'Ken Burns style - bringing history to life',
      references: ['The Civil War', 'The Last Dance', 'Apollo 11']
    },
    // Educational Genres
    'educational-explainer': {
      id: 'educational-explainer',
      name: 'Explainer',
      category: 'educational',
      icon: 'ðŸ’¡',
      description: 'Complex topics made simple and engaging',
      references: ['Kurzgesagt', 'Vox', 'Wendover Productions']
    },
    'educational-tutorial': {
      id: 'educational-tutorial',
      name: 'Tutorial/How-To',
      category: 'educational',
      icon: 'ðŸ› ï¸',
      description: 'Personality-driven teaching and making',
      references: ['Mark Rober', 'Binging with Babish', 'Adam Savage']
    },
    'educational-science': {
      id: 'educational-science',
      name: 'Science/Tech',
      category: 'educational',
      icon: 'ðŸ”¬',
      description: 'Wonder-filled exploration of how things work',
      references: ['Veritasium', 'SmarterEveryDay', 'Vsauce']
    },
    // Entertainment Genres
    'entertainment-comedy': {
      id: 'entertainment-comedy',
      name: 'Comedy',
      category: 'entertainment',
      icon: 'ðŸ˜‚',
      description: 'Timing, subversion, and commitment to the bit',
      references: ['The Office', 'Key & Peele', 'Brooklyn 99']
    },
    'entertainment-drama': {
      id: 'entertainment-drama',
      name: 'Drama',
      category: 'entertainment',
      icon: 'ðŸŽ­',
      description: 'Stakes, conflict, and moral complexity',
      references: ['Breaking Bad', 'Succession', 'Chernobyl']
    },
    'entertainment-horror': {
      id: 'entertainment-horror',
      name: 'Horror/Thriller',
      category: 'entertainment',
      icon: 'ðŸ‘»',
      description: 'Dread, psychological tension, and the unknown',
      references: ['Black Mirror', 'Get Out', 'A Quiet Place']
    },
    // Business Genres
    'business-brand': {
      id: 'business-brand',
      name: 'Brand Story',
      category: 'business',
      icon: 'âœ¨',
      description: 'Values-driven, aspirational brand content',
      references: ['Apple keynotes', 'Nike campaigns', 'Patagonia']
    },
    'business-product': {
      id: 'business-product',
      name: 'Product Launch',
      category: 'business',
      icon: 'ðŸš€',
      description: 'Anticipation, reveals, and product excellence',
      references: ['Apple reveals', 'Tesla unveilings', 'MKBHD']
    },
    'business-testimonial': {
      id: 'business-testimonial',
      name: 'Testimonial',
      category: 'business',
      icon: 'ðŸ’¬',
      description: 'Real people, real results, authentic stories',
      references: ['Salesforce stories', 'Shot on iPhone']
    },
    // Social Media Native Genres
    'social-viral': {
      id: 'social-viral',
      name: 'Viral/Hook-Driven',
      category: 'social',
      icon: 'ðŸ”¥',
      description: 'Stop-scrolling hooks, instant value',
      references: ['MrBeast', 'TikTok trends', 'Top Reels']
    },
    'social-storytime': {
      id: 'social-storytime',
      name: 'Storytime',
      category: 'social',
      icon: 'ðŸ“–',
      description: 'Bingeable narratives with personality',
      references: ['Reddit stories', 'Commentary channels']
    },
    // Series Genres
    'series-docuseries': {
      id: 'series-docuseries',
      name: 'Docuseries',
      category: 'series',
      icon: 'ðŸ“º',
      description: 'Episodic storytelling with overarching narrative',
      references: ['The Last Dance', 'Drive to Survive', 'Chef\'s Table']
    }
  };

  const contentFormats = {
    'short-form': {
      id: 'short-form',
      name: 'Short-Form',
      duration: '< 60s',
      icon: 'âš¡',
      platforms: ['TikTok', 'Reels', 'Shorts'],
      description: 'Hook in 0.5s, one key idea, strong loop'
    },
    'medium-form': {
      id: 'medium-form',
      name: 'Medium-Form',
      duration: '1-5 min',
      icon: 'ðŸ“±',
      platforms: ['YouTube', 'Instagram', 'LinkedIn'],
      description: 'Full narrative arc, room for nuance'
    },
    'long-form': {
      id: 'long-form',
      name: 'Long-Form',
      duration: '5-20 min',
      icon: 'ðŸŽ¬',
      platforms: ['YouTube', 'Podcasts', 'Courses'],
      description: 'Deep exploration, varied pacing'
    },
    'episodic': {
      id: 'episodic',
      name: 'Episodic',
      duration: 'Series',
      icon: 'ðŸ“º',
      platforms: ['YouTube series', 'Courses'],
      description: 'Connected episodes, cliffhangers'
    }
  };

  const categories = [
    { id: 'documentary', name: 'Documentary', icon: 'ðŸŽ¬' },
    { id: 'educational', name: 'Educational', icon: 'ðŸ’¡' },
    { id: 'entertainment', name: 'Entertainment', icon: 'ðŸŽ­' },
    { id: 'business', name: 'Business', icon: 'ðŸ’¼' },
    { id: 'social', name: 'Social Native', icon: 'ðŸ“±' },
    { id: 'series', name: 'Series', icon: 'ðŸ“º' }
  ];

  return {
    success: true,
    genres,
    contentFormats,
    categories
  };
});

// ==============================================
// CREATION WIZARD - PHASE 5: ASSEMBLY
// ==============================================

// ==============================================
// AUDIO INTELLIGENCE SYSTEM - Phase 1
// ==============================================

/**
 * Genre to Audio Mapping - Maps video genres to audio characteristics
 * Used by the Audio Intelligence Engine to auto-select appropriate audio
 */
const GENRE_AUDIO_MAPPING = {
  // Horror & Thriller
  'horror': {
    musicMoods: ['dark', 'tense', 'suspenseful', 'eerie'],
    musicCategories: ['cinematic', 'dark'],
    sfxStyle: 'horror',
    sfxTypes: ['impact-deep', 'whoosh-dark', 'tension-rise', 'heartbeat'],
    ambienceTypes: ['wind-howling', 'creaking', 'distant-thunder', 'whispers'],
    bpmRange: { min: 60, max: 100 },
    energyCurve: 'building-tension',
    recommendedVolumes: { music: 35, sfx: 50, ambience: 25 }
  },
  'thriller': {
    musicMoods: ['tense', 'suspenseful', 'dramatic', 'urgent'],
    musicCategories: ['cinematic', 'dramatic'],
    sfxStyle: 'cinematic',
    sfxTypes: ['impact-dramatic', 'whoosh-heavy', 'bass-drop', 'tension-hit'],
    ambienceTypes: ['urban-night', 'rain-heavy', 'wind'],
    bpmRange: { min: 80, max: 130 },
    energyCurve: 'escalating',
    recommendedVolumes: { music: 40, sfx: 45, ambience: 20 }
  },

  // Documentary & Educational
  'documentary': {
    musicMoods: ['inspiring', 'emotional', 'epic', 'reflective'],
    musicCategories: ['cinematic', 'orchestral', 'ambient'],
    sfxStyle: 'subtle',
    sfxTypes: ['whoosh-soft', 'transition-smooth', 'rise-gentle'],
    ambienceTypes: ['nature', 'urban-light', 'room-tone'],
    bpmRange: { min: 70, max: 110 },
    energyCurve: 'narrative-wave',
    recommendedVolumes: { music: 25, sfx: 30, ambience: 15 }
  },
  'educational': {
    musicMoods: ['neutral', 'focus', 'light', 'positive'],
    musicCategories: ['corporate', 'ambient', 'electronic-soft'],
    sfxStyle: 'minimal',
    sfxTypes: ['click-soft', 'notification', 'pop-light'],
    ambienceTypes: ['quiet-room', 'library'],
    bpmRange: { min: 80, max: 110 },
    energyCurve: 'steady',
    recommendedVolumes: { music: 20, sfx: 25, ambience: 10 }
  },

  // Tech & Innovation
  'tech': {
    musicMoods: ['modern', 'electronic', 'innovative', 'futuristic'],
    musicCategories: ['electronic', 'corporate', 'modern'],
    sfxStyle: 'tech',
    sfxTypes: ['glitch', 'digital-beep', 'tech-swoosh', 'data-stream'],
    ambienceTypes: ['digital-hum', 'server-room', 'subtle-electronic'],
    bpmRange: { min: 100, max: 140 },
    energyCurve: 'pulsing',
    recommendedVolumes: { music: 35, sfx: 40, ambience: 15 }
  },

  // Business & Corporate
  'corporate': {
    musicMoods: ['professional', 'motivational', 'confident', 'uplifting'],
    musicCategories: ['corporate', 'upbeat', 'inspiring'],
    sfxStyle: 'clean',
    sfxTypes: ['whoosh-medium', 'rise-corporate', 'success-ding'],
    ambienceTypes: ['office-subtle', 'conference'],
    bpmRange: { min: 100, max: 130 },
    energyCurve: 'positive-build',
    recommendedVolumes: { music: 30, sfx: 35, ambience: 10 }
  },

  // Motivational & Inspirational
  'motivational': {
    musicMoods: ['uplifting', 'powerful', 'triumphant', 'epic'],
    musicCategories: ['cinematic', 'epic', 'inspiring'],
    sfxStyle: 'powerful',
    sfxTypes: ['impact-dramatic', 'rise-epic', 'boom', 'whoosh-heavy'],
    ambienceTypes: ['crowd-cheer', 'stadium', 'nature-open'],
    bpmRange: { min: 90, max: 140 },
    energyCurve: 'hero-journey',
    recommendedVolumes: { music: 45, sfx: 50, ambience: 20 }
  },

  // Lifestyle & Vlog
  'lifestyle': {
    musicMoods: ['chill', 'happy', 'acoustic', 'warm'],
    musicCategories: ['acoustic', 'indie', 'chill'],
    sfxStyle: 'playful',
    sfxTypes: ['pop-soft', 'swoosh-light', 'spring', 'sparkle'],
    ambienceTypes: ['cafe', 'nature-birds', 'city-day'],
    bpmRange: { min: 90, max: 120 },
    energyCurve: 'casual-flow',
    recommendedVolumes: { music: 35, sfx: 30, ambience: 20 }
  },

  // Cinematic & Film
  'cinematic': {
    musicMoods: ['orchestral', 'epic', 'dramatic', 'emotional'],
    musicCategories: ['cinematic', 'orchestral', 'epic'],
    sfxStyle: 'cinematic',
    sfxTypes: ['boom', 'whoosh-heavy', 'bass-drop', 'impact-cinematic'],
    ambienceTypes: ['wind', 'rain', 'thunder', 'nature-dramatic'],
    bpmRange: { min: 60, max: 120 },
    energyCurve: 'cinematic-arc',
    recommendedVolumes: { music: 50, sfx: 55, ambience: 30 }
  },

  // Comedy & Entertainment
  'comedy': {
    musicMoods: ['playful', 'quirky', 'fun', 'upbeat'],
    musicCategories: ['comedy', 'quirky', 'upbeat'],
    sfxStyle: 'comedic',
    sfxTypes: ['boing', 'slide-whistle', 'pop-cartoon', 'fail-horn'],
    ambienceTypes: ['laugh-track', 'applause'],
    bpmRange: { min: 100, max: 150 },
    energyCurve: 'bouncy',
    recommendedVolumes: { music: 35, sfx: 60, ambience: 15 }
  },

  // Gaming
  'gaming': {
    musicMoods: ['energetic', 'electronic', 'intense', 'action'],
    musicCategories: ['electronic', 'gaming', 'action'],
    sfxStyle: 'gaming',
    sfxTypes: ['power-up', 'hit-marker', 'level-up', 'explosion-8bit'],
    ambienceTypes: ['digital-atmosphere', 'arcade'],
    bpmRange: { min: 120, max: 180 },
    energyCurve: 'high-energy',
    recommendedVolumes: { music: 45, sfx: 55, ambience: 15 }
  },

  // Romance & Drama
  'romance': {
    musicMoods: ['romantic', 'emotional', 'soft', 'tender'],
    musicCategories: ['romantic', 'piano', 'orchestral-soft'],
    sfxStyle: 'delicate',
    sfxTypes: ['sparkle', 'chime-soft', 'heartbeat-soft'],
    ambienceTypes: ['wind-gentle', 'rain-soft', 'nature-peaceful'],
    bpmRange: { min: 60, max: 90 },
    energyCurve: 'emotional-wave',
    recommendedVolumes: { music: 40, sfx: 25, ambience: 25 }
  },

  // Action & Sports
  'action': {
    musicMoods: ['intense', 'powerful', 'driving', 'aggressive'],
    musicCategories: ['action', 'rock', 'electronic-hard'],
    sfxStyle: 'action',
    sfxTypes: ['impact-heavy', 'explosion', 'whoosh-fast', 'hit'],
    ambienceTypes: ['engine-rev', 'crowd-intense', 'wind-rushing'],
    bpmRange: { min: 130, max: 180 },
    energyCurve: 'adrenaline',
    recommendedVolumes: { music: 50, sfx: 60, ambience: 20 }
  },

  // News & Commentary
  'news': {
    musicMoods: ['serious', 'professional', 'urgent', 'neutral'],
    musicCategories: ['news', 'corporate', 'minimal'],
    sfxStyle: 'news',
    sfxTypes: ['whoosh-news', 'transition-news', 'notification-urgent'],
    ambienceTypes: ['newsroom', 'office'],
    bpmRange: { min: 90, max: 120 },
    energyCurve: 'professional',
    recommendedVolumes: { music: 20, sfx: 35, ambience: 10 }
  },

  // Default/General
  'general': {
    musicMoods: ['neutral', 'light', 'positive'],
    musicCategories: ['corporate', 'ambient'],
    sfxStyle: 'subtle',
    sfxTypes: ['whoosh-soft', 'transition-smooth'],
    ambienceTypes: ['room-tone'],
    bpmRange: { min: 80, max: 120 },
    energyCurve: 'steady',
    recommendedVolumes: { music: 25, sfx: 30, ambience: 15 }
  }
};

/**
 * Pacing to BPM Mapping - Correlates video pacing with music tempo
 */
const PACING_BPM_CONFIG = {
  'fast': {
    bpmRange: { min: 120, max: 160 },
    preferredBPM: 140,
    sceneTransitionSpeed: 'quick',
    sfxIntensity: 'high',
    recommendedMusicVolume: 40
  },
  'balanced': {
    bpmRange: { min: 90, max: 120 },
    preferredBPM: 105,
    sceneTransitionSpeed: 'moderate',
    sfxIntensity: 'medium',
    recommendedMusicVolume: 30
  },
  'contemplative': {
    bpmRange: { min: 60, max: 90 },
    preferredBPM: 75,
    sceneTransitionSpeed: 'slow',
    sfxIntensity: 'low',
    recommendedMusicVolume: 25
  }
};

/**
 * Emotional Journey Audio Curves - Maps narrative arcs to audio energy
 */
const EMOTIONAL_AUDIO_CURVES = {
  'hero-journey': {
    phases: [
      { name: 'ordinary-world', position: 0.0, energy: 0.3, mood: 'neutral' },
      { name: 'call-to-adventure', position: 0.15, energy: 0.5, mood: 'curious' },
      { name: 'crossing-threshold', position: 0.25, energy: 0.6, mood: 'determined' },
      { name: 'tests-allies', position: 0.4, energy: 0.7, mood: 'building' },
      { name: 'ordeal', position: 0.6, energy: 0.9, mood: 'intense' },
      { name: 'reward', position: 0.75, energy: 1.0, mood: 'triumphant' },
      { name: 'return', position: 0.9, energy: 0.6, mood: 'reflective' },
      { name: 'resolution', position: 1.0, energy: 0.5, mood: 'satisfied' }
    ]
  },
  'problem-solution': {
    phases: [
      { name: 'problem-intro', position: 0.0, energy: 0.4, mood: 'concerned' },
      { name: 'problem-deep', position: 0.2, energy: 0.6, mood: 'tense' },
      { name: 'exploration', position: 0.4, energy: 0.5, mood: 'curious' },
      { name: 'solution-reveal', position: 0.6, energy: 0.8, mood: 'hopeful' },
      { name: 'implementation', position: 0.8, energy: 0.9, mood: 'confident' },
      { name: 'success', position: 1.0, energy: 0.7, mood: 'satisfied' }
    ]
  },
  'tension-release': {
    phases: [
      { name: 'setup', position: 0.0, energy: 0.3, mood: 'calm' },
      { name: 'building', position: 0.3, energy: 0.6, mood: 'anticipation' },
      { name: 'peak-tension', position: 0.6, energy: 1.0, mood: 'intense' },
      { name: 'release', position: 0.8, energy: 0.4, mood: 'relief' },
      { name: 'resolution', position: 1.0, energy: 0.5, mood: 'peaceful' }
    ]
  },
  'steady': {
    phases: [
      { name: 'intro', position: 0.0, energy: 0.5, mood: 'neutral' },
      { name: 'main', position: 0.5, energy: 0.5, mood: 'consistent' },
      { name: 'outro', position: 1.0, energy: 0.5, mood: 'neutral' }
    ]
  },
  'building-climax': {
    phases: [
      { name: 'intro', position: 0.0, energy: 0.3, mood: 'calm' },
      { name: 'rising', position: 0.3, energy: 0.5, mood: 'building' },
      { name: 'intensifying', position: 0.6, energy: 0.7, mood: 'intense' },
      { name: 'climax', position: 0.85, energy: 1.0, mood: 'peak' },
      { name: 'resolution', position: 1.0, energy: 0.6, mood: 'satisfied' }
    ]
  }
};

/**
 * Sound Effects Library - Categorized SFX for different use cases
 */
const SFX_LIBRARY = [
  // Transitions - Whooshes
  { id: 'whoosh-soft', name: 'Soft Whoosh', category: 'transition', intensity: 'low', duration: 0.8, tags: ['subtle', 'smooth'] },
  { id: 'whoosh-medium', name: 'Medium Whoosh', category: 'transition', intensity: 'medium', duration: 0.6, tags: ['standard', 'clean'] },
  { id: 'whoosh-heavy', name: 'Heavy Whoosh', category: 'transition', intensity: 'high', duration: 0.5, tags: ['dramatic', 'powerful'] },
  { id: 'whoosh-dark', name: 'Dark Whoosh', category: 'transition', intensity: 'medium', duration: 0.7, tags: ['horror', 'mysterious'] },
  { id: 'whoosh-fast', name: 'Fast Whoosh', category: 'transition', intensity: 'high', duration: 0.3, tags: ['quick', 'action'] },

  // Transitions - Swooshes
  { id: 'swoosh-magical', name: 'Magical Swoosh', category: 'transition', intensity: 'medium', duration: 1.0, tags: ['fantasy', 'sparkle'] },
  { id: 'swoosh-light', name: 'Light Swoosh', category: 'transition', intensity: 'low', duration: 0.5, tags: ['gentle', 'airy'] },
  { id: 'tech-swoosh', name: 'Tech Swoosh', category: 'transition', intensity: 'medium', duration: 0.6, tags: ['digital', 'modern'] },

  // Impacts
  { id: 'impact-dramatic', name: 'Dramatic Impact', category: 'impact', intensity: 'high', duration: 1.5, tags: ['cinematic', 'powerful'] },
  { id: 'impact-deep', name: 'Deep Impact', category: 'impact', intensity: 'high', duration: 2.0, tags: ['bass', 'horror'] },
  { id: 'impact-cinematic', name: 'Cinematic Impact', category: 'impact', intensity: 'high', duration: 1.8, tags: ['movie', 'epic'] },
  { id: 'impact-light', name: 'Light Impact', category: 'impact', intensity: 'low', duration: 0.8, tags: ['subtle', 'accent'] },

  // Tech & Digital
  { id: 'glitch', name: 'Glitch', category: 'tech', intensity: 'medium', duration: 0.5, tags: ['digital', 'error', 'modern'] },
  { id: 'digital-beep', name: 'Digital Beep', category: 'tech', intensity: 'low', duration: 0.3, tags: ['notification', 'ui'] },
  { id: 'data-stream', name: 'Data Stream', category: 'tech', intensity: 'low', duration: 1.5, tags: ['cyber', 'processing'] },

  // Rises & Builds
  { id: 'rise-epic', name: 'Epic Rise', category: 'rise', intensity: 'high', duration: 3.0, tags: ['building', 'anticipation'] },
  { id: 'rise-gentle', name: 'Gentle Rise', category: 'rise', intensity: 'low', duration: 2.0, tags: ['subtle', 'smooth'] },
  { id: 'tension-rise', name: 'Tension Rise', category: 'rise', intensity: 'medium', duration: 2.5, tags: ['suspense', 'building'] },

  // Bass & Drops
  { id: 'bass-drop', name: 'Bass Drop', category: 'drop', intensity: 'high', duration: 1.5, tags: ['edm', 'powerful'] },
  { id: 'sub-drop', name: 'Sub Drop', category: 'drop', intensity: 'high', duration: 2.0, tags: ['deep', 'cinematic'] },
  { id: 'boom', name: 'Boom', category: 'drop', intensity: 'high', duration: 1.0, tags: ['explosion', 'dramatic'] },

  // UI & Notifications
  { id: 'click-soft', name: 'Soft Click', category: 'ui', intensity: 'low', duration: 0.1, tags: ['interface', 'subtle'] },
  { id: 'notification', name: 'Notification', category: 'ui', intensity: 'low', duration: 0.5, tags: ['alert', 'ping'] },
  { id: 'success-ding', name: 'Success Ding', category: 'ui', intensity: 'low', duration: 0.6, tags: ['positive', 'complete'] },
  { id: 'pop-soft', name: 'Soft Pop', category: 'ui', intensity: 'low', duration: 0.2, tags: ['bubble', 'light'] },

  // Playful & Comedy
  { id: 'boing', name: 'Boing', category: 'comedy', intensity: 'medium', duration: 0.5, tags: ['cartoon', 'bounce'] },
  { id: 'spring', name: 'Spring', category: 'comedy', intensity: 'medium', duration: 0.4, tags: ['playful', 'fun'] },
  { id: 'sparkle', name: 'Sparkle', category: 'magic', intensity: 'low', duration: 0.8, tags: ['magical', 'shine'] },

  // Tape & Vinyl
  { id: 'tape-stop', name: 'Tape Stop', category: 'retro', intensity: 'medium', duration: 1.0, tags: ['vinyl', 'slowdown'] },
  { id: 'record-scratch', name: 'Record Scratch', category: 'retro', intensity: 'medium', duration: 0.5, tags: ['vinyl', 'interrupt'] }
];

/**
 * Ambience Library - Background atmosphere sounds
 */
const AMBIENCE_LIBRARY = [
  // Nature
  { id: 'nature-forest', name: 'Forest Ambience', category: 'nature', loopable: true, tags: ['peaceful', 'birds', 'trees'] },
  { id: 'nature-ocean', name: 'Ocean Waves', category: 'nature', loopable: true, tags: ['beach', 'calm', 'water'] },
  { id: 'nature-rain', name: 'Rain', category: 'nature', loopable: true, tags: ['weather', 'cozy', 'relaxing'] },
  { id: 'nature-rain-heavy', name: 'Heavy Rain', category: 'nature', loopable: true, tags: ['storm', 'intense'] },
  { id: 'nature-thunder', name: 'Distant Thunder', category: 'nature', loopable: true, tags: ['storm', 'dramatic'] },
  { id: 'nature-wind', name: 'Wind', category: 'nature', loopable: true, tags: ['air', 'movement'] },
  { id: 'nature-wind-howling', name: 'Howling Wind', category: 'nature', loopable: true, tags: ['eerie', 'horror'] },
  { id: 'nature-birds', name: 'Bird Songs', category: 'nature', loopable: true, tags: ['morning', 'peaceful'] },

  // Urban
  { id: 'urban-city', name: 'City Ambience', category: 'urban', loopable: true, tags: ['traffic', 'busy', 'street'] },
  { id: 'urban-night', name: 'City Night', category: 'urban', loopable: true, tags: ['quiet', 'distant', 'evening'] },
  { id: 'urban-cafe', name: 'Cafe Ambience', category: 'urban', loopable: true, tags: ['coffee', 'chatter', 'cozy'] },
  { id: 'urban-office', name: 'Office', category: 'urban', loopable: true, tags: ['typing', 'quiet', 'professional'] },

  // Indoor
  { id: 'room-tone', name: 'Room Tone', category: 'indoor', loopable: true, tags: ['quiet', 'neutral', 'subtle'] },
  { id: 'library', name: 'Library', category: 'indoor', loopable: true, tags: ['quiet', 'pages', 'study'] },

  // Tech
  { id: 'tech-server', name: 'Server Room', category: 'tech', loopable: true, tags: ['hum', 'fans', 'digital'] },
  { id: 'tech-digital', name: 'Digital Atmosphere', category: 'tech', loopable: true, tags: ['electronic', 'subtle'] },

  // Crowds
  { id: 'crowd-cheer', name: 'Crowd Cheering', category: 'crowd', loopable: true, tags: ['celebration', 'victory'] },
  { id: 'crowd-applause', name: 'Applause', category: 'crowd', loopable: false, tags: ['clapping', 'approval'] },

  // Special
  { id: 'heartbeat', name: 'Heartbeat', category: 'special', loopable: true, tags: ['tension', 'suspense', 'horror'] },
  { id: 'clock-ticking', name: 'Clock Ticking', category: 'special', loopable: true, tags: ['time', 'suspense', 'pressure'] }
];

/**
 * analyzeContentForAudio - Analyzes video content and returns comprehensive audio recommendations
 * This is the core Audio Intelligence Engine function
 */
exports.analyzeContentForAudio = functions.https.onCall(async (data, context) => {
  await verifyAuth(context);

  const {
    genre,
    mood: contentMood,
    pacing,
    scenes,
    totalDuration,
    narrativeArc,
    emotionalJourney,
    platform,
    style
  } = data;

  try {
    // 1. Get genre-specific audio mapping
    const genreConfig = GENRE_AUDIO_MAPPING[genre] || GENRE_AUDIO_MAPPING['general'];
    const pacingConfig = PACING_BPM_CONFIG[pacing] || PACING_BPM_CONFIG['balanced'];

    // 2. Calculate optimal BPM range (intersection of genre and pacing)
    const optimalBPM = {
      min: Math.max(genreConfig.bpmRange.min, pacingConfig.bpmRange.min),
      max: Math.min(genreConfig.bpmRange.max, pacingConfig.bpmRange.max),
      preferred: Math.round((pacingConfig.preferredBPM + (genreConfig.bpmRange.min + genreConfig.bpmRange.max) / 2) / 2)
    };

    // 3. Get emotional curve for the narrative
    const emotionalCurve = EMOTIONAL_AUDIO_CURVES[emotionalJourney] ||
                          EMOTIONAL_AUDIO_CURVES[narrativeArc] ||
                          EMOTIONAL_AUDIO_CURVES['steady'];

    // 4. Calculate per-scene audio assignments
    const sceneAudioAssignments = (scenes || []).map((scene, index) => {
      const scenePosition = scenes.length > 1 ? index / (scenes.length - 1) : 0.5;

      // Find the emotional phase for this scene position
      const phases = emotionalCurve.phases;
      let currentPhase = phases[0];
      for (const phase of phases) {
        if (scenePosition >= phase.position) {
          currentPhase = phase;
        }
      }

      // Select appropriate SFX based on genre and intensity
      const isLastScene = index === scenes.length - 1;
      const sfxIntensity = currentPhase.energy > 0.7 ? 'high' : currentPhase.energy > 0.4 ? 'medium' : 'low';

      // Find matching transition SFX
      const transitionSfx = !isLastScene ? selectBestSfx(genreConfig.sfxTypes, sfxIntensity) : null;

      // Find scene-appropriate ambience
      const sceneAmbience = selectBestAmbience(genreConfig.ambienceTypes, scene, currentPhase.mood);

      return {
        sceneId: scene.id,
        sceneIndex: index,
        position: scenePosition,
        emotionalPhase: currentPhase.name,
        energy: currentPhase.energy,
        mood: currentPhase.mood,
        transitionSfx: transitionSfx,
        ambience: sceneAmbience,
        suggestedMusicVolume: Math.round(genreConfig.recommendedVolumes.music * currentPhase.energy)
      };
    });

    // 5. Build music search criteria
    const musicCriteria = {
      moods: genreConfig.musicMoods,
      categories: genreConfig.musicCategories,
      bpmRange: optimalBPM,
      minDuration: (totalDuration || 60) + 10, // Add 10s buffer for fade
      loopable: (totalDuration || 60) > 180,
      tags: [genre, contentMood, style].filter(Boolean)
    };

    // 6. Find best matching tracks from library
    const recommendedTracks = findMatchingTracks(musicCriteria, MUSIC_LIBRARY);

    // 7. Calculate global mix settings
    const mixSettings = {
      voiceVolume: 100,
      musicVolume: pacingConfig.recommendedMusicVolume,
      sfxVolume: genreConfig.recommendedVolumes.sfx,
      ambienceVolume: genreConfig.recommendedVolumes.ambience,
      // Auto-duck music during speech
      autoDuck: true,
      duckLevel: 0.4, // Duck to 40% during speech
      duckAttack: 200, // 200ms fade down
      duckRelease: 500 // 500ms fade up
    };

    // 8. Compile full audio profile
    const audioProfile = {
      // Overall characteristics
      genre: genre,
      pacing: pacing,
      energyCurve: genreConfig.energyCurve,

      // Music recommendations
      music: {
        criteria: musicCriteria,
        recommendedTracks: recommendedTracks.slice(0, 5), // Top 5 matches
        topPick: recommendedTracks[0] || null,
        bpmRange: optimalBPM,
        fadeIn: 2000,
        fadeOut: 3000
      },

      // Per-scene audio
      sceneAudio: sceneAudioAssignments,

      // SFX style
      sfx: {
        style: genreConfig.sfxStyle,
        availableTypes: genreConfig.sfxTypes,
        intensity: pacingConfig.sfxIntensity
      },

      // Ambience
      ambience: {
        recommendedTypes: genreConfig.ambienceTypes,
        primaryAmbience: genreConfig.ambienceTypes[0] || null
      },

      // Mix settings
      mix: mixSettings,

      // Emotional journey
      emotionalCurve: emotionalCurve
    };

    return {
      success: true,
      audioProfile: audioProfile,
      message: `Audio profile generated for ${genre} ${pacing} content`
    };

  } catch (error) {
    console.error('[analyzeContentForAudio] Error:', error);
    throw new functions.https.HttpsError('internal', 'Failed to analyze content for audio');
  }
});

/**
 * Helper: Select best SFX based on types and intensity
 */
function selectBestSfx(sfxTypes, intensity) {
  for (const sfxType of sfxTypes) {
    const sfx = SFX_LIBRARY.find(s => s.id === sfxType || s.id.includes(sfxType));
    if (sfx && (sfx.intensity === intensity || !intensity)) {
      return sfx;
    }
  }
  // Fallback to first type
  return SFX_LIBRARY.find(s => s.id === sfxTypes[0]) || SFX_LIBRARY[0];
}

/**
 * Helper: Select best ambience based on types and mood
 */
function selectBestAmbience(ambienceTypes, scene, mood) {
  for (const ambienceType of ambienceTypes) {
    const ambience = AMBIENCE_LIBRARY.find(a =>
      a.id === ambienceType ||
      a.id.includes(ambienceType) ||
      a.tags.some(t => ambienceType.includes(t))
    );
    if (ambience) {
      return ambience;
    }
  }
  return null;
}

/**
 * Helper: Find tracks matching criteria
 */
function findMatchingTracks(criteria, library) {
  return library
    .map(track => {
      let score = 0;

      // Mood matching
      if (criteria.moods) {
        const trackMoodLower = (track.mood || '').toLowerCase();
        for (const mood of criteria.moods) {
          if (trackMoodLower.includes(mood.toLowerCase())) {
            score += 20;
          }
        }
      }

      // Category matching
      if (criteria.categories && track.category) {
        if (criteria.categories.includes(track.category)) {
          score += 15;
        }
      }

      // BPM matching
      if (criteria.bpmRange && track.bpm) {
        if (track.bpm >= criteria.bpmRange.min && track.bpm <= criteria.bpmRange.max) {
          score += 25;
          // Bonus for being close to preferred
          if (criteria.bpmRange.preferred) {
            const bpmDiff = Math.abs(track.bpm - criteria.bpmRange.preferred);
            score += Math.max(0, 10 - bpmDiff / 5);
          }
        }
      }

      // Duration matching
      if (criteria.minDuration && track.duration) {
        if (track.duration >= criteria.minDuration) {
          score += 10;
        }
      }

      // Tag matching
      if (criteria.tags && track.tags) {
        for (const tag of criteria.tags) {
          if (tag && track.tags.some(t => t.toLowerCase().includes(tag.toLowerCase()))) {
            score += 5;
          }
        }
      }

      return { ...track, matchScore: score };
    })
    .filter(track => track.matchScore > 0)
    .sort((a, b) => b.matchScore - a.matchScore);
}

/**
 * Royalty-free music library for video creation
 * These are curated tracks that can be used freely
 */
const MUSIC_LIBRARY = [
  // Upbeat & Energetic
  {
    id: 'upbeat-corporate',
    name: 'Corporate Success',
    category: 'upbeat',
    mood: 'Motivational, Professional',
    duration: 120,
    bpm: 120,
    tags: ['corporate', 'business', 'success', 'motivational']
  },
  {
    id: 'upbeat-tech',
    name: 'Tech Innovation',
    category: 'upbeat',
    mood: 'Modern, Innovative',
    duration: 90,
    bpm: 128,
    tags: ['technology', 'innovation', 'modern', 'electronic']
  },
  {
    id: 'upbeat-happy',
    name: 'Happy Days',
    category: 'upbeat',
    mood: 'Cheerful, Positive',
    duration: 150,
    bpm: 115,
    tags: ['happy', 'cheerful', 'fun', 'positive']
  },
  // Calm & Ambient
  {
    id: 'calm-ambient',
    name: 'Peaceful Ambient',
    category: 'calm',
    mood: 'Relaxing, Peaceful',
    duration: 180,
    bpm: 70,
    tags: ['ambient', 'relaxing', 'meditation', 'peaceful']
  },
  {
    id: 'calm-piano',
    name: 'Gentle Piano',
    category: 'calm',
    mood: 'Emotional, Soft',
    duration: 150,
    bpm: 60,
    tags: ['piano', 'emotional', 'soft', 'gentle']
  },
  {
    id: 'calm-nature',
    name: 'Nature Sounds',
    category: 'calm',
    mood: 'Natural, Organic',
    duration: 200,
    bpm: 0,
    tags: ['nature', 'birds', 'water', 'forest']
  },
  // Dramatic & Cinematic
  {
    id: 'dramatic-epic',
    name: 'Epic Cinematic',
    category: 'dramatic',
    mood: 'Epic, Powerful',
    duration: 120,
    bpm: 100,
    tags: ['cinematic', 'epic', 'trailer', 'powerful']
  },
  {
    id: 'dramatic-tension',
    name: 'Building Tension',
    category: 'dramatic',
    mood: 'Suspenseful, Intense',
    duration: 90,
    bpm: 90,
    tags: ['tension', 'suspense', 'thriller', 'intense']
  },
  {
    id: 'dramatic-emotional',
    name: 'Emotional Journey',
    category: 'dramatic',
    mood: 'Moving, Inspirational',
    duration: 180,
    bpm: 80,
    tags: ['emotional', 'inspirational', 'moving', 'heartfelt']
  },
  // Electronic & Modern
  {
    id: 'electronic-edm',
    name: 'EDM Energy',
    category: 'electronic',
    mood: 'High Energy, Dance',
    duration: 120,
    bpm: 140,
    tags: ['edm', 'electronic', 'dance', 'energy']
  },
  {
    id: 'electronic-chill',
    name: 'Chill Beats',
    category: 'electronic',
    mood: 'Chill, Lofi',
    duration: 180,
    bpm: 85,
    tags: ['lofi', 'chill', 'beats', 'study']
  },
  {
    id: 'electronic-synthwave',
    name: 'Retro Synthwave',
    category: 'electronic',
    mood: 'Retro, Nostalgic',
    duration: 150,
    bpm: 110,
    tags: ['synthwave', 'retro', '80s', 'nostalgic']
  },
  // Hip Hop & Urban
  {
    id: 'hiphop-trap',
    name: 'Trap Vibes',
    category: 'hiphop',
    mood: 'Urban, Cool',
    duration: 120,
    bpm: 140,
    tags: ['trap', 'hiphop', 'urban', 'beats']
  },
  {
    id: 'hiphop-boom',
    name: 'Boom Bap Classic',
    category: 'hiphop',
    mood: 'Classic, Groovy',
    duration: 150,
    bpm: 90,
    tags: ['boombap', 'classic', 'hiphop', 'groovy']
  },
  // No Music Option
  {
    id: 'none',
    name: 'No Background Music',
    category: 'none',
    mood: 'Voice Only',
    duration: 0,
    bpm: 0,
    tags: ['none', 'silent', 'voice-only']
  }
];

/**
 * Transition types available for video assembly
 */
const TRANSITION_TYPES = [
  { id: 'cut', name: 'Cut', description: 'Instant switch between scenes', duration: 0 },
  { id: 'fade', name: 'Fade', description: 'Smooth fade to black and back', duration: 500 },
  { id: 'crossfade', name: 'Crossfade', description: 'Blend between scenes', duration: 750 },
  { id: 'dissolve', name: 'Dissolve', description: 'Gradual dissolve transition', duration: 1000 },
  { id: 'wipe-left', name: 'Wipe Left', description: 'Wipe from right to left', duration: 500 },
  { id: 'wipe-right', name: 'Wipe Right', description: 'Wipe from left to right', duration: 500 },
  { id: 'zoom-in', name: 'Zoom In', description: 'Zoom into next scene', duration: 600 },
  { id: 'zoom-out', name: 'Zoom Out', description: 'Zoom out to next scene', duration: 600 },
  { id: 'slide-up', name: 'Slide Up', description: 'Next scene slides up', duration: 500 },
  { id: 'slide-down', name: 'Slide Down', description: 'Next scene slides down', duration: 500 }
];

/**
 * Caption style presets
 */
const CAPTION_STYLES = [
  { id: 'karaoke', name: 'Karaoke', description: 'Words highlight as spoken', font: 'bold', animation: 'highlight' },
  { id: 'subtitle', name: 'Subtitle', description: 'Classic subtitle style', font: 'regular', animation: 'fade' },
  { id: 'dynamic', name: 'Dynamic', description: 'Words pop in dynamically', font: 'bold', animation: 'pop' },
  { id: 'minimal', name: 'Minimal', description: 'Clean minimal text', font: 'light', animation: 'slide' },
  { id: 'bold', name: 'Bold Impact', description: 'Large bold text overlay', font: 'extra-bold', animation: 'scale' },
  { id: 'none', name: 'No Captions', description: 'Hide captions', font: null, animation: null }
];

/**
 * creationWizardGetMusicLibrary - Get available music tracks for video
 */
exports.creationWizardGetMusicLibrary = functions.https.onCall(async (data, context) => {
  await verifyAuth(context);
  const { category, search } = data || {};

  let tracks = [...MUSIC_LIBRARY];

  // Filter by category if specified
  if (category && category !== 'all') {
    tracks = tracks.filter(t => t.category === category);
  }

  // Filter by search term if specified
  if (search) {
    const searchLower = search.toLowerCase();
    tracks = tracks.filter(t =>
      t.name.toLowerCase().includes(searchLower) ||
      t.mood.toLowerCase().includes(searchLower) ||
      t.tags.some(tag => tag.includes(searchLower))
    );
  }

  return {
    success: true,
    tracks,
    categories: ['all', 'upbeat', 'calm', 'dramatic', 'electronic', 'hiphop', 'none'],
    transitionTypes: TRANSITION_TYPES,
    captionStyles: CAPTION_STYLES
  };
});

/**
 * creationWizardUpdateAssembly - Save assembly settings to project
 */
exports.creationWizardUpdateAssembly = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const { projectId, assembly } = data;

  if (!projectId) {
    throw new functions.https.HttpsError('invalid-argument', 'Project ID required');
  }

  try {
    const projectRef = db.collection('creationProjects').doc(projectId);
    const projectDoc = await projectRef.get();

    if (!projectDoc.exists) {
      throw new functions.https.HttpsError('not-found', 'Project not found');
    }

    if (projectDoc.data().userId !== uid) {
      throw new functions.https.HttpsError('permission-denied', 'Not authorized');
    }

    // Validate and sanitize assembly data
    const sanitizedAssembly = {
      status: assembly.status || 'in_progress',
      sceneOrder: Array.isArray(assembly.sceneOrder) ? assembly.sceneOrder : [],
      transitions: assembly.transitions || {},
      music: {
        enabled: !!assembly.music?.enabled,
        trackId: assembly.music?.trackId || null,
        volume: Math.min(100, Math.max(0, assembly.music?.volume || 30))
      },
      captions: {
        enabled: assembly.captions?.enabled !== false,
        style: assembly.captions?.style || 'karaoke',
        position: assembly.captions?.position || 'bottom',
        fontSize: assembly.captions?.fontSize || 'medium'
      },
      audioMix: {
        voiceVolume: Math.min(100, Math.max(0, assembly.audioMix?.voiceVolume || 100)),
        musicVolume: Math.min(100, Math.max(0, assembly.audioMix?.musicVolume || 30))
      },
      updatedAt: new Date().toISOString()
    };

    await projectRef.update({
      assembly: sanitizedAssembly,
      updatedAt: admin.firestore.FieldValue.serverTimestamp()
    });

    return { success: true, assembly: sanitizedAssembly };

  } catch (error) {
    console.error('[creationWizardUpdateAssembly] Error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to update assembly'));
  }
});

/**
 * creationWizardGetAssemblyPreview - Generate preview data for assembly
 * Returns timing information for video preview
 */
exports.creationWizardGetAssemblyPreview = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const { projectId } = data;

  if (!projectId) {
    throw new functions.https.HttpsError('invalid-argument', 'Project ID required');
  }

  try {
    const projectRef = db.collection('creationProjects').doc(projectId);
    const projectDoc = await projectRef.get();

    if (!projectDoc.exists) {
      throw new functions.https.HttpsError('not-found', 'Project not found');
    }

    const project = projectDoc.data();
    if (project.userId !== uid) {
      throw new functions.https.HttpsError('permission-denied', 'Not authorized');
    }

    // Calculate total duration and scene timings
    const scriptScenes = project.script?.scenes || [];
    const animationScenes = project.animation?.scenes || [];
    const assembly = project.assembly || {};
    const sceneOrder = assembly.sceneOrder?.length > 0
      ? assembly.sceneOrder
      : scriptScenes.map(s => s.id);

    let currentTime = 0;
    const timeline = [];

    for (const sceneId of sceneOrder) {
      const scriptScene = scriptScenes.find(s => s.id === sceneId);
      const animScene = animationScenes.find(s => s.sceneId === sceneId);
      const transition = assembly.transitions?.[sceneId] || { type: 'cut', duration: 0 };

      if (scriptScene) {
        const sceneDuration = scriptScene.duration || 8;
        const transitionDuration = TRANSITION_TYPES.find(t => t.id === transition.type)?.duration || 0;

        timeline.push({
          sceneId,
          startTime: currentTime,
          duration: sceneDuration * 1000,
          endTime: currentTime + (sceneDuration * 1000),
          transitionType: transition.type,
          transitionDuration,
          hasVideo: !!animScene?.videoUrl,
          hasAudio: !!animScene?.voiceoverUrl,
          narration: scriptScene.narration
        });

        currentTime += (sceneDuration * 1000) + transitionDuration;
      }
    }

    // Get selected music track info
    const selectedTrack = assembly.music?.trackId
      ? MUSIC_LIBRARY.find(t => t.id === assembly.music.trackId)
      : null;

    return {
      success: true,
      timeline,
      totalDuration: currentTime,
      music: selectedTrack ? {
        ...selectedTrack,
        enabled: assembly.music?.enabled,
        volume: assembly.music?.volume
      } : null,
      captions: assembly.captions,
      audioMix: assembly.audioMix || { voiceVolume: 100, musicVolume: 30 }
    };

  } catch (error) {
    console.error('[creationWizardGetAssemblyPreview] Error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to get assembly preview'));
  }
});

// ==============================================
// CREATION WIZARD - PHASE 6: EXPORT
// ==============================================

/**
 * Quality presets for video export
 */
const EXPORT_QUALITY_PRESETS = {
  '720p': {
    name: '720p HD',
    resolution: { width: 1280, height: 720 },
    bitrate: '4M',
    description: 'Good quality, smaller file size'
  },
  '1080p': {
    name: '1080p Full HD',
    resolution: { width: 1920, height: 1080 },
    bitrate: '8M',
    description: 'High quality, recommended'
  },
  '4k': {
    name: '4K Ultra HD',
    resolution: { width: 3840, height: 2160 },
    bitrate: '20M',
    description: 'Maximum quality, large file size'
  }
};

/**
 * creationWizardStartExport - Start video export/rendering job
 */
exports.creationWizardStartExport = functions
  .runWith({ timeoutSeconds: 540, memory: '2GB' })
  .https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const { projectId, quality = '1080p', format = 'mp4', timelineState = null, renderQuality = 'balanced' } = data;

  if (!projectId) {
    throw new functions.https.HttpsError('invalid-argument', 'Project ID required');
  }

  // Validate quality
  const validQualities = Object.keys(EXPORT_QUALITY_PRESETS);
  const outputQuality = validQualities.includes(quality) ? quality : '1080p';
  const qualityPreset = EXPORT_QUALITY_PRESETS[outputQuality];

  try {
    // Get project data
    const projectRef = db.collection('creationProjects').doc(projectId);
    const projectDoc = await projectRef.get();

    if (!projectDoc.exists) {
      throw new functions.https.HttpsError('not-found', 'Project not found');
    }

    const project = projectDoc.data();
    if (project.userId !== uid) {
      throw new functions.https.HttpsError('permission-denied', 'Not authorized');
    }

    // Validate project has all required data
    const scriptScenes = project.script?.scenes || [];
    const animationScenes = project.animation?.scenes || [];
    const storyboardScenes = project.storyboard?.scenes || [];
    const assembly = project.assembly || {};

    if (scriptScenes.length === 0) {
      throw new functions.https.HttpsError('failed-precondition', 'No script scenes found');
    }

    // Check for visual content - either animated videos OR storyboard images
    const animatedScenes = animationScenes.filter(s => s.videoUrl);
    const imageScenes = storyboardScenes.filter(s => s.imageUrl);

    // Allow export if we have EITHER animated videos OR storyboard images
    if (animatedScenes.length === 0 && imageScenes.length === 0) {
      throw new functions.https.HttpsError('failed-precondition', 'No visual content found. Please generate images in the Storyboard step or animate scenes in the Animation step.');
    }

    // Get scene order - prefer timeline state if available
    const sceneOrder = timelineState?.sceneOrder?.length > 0
      ? timelineState.sceneOrder
      : (assembly.sceneOrder?.length > 0 ? assembly.sceneOrder : scriptScenes.map(s => s.id));

    // Build a map of timeline scene data for quick lookup
    const timelineSceneMap = {};
    if (timelineState?.scenes) {
      timelineState.scenes.forEach(s => {
        timelineSceneMap[s.id] = s;
      });
    }

    // Build export manifest with timeline edits applied
    // Format matches what creation-processor.js expects
    const exportManifest = {
      scenes: sceneOrder.map((sceneId, index) => {
        const scriptScene = scriptScenes.find(s => s.id === sceneId);
        const animScene = animationScenes.find(s => s.sceneId === sceneId);
        const storyboardScene = storyboardScenes.find(s => s.sceneId === sceneId);
        const timelineScene = timelineSceneMap[sceneId];

        // Use timeline edits if available, otherwise fall back to assembly/default
        const transition = timelineScene
          ? { type: timelineScene.transition, duration: timelineScene.transitionDuration }
          : (assembly.transitions?.[sceneId] || { type: 'fade', duration: 0.5 });

        // Generate Ken Burns parameters (randomized for visual interest)
        // These control the zoom/pan effect on images
        const kenBurns = storyboardScene?.kenBurns || {
          startScale: 1.0 + Math.random() * 0.15,  // 1.0 - 1.15
          endScale: 1.1 + Math.random() * 0.15,    // 1.1 - 1.25
          startX: 0.4 + Math.random() * 0.2,       // 0.4 - 0.6 (center area)
          startY: 0.4 + Math.random() * 0.2,       // 0.4 - 0.6
          endX: 0.4 + Math.random() * 0.2,         // 0.4 - 0.6
          endY: 0.4 + Math.random() * 0.2          // 0.4 - 0.6
        };

        // Get scene duration - timeline stores in milliseconds, we need seconds
        // If duration > 300 (5 min), assume it's in milliseconds and convert
        let sceneDuration = timelineScene?.duration || scriptScene?.duration || 8;
        if (sceneDuration > 300) {
          sceneDuration = sceneDuration / 1000; // Convert ms to seconds
        }

        return {
          id: sceneId,
          sceneId,
          index,
          duration: sceneDuration,
          // Video sources - prefer animated video, fall back to image
          videoUrl: animScene?.videoUrl || null,
          imageUrl: storyboardScene?.imageUrl || null,
          // Audio - voiceover URL for creation-processor.js
          voiceoverUrl: animScene?.voiceoverUrl || null,
          audioUrl: animScene?.voiceoverUrl || null, // Legacy field
          narration: scriptScene?.narration || '',
          // Ken Burns effect parameters for images
          kenBurns,
          // Transition settings
          transition: transition.type,
          transitionDuration: transition.duration || 0.5,
          voiceoverOffset: timelineScene?.voiceoverOffset || 0
        };
      }),
      // Music settings for creation-processor.js
      music: {
        enabled: timelineState?.music?.enabled || assembly.music?.enabled || false,
        url: assembly.music?.trackUrl || null,
        volume: (timelineState?.audioMix?.musicVolume || assembly.audioMix?.musicVolume || 30) / 100
      },
      // Captions settings for subtitle generation (matching video-wizard format)
      captions: {
        enabled: timelineState?.captions?.enabled ?? assembly.captions?.enabled ?? true,
        style: timelineState?.captions?.style || assembly.captions?.style || 'karaoke',
        position: timelineState?.captions?.position || assembly.captions?.position || 'bottom',
        size: parseFloat(timelineState?.captions?.size || assembly.captions?.size) || 1  // Numeric 0.7-1.5, matching video-wizard
      },
      // Audio mix settings
      audioMix: timelineState?.audioMix || assembly.audioMix || { voiceVolume: 100, musicVolume: 30 },
      // Platform and format
      platform: project.platform?.selected || 'youtube-long',
      aspectRatio: project.platform?.aspectRatio || '16:9',
      // Include timeline metadata
      timelineVersion: timelineState?.version || 0,
      hasTimelineEdits: timelineState?.editHistory?.hasEdits || false
    };

    // Output settings for creation-processor.js
    // Validate renderQuality option
    const validRenderQualities = ['fast', 'balanced', 'best'];
    const outputRenderQuality = validRenderQualities.includes(renderQuality) ? renderQuality : 'balanced';

    const outputSettings = {
      quality: outputQuality,
      aspectRatio: project.platform?.aspectRatio || '16:9',
      fps: 30,
      format,
      resolution: qualityPreset.resolution,
      bitrate: qualityPreset.bitrate,
      renderQuality: outputRenderQuality  // fast, balanced, or best - affects Ken Burns processing speed
    };

    // Calculate total duration
    const totalDuration = exportManifest.scenes.reduce((sum, s) => sum + s.duration, 0);

    // Create export job document
    // Structure matches what creation-processor.js expects
    const jobRef = db.collection('creationExportJobs').doc();
    const jobData = {
      id: jobRef.id,
      projectId,
      userId: uid,
      status: 'pending',
      progress: 0,
      currentStage: 'Queued for processing...',
      // Manifest contains all scene data
      manifest: exportManifest,
      // Output settings for FFmpeg
      output: outputSettings,
      totalDuration,
      quality: outputQuality,
      format,
      createdAt: admin.firestore.FieldValue.serverTimestamp(),
      updatedAt: admin.firestore.FieldValue.serverTimestamp(),
      outputUrl: null,
      error: null
    };

    await jobRef.set(jobData);

    // Update project with export job reference
    await projectRef.update({
      'export.status': 'exporting',
      'export.jobId': jobRef.id,
      'export.progress': 0,
      'export.startedAt': new Date().toISOString(),
      updatedAt: admin.firestore.FieldValue.serverTimestamp()
    });

    // Trigger video processor service (if available)
    const videoProcessorUrl = functions.config().videoprocessor?.url;
    if (videoProcessorUrl) {
      try {
        // Only send jobId - processor fetches full job data from Firestore
        const requestBody = { jobId: jobRef.id };

        // Fire and forget - processor will update job status
        axios.post(`${videoProcessorUrl}/creation-export`, requestBody, {
          timeout: 5000,
          headers: { 'Content-Type': 'application/json' }
        }).catch(err => {
          console.log('Video processor trigger sent (async):', err.message || 'pending');
        });

        console.log(`[creationWizardStartExport] Triggered video processor for job: ${jobRef.id}`);
        console.log(`[creationWizardStartExport] Scenes: ${exportManifest.scenes.length}, Duration: ${totalDuration}s, Quality: ${outputQuality}`);
      } catch (triggerError) {
        console.log('[creationWizardStartExport] Video processor trigger note:', triggerError.message);
      }
    } else {
      console.log('[creationWizardStartExport] Video processor URL not configured - using fallback simulation');
      console.log('[creationWizardStartExport] To enable server-side rendering, configure: firebase functions:config:set videoprocessor.url="YOUR_CLOUD_RUN_URL"');

      // Simulate export progress for demo (when no video processor is available)
      simulateExportProgress(jobRef.id, projectId, uid, exportManifest, totalDuration);
    }

    return {
      success: true,
      jobId: jobRef.id,
      status: 'pending',
      totalDuration,
      quality: outputQuality,
      estimatedTime: Math.ceil(totalDuration * 2) // Rough estimate: 2x realtime
    };

  } catch (error) {
    console.error('[creationWizardStartExport] Error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to start export'));
  }
});

/**
 * Simulate export progress when video processor is not available
 * This is for demo/development purposes
 */
async function simulateExportProgress(jobId, projectId, uid, manifest, totalDuration) {
  const jobRef = db.collection('creationExportJobs').doc(jobId);
  const projectRef = db.collection('creationProjects').doc(projectId);

  const stages = [
    { progress: 10, status: 'processing', message: 'Preparing scenes...' },
    { progress: 30, status: 'processing', message: 'Combining videos...' },
    { progress: 50, status: 'processing', message: 'Adding transitions...' },
    { progress: 70, status: 'processing', message: 'Mixing audio...' },
    { progress: 85, status: 'processing', message: 'Adding captions...' },
    { progress: 95, status: 'processing', message: 'Finalizing...' },
    { progress: 100, status: 'completed', message: 'Export complete!' }
  ];

  for (const stage of stages) {
    await new Promise(resolve => setTimeout(resolve, 2000)); // 2 second delay between stages

    const updateData = {
      progress: stage.progress,
      status: stage.status,
      currentStage: stage.message,
      updatedAt: admin.firestore.FieldValue.serverTimestamp()
    };

    if (stage.status === 'completed') {
      // For demo/simulation: use available video content
      // In production, the video processor would render a combined video
      const animatedScenes = manifest.scenes.filter(s => s.videoUrl);
      const imageScenes = manifest.scenes.filter(s => s.imageUrl);

      if (animatedScenes.length > 0) {
        // Use first animated video as preview
        // In production, all scenes would be combined
        updateData.outputUrl = animatedScenes[0].videoUrl;
        updateData.exportType = 'animated-preview';
        updateData.exportNote = `Preview: ${animatedScenes.length} animated scene(s). Full video rendering requires video processor service.`;
      } else if (imageScenes.length > 0) {
        // Image-only export - can't create video without processor
        // Mark as complete but indicate it's images-only
        updateData.exportType = 'images-only';
        updateData.exportNote = 'Image slideshow export. Video processor not configured - download images individually or configure video processor for Ken Burns video export.';
        // Use first image as thumbnail preview
        updateData.previewThumbnail = imageScenes[0].imageUrl;
        // Set outputUrl to null to indicate no video available
        updateData.outputUrl = null;
        updateData.imagesReady = true;
        updateData.imageUrls = imageScenes.map(s => s.imageUrl);
      } else {
        updateData.exportType = 'error';
        updateData.error = 'No visual content found';
        updateData.outputUrl = null;
      }

      updateData.completedAt = admin.firestore.FieldValue.serverTimestamp();
    }

    await jobRef.update(updateData);
    await projectRef.update({
      'export.progress': stage.progress,
      'export.status': stage.status,
      'export.currentStage': stage.message,
      updatedAt: admin.firestore.FieldValue.serverTimestamp()
    });
  }
}

/**
 * creationWizardCheckExportStatus - Check export job status
 */
exports.creationWizardCheckExportStatus = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const { jobId, projectId } = data;

  if (!jobId && !projectId) {
    throw new functions.https.HttpsError('invalid-argument', 'Job ID or Project ID required');
  }

  try {
    let jobDoc;

    if (jobId) {
      jobDoc = await db.collection('creationExportJobs').doc(jobId).get();
    } else {
      // Find job by project ID
      const jobsQuery = await db.collection('creationExportJobs')
        .where('projectId', '==', projectId)
        .where('userId', '==', uid)
        .orderBy('createdAt', 'desc')
        .limit(1)
        .get();

      if (!jobsQuery.empty) {
        jobDoc = jobsQuery.docs[0];
      }
    }

    if (!jobDoc || !jobDoc.exists) {
      return {
        success: true,
        status: 'not_found',
        message: 'No export job found'
      };
    }

    const job = jobDoc.data();

    // Verify ownership
    if (job.userId !== uid) {
      throw new functions.https.HttpsError('permission-denied', 'Not authorized');
    }

    return {
      success: true,
      jobId: job.id,
      status: job.status,
      progress: job.progress || 0,
      currentStage: job.currentStage || null,
      outputUrl: job.outputUrl || null,
      error: job.error || null,
      totalDuration: job.totalDuration,
      quality: job.quality,
      // Additional export type info
      exportType: job.exportType || null,
      exportNote: job.exportNote || null,
      imageUrls: job.imageUrls || null,
      previewThumbnail: job.previewThumbnail || null,
      imagesReady: job.imagesReady || false,
      // Scene progress tracking for parallel processing UI
      scenesCompleted: job.scenesCompleted || 0,
      scenesTotal: job.scenesTotal || 0,
      sceneStatuses: job.sceneStatuses || [],
      lastSceneCompleted: job.lastSceneCompleted || 0,
      createdAt: job.createdAt?.toDate?.()?.toISOString() || null,
      completedAt: job.completedAt?.toDate?.()?.toISOString() || null
    };

  } catch (error) {
    console.error('[creationWizardCheckExportStatus] Error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to check export status'));
  }
});

/**
 * creationWizardGetExportHistory - Get user's export history
 */
exports.creationWizardGetExportHistory = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const { limit: queryLimit = 10 } = data || {};

  try {
    const jobsQuery = await db.collection('creationExportJobs')
      .where('userId', '==', uid)
      .where('status', '==', 'completed')
      .orderBy('completedAt', 'desc')
      .limit(Math.min(queryLimit, 50))
      .get();

    const exports = jobsQuery.docs.map(doc => {
      const job = doc.data();
      return {
        jobId: job.id,
        projectId: job.projectId,
        outputUrl: job.outputUrl,
        quality: job.quality,
        format: job.format,
        totalDuration: job.totalDuration,
        completedAt: job.completedAt?.toDate?.()?.toISOString() || null
      };
    });

    return {
      success: true,
      exports,
      count: exports.length
    };

  } catch (error) {
    console.error('[creationWizardGetExportHistory] Error:', error);
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to get export history'));
  }
});

/**
 * creationWizardCancelExport - Cancel an in-progress export job
 */
exports.creationWizardCancelExport = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const { jobId } = data;

  if (!jobId) {
    throw new functions.https.HttpsError('invalid-argument', 'Job ID required');
  }

  try {
    const jobRef = db.collection('creationExportJobs').doc(jobId);
    const jobDoc = await jobRef.get();

    if (!jobDoc.exists) {
      throw new functions.https.HttpsError('not-found', 'Export job not found');
    }

    const job = jobDoc.data();
    if (job.userId !== uid) {
      throw new functions.https.HttpsError('permission-denied', 'Not authorized');
    }

    if (job.status === 'completed') {
      throw new functions.https.HttpsError('failed-precondition', 'Cannot cancel completed export');
    }

    // Update job status
    await jobRef.update({
      status: 'cancelled',
      cancelledAt: admin.firestore.FieldValue.serverTimestamp(),
      updatedAt: admin.firestore.FieldValue.serverTimestamp()
    });

    // Update project export status
    const projectRef = db.collection('creationProjects').doc(job.projectId);
    await projectRef.update({
      'export.status': 'cancelled',
      'export.jobId': null,
      updatedAt: admin.firestore.FieldValue.serverTimestamp()
    });

    return { success: true, message: 'Export cancelled' };

  } catch (error) {
    console.error('[creationWizardCancelExport] Error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to cancel export'));
  }
});

// ==============================================
// STOCK MEDIA API INTEGRATION
// Free royalty-free media from Pexels, Pixabay
// ==============================================

/**
 * generateSmartStockQueries - AI-powered stock search query generator
 *
 * Uses Gemini to understand the visual intent of a scene description
 * and generate optimized search queries for stock media sites.
 *
 * @param {string} sceneDescription - The visual description of the scene
 * @param {string} narration - Optional narration text for context
 * @param {string} mediaType - 'image' | 'video' - affects query optimization
 */
exports.generateSmartStockQueries = functions.https.onCall(async (data, context) => {
  await verifyAuth(context);

  const { sceneDescription, narration = '', mediaType = 'image' } = data;

  if (!sceneDescription || sceneDescription.trim().length < 5) {
    throw new functions.https.HttpsError('invalid-argument', 'Scene description is required');
  }

  const geminiKey = functions.config().gemini?.key;
  if (!geminiKey) {
    // Fallback to basic extraction if no Gemini key
    console.log('[generateSmartStockQueries] No Gemini key, using fallback');
    return {
      success: true,
      queries: [extractBasicKeywords(sceneDescription)],
      primaryQuery: extractBasicKeywords(sceneDescription),
      category: 'general',
      fallback: true
    };
  }

  try {
    const genAI = new GoogleGenAI({ apiKey: geminiKey });

    const prompt = `You are an expert at finding stock ${mediaType}s. Analyze this scene description and generate optimal search queries for stock media websites (Pexels, Pixabay).

SCENE DESCRIPTION: "${sceneDescription}"
${narration ? `NARRATION CONTEXT: "${narration}"` : ''}
MEDIA TYPE: ${mediaType}

Your task:
1. Understand the VISUAL INTENT - what should the viewer SEE (not abstract concepts)
2. Generate search queries that will find RELEVANT stock ${mediaType}s
3. Focus on concrete visual elements, not metaphors

IMPORTANT RULES:
- Stock sites have photos/videos of REAL things - not abstract concepts
- "bar graph" should search for "chart", "data visualization", "statistics screen" - NOT "bar" (drinking establishment)
- "dollar signs" should search for "money", "currency", "finance", "cash" - NOT abstract symbols
- "animated" is usually NOT searchable - focus on the SUBJECT instead
- Think about what a photographer would actually capture
- For abstract concepts, find visual METAPHORS (e.g., "growth" â†’ "plant growing", "arrow up", "stairs")

Return a JSON object with this exact structure:
{
  "primaryQuery": "the single best 2-3 word search query",
  "alternativeQueries": ["query2", "query3", "query4"],
  "category": "one of: nature, business, technology, people, city, abstract, motion, aerial, food, health, education, entertainment",
  "visualElements": ["main visual element 1", "element 2"],
  "avoid": ["words that might return wrong results"]
}

Return ONLY the JSON object, no markdown or explanation.`;

    const result = await genAI.models.generateContent({
      model: 'gemini-2.0-flash',
      contents: prompt
    });

    const responseText = result.text.trim();

    // Parse JSON from response (handle potential markdown wrapping)
    let jsonStr = responseText;
    if (responseText.includes('```')) {
      const jsonMatch = responseText.match(/```(?:json)?\s*([\s\S]*?)```/);
      if (jsonMatch) jsonStr = jsonMatch[1].trim();
    }

    const parsed = JSON.parse(jsonStr);

    // Validate and structure the response
    const queries = [
      parsed.primaryQuery,
      ...(parsed.alternativeQueries || [])
    ].filter(q => q && typeof q === 'string' && q.trim().length > 0).slice(0, 5);

    return {
      success: true,
      primaryQuery: parsed.primaryQuery || queries[0] || 'cinematic background',
      queries: queries,
      category: parsed.category || 'general',
      visualElements: parsed.visualElements || [],
      avoid: parsed.avoid || [],
      aiGenerated: true
    };

  } catch (error) {
    console.error('[generateSmartStockQueries] Error:', error);

    // Fallback to basic extraction on error
    const fallbackQuery = extractBasicKeywords(sceneDescription);
    return {
      success: true,
      queries: [fallbackQuery],
      primaryQuery: fallbackQuery,
      category: 'general',
      fallback: true,
      error: error.message
    };
  }
});

// Helper function for basic keyword extraction (fallback)
function extractBasicKeywords(text) {
  const stopWords = new Set(['the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been', 'being',
    'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might',
    'must', 'shall', 'can', 'need', 'to', 'of', 'in', 'for', 'on', 'with', 'at', 'by', 'from',
    'up', 'about', 'into', 'over', 'after', 'show', 'showing', 'shows', 'scene', 'image', 'video',
    'clip', 'animated', 'animation', 'illustration', 'style', 'mood', 'feeling']);

  const words = text.toLowerCase()
    .replace(/[^a-z\s]/g, '')
    .split(/\s+/)
    .filter(word => word.length > 2 && !stopWords.has(word));

  // Return first 3 meaningful words
  return words.slice(0, 3).join(' ') || 'cinematic background';
}

/**
 * searchStockMedia - Search for royalty-free images and videos
 * Sources: Pexels, Pixabay
 *
 * @param {string} query - Search query
 * @param {string} type - 'image' | 'video'
 * @param {string} source - 'pexels' | 'pixabay' | 'all'
 * @param {object} filters - { orientation, size, color, page, perPage }
 */
exports.searchStockMedia = functions.https.onCall(async (data, context) => {
  await verifyAuth(context);

  const {
    query,
    type = 'image',
    source = 'all',
    filters = {}
  } = data;

  if (!query || query.trim().length < 2) {
    throw new functions.https.HttpsError('invalid-argument', 'Search query required (min 2 chars)');
  }

  const results = {
    pexels: [],
    pixabay: [],
    total: 0
  };

  const { orientation = 'landscape', page = 1, perPage = 20 } = filters;

  // Pexels API
  const pexelsKey = functions.config().pexels?.key;
  if (pexelsKey && (source === 'pexels' || source === 'all')) {
    try {
      const pexelsEndpoint = type === 'video'
        ? 'https://api.pexels.com/videos/search'
        : 'https://api.pexels.com/v1/search';

      const pexelsResponse = await axios.get(pexelsEndpoint, {
        params: {
          query: query,
          orientation: orientation,
          page: page,
          per_page: Math.min(perPage, 40)
        },
        headers: {
          'Authorization': pexelsKey
        },
        timeout: 10000
      });

      if (type === 'video') {
        results.pexels = (pexelsResponse.data.videos || []).map(v => {
          const hdVideo = v.video_files?.find(f => f.quality === 'hd');
          const sdVideo = v.video_files?.find(f => f.quality === 'sd');
          const videoUrl = hdVideo?.link || sdVideo?.link || v.video_files?.[0]?.link;
          return {
            id: `pexels-${v.id}`,
            source: 'pexels',
            type: 'video',
            thumbnail: v.image, // Pexels provides image thumbnail
            preview: sdVideo?.link || v.video_files?.[0]?.link,
            url: videoUrl,
            videoUrl: videoUrl, // Explicit video URL field
            width: v.width,
            height: v.height,
            duration: v.duration,
            author: v.user?.name || 'Pexels',
            authorUrl: v.user?.url,
            license: 'Pexels License (Free)',
            originalUrl: v.url
          };
        });
      } else {
        results.pexels = (pexelsResponse.data.photos || []).map(p => ({
          id: `pexels-${p.id}`,
          source: 'pexels',
          type: 'image',
          thumbnail: p.src?.small || p.src?.medium,
          preview: p.src?.medium || p.src?.large,
          url: p.src?.large2x || p.src?.original,
          width: p.width,
          height: p.height,
          author: p.photographer,
          authorUrl: p.photographer_url,
          license: 'Pexels License (Free)',
          originalUrl: p.url,
          avgColor: p.avg_color
        }));
      }

      results.total += pexelsResponse.data.total_results || results.pexels.length;
    } catch (pexelsError) {
      console.error('[searchStockMedia] Pexels error:', pexelsError.message);
    }
  }

  // Pixabay API
  const pixabayKey = functions.config().pixabay?.key;
  if (pixabayKey && (source === 'pixabay' || source === 'all')) {
    try {
      const pixabayEndpoint = type === 'video'
        ? 'https://pixabay.com/api/videos/'
        : 'https://pixabay.com/api/';

      const pixabayParams = {
        key: pixabayKey,
        q: query,
        orientation: orientation === 'landscape' ? 'horizontal' : orientation === 'portrait' ? 'vertical' : 'all',
        page: page,
        per_page: Math.min(perPage, 40),
        safesearch: true
      };

      const pixabayResponse = await axios.get(pixabayEndpoint, {
        params: pixabayParams,
        timeout: 10000
      });

      if (type === 'video') {
        results.pixabay = (pixabayResponse.data.hits || []).map(v => {
          const videoUrl = v.videos?.large?.url || v.videos?.medium?.url || v.videos?.small?.url;
          // Pixabay video thumbnails - try multiple formats
          // picture_id can be used with Vimeo CDN or we use a fallback
          const thumbnailFormats = [
            `https://i.vimeocdn.com/video/${v.picture_id}_640x360.jpg`,
            `https://i.vimeocdn.com/video/${v.picture_id}_295x166.jpg`,
            v.userImageURL // Fallback to user image if needed
          ];
          return {
            id: `pixabay-${v.id}`,
            source: 'pixabay',
            type: 'video',
            thumbnail: thumbnailFormats[0],
            thumbnailFallback: thumbnailFormats[1], // Provide fallback
            preview: v.videos?.small?.url || v.videos?.tiny?.url,
            url: videoUrl,
            videoUrl: videoUrl, // Explicit video URL field
            width: v.videos?.large?.width || v.videos?.medium?.width || 1920,
            height: v.videos?.large?.height || v.videos?.medium?.height || 1080,
            duration: v.duration,
            author: v.user,
            authorUrl: `https://pixabay.com/users/${v.user}-${v.user_id}/`,
            license: 'Pixabay License (Free)',
            originalUrl: v.pageURL,
            tags: v.tags,
            pictureId: v.picture_id // Include for debugging
          };
        });
      } else {
        results.pixabay = (pixabayResponse.data.hits || []).map(p => ({
          id: `pixabay-${p.id}`,
          source: 'pixabay',
          type: 'image',
          thumbnail: p.previewURL,
          preview: p.webformatURL,
          url: p.largeImageURL,
          width: p.imageWidth,
          height: p.imageHeight,
          author: p.user,
          authorUrl: `https://pixabay.com/users/${p.user}-${p.user_id}/`,
          license: 'Pixabay License (Free)',
          originalUrl: p.pageURL,
          tags: p.tags
        }));
      }

      results.total += pixabayResponse.data.totalHits || results.pixabay.length;
    } catch (pixabayError) {
      console.error('[searchStockMedia] Pixabay error:', pixabayError.message);
    }
  }

  // Combine and interleave results
  const combined = [];
  const maxLen = Math.max(results.pexels.length, results.pixabay.length);
  for (let i = 0; i < maxLen; i++) {
    if (results.pexels[i]) combined.push(results.pexels[i]);
    if (results.pixabay[i]) combined.push(results.pixabay[i]);
  }

  return {
    success: true,
    results: combined,
    total: results.total,
    page,
    perPage,
    sources: {
      pexels: results.pexels.length,
      pixabay: results.pixabay.length
    }
  };
});

/**
 * searchStockMusic - Search for royalty-free music
 * Source: Pixabay Music
 *
 * @param {string} query - Search query
 * @param {object} filters - { genre, mood, duration, page, perPage }
 */
exports.searchStockMusic = functions.https.onCall(async (data, context) => {
  await verifyAuth(context);

  const { query = '', filters = {} } = data;
  const { genre, mood, minDuration, maxDuration, page = 1, perPage = 20 } = filters;

  const pixabayKey = functions.config().pixabay?.key;
  if (!pixabayKey) {
    throw new functions.https.HttpsError('failed-precondition', 'Pixabay API key not configured');
  }

  try {
    // Build search query
    let searchQuery = query || 'background music';
    if (genre) searchQuery += ` ${genre}`;
    if (mood) searchQuery += ` ${mood}`;

    const response = await axios.get('https://pixabay.com/api/', {
      params: {
        key: pixabayKey,
        q: searchQuery,
        // Pixabay doesn't have a dedicated music endpoint in free API
        // Using image search as placeholder - in production use Pixabay Music or Freesound
        page: page,
        per_page: Math.min(perPage, 40),
        safesearch: true
      },
      timeout: 10000
    });

    // For now, return curated music library since Pixabay free API doesn't include music
    // In production, integrate with Pixabay Music API or Freesound
    const curatedTracks = MUSIC_LIBRARY.filter(track => {
      const matchesQuery = !query ||
        track.name.toLowerCase().includes(query.toLowerCase()) ||
        track.tags.some(t => t.toLowerCase().includes(query.toLowerCase()));
      const matchesMood = !mood || track.mood.toLowerCase().includes(mood.toLowerCase());
      const matchesDuration = (!minDuration || track.duration >= minDuration) &&
                              (!maxDuration || track.duration <= maxDuration);
      return matchesQuery && matchesMood && matchesDuration;
    });

    return {
      success: true,
      results: curatedTracks.slice((page - 1) * perPage, page * perPage),
      total: curatedTracks.length,
      page,
      perPage,
      note: 'Using curated library. For full music search, configure Pixabay Music API.'
    };

  } catch (error) {
    console.error('[searchStockMusic] Error:', error);
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to search music'));
  }
});

// ==============================================
// AUDIO INTELLIGENCE - PHASE 3: STOCK AUDIO API
// ==============================================

/**
 * Enhanced Music Library with preview URLs and detailed metadata
 * Using royalty-free sources (Pixabay, Freesound)
 */
const ENHANCED_MUSIC_LIBRARY = [
  // Corporate & Business
  {
    id: 'corporate-inspiring-01',
    name: 'Inspiring Corporate',
    artist: 'Stock Audio',
    category: 'corporate',
    subcategory: 'inspiring',
    mood: 'Uplifting, Professional',
    energy: 0.7,
    duration: 147,
    bpm: 110,
    key: 'C major',
    tags: ['corporate', 'business', 'inspiring', 'presentation', 'success'],
    instruments: ['piano', 'strings', 'light percussion'],
    previewUrl: null, // Will be populated from API or storage
    source: 'curated',
    license: 'royalty-free'
  },
  {
    id: 'corporate-tech-01',
    name: 'Tech Innovation',
    artist: 'Stock Audio',
    category: 'corporate',
    subcategory: 'tech',
    mood: 'Modern, Innovative',
    energy: 0.6,
    duration: 135,
    bpm: 118,
    key: 'G major',
    tags: ['tech', 'innovation', 'startup', 'digital', 'modern'],
    instruments: ['synth', 'electronic drums', 'bass'],
    previewUrl: null,
    source: 'curated',
    license: 'royalty-free'
  },
  {
    id: 'corporate-motivational-01',
    name: 'Motivational Achievement',
    artist: 'Stock Audio',
    category: 'corporate',
    subcategory: 'motivational',
    mood: 'Powerful, Triumphant',
    energy: 0.8,
    duration: 180,
    bpm: 125,
    key: 'D major',
    tags: ['motivational', 'achievement', 'success', 'triumph', 'corporate'],
    instruments: ['orchestra', 'drums', 'brass'],
    previewUrl: null,
    source: 'curated',
    license: 'royalty-free'
  },

  // Cinematic & Epic
  {
    id: 'cinematic-epic-01',
    name: 'Epic Cinematic Trailer',
    artist: 'Stock Audio',
    category: 'cinematic',
    subcategory: 'epic',
    mood: 'Epic, Powerful',
    energy: 0.9,
    duration: 120,
    bpm: 95,
    key: 'D minor',
    tags: ['cinematic', 'epic', 'trailer', 'movie', 'dramatic'],
    instruments: ['orchestra', 'choir', 'percussion', 'brass'],
    previewUrl: null,
    source: 'curated',
    license: 'royalty-free'
  },
  {
    id: 'cinematic-emotional-01',
    name: 'Emotional Piano',
    artist: 'Stock Audio',
    category: 'cinematic',
    subcategory: 'emotional',
    mood: 'Emotional, Moving',
    energy: 0.5,
    duration: 195,
    bpm: 72,
    key: 'A minor',
    tags: ['emotional', 'piano', 'sad', 'touching', 'cinematic'],
    instruments: ['piano', 'strings'],
    previewUrl: null,
    source: 'curated',
    license: 'royalty-free'
  },
  {
    id: 'cinematic-tension-01',
    name: 'Building Tension',
    artist: 'Stock Audio',
    category: 'cinematic',
    subcategory: 'tension',
    mood: 'Suspenseful, Intense',
    energy: 0.7,
    duration: 150,
    bpm: 85,
    key: 'E minor',
    tags: ['tension', 'suspense', 'thriller', 'mystery', 'dark'],
    instruments: ['strings', 'synth', 'percussion'],
    previewUrl: null,
    source: 'curated',
    license: 'royalty-free'
  },

  // Ambient & Chill
  {
    id: 'ambient-peaceful-01',
    name: 'Peaceful Meditation',
    artist: 'Stock Audio',
    category: 'ambient',
    subcategory: 'peaceful',
    mood: 'Calm, Relaxing',
    energy: 0.2,
    duration: 240,
    bpm: 60,
    key: 'F major',
    tags: ['ambient', 'meditation', 'peaceful', 'relaxing', 'calm'],
    instruments: ['synth pads', 'nature sounds'],
    previewUrl: null,
    source: 'curated',
    license: 'royalty-free'
  },
  {
    id: 'ambient-lofi-01',
    name: 'Lo-Fi Chill Beats',
    artist: 'Stock Audio',
    category: 'ambient',
    subcategory: 'lofi',
    mood: 'Chill, Relaxed',
    energy: 0.4,
    duration: 180,
    bpm: 85,
    key: 'C major',
    tags: ['lofi', 'chill', 'study', 'beats', 'relaxed'],
    instruments: ['vinyl crackle', 'piano', 'drums'],
    previewUrl: null,
    source: 'curated',
    license: 'royalty-free'
  },

  // Electronic & EDM
  {
    id: 'electronic-energetic-01',
    name: 'Electronic Energy',
    artist: 'Stock Audio',
    category: 'electronic',
    subcategory: 'energetic',
    mood: 'High Energy, Exciting',
    energy: 0.9,
    duration: 165,
    bpm: 140,
    key: 'F minor',
    tags: ['electronic', 'edm', 'energy', 'dance', 'exciting'],
    instruments: ['synth', 'bass', 'drums'],
    previewUrl: null,
    source: 'curated',
    license: 'royalty-free'
  },
  {
    id: 'electronic-future-01',
    name: 'Future Bass',
    artist: 'Stock Audio',
    category: 'electronic',
    subcategory: 'future',
    mood: 'Modern, Uplifting',
    energy: 0.8,
    duration: 150,
    bpm: 150,
    key: 'G minor',
    tags: ['future bass', 'electronic', 'modern', 'uplifting'],
    instruments: ['synth', 'vocal chops', 'bass'],
    previewUrl: null,
    source: 'curated',
    license: 'royalty-free'
  },

  // Acoustic & Indie
  {
    id: 'acoustic-warm-01',
    name: 'Warm Acoustic',
    artist: 'Stock Audio',
    category: 'acoustic',
    subcategory: 'warm',
    mood: 'Warm, Friendly',
    energy: 0.5,
    duration: 175,
    bpm: 95,
    key: 'G major',
    tags: ['acoustic', 'guitar', 'warm', 'friendly', 'organic'],
    instruments: ['acoustic guitar', 'light percussion'],
    previewUrl: null,
    source: 'curated',
    license: 'royalty-free'
  },
  {
    id: 'acoustic-upbeat-01',
    name: 'Upbeat Acoustic',
    artist: 'Stock Audio',
    category: 'acoustic',
    subcategory: 'upbeat',
    mood: 'Happy, Positive',
    energy: 0.7,
    duration: 145,
    bpm: 115,
    key: 'C major',
    tags: ['acoustic', 'happy', 'positive', 'cheerful', 'upbeat'],
    instruments: ['acoustic guitar', 'ukulele', 'claps'],
    previewUrl: null,
    source: 'curated',
    license: 'royalty-free'
  },

  // Dark & Horror
  {
    id: 'dark-horror-01',
    name: 'Dark Horror Ambience',
    artist: 'Stock Audio',
    category: 'dark',
    subcategory: 'horror',
    mood: 'Dark, Eerie',
    energy: 0.4,
    duration: 200,
    bpm: 70,
    key: 'D minor',
    tags: ['horror', 'dark', 'eerie', 'scary', 'suspense'],
    instruments: ['drones', 'strings', 'fx'],
    previewUrl: null,
    source: 'curated',
    license: 'royalty-free'
  },
  {
    id: 'dark-mysterious-01',
    name: 'Mysterious Dark',
    artist: 'Stock Audio',
    category: 'dark',
    subcategory: 'mysterious',
    mood: 'Mysterious, Intriguing',
    energy: 0.5,
    duration: 165,
    bpm: 80,
    key: 'B minor',
    tags: ['mysterious', 'dark', 'intrigue', 'cinematic'],
    instruments: ['piano', 'strings', 'synth'],
    previewUrl: null,
    source: 'curated',
    license: 'royalty-free'
  },

  // News & Documentary
  {
    id: 'news-urgent-01',
    name: 'Breaking News',
    artist: 'Stock Audio',
    category: 'news',
    subcategory: 'urgent',
    mood: 'Urgent, Professional',
    energy: 0.6,
    duration: 90,
    bpm: 120,
    key: 'C major',
    tags: ['news', 'breaking', 'urgent', 'broadcast', 'professional'],
    instruments: ['synth', 'percussion'],
    previewUrl: null,
    source: 'curated',
    license: 'royalty-free'
  },
  {
    id: 'documentary-nature-01',
    name: 'Nature Documentary',
    artist: 'Stock Audio',
    category: 'documentary',
    subcategory: 'nature',
    mood: 'Majestic, Natural',
    energy: 0.6,
    duration: 210,
    bpm: 85,
    key: 'G major',
    tags: ['documentary', 'nature', 'wildlife', 'majestic', 'earth'],
    instruments: ['orchestra', 'ethnic percussion', 'flute'],
    previewUrl: null,
    source: 'curated',
    license: 'royalty-free'
  }
];

/**
 * Music categories with metadata
 */
const MUSIC_CATEGORIES = [
  { id: 'corporate', name: 'Corporate & Business', icon: 'ðŸ’¼', count: 0 },
  { id: 'cinematic', name: 'Cinematic & Epic', icon: 'ðŸŽ¬', count: 0 },
  { id: 'ambient', name: 'Ambient & Chill', icon: 'ðŸŒ¿', count: 0 },
  { id: 'electronic', name: 'Electronic & EDM', icon: 'ðŸŽ§', count: 0 },
  { id: 'acoustic', name: 'Acoustic & Indie', icon: 'ðŸŽ¸', count: 0 },
  { id: 'dark', name: 'Dark & Horror', icon: 'ðŸŒ‘', count: 0 },
  { id: 'news', name: 'News & Documentary', icon: 'ðŸ“°', count: 0 },
  { id: 'documentary', name: 'Documentary', icon: 'ðŸŽ¥', count: 0 }
];

/**
 * searchEnhancedMusicLibrary - Advanced music search with AI matching
 */
exports.searchEnhancedMusicLibrary = functions.https.onCall(async (data, context) => {
  await verifyAuth(context);

  const {
    query = '',
    category,
    subcategory,
    mood,
    minBpm,
    maxBpm,
    minDuration,
    maxDuration,
    energy, // 0-1 scale
    tags = [],
    sortBy = 'relevance', // 'relevance', 'duration', 'bpm', 'energy'
    page = 1,
    perPage = 20
  } = data;

  try {
    let results = [...ENHANCED_MUSIC_LIBRARY, ...MUSIC_LIBRARY];

    // Text search
    if (query) {
      const queryLower = query.toLowerCase();
      results = results.filter(track =>
        track.name.toLowerCase().includes(queryLower) ||
        (track.mood || '').toLowerCase().includes(queryLower) ||
        (track.tags || []).some(t => t.toLowerCase().includes(queryLower)) ||
        (track.instruments || []).some(i => i.toLowerCase().includes(queryLower))
      );
    }

    // Category filter
    if (category) {
      results = results.filter(track => track.category === category);
    }

    // Subcategory filter
    if (subcategory) {
      results = results.filter(track => track.subcategory === subcategory);
    }

    // Mood filter
    if (mood) {
      const moodLower = mood.toLowerCase();
      results = results.filter(track =>
        (track.mood || '').toLowerCase().includes(moodLower)
      );
    }

    // BPM range
    if (minBpm !== undefined) {
      results = results.filter(track => !track.bpm || track.bpm >= minBpm);
    }
    if (maxBpm !== undefined) {
      results = results.filter(track => !track.bpm || track.bpm <= maxBpm);
    }

    // Duration range
    if (minDuration !== undefined) {
      results = results.filter(track => track.duration >= minDuration);
    }
    if (maxDuration !== undefined) {
      results = results.filter(track => track.duration <= maxDuration);
    }

    // Energy filter (with tolerance)
    if (energy !== undefined) {
      const tolerance = 0.2;
      results = results.filter(track =>
        !track.energy || Math.abs(track.energy - energy) <= tolerance
      );
    }

    // Tag filtering
    if (tags.length > 0) {
      results = results.filter(track =>
        track.tags && tags.some(tag =>
          track.tags.some(t => t.toLowerCase().includes(tag.toLowerCase()))
        )
      );
    }

    // Sorting
    switch (sortBy) {
      case 'duration':
        results.sort((a, b) => a.duration - b.duration);
        break;
      case 'bpm':
        results.sort((a, b) => (a.bpm || 0) - (b.bpm || 0));
        break;
      case 'energy':
        results.sort((a, b) => (b.energy || 0.5) - (a.energy || 0.5));
        break;
      case 'relevance':
      default:
        // Keep original order (most relevant first from search)
        break;
    }

    // Pagination
    const total = results.length;
    const paginatedResults = results.slice((page - 1) * perPage, page * perPage);

    // Calculate category counts
    const categoryCounts = {};
    [...ENHANCED_MUSIC_LIBRARY, ...MUSIC_LIBRARY].forEach(track => {
      categoryCounts[track.category] = (categoryCounts[track.category] || 0) + 1;
    });

    const categoriesWithCounts = MUSIC_CATEGORIES.map(cat => ({
      ...cat,
      count: categoryCounts[cat.id] || 0
    }));

    return {
      success: true,
      results: paginatedResults,
      total,
      page,
      perPage,
      categories: categoriesWithCounts,
      filters: {
        category,
        subcategory,
        mood,
        minBpm,
        maxBpm,
        energy
      }
    };

  } catch (error) {
    console.error('[searchEnhancedMusicLibrary] Error:', error);
    throw new functions.https.HttpsError('internal', 'Failed to search music library');
  }
});

/**
 * getAudioRecommendationsForScene - Get AI-powered audio recommendations for specific scenes
 */
exports.getAudioRecommendationsForScene = functions.https.onCall(async (data, context) => {
  await verifyAuth(context);

  const {
    sceneId,
    sceneContent, // { visual, narration, mood }
    genre,
    pacing,
    position, // 0-1 position in video
    emotionalPhase
  } = data;

  try {
    // Get genre config
    const genreConfig = GENRE_AUDIO_MAPPING[genre] || GENRE_AUDIO_MAPPING['general'];

    // Calculate energy based on position and emotional phase
    const emotionalCurve = EMOTIONAL_AUDIO_CURVES['hero-journey'];
    let targetEnergy = 0.5;
    for (const phase of emotionalCurve.phases) {
      if (position >= phase.position) {
        targetEnergy = phase.energy;
      }
    }

    // Find best matching tracks
    const allTracks = [...ENHANCED_MUSIC_LIBRARY, ...MUSIC_LIBRARY];
    const scoredTracks = allTracks.map(track => {
      let score = 0;

      // Genre mood matching
      for (const genreMood of genreConfig.musicMoods) {
        if ((track.mood || '').toLowerCase().includes(genreMood)) {
          score += 25;
        }
      }

      // Category matching
      if (genreConfig.musicCategories.includes(track.category)) {
        score += 20;
      }

      // Energy matching
      if (track.energy) {
        const energyDiff = Math.abs(track.energy - targetEnergy);
        score += Math.max(0, 15 - energyDiff * 30);
      }

      // BPM matching
      const pacingConfig = PACING_BPM_CONFIG[pacing] || PACING_BPM_CONFIG['balanced'];
      if (track.bpm && track.bpm >= pacingConfig.bpmRange.min && track.bpm <= pacingConfig.bpmRange.max) {
        score += 15;
      }

      return { ...track, sceneMatchScore: Math.round(score) };
    });

    // Sort by score and get top recommendations
    const recommendations = scoredTracks
      .filter(t => t.sceneMatchScore > 20)
      .sort((a, b) => b.sceneMatchScore - a.sceneMatchScore)
      .slice(0, 5);

    // Get SFX recommendations
    const sfxRecommendations = genreConfig.sfxTypes.slice(0, 3).map(sfxType => {
      return SFX_LIBRARY.find(s => s.id === sfxType || s.id.includes(sfxType));
    }).filter(Boolean);

    // Get ambience recommendations
    const ambienceRecommendations = genreConfig.ambienceTypes.slice(0, 2).map(ambType => {
      return AMBIENCE_LIBRARY.find(a => a.id === ambType || a.id.includes(ambType));
    }).filter(Boolean);

    return {
      success: true,
      sceneId,
      recommendations: {
        music: recommendations,
        sfx: sfxRecommendations,
        ambience: ambienceRecommendations,
        suggestedEnergy: targetEnergy,
        emotionalPhase
      }
    };

  } catch (error) {
    console.error('[getAudioRecommendationsForScene] Error:', error);
    throw new functions.https.HttpsError('internal', 'Failed to get audio recommendations');
  }
});

/**
 * importStockAudio - Download and cache stock audio to Firebase Storage
 */
exports.importStockAudio = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);

  const {
    audioId,
    sourceUrl,
    type = 'music', // 'music' | 'sfx' | 'ambience'
    projectId,
    metadata = {}
  } = data;

  if (!sourceUrl) {
    throw new functions.https.HttpsError('invalid-argument', 'Audio source URL required');
  }

  try {
    // Download the audio file
    const response = await axios.get(sourceUrl, {
      responseType: 'arraybuffer',
      timeout: 60000,
      headers: {
        'User-Agent': 'Mozilla/5.0 (compatible; VideoWizard/1.0)'
      }
    });

    const buffer = Buffer.from(response.data);
    const contentType = response.headers['content-type'] || 'audio/mpeg';

    // Determine file extension
    let extension = 'mp3';
    if (contentType.includes('wav')) extension = 'wav';
    else if (contentType.includes('ogg')) extension = 'ogg';
    else if (contentType.includes('m4a') || contentType.includes('mp4')) extension = 'm4a';

    // Generate storage path
    const timestamp = Date.now();
    const filename = `${audioId || 'audio'}-${timestamp}.${extension}`;
    const storagePath = `users/${uid}/audio/${type}/${filename}`;

    // Upload to Firebase Storage
    const bucket = admin.storage().bucket();
    const file = bucket.file(storagePath);

    await file.save(buffer, {
      metadata: {
        contentType,
        metadata: {
          originalUrl: sourceUrl,
          audioId,
          type,
          projectId: projectId || '',
          ...metadata
        }
      }
    });

    // Make file publicly accessible (or use signed URL)
    await file.makePublic();
    const publicUrl = `https://storage.googleapis.com/${bucket.name}/${storagePath}`;

    // Also generate a signed URL for preview (expires in 1 hour)
    const [signedUrl] = await file.getSignedUrl({
      action: 'read',
      expires: Date.now() + 3600000 // 1 hour
    });

    return {
      success: true,
      audioId,
      type,
      url: publicUrl,
      previewUrl: signedUrl,
      storagePath,
      contentType,
      size: buffer.length
    };

  } catch (error) {
    console.error('[importStockAudio] Error:', error);
    throw new functions.https.HttpsError('internal', 'Failed to import audio');
  }
});

// ==============================================
// FREESOUND.ORG AUDIO API INTEGRATION
// ==============================================

/**
 * searchFreesoundAudio - Search Freesound.org for music, SFX, and ambience
 * Freesound provides royalty-free audio with preview URLs
 *
 * @param {string} query - Search term
 * @param {object} filters - { type, minDuration, maxDuration, page, perPage }
 * type: 'music' | 'sfx' | 'ambience' | 'all'
 */
exports.searchFreesoundAudio = functions.https.onCall(async (data, context) => {
  await verifyAuth(context);

  const { query = '', filters = {} } = data;
  const {
    type = 'all',
    minDuration,
    maxDuration,
    page = 1,
    perPage = 20,
    sort = 'score' // 'score', 'duration_asc', 'duration_desc', 'created_asc', 'created_desc', 'downloads_asc', 'downloads_desc'
  } = filters;

  const freesoundKey = functions.config().freesound?.key;
  if (!freesoundKey) {
    // Fallback to curated library if no API key
    console.log('[searchFreesoundAudio] No API key, using curated library fallback');
    return searchCuratedAudioLibrary(query, filters);
  }

  try {
    // Build filter string for Freesound
    let filterParts = [];

    // Duration filter
    if (minDuration || maxDuration) {
      const min = minDuration || 0;
      const max = maxDuration || 600; // 10 minutes max
      filterParts.push(`duration:[${min} TO ${max}]`);
    }

    // Type-specific tags
    const typeTagMap = {
      'music': 'music OR song OR melody OR instrumental OR background-music',
      'sfx': 'sound-effect OR sfx OR foley OR transition OR whoosh OR impact',
      'ambience': 'ambience OR ambient OR atmosphere OR nature OR environment OR background'
    };

    if (type !== 'all' && typeTagMap[type]) {
      filterParts.push(`tag:(${typeTagMap[type]})`);
    }

    // Build the search query
    let searchQuery = query || (type === 'music' ? 'background music' : type === 'sfx' ? 'sound effect' : type === 'ambience' ? 'ambient' : 'audio');

    // Request fields we need
    const fields = 'id,name,description,duration,tags,previews,username,license,avg_rating,num_ratings,num_downloads';

    const response = await axios.get('https://freesound.org/apiv2/search/text/', {
      params: {
        token: freesoundKey,
        query: searchQuery,
        filter: filterParts.join(' '),
        fields: fields,
        sort: sort,
        page: page,
        page_size: Math.min(perPage, 50) // Freesound max is 150, but we use 50
      },
      timeout: 15000
    });

    const results = (response.data.results || []).map(sound => ({
      id: `freesound-${sound.id}`,
      name: sound.name,
      description: sound.description?.substring(0, 200) || '',
      duration: Math.round(sound.duration),
      tags: sound.tags?.slice(0, 10) || [],
      author: sound.username,
      license: sound.license,
      rating: sound.avg_rating,
      downloads: sound.num_downloads,
      source: 'freesound',
      // Preview URLs - Freesound provides mp3 and ogg previews
      previewUrl: sound.previews?.['preview-hq-mp3'] || sound.previews?.['preview-lq-mp3'],
      previewUrlOgg: sound.previews?.['preview-hq-ogg'] || sound.previews?.['preview-lq-ogg'],
      // Type classification based on tags
      type: classifyAudioType(sound.tags),
      // For compatibility with existing UI
      category: classifyAudioCategory(sound.tags),
      mood: extractMoodFromTags(sound.tags)
    }));

    return {
      success: true,
      results,
      total: response.data.count,
      page,
      perPage,
      hasMore: response.data.next !== null,
      source: 'freesound'
    };

  } catch (error) {
    console.error('[searchFreesoundAudio] Error:', error.response?.data || error.message);

    // Fallback to curated library on error
    console.log('[searchFreesoundAudio] Falling back to curated library');
    return searchCuratedAudioLibrary(query, filters);
  }
});

/**
 * Helper function to classify audio type from tags
 */
function classifyAudioType(tags) {
  const tagString = (tags || []).join(' ').toLowerCase();

  if (tagString.includes('music') || tagString.includes('song') || tagString.includes('melody') ||
      tagString.includes('instrumental') || tagString.includes('piano') || tagString.includes('guitar')) {
    return 'music';
  }
  if (tagString.includes('ambien') || tagString.includes('atmosphere') || tagString.includes('nature') ||
      tagString.includes('rain') || tagString.includes('wind') || tagString.includes('forest')) {
    return 'ambience';
  }
  return 'sfx';
}

/**
 * Helper function to classify audio category from tags
 */
function classifyAudioCategory(tags) {
  const tagString = (tags || []).join(' ').toLowerCase();

  // Music categories
  if (tagString.includes('epic') || tagString.includes('cinematic') || tagString.includes('trailer')) return 'cinematic';
  if (tagString.includes('corporate') || tagString.includes('business') || tagString.includes('presentation')) return 'corporate';
  if (tagString.includes('ambient') || tagString.includes('chill') || tagString.includes('relaxing')) return 'ambient';
  if (tagString.includes('electronic') || tagString.includes('edm') || tagString.includes('synth')) return 'electronic';
  if (tagString.includes('horror') || tagString.includes('dark') || tagString.includes('scary')) return 'dark';
  if (tagString.includes('happy') || tagString.includes('upbeat') || tagString.includes('cheerful')) return 'upbeat';

  // SFX categories
  if (tagString.includes('whoosh') || tagString.includes('transition') || tagString.includes('swipe')) return 'transitions';
  if (tagString.includes('impact') || tagString.includes('hit') || tagString.includes('punch')) return 'impacts';
  if (tagString.includes('tech') || tagString.includes('digital') || tagString.includes('glitch')) return 'tech';

  return 'general';
}

/**
 * Helper function to extract mood from tags
 */
function extractMoodFromTags(tags) {
  const tagString = (tags || []).join(' ').toLowerCase();

  if (tagString.includes('epic') || tagString.includes('powerful')) return 'Epic, Powerful';
  if (tagString.includes('calm') || tagString.includes('peaceful') || tagString.includes('relaxing')) return 'Calm, Peaceful';
  if (tagString.includes('happy') || tagString.includes('cheerful')) return 'Happy, Cheerful';
  if (tagString.includes('sad') || tagString.includes('emotional')) return 'Emotional, Moving';
  if (tagString.includes('tense') || tagString.includes('suspense')) return 'Tense, Suspenseful';
  if (tagString.includes('dark') || tagString.includes('scary')) return 'Dark, Mysterious';
  if (tagString.includes('upbeat') || tagString.includes('energetic')) return 'Energetic, Upbeat';

  return 'Neutral';
}

/**
 * Fallback search using curated library when Freesound API is unavailable
 */
function searchCuratedAudioLibrary(query, filters) {
  const { type = 'all', minDuration, maxDuration, page = 1, perPage = 20 } = filters;

  // Combine all audio libraries
  const allAudio = [
    ...MUSIC_LIBRARY.map(t => ({ ...t, type: 'music', source: 'curated' })),
    ...SFX_LIBRARY.map(t => ({ ...t, type: 'sfx', source: 'curated' })),
    ...AMBIENCE_LIBRARY.map(t => ({ ...t, type: 'ambience', source: 'curated' }))
  ];

  // Filter by type
  let filtered = type === 'all' ? allAudio : allAudio.filter(a => a.type === type);

  // Filter by query
  if (query) {
    const q = query.toLowerCase();
    filtered = filtered.filter(a =>
      a.name?.toLowerCase().includes(q) ||
      a.tags?.some(t => t.toLowerCase().includes(q)) ||
      a.category?.toLowerCase().includes(q) ||
      a.mood?.toLowerCase().includes(q)
    );
  }

  // Filter by duration
  if (minDuration) filtered = filtered.filter(a => (a.duration || 0) >= minDuration);
  if (maxDuration) filtered = filtered.filter(a => (a.duration || 999) <= maxDuration);

  // Paginate
  const start = (page - 1) * perPage;
  const results = filtered.slice(start, start + perPage);

  return {
    success: true,
    results,
    total: filtered.length,
    page,
    perPage,
    hasMore: start + perPage < filtered.length,
    source: 'curated',
    note: 'Using curated library. Configure Freesound API key for live search.'
  };
}

/**
 * searchPixabayMusic - Search Pixabay for royalty-free music
 * Pixabay has a music section but no official API, so we provide curated tracks
 * with actual Pixabay URLs
 */
exports.searchPixabayMusic = functions.https.onCall(async (data, context) => {
  await verifyAuth(context);

  const { query = '', filters = {} } = data;
  const { category, mood, page = 1, perPage = 20 } = filters;

  // Curated Pixabay music tracks with real URLs
  // These are popular royalty-free tracks from Pixabay's music library
  const PIXABAY_MUSIC_TRACKS = [
    {
      id: 'pixabay-inspiring-cinematic',
      name: 'Inspiring Cinematic Ambient',
      artist: 'Lexin_Music',
      category: 'cinematic',
      mood: 'Inspiring, Uplifting',
      duration: 127,
      bpm: 80,
      tags: ['cinematic', 'inspiring', 'ambient', 'background'],
      previewUrl: 'https://cdn.pixabay.com/download/audio/2022/05/27/audio_1808fbf07a.mp3',
      source: 'pixabay',
      license: 'Pixabay License (Free)'
    },
    {
      id: 'pixabay-documentary',
      name: 'Documentary',
      artist: 'Lexin_Music',
      category: 'cinematic',
      mood: 'Serious, Professional',
      duration: 122,
      bpm: 90,
      tags: ['documentary', 'cinematic', 'serious', 'professional'],
      previewUrl: 'https://cdn.pixabay.com/download/audio/2022/10/25/audio_946b0939c3.mp3',
      source: 'pixabay',
      license: 'Pixabay License (Free)'
    },
    {
      id: 'pixabay-upbeat-corporate',
      name: 'Upbeat Corporate Technology',
      artist: 'Daddy_s_Music',
      category: 'corporate',
      mood: 'Upbeat, Professional',
      duration: 152,
      bpm: 120,
      tags: ['corporate', 'technology', 'upbeat', 'business'],
      previewUrl: 'https://cdn.pixabay.com/download/audio/2023/05/16/audio_166b63e854.mp3',
      source: 'pixabay',
      license: 'Pixabay License (Free)'
    },
    {
      id: 'pixabay-ambient-piano',
      name: 'Ambient Piano',
      artist: 'Music_For_Videos',
      category: 'ambient',
      mood: 'Calm, Peaceful',
      duration: 186,
      bpm: 70,
      tags: ['ambient', 'piano', 'calm', 'peaceful', 'relaxing'],
      previewUrl: 'https://cdn.pixabay.com/download/audio/2022/01/18/audio_d0a13f69d2.mp3',
      source: 'pixabay',
      license: 'Pixabay License (Free)'
    },
    {
      id: 'pixabay-electronic-future',
      name: 'Electronic Future',
      artist: 'SoulProdMusic',
      category: 'electronic',
      mood: 'Modern, Energetic',
      duration: 145,
      bpm: 128,
      tags: ['electronic', 'future', 'modern', 'technology', 'energetic'],
      previewUrl: 'https://cdn.pixabay.com/download/audio/2023/03/11/audio_0e0a42a1c3.mp3',
      source: 'pixabay',
      license: 'Pixabay License (Free)'
    },
    {
      id: 'pixabay-epic-trailer',
      name: 'Epic Cinematic Trailer',
      artist: 'Lexin_Music',
      category: 'cinematic',
      mood: 'Epic, Powerful',
      duration: 71,
      bpm: 95,
      tags: ['epic', 'cinematic', 'trailer', 'powerful', 'dramatic'],
      previewUrl: 'https://cdn.pixabay.com/download/audio/2022/05/16/audio_bd8740feb5.mp3',
      source: 'pixabay',
      license: 'Pixabay License (Free)'
    },
    {
      id: 'pixabay-happy-upbeat',
      name: 'Happy Day',
      artist: 'Top-Flow',
      category: 'upbeat',
      mood: 'Happy, Cheerful',
      duration: 120,
      bpm: 115,
      tags: ['happy', 'upbeat', 'cheerful', 'positive', 'fun'],
      previewUrl: 'https://cdn.pixabay.com/download/audio/2022/01/20/audio_570dc28ec0.mp3',
      source: 'pixabay',
      license: 'Pixabay License (Free)'
    },
    {
      id: 'pixabay-dark-ambient',
      name: 'Dark Ambient Horror',
      artist: 'Lexin_Music',
      category: 'dark',
      mood: 'Dark, Mysterious',
      duration: 180,
      bpm: 60,
      tags: ['dark', 'ambient', 'horror', 'mysterious', 'scary'],
      previewUrl: 'https://cdn.pixabay.com/download/audio/2022/03/10/audio_c55cf59e18.mp3',
      source: 'pixabay',
      license: 'Pixabay License (Free)'
    },
    {
      id: 'pixabay-tension-suspense',
      name: 'Tension Suspense',
      artist: 'SoundGalleryByDimitri',
      category: 'dramatic',
      mood: 'Tense, Suspenseful',
      duration: 120,
      bpm: 85,
      tags: ['tension', 'suspense', 'thriller', 'intense', 'dramatic'],
      previewUrl: 'https://cdn.pixabay.com/download/audio/2022/10/30/audio_dd0e5e7f2a.mp3',
      source: 'pixabay',
      license: 'Pixabay License (Free)'
    },
    {
      id: 'pixabay-nature-documentary',
      name: 'Nature Documentary',
      artist: 'Daddy_s_Music',
      category: 'cinematic',
      mood: 'Inspiring, Natural',
      duration: 195,
      bpm: 75,
      tags: ['nature', 'documentary', 'inspiring', 'natural', 'cinematic'],
      previewUrl: 'https://cdn.pixabay.com/download/audio/2023/02/28/audio_a46a09fe9e.mp3',
      source: 'pixabay',
      license: 'Pixabay License (Free)'
    },
    {
      id: 'pixabay-motivational',
      name: 'Motivational Epic',
      artist: 'Lexin_Music',
      category: 'cinematic',
      mood: 'Motivational, Epic',
      duration: 156,
      bpm: 110,
      tags: ['motivational', 'epic', 'inspiring', 'powerful', 'success'],
      previewUrl: 'https://cdn.pixabay.com/download/audio/2022/08/31/audio_419273eb64.mp3',
      source: 'pixabay',
      license: 'Pixabay License (Free)'
    },
    {
      id: 'pixabay-lofi-chill',
      name: 'Lofi Study',
      artist: 'FASSounds',
      category: 'ambient',
      mood: 'Chill, Relaxing',
      duration: 145,
      bpm: 85,
      tags: ['lofi', 'chill', 'study', 'relaxing', 'calm'],
      previewUrl: 'https://cdn.pixabay.com/download/audio/2022/05/13/audio_257112ec40.mp3',
      source: 'pixabay',
      license: 'Pixabay License (Free)'
    },
    {
      id: 'pixabay-tech-innovation',
      name: 'Innovation Technology',
      artist: 'Lexin_Music',
      category: 'corporate',
      mood: 'Modern, Innovative',
      duration: 138,
      bpm: 118,
      tags: ['technology', 'innovation', 'corporate', 'modern', 'startup'],
      previewUrl: 'https://cdn.pixabay.com/download/audio/2022/03/24/audio_fc47d46315.mp3',
      source: 'pixabay',
      license: 'Pixabay License (Free)'
    },
    {
      id: 'pixabay-emotional-piano',
      name: 'Emotional Cinematic Piano',
      artist: 'Ahjay_Stelino',
      category: 'cinematic',
      mood: 'Emotional, Moving',
      duration: 168,
      bpm: 72,
      tags: ['emotional', 'piano', 'cinematic', 'moving', 'sad'],
      previewUrl: 'https://cdn.pixabay.com/download/audio/2022/11/22/audio_d6a3e34ab2.mp3',
      source: 'pixabay',
      license: 'Pixabay License (Free)'
    },
    {
      id: 'pixabay-adventure',
      name: 'Adventure Awaits',
      artist: 'Daddy_s_Music',
      category: 'cinematic',
      mood: 'Adventurous, Exciting',
      duration: 142,
      bpm: 100,
      tags: ['adventure', 'exciting', 'travel', 'exploration', 'cinematic'],
      previewUrl: 'https://cdn.pixabay.com/download/audio/2023/08/31/audio_c0dfe4e9f4.mp3',
      source: 'pixabay',
      license: 'Pixabay License (Free)'
    }
  ];

  // Filter tracks
  let filtered = [...PIXABAY_MUSIC_TRACKS];

  if (query) {
    const q = query.toLowerCase();
    filtered = filtered.filter(t =>
      t.name.toLowerCase().includes(q) ||
      t.tags.some(tag => tag.toLowerCase().includes(q)) ||
      t.mood.toLowerCase().includes(q)
    );
  }

  if (category) {
    filtered = filtered.filter(t => t.category === category);
  }

  if (mood) {
    filtered = filtered.filter(t => t.mood.toLowerCase().includes(mood.toLowerCase()));
  }

  // Paginate
  const start = (page - 1) * perPage;
  const results = filtered.slice(start, start + perPage);

  return {
    success: true,
    results,
    total: filtered.length,
    page,
    perPage,
    hasMore: start + perPage < filtered.length,
    source: 'pixabay'
  };
});

/**
 * getAudioLibrary - Get combined audio library with real URLs
 * This is the main endpoint for the audio browser
 * Returns Pixabay music tracks + curated library tracks
 */
exports.getAudioLibrary = functions.https.onCall(async (data, context) => {
  await verifyAuth(context);

  const { type = 'all', source = 'all', category } = data;

  // Combine all audio sources
  let results = [
    ...MUSIC_LIBRARY.map(t => ({ ...t, type: 'music', source: 'curated' })),
    ...SFX_LIBRARY.map(t => ({ ...t, type: 'sfx', source: 'curated' })),
    ...AMBIENCE_LIBRARY.map(t => ({ ...t, type: 'ambience', source: 'curated' }))
  ];

  // Filter by type
  if (type !== 'all') {
    results = results.filter(r => r.type === type);
  }

  // Filter by category
  if (category) {
    results = results.filter(r => r.category === category);
  }

  // Filter by source
  if (source !== 'all') {
    results = results.filter(r => r.source === source);
  }

  return {
    success: true,
    results,
    total: results.length,
    categories: ['cinematic', 'corporate', 'ambient', 'electronic', 'dark', 'upbeat', 'dramatic'],
    sources: ['pixabay', 'freesound', 'curated']
  };
});

/**
 * getMusicCategories - Get all music categories with counts
 */
exports.getMusicCategories = functions.https.onCall(async (data, context) => {
  await verifyAuth(context);

  const allTracks = [...ENHANCED_MUSIC_LIBRARY, ...MUSIC_LIBRARY];

  // Calculate counts per category
  const categoryCounts = {};
  const subcategoryCounts = {};

  allTracks.forEach(track => {
    categoryCounts[track.category] = (categoryCounts[track.category] || 0) + 1;
    if (track.subcategory) {
      const key = `${track.category}:${track.subcategory}`;
      subcategoryCounts[key] = (subcategoryCounts[key] || 0) + 1;
    }
  });

  const categories = MUSIC_CATEGORIES.map(cat => ({
    ...cat,
    count: categoryCounts[cat.id] || 0,
    subcategories: Object.entries(subcategoryCounts)
      .filter(([key]) => key.startsWith(cat.id + ':'))
      .map(([key, count]) => ({
        id: key.split(':')[1],
        name: key.split(':')[1].charAt(0).toUpperCase() + key.split(':')[1].slice(1),
        count
      }))
  }));

  return {
    success: true,
    categories,
    totalTracks: allTracks.length
  };
});

// ==========================================
// PHASE 5: BEAT SYNCHRONIZATION ENGINE
// ==========================================

/**
 * BEAT_SYNC_PRESETS - Pre-defined beat maps for common music patterns
 * Since client-side beat detection is complex, we use BPM-based estimation
 */
const BEAT_SYNC_PRESETS = {
  // Common time signatures and structures
  '4/4': {
    beatsPerMeasure: 4,
    measureDuration: (bpm) => (60 / bpm) * 4, // seconds per measure
    strongBeats: [1], // Downbeat
    mediumBeats: [3], // Backbeat
    weakBeats: [2, 4]
  },
  '3/4': {
    beatsPerMeasure: 3,
    measureDuration: (bpm) => (60 / bpm) * 3,
    strongBeats: [1],
    mediumBeats: [],
    weakBeats: [2, 3]
  },
  '6/8': {
    beatsPerMeasure: 6,
    measureDuration: (bpm) => (60 / bpm) * 6,
    strongBeats: [1, 4],
    mediumBeats: [],
    weakBeats: [2, 3, 5, 6]
  }
};

/**
 * MUSIC_STRUCTURE_PATTERNS - Common song structures for section detection
 */
const MUSIC_STRUCTURE_PATTERNS = {
  'pop': {
    sections: [
      { type: 'intro', typicalBars: 4, position: 0 },
      { type: 'verse', typicalBars: 8, position: 0.1 },
      { type: 'chorus', typicalBars: 8, position: 0.3 },
      { type: 'verse2', typicalBars: 8, position: 0.45 },
      { type: 'chorus2', typicalBars: 8, position: 0.6 },
      { type: 'bridge', typicalBars: 4, position: 0.75 },
      { type: 'chorus3', typicalBars: 8, position: 0.85 },
      { type: 'outro', typicalBars: 4, position: 0.95 }
    ]
  },
  'cinematic': {
    sections: [
      { type: 'intro', typicalBars: 8, position: 0 },
      { type: 'build', typicalBars: 16, position: 0.15 },
      { type: 'climax', typicalBars: 8, position: 0.5 },
      { type: 'sustain', typicalBars: 8, position: 0.65 },
      { type: 'resolve', typicalBars: 8, position: 0.8 },
      { type: 'outro', typicalBars: 4, position: 0.92 }
    ]
  },
  'ambient': {
    sections: [
      { type: 'intro', typicalBars: 4, position: 0 },
      { type: 'develop', typicalBars: 16, position: 0.1 },
      { type: 'peak', typicalBars: 8, position: 0.5 },
      { type: 'fade', typicalBars: 16, position: 0.7 },
      { type: 'outro', typicalBars: 4, position: 0.9 }
    ]
  },
  'corporate': {
    sections: [
      { type: 'intro', typicalBars: 4, position: 0 },
      { type: 'main', typicalBars: 16, position: 0.1 },
      { type: 'variation', typicalBars: 8, position: 0.5 },
      { type: 'main2', typicalBars: 8, position: 0.7 },
      { type: 'outro', typicalBars: 4, position: 0.9 }
    ]
  }
};

/**
 * generateBeatMap - Analyzes music track and generates beat timestamps
 * Uses BPM-based calculation for reliable beat mapping
 */
exports.generateBeatMap = functions.https.onCall(async (data, context) => {
  await verifyAuth(context);

  const {
    trackId,
    bpm = 120,
    duration,
    timeSignature = '4/4',
    musicStyle = 'corporate'
  } = data;

  if (!duration) {
    throw new functions.https.HttpsError('invalid-argument', 'Track duration required');
  }

  try {
    const durationMs = duration * 1000;
    const beatDurationMs = (60 / bpm) * 1000; // Duration of one beat in ms
    const signature = BEAT_SYNC_PRESETS[timeSignature] || BEAT_SYNC_PRESETS['4/4'];
    const measureDurationMs = signature.measureDuration(bpm) * 1000;

    // Generate all beat timestamps
    const beats = [];
    const measures = [];
    const downbeats = [];

    let currentTime = 0;
    let beatCount = 0;
    let measureCount = 0;

    while (currentTime < durationMs) {
      beatCount++;
      const beatInMeasure = ((beatCount - 1) % signature.beatsPerMeasure) + 1;

      beats.push({
        time: Math.round(currentTime),
        beatNumber: beatCount,
        beatInMeasure,
        isDownbeat: beatInMeasure === 1,
        isStrong: signature.strongBeats.includes(beatInMeasure),
        isMedium: signature.mediumBeats.includes(beatInMeasure)
      });

      // Track measure boundaries (downbeats)
      if (beatInMeasure === 1) {
        measureCount++;
        measures.push({
          measureNumber: measureCount,
          time: Math.round(currentTime)
        });
        downbeats.push(Math.round(currentTime));
      }

      currentTime += beatDurationMs;
    }

    // Generate section markers based on music style
    const structure = MUSIC_STRUCTURE_PATTERNS[musicStyle] || MUSIC_STRUCTURE_PATTERNS['corporate'];
    const sections = structure.sections.map(section => ({
      type: section.type,
      startTime: Math.round(durationMs * section.position),
      // Find nearest downbeat
      nearestDownbeat: downbeats.reduce((nearest, db) =>
        Math.abs(db - durationMs * section.position) < Math.abs(nearest - durationMs * section.position) ? db : nearest
      , downbeats[0])
    }));

    // Calculate optimal cut points (every 2, 4, or 8 measures for clean cuts)
    const optimalCutPoints = {
      tight: [], // Every 2 measures
      balanced: [], // Every 4 measures
      relaxed: [] // Every 8 measures
    };

    measures.forEach((measure, i) => {
      if ((i + 1) % 2 === 0) optimalCutPoints.tight.push(measure.time);
      if ((i + 1) % 4 === 0) optimalCutPoints.balanced.push(measure.time);
      if ((i + 1) % 8 === 0) optimalCutPoints.relaxed.push(measure.time);
    });

    return {
      success: true,
      beatMap: {
        trackId,
        bpm,
        timeSignature,
        duration: durationMs,
        beatCount: beats.length,
        measureCount: measures.length,
        beatDurationMs: Math.round(beatDurationMs),
        measureDurationMs: Math.round(measureDurationMs),
        beats: beats.slice(0, 500), // Limit to first 500 beats for performance
        measures: measures.slice(0, 100), // Limit measures
        downbeats: downbeats.slice(0, 100),
        sections,
        optimalCutPoints
      }
    };

  } catch (error) {
    console.error('[generateBeatMap] Error:', error);
    throw new functions.https.HttpsError('internal', 'Failed to generate beat map');
  }
});

/**
 * suggestBeatSyncCuts - Analyzes scenes and suggests optimal cut points aligned with music beats
 */
exports.suggestBeatSyncCuts = functions.https.onCall(async (data, context) => {
  await verifyAuth(context);

  const {
    scenes,
    beatMap,
    syncMode = 'balanced', // 'tight' | 'balanced' | 'relaxed'
    preserveMinDuration = 3000, // Minimum scene duration in ms
    preserveMaxAdjustment = 2000 // Maximum adjustment allowed in ms
  } = data;

  if (!scenes || !scenes.length) {
    throw new functions.https.HttpsError('invalid-argument', 'Scenes array required');
  }

  if (!beatMap || !beatMap.downbeats) {
    throw new functions.https.HttpsError('invalid-argument', 'Beat map with downbeats required');
  }

  try {
    const { downbeats, measures, optimalCutPoints, measureDurationMs, beatDurationMs } = beatMap;
    const cutPoints = optimalCutPoints[syncMode] || optimalCutPoints.balanced;

    // Calculate current scene end times
    let currentTime = 0;
    const sceneEndTimes = scenes.map(scene => {
      const duration = (scene.duration || 5) * 1000;
      currentTime += duration;
      return {
        sceneId: scene.id || scene.sceneId,
        originalEnd: currentTime,
        originalDuration: duration
      };
    });

    // Suggest adjustments for each scene
    const suggestions = sceneEndTimes.map((scene, index) => {
      const isLastScene = index === sceneEndTimes.length - 1;

      // Find nearest downbeat to current scene end
      const nearestDownbeat = downbeats.reduce((nearest, db) =>
        Math.abs(db - scene.originalEnd) < Math.abs(nearest - scene.originalEnd) ? db : nearest
      , downbeats[0]);

      // Find nearest optimal cut point
      const nearestCutPoint = cutPoints.reduce((nearest, cp) =>
        Math.abs(cp - scene.originalEnd) < Math.abs(nearest - scene.originalEnd) ? cp : nearest
      , cutPoints[0] || nearestDownbeat);

      // Find nearest measure boundary
      const nearestMeasure = measures.reduce((nearest, m) =>
        Math.abs(m.time - scene.originalEnd) < Math.abs(nearest.time - scene.originalEnd) ? m : nearest
      , measures[0]);

      // Calculate adjustments
      const downbeatAdjustment = nearestDownbeat - scene.originalEnd;
      const cutPointAdjustment = nearestCutPoint - scene.originalEnd;
      const measureAdjustment = nearestMeasure.time - scene.originalEnd;

      // Determine best suggestion based on constraints
      let suggestedEnd = scene.originalEnd;
      let adjustment = 0;
      let quality = 'none';
      let syncType = 'none';

      // Priority: optimal cut point > measure > downbeat
      if (Math.abs(cutPointAdjustment) <= preserveMaxAdjustment) {
        const newDuration = scene.originalDuration + cutPointAdjustment;
        if (newDuration >= preserveMinDuration) {
          suggestedEnd = nearestCutPoint;
          adjustment = cutPointAdjustment;
          quality = 'perfect';
          syncType = 'phrase';
        }
      }

      if (quality === 'none' && Math.abs(measureAdjustment) <= preserveMaxAdjustment) {
        const newDuration = scene.originalDuration + measureAdjustment;
        if (newDuration >= preserveMinDuration) {
          suggestedEnd = nearestMeasure.time;
          adjustment = measureAdjustment;
          quality = 'great';
          syncType = 'measure';
        }
      }

      if (quality === 'none' && Math.abs(downbeatAdjustment) <= preserveMaxAdjustment) {
        const newDuration = scene.originalDuration + downbeatAdjustment;
        if (newDuration >= preserveMinDuration) {
          suggestedEnd = nearestDownbeat;
          adjustment = downbeatAdjustment;
          quality = 'good';
          syncType = 'downbeat';
        }
      }

      // If no good sync point found, keep original
      if (quality === 'none') {
        quality = 'keep';
        syncType = 'original';
      }

      return {
        sceneId: scene.sceneId,
        sceneIndex: index,
        originalDuration: scene.originalDuration,
        suggestedDuration: scene.originalDuration + adjustment,
        originalEnd: scene.originalEnd,
        suggestedEnd,
        adjustment,
        adjustmentSeconds: Math.round(adjustment / 100) / 10,
        quality, // 'perfect' | 'great' | 'good' | 'keep'
        syncType, // 'phrase' | 'measure' | 'downbeat' | 'original'
        nearestDownbeat,
        nearestMeasure: nearestMeasure.measureNumber,
        isLastScene
      };
    });

    // Calculate overall sync score
    const qualityScores = { perfect: 100, great: 80, good: 60, keep: 40 };
    const avgScore = suggestions.reduce((sum, s) => sum + qualityScores[s.quality], 0) / suggestions.length;

    // Recalculate adjusted timeline
    let adjustedTime = 0;
    const adjustedTimeline = suggestions.map(s => {
      adjustedTime += s.suggestedDuration;
      return {
        sceneId: s.sceneId,
        endTime: adjustedTime
      };
    });

    return {
      success: true,
      suggestions,
      summary: {
        totalScenes: scenes.length,
        perfectSyncs: suggestions.filter(s => s.quality === 'perfect').length,
        greatSyncs: suggestions.filter(s => s.quality === 'great').length,
        goodSyncs: suggestions.filter(s => s.quality === 'good').length,
        keptOriginal: suggestions.filter(s => s.quality === 'keep').length,
        overallScore: Math.round(avgScore),
        totalAdjustment: suggestions.reduce((sum, s) => sum + Math.abs(s.adjustment), 0),
        newTotalDuration: adjustedTime,
        originalTotalDuration: sceneEndTimes[sceneEndTimes.length - 1].originalEnd
      },
      adjustedTimeline,
      syncMode,
      bpm: beatMap.bpm
    };

  } catch (error) {
    console.error('[suggestBeatSyncCuts] Error:', error);
    throw new functions.https.HttpsError('internal', 'Failed to generate beat sync suggestions');
  }
});

/**
 * applyBeatSync - Applies beat sync adjustments to scene durations
 */
exports.applyBeatSync = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);

  const {
    projectId,
    suggestions,
    applyAll = true,
    selectedSceneIds = []
  } = data;

  if (!projectId) {
    throw new functions.https.HttpsError('invalid-argument', 'Project ID required');
  }

  if (!suggestions || !suggestions.length) {
    throw new functions.https.HttpsError('invalid-argument', 'Suggestions array required');
  }

  try {
    // Filter suggestions to apply
    const toApply = applyAll
      ? suggestions.filter(s => s.quality !== 'keep')
      : suggestions.filter(s => selectedSceneIds.includes(s.sceneId));

    // Get project
    const projectRef = db.collection('creationProjects').doc(projectId);
    const projectDoc = await projectRef.get();

    if (!projectDoc.exists) {
      throw new functions.https.HttpsError('not-found', 'Project not found');
    }

    const projectData = projectDoc.data();
    if (projectData.userId !== uid) {
      throw new functions.https.HttpsError('permission-denied', 'Not your project');
    }

    // Update scene durations in script
    const scriptScenes = projectData.state?.script?.scenes || [];
    const updatedScenes = scriptScenes.map(scene => {
      const suggestion = toApply.find(s => s.sceneId === scene.id);
      if (suggestion) {
        return {
          ...scene,
          duration: Math.round(suggestion.suggestedDuration / 1000 * 10) / 10, // Convert to seconds
          beatSynced: true,
          originalDuration: scene.duration,
          syncQuality: suggestion.quality,
          syncType: suggestion.syncType
        };
      }
      return scene;
    });

    // Update project
    await projectRef.update({
      'state.script.scenes': updatedScenes,
      'state.assembly.beatSyncApplied': true,
      'state.assembly.beatSyncTimestamp': admin.firestore.FieldValue.serverTimestamp(),
      'state.assembly.beatSyncStats': {
        appliedCount: toApply.length,
        totalScenes: scriptScenes.length
      },
      updatedAt: admin.firestore.FieldValue.serverTimestamp()
    });

    return {
      success: true,
      appliedCount: toApply.length,
      updatedScenes: updatedScenes.filter(s => s.beatSynced).map(s => ({
        sceneId: s.id,
        newDuration: s.duration,
        originalDuration: s.originalDuration,
        syncQuality: s.syncQuality
      }))
    };

  } catch (error) {
    console.error('[applyBeatSync] Error:', error);
    throw new functions.https.HttpsError('internal', 'Failed to apply beat sync');
  }
});

/**
 * getBeatSyncPreview - Quick preview of beat sync without full analysis
 * Returns simple alignment info for UI preview
 */
exports.getBeatSyncPreview = functions.https.onCall(async (data, context) => {
  await verifyAuth(context);

  const {
    sceneDurations, // Array of durations in seconds
    bpm = 120,
    syncMode = 'balanced'
  } = data;

  if (!sceneDurations || !sceneDurations.length) {
    throw new functions.https.HttpsError('invalid-argument', 'Scene durations required');
  }

  try {
    const measureDuration = (60 / bpm) * 4; // 4/4 time
    const beatDuration = 60 / bpm;

    let currentTime = 0;
    const preview = sceneDurations.map((duration, index) => {
      currentTime += duration;

      // Find how close to nearest measure boundary
      const nearestMeasure = Math.round(currentTime / measureDuration) * measureDuration;
      const measureOffset = currentTime - nearestMeasure;

      // Find how close to nearest beat
      const nearestBeat = Math.round(currentTime / beatDuration) * beatDuration;
      const beatOffset = currentTime - nearestBeat;

      // Determine sync quality
      let quality;
      if (Math.abs(measureOffset) < 0.1) {
        quality = 'perfect'; // Within 100ms of measure
      } else if (Math.abs(measureOffset) < 0.3) {
        quality = 'great'; // Within 300ms of measure
      } else if (Math.abs(beatOffset) < 0.15) {
        quality = 'good'; // Within 150ms of beat
      } else {
        quality = 'off'; // Not synced
      }

      return {
        sceneIndex: index,
        currentEnd: Math.round(currentTime * 100) / 100,
        nearestMeasure: Math.round(nearestMeasure * 100) / 100,
        measureOffset: Math.round(measureOffset * 100) / 100,
        quality,
        suggestedAdjustment: -measureOffset
      };
    });

    // Calculate overall alignment score
    const qualityScores = { perfect: 100, great: 75, good: 50, off: 0 };
    const avgScore = preview.reduce((sum, p) => sum + qualityScores[p.quality], 0) / preview.length;

    return {
      success: true,
      preview,
      bpm,
      measureDuration: Math.round(measureDuration * 100) / 100,
      beatDuration: Math.round(beatDuration * 100) / 100,
      overallAlignment: Math.round(avgScore),
      alignmentLabel: avgScore >= 80 ? 'Excellent' : avgScore >= 60 ? 'Good' : avgScore >= 40 ? 'Fair' : 'Needs Adjustment'
    };

  } catch (error) {
    console.error('[getBeatSyncPreview] Error:', error);
    throw new functions.https.HttpsError('internal', 'Failed to generate beat sync preview');
  }
});

/**
 * importStockMedia - Download and cache stock media to Firebase Storage
 * Needed because some APIs don't allow hotlinking
 *
 * @param {string} mediaId - Stock media ID (e.g., 'pexels-12345')
 * @param {string} url - Original media URL
 * @param {string} type - 'image' | 'video'
 * @param {string} projectId - Project to associate with
 */
exports.importStockMedia = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);

  const {
    mediaId,
    url,
    type = 'image',
    projectId,
    // Video-specific options
    thumbnailUrl,
    trimStart = 0,
    trimEnd = null,
    originalDuration = null
  } = data;

  if (!url) {
    throw new functions.https.HttpsError('invalid-argument', 'Media URL required');
  }

  try {
    // Download the media
    const response = await axios.get(url, {
      responseType: 'arraybuffer',
      timeout: 120000, // 120s for large videos
      headers: {
        'User-Agent': 'Mozilla/5.0 (compatible; VideoWizard/1.0)'
      }
    });

    const buffer = Buffer.from(response.data);
    const contentType = response.headers['content-type'] || (type === 'video' ? 'video/mp4' : 'image/jpeg');
    const extension = type === 'video' ? 'mp4' : 'jpg';

    // Generate filename - include trim info for videos
    const timestamp = Date.now();
    const trimSuffix = type === 'video' && (trimStart > 0 || trimEnd) ? `_trim${trimStart}-${trimEnd || 'end'}` : '';
    const fileName = `creation-projects/${projectId || uid}/stock/${mediaId}${trimSuffix}_${timestamp}.${extension}`;

    // Upload to Firebase Storage
    const bucket = admin.storage().bucket();
    const file = bucket.file(fileName);

    // Prepare metadata - include trim info for videos
    const fileMetadata = {
      originalUrl: url,
      mediaId: mediaId,
      importedAt: new Date().toISOString(),
      source: mediaId.split('-')[0] // 'pexels' or 'pixabay'
    };

    // Add video-specific metadata
    if (type === 'video') {
      fileMetadata.trimStart = String(trimStart || 0);
      fileMetadata.trimEnd = trimEnd ? String(trimEnd) : '';
      fileMetadata.originalDuration = originalDuration ? String(originalDuration) : '';
      fileMetadata.clipDuration = trimEnd ? String(trimEnd - trimStart) : '';
      if (thumbnailUrl) {
        fileMetadata.thumbnailUrl = thumbnailUrl;
      }
    }

    await file.save(buffer, {
      metadata: {
        contentType: contentType,
        metadata: fileMetadata
      }
    });

    // Make file public and get URL
    await file.makePublic();
    const publicUrl = `https://storage.googleapis.com/${bucket.name}/${fileName}`;

    // Log usage
    await db.collection('apiUsage').add({
      userId: uid,
      type: 'stock_media_import',
      source: mediaId.split('-')[0],
      mediaType: type,
      originalUrl: url,
      cachedUrl: publicUrl,
      ...(type === 'video' && {
        trimStart: trimStart || 0,
        trimEnd: trimEnd,
        originalDuration: originalDuration,
        clipDuration: trimEnd ? trimEnd - trimStart : null
      }),
      timestamp: admin.firestore.FieldValue.serverTimestamp()
    });

    // Return response with trim info for videos
    const result = {
      success: true,
      url: publicUrl,
      mediaId,
      type,
      cached: true
    };

    // Add video-specific info to response
    if (type === 'video') {
      result.trimStart = trimStart || 0;
      result.trimEnd = trimEnd;
      result.originalDuration = originalDuration;
      result.clipDuration = trimEnd ? trimEnd - trimStart : originalDuration;
      result.thumbnailUrl = thumbnailUrl; // Pass through for frontend reference
      // Note: Actual trimming will be applied during video assembly/export
      result.trimNote = 'Trim settings stored. Will be applied during final video assembly.';
    }

    return result;

  } catch (error) {
    console.error('[importStockMedia] Error:', error);
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to import media'));
  }
});

// =============================================================================
// PHASE 8-11: PROMPT CHAIN ARCHITECTURE
// The Golden Chain of Quality - Script â†’ Image Prompt â†’ Video Prompt â†’ Final
// =============================================================================

/**
 * PROMPT CHAIN ARCHITECTURE - THE GOLDEN CHAIN OF QUALITY
 *
 * This system ensures that quality compounds at each step rather than degrades.
 * The script is the master source - if the script is mediocre, everything fails.
 *
 * Chain: User Choices â†’ Script â†’ Visual Intent â†’ Image Prompt â†’ Generated Image
 *        â†’ Video Prompt (Image + Action) â†’ Generated Video â†’ Transition â†’ Next Scene
 */

// =============================================================================
// SECTION 8.1: SCENE SCRIPT STRUCTURE - THE VISUAL BLUEPRINT
// =============================================================================

/**
 * Scene Script Structure - Complete Visual Blueprint
 * The script is NOT just dialogue and action - it's a complete visual blueprint
 */
const SCENE_SCRIPT_STRUCTURE = {
  // Define the structure schema for scene scripts
  schema: {
    sceneId: 'string', // e.g., 'S01_001'
    episodeNumber: 'number|null', // For series

    timing: {
      duration: 'number', // seconds
      pacingNote: 'string' // e.g., 'slow build to sudden action'
    },

    // THE VISUAL INTENT (becomes IMAGE PROMPT)
    visualBlueprint: {
      // Camera
      shotType: ['extreme-wide', 'wide', 'medium-wide', 'medium', 'medium-close', 'close-up', 'extreme-close-up', 'establishing'],
      cameraAngle: ['eye-level', 'low-angle', 'high-angle', 'dutch-angle', 'birds-eye', 'worms-eye', 'over-shoulder'],
      cameraMovement: ['static', 'push-in', 'pull-out', 'pan-left', 'pan-right', 'tilt-up', 'tilt-down', 'tracking', 'crane', 'handheld'],

      // Composition
      subjectPlacement: ['center', 'rule-of-thirds-left', 'rule-of-thirds-right', 'bottom-third', 'top-third', 'leading-space'],
      foreground: 'string',
      midground: 'string',
      background: 'string',

      // Lighting
      keyLight: 'string', // e.g., 'harsh-neon-pink-from-left'
      fillLight: 'string', // e.g., 'cyan-ambient-from-screens'
      practicalLights: 'array', // e.g., ['holographic-ads', 'distant-windows']

      // Atmosphere
      weather: 'string|null',
      particles: 'string|null',
      mood: 'string',

      // Color
      dominantColors: 'array',
      colorTemperature: 'string'
    },

    // THE ACTION INTENT (becomes VIDEO PROMPT motion)
    actionBlueprint: {
      characterAction: {
        who: 'string',
        startPose: 'string',
        action: 'string',
        endPose: 'string',
        timing: 'string'
      },
      environmentAction: 'array', // Array of environment animations
      cameraAction: {
        movement: 'string',
        speed: 'string',
        focus: 'string'
      }
    },

    // AUDIO INTENT (for audio generation)
    audioBlueprint: {
      ambience: 'array',
      music: 'string',
      sfx: 'array',
      dialogue: 'string|null',
      voiceover: 'string|null'
    },

    // DIALOGUE (if any)
    dialogue: 'array', // Array of { character, line, delivery }

    // TRANSITION TO NEXT
    transitionOut: {
      type: ['cut', 'dissolve', 'wipe', 'match-cut', 'j-cut', 'l-cut', 'fade-to-black', 'fade-from-black'],
      timing: 'string',
      visualBridge: 'string|null'
    }
  },

  // Shot type descriptions for prompt generation
  shotTypes: {
    'extreme-wide': {
      prompt: 'extreme wide shot, vast landscape, subject very small in frame',
      useFor: ['establishing scale', 'showing environment', 'isolation']
    },
    'wide': {
      prompt: 'wide shot, full body visible, environment context',
      useFor: ['action sequences', 'group shots', 'movement']
    },
    'medium-wide': {
      prompt: 'medium wide shot, waist up, good balance of subject and environment',
      useFor: ['dialogue with context', 'walking scenes']
    },
    'medium': {
      prompt: 'medium shot, waist to head, conversational distance',
      useFor: ['dialogue', 'standard coverage', 'interviews']
    },
    'medium-close': {
      prompt: 'medium close-up, chest and head, intimate but not intrusive',
      useFor: ['emotional moments', 'important dialogue']
    },
    'close-up': {
      prompt: 'close-up shot, face fills frame, intense focus on expression',
      useFor: ['emotional peaks', 'reactions', 'detail reveals']
    },
    'extreme-close-up': {
      prompt: 'extreme close-up, single feature fills frame (eye, hand, object)',
      useFor: ['dramatic emphasis', 'symbolic moments', 'tension']
    },
    'establishing': {
      prompt: 'establishing shot, wide view introducing the location',
      useFor: ['scene beginnings', 'location changes', 'time passages']
    }
  },

  // Camera angle descriptions
  cameraAngles: {
    'eye-level': {
      prompt: 'eye-level shot, neutral and objective perspective',
      psychology: 'equality, normalcy, relatability'
    },
    'low-angle': {
      prompt: 'low angle shot, camera looking up at subject',
      psychology: 'power, dominance, heroism, intimidation'
    },
    'high-angle': {
      prompt: 'high angle shot, camera looking down at subject',
      psychology: 'vulnerability, weakness, surveillance, overview'
    },
    'dutch-angle': {
      prompt: 'dutch angle shot, tilted frame creating unease',
      psychology: 'disorientation, tension, instability, madness'
    },
    'birds-eye': {
      prompt: 'bird\'s eye view, directly overhead looking down',
      psychology: 'god-like perspective, fate, insignificance'
    },
    'worms-eye': {
      prompt: 'worm\'s eye view, extreme low angle from ground level',
      psychology: 'extreme power, monumentality, childlike wonder'
    },
    'over-shoulder': {
      prompt: 'over-the-shoulder shot, POV of conversation participant',
      psychology: 'connection, dialogue intimacy, suspense'
    }
  },

  // Lighting setups
  lightingSetups: {
    'three-point': {
      prompt: 'three-point lighting, professional balanced illumination',
      mood: 'professional, neutral, clear'
    },
    'high-key': {
      prompt: 'high-key lighting, bright and evenly lit, minimal shadows',
      mood: 'optimistic, happy, clean, commercial'
    },
    'low-key': {
      prompt: 'low-key lighting, dramatic shadows, high contrast',
      mood: 'mysterious, dramatic, noir, tension'
    },
    'chiaroscuro': {
      prompt: 'chiaroscuro lighting, extreme contrast between light and dark',
      mood: 'dramatic, artistic, renaissance, profound'
    },
    'motivated': {
      prompt: 'motivated lighting from visible source (window, lamp, screen)',
      mood: 'realistic, grounded, natural'
    },
    'neon': {
      prompt: 'neon lighting, vibrant colored light sources',
      mood: 'futuristic, cyberpunk, nightlife, edgy'
    },
    'golden-hour': {
      prompt: 'golden hour lighting, warm sunset/sunrise glow',
      mood: 'romantic, nostalgic, magical, peaceful'
    },
    'blue-hour': {
      prompt: 'blue hour lighting, cool twilight ambiance',
      mood: 'melancholic, contemplative, mysterious'
    },
    'practical': {
      prompt: 'practical lighting from in-scene sources only',
      mood: 'realistic, immersive, cinematic'
    }
  }
};

// =============================================================================
// SECTION 9.0: BLUEPRINT EXTRACTOR - Extract Cinematography from Visual Text
// =============================================================================

/**
 * BLUEPRINT_EXTRACTOR
 * Extracts structured cinematographic data from plain-text visual descriptions.
 * This bridges the gap between simple scene.visual text and the structured
 * visualBlueprint/actionBlueprint that IMAGE_PROMPT_GENERATOR expects.
 */
const BLUEPRINT_EXTRACTOR = {

  /**
   * Quick pattern-based extraction for common cinematography elements
   * Used for immediate prompt enhancement without AI calls
   */
  quickExtract: (sceneVisual) => {
    if (!sceneVisual || typeof sceneVisual !== 'string') {
      return { visualBlueprint: {}, actionBlueprint: {} };
    }

    const visual = sceneVisual.toLowerCase();
    const blueprint = {
      visualBlueprint: {
        rawVisual: sceneVisual.replace(/\[.*?\]\s*/, '') // Store cleaned visual
      },
      actionBlueprint: {}
    };

    // Extract camera movement from brackets [Push in, etc]
    const cameraMatch = sceneVisual.match(/\[(.*?)\]/);
    if (cameraMatch) {
      const movement = cameraMatch[1].toLowerCase();
      if (movement.includes('push') && movement.includes('in')) blueprint.visualBlueprint.cameraMovement = 'push-in';
      else if (movement.includes('pull') || movement.includes('out')) blueprint.visualBlueprint.cameraMovement = 'pull-out';
      else if (movement.includes('pan') && movement.includes('left')) blueprint.visualBlueprint.cameraMovement = 'pan-left';
      else if (movement.includes('pan') && movement.includes('right')) blueprint.visualBlueprint.cameraMovement = 'pan-right';
      else if (movement.includes('track')) blueprint.visualBlueprint.cameraMovement = 'tracking';
      else if (movement.includes('static') || movement.includes('hold')) blueprint.visualBlueprint.cameraMovement = 'static';
      else if (movement.includes('crane')) blueprint.visualBlueprint.cameraMovement = 'crane';
      else if (movement.includes('handheld')) blueprint.visualBlueprint.cameraMovement = 'handheld';
      else if (movement.includes('zoom') && movement.includes('in')) blueprint.visualBlueprint.cameraMovement = 'push-in';
      else if (movement.includes('tilt') && movement.includes('up')) blueprint.visualBlueprint.cameraMovement = 'tilt-up';
      else if (movement.includes('tilt') && movement.includes('down')) blueprint.visualBlueprint.cameraMovement = 'tilt-down';

      // Also store for action blueprint
      blueprint.actionBlueprint.cameraAction = {
        movement: blueprint.visualBlueprint.cameraMovement || movement,
        speed: movement.includes('slow') ? 'slow' : movement.includes('fast') ? 'fast' : 'medium'
      };
    }

    // Extract lighting keywords
    if (visual.includes('golden hour') || visual.includes('sunset') || visual.includes('sunrise')) {
      blueprint.visualBlueprint.lightingSetup = 'golden-hour';
      blueprint.visualBlueprint.colorTemperature = 'warm 3000K-4000K';
    } else if (visual.includes('blue hour') || visual.includes('twilight') || visual.includes('dusk')) {
      blueprint.visualBlueprint.lightingSetup = 'blue-hour';
      blueprint.visualBlueprint.colorTemperature = 'cool 6000K-8000K';
    } else if (visual.includes('shadow') || visual.includes('noir') || visual.includes('dark') || visual.includes('chiaroscuro')) {
      blueprint.visualBlueprint.lightingSetup = 'low-key';
    } else if (visual.includes('neon') || visual.includes('cyberpunk') || visual.includes('electric')) {
      blueprint.visualBlueprint.lightingSetup = 'neon';
    } else if (visual.includes('bright') || visual.includes('even light') || visual.includes('well-lit')) {
      blueprint.visualBlueprint.lightingSetup = 'high-key';
    } else if (visual.includes('lamp') || visual.includes('screen') || visual.includes('fire') || visual.includes('candle')) {
      blueprint.visualBlueprint.lightingSetup = 'practical';
    }

    // Extract shot type keywords
    if (visual.includes('extreme close') || visual.includes('macro')) {
      blueprint.visualBlueprint.shotType = 'extreme-close-up';
    } else if (visual.includes('close-up') || visual.includes('closeup') || visual.includes('close up')) {
      blueprint.visualBlueprint.shotType = 'close-up';
    } else if (visual.includes('extreme wide') || visual.includes('vast') || visual.includes('panoramic')) {
      blueprint.visualBlueprint.shotType = 'extreme-wide';
    } else if (visual.includes('wide shot') || visual.includes('wide-shot') || visual.includes('establishing')) {
      blueprint.visualBlueprint.shotType = 'wide';
    } else if (visual.includes('medium close') || visual.includes('mid-close')) {
      blueprint.visualBlueprint.shotType = 'medium-close';
    } else if (visual.includes('medium shot') || visual.includes('medium-shot') || visual.includes('mid shot')) {
      blueprint.visualBlueprint.shotType = 'medium';
    }

    // Extract camera angle
    if (visual.includes('low angle') || visual.includes('low-angle') || visual.includes('looking up')) {
      blueprint.visualBlueprint.cameraAngle = 'low-angle';
    } else if (visual.includes('high angle') || visual.includes('high-angle') || visual.includes('looking down') || visual.includes('bird')) {
      blueprint.visualBlueprint.cameraAngle = 'high-angle';
    } else if (visual.includes('dutch') || visual.includes('tilted') || visual.includes('canted')) {
      blueprint.visualBlueprint.cameraAngle = 'dutch-angle';
    } else if (visual.includes('over shoulder') || visual.includes('over-shoulder') || visual.includes('over the shoulder')) {
      blueprint.visualBlueprint.cameraAngle = 'over-shoulder';
    }

    // Extract atmospheric elements
    const particles = [];
    if (visual.includes('dust') || visual.includes('motes')) particles.push('dust motes floating in light');
    if (visual.includes('smoke') || visual.includes('smoking')) particles.push('wisps of smoke');
    if (visual.includes('ember') || visual.includes('spark')) particles.push('floating embers and sparks');
    if (visual.includes('rain')) particles.push('rain droplets');
    if (visual.includes('mist') || visual.includes('fog')) particles.push('atmospheric mist');
    if (visual.includes('snow')) particles.push('falling snowflakes');
    if (visual.includes('god ray') || visual.includes('light beam') || visual.includes('rays of light')) particles.push('volumetric god rays');
    if (particles.length > 0) {
      blueprint.visualBlueprint.particles = particles.join(', ');
    }

    // Extract mood
    if (visual.includes('tense') || visual.includes('tension') || visual.includes('anxious')) {
      blueprint.visualBlueprint.mood = 'tense';
    } else if (visual.includes('contemplative') || visual.includes('thoughtful') || visual.includes('reflective')) {
      blueprint.visualBlueprint.mood = 'contemplative';
    } else if (visual.includes('hopeful') || visual.includes('optimistic') || visual.includes('uplifting')) {
      blueprint.visualBlueprint.mood = 'hopeful';
    } else if (visual.includes('mysterious') || visual.includes('enigmatic') || visual.includes('cryptic')) {
      blueprint.visualBlueprint.mood = 'mysterious';
    } else if (visual.includes('epic') || visual.includes('grand') || visual.includes('majestic')) {
      blueprint.visualBlueprint.mood = 'epic';
    } else if (visual.includes('intimate') || visual.includes('personal') || visual.includes('close')) {
      blueprint.visualBlueprint.mood = 'intimate';
    } else if (visual.includes('melanchol') || visual.includes('sad') || visual.includes('somber')) {
      blueprint.visualBlueprint.mood = 'melancholic';
    }

    // Extract environment layers using pattern matching
    const fgMatch = sceneVisual.match(/foreground[:\s]+([^.]+)/i);
    const mgMatch = sceneVisual.match(/midground[:\s]+([^.]+)/i);
    const bgMatch = sceneVisual.match(/background[:\s]+([^.]+)/i);

    if (fgMatch) blueprint.visualBlueprint.foreground = fgMatch[1].trim();
    if (mgMatch) blueprint.visualBlueprint.midground = mgMatch[1].trim();
    if (bgMatch) blueprint.visualBlueprint.background = bgMatch[1].trim();

    // If no layers found, try to identify them from context
    if (!blueprint.visualBlueprint.foreground && !blueprint.visualBlueprint.midground) {
      blueprint.visualBlueprint.midground = sceneVisual.replace(/\[.*?\]\s*/, '').trim();
    }

    return blueprint;
  },

  /**
   * Apply genre-specific cinematography defaults
   */
  applyGenreDefaults: (blueprint, genre, productionMode) => {
    const defaults = {
      'documentary-nature': {
        shotType: 'wide',
        cameraAngle: 'eye-level',
        lightingSetup: 'golden-hour',
        colorTemperature: 'warm natural',
        mood: 'epic',
        filmLook: 'National Geographic cinematography, BBC Earth quality'
      },
      'thriller': {
        shotType: 'close-up',
        cameraAngle: 'dutch-angle',
        lightingSetup: 'low-key',
        colorTemperature: 'cool desaturated',
        mood: 'tense',
        filmLook: 'David Fincher aesthetic, paranoid framing'
      },
      'cinematic': {
        shotType: 'medium-wide',
        cameraAngle: 'eye-level',
        lightingSetup: 'practical',
        colorTemperature: 'cinematic balanced',
        mood: 'epic',
        filmLook: 'Roger Deakins cinematography, Denis Villeneuve composition'
      },
      'documentary': {
        shotType: 'medium',
        cameraAngle: 'eye-level',
        lightingSetup: 'motivated',
        colorTemperature: 'natural',
        mood: 'authentic',
        filmLook: 'documentary realism, observational style'
      },
      'inspirational': {
        shotType: 'wide',
        cameraAngle: 'low-angle',
        lightingSetup: 'golden-hour',
        colorTemperature: 'warm 4000K',
        mood: 'hopeful',
        filmLook: 'aspirational imagery, uplifting visual style'
      },
      'story': {
        shotType: 'medium',
        cameraAngle: 'eye-level',
        lightingSetup: 'motivated',
        colorTemperature: 'natural',
        mood: 'engaging',
        filmLook: 'prestige television cinematography'
      },
      'horror': {
        shotType: 'close-up',
        cameraAngle: 'dutch-angle',
        lightingSetup: 'low-key',
        colorTemperature: 'cool blue-green',
        mood: 'dread',
        filmLook: 'atmospheric horror, deep shadows, negative space'
      }
    };

    const genreDefaults = defaults[genre] || defaults[productionMode] || defaults['cinematic'];

    // Apply defaults only for missing properties
    const vb = blueprint.visualBlueprint;
    if (!vb.shotType) vb.shotType = genreDefaults.shotType;
    if (!vb.cameraAngle) vb.cameraAngle = genreDefaults.cameraAngle;
    if (!vb.lightingSetup) vb.lightingSetup = genreDefaults.lightingSetup;
    if (!vb.colorTemperature) vb.colorTemperature = genreDefaults.colorTemperature;
    if (!vb.mood) vb.mood = genreDefaults.mood;
    if (!vb.filmLook) vb.filmLook = genreDefaults.filmLook;

    return blueprint;
  },

  /**
   * Full extraction: pattern matching + genre defaults
   * This is the main entry point
   */
  extractFromScene: (scene, genre, productionMode) => {
    const visual = scene.visual || scene.visualPrompt || '';

    // First, do pattern-based quick extraction
    let blueprint = BLUEPRINT_EXTRACTOR.quickExtract(visual);

    // Apply genre-specific defaults for missing elements
    blueprint = BLUEPRINT_EXTRACTOR.applyGenreDefaults(blueprint, genre, productionMode);

    // Merge with any existing blueprint data from the scene
    if (scene.visualBlueprint) {
      blueprint.visualBlueprint = { ...blueprint.visualBlueprint, ...scene.visualBlueprint };
    }
    if (scene.actionBlueprint) {
      blueprint.actionBlueprint = { ...blueprint.actionBlueprint, ...scene.actionBlueprint };
    }

    return blueprint;
  }
};

// =============================================================================
// PRODUCTION BIBLE CONSTANTS - Module-level for use by IMAGE_PROMPT_GENERATOR
// =============================================================================

/**
 * SHOT_COMPOSITION_TEMPLATES - Cinematic framing templates for scene types
 * These define camera positioning, depth of field, and visual rhythm
 */
const SHOT_COMPOSITION_TEMPLATES_GLOBAL = {
  'hero_entrance': {
    description: 'Character enters frame with impact',
    composition: 'Low angle, character silhouette against sky/light, slow push in',
    dof: 'Deep focus transitioning to shallow on subject',
    movement: 'Slow dolly forward or crane up',
    timing: 'Hold wide for 2 beats, then reveal'
  },
  'confrontation': {
    description: 'Two opposing forces face off',
    composition: 'Over-the-shoulder alternating, tight close-ups, eye-level',
    dof: 'Shallow on active speaker, rack focus on reaction',
    movement: 'Static with subtle push on tension moments',
    timing: 'Quick cuts building pace, then hold on decisive moment'
  },
  'emotional_peak': {
    description: 'Character experiences breakthrough or breakdown',
    composition: 'Extreme close-up on eyes, then pull back to reveal',
    dof: 'Ultra-shallow, background completely dissolved',
    movement: 'Subtle orbit or slow zoom out',
    timing: 'Extended hold, breathing room'
  },
  'action_sequence': {
    description: 'High energy physical action',
    composition: 'Wide establishing, then tight coverage, Dutch angles for chaos',
    dof: 'Deep focus for spatial awareness',
    movement: 'Handheld energy, whip pans, tracking shots',
    timing: 'Rapid cuts, 1-2 second average shot length'
  },
  'revelation': {
    description: 'Secret or truth is exposed',
    composition: 'Start tight on reactor, then reveal the cause',
    dof: 'Rack focus from face to revealed object/person',
    movement: 'Slow push or pull depending on emotional valence',
    timing: 'Build anticipation, then snap reveal'
  },
  'intimate_dialogue': {
    description: 'Two characters share a personal moment',
    composition: 'Two-shot favoring space between, soft lighting',
    dof: 'Medium shallow, both subjects in focus',
    movement: 'Static or gentle float',
    timing: 'Longer takes, let performances breathe'
  },
  'establishing_world': {
    description: 'Introduce location or setting',
    composition: 'Epic wide, then guided tour of details',
    dof: 'Deep focus showcasing environment',
    movement: 'Crane, drone, or slow pan revealing scope',
    timing: 'Allow time to absorb visual information'
  },
  'montage_beat': {
    description: 'Single moment in a sequence',
    composition: 'Centered subject, clean background, iconic framing',
    dof: 'Medium depth, subject isolated',
    movement: 'Static or simple push/pull',
    timing: '2-4 seconds per beat, rhythm driven'
  },
  'tension_build': {
    description: 'Something bad is about to happen',
    composition: 'Wide with empty space suggesting threat, or claustrophobic tight',
    dof: 'Deep focus creating paranoia of unseen threat',
    movement: 'Slow creeping dolly, or locked-off stillness',
    timing: 'Extended takes, uncomfortable holds'
  },
  'resolution': {
    description: 'Story threads come together',
    composition: 'Balanced frame, subjects in harmony, natural light',
    dof: 'Medium depth, world in soft focus behind',
    movement: 'Gentle pullback or ascending crane',
    timing: 'Measured pace, satisfying resolution beats'
  }
};

/**
 * VISUAL_MOTIFS_GLOBAL - Recurring symbols and imagery for thematic consistency
 */
const VISUAL_MOTIFS_GLOBAL = {
  'light_shadow': {
    meaning: 'Truth vs deception, hope vs despair, revelation',
    manifestations: [
      'Character stepping from shadow into light',
      'Shadows falling across faces during moral conflict',
      'Light sources representing knowledge or safety',
      'Darkness encroaching during threat'
    ]
  },
  'reflections': {
    meaning: 'Self-examination, duality, hidden truth, alternate reality',
    manifestations: [
      'Character seeing reflection in mirror/water/glass',
      'Distorted reflections during identity crisis',
      'Broken mirrors for shattered self',
      'Perfect reflections for harmony/acceptance'
    ]
  },
  'doorways_thresholds': {
    meaning: 'Transition, choice, point of no return, new chapter',
    manifestations: [
      'Character pausing at threshold',
      'Looking back before crossing',
      'Doors closing behind (finality)',
      'Multiple doors (choice/destiny)'
    ]
  },
  'water': {
    meaning: 'Emotion, purification, danger, the subconscious',
    manifestations: [
      'Rain during emotional release',
      'Still water for peace/reflection',
      'Turbulent water for inner turmoil',
      'Submersion as transformation/rebirth'
    ]
  },
  'fire': {
    meaning: 'Passion, destruction, renewal, danger, warmth',
    manifestations: [
      'Flames during intense emotion',
      'Candles for intimacy/hope',
      'Conflagration as climax/cleansing',
      'Embers as fading hope or smoldering threat'
    ]
  },
  'clocks_time': {
    meaning: 'Mortality, urgency, memory, fate',
    manifestations: [
      'Clock prominently ticking',
      'Time-lapse sequences',
      'Broken/stopped clocks for frozen moment',
      'Watches as personal time markers'
    ]
  }
};

/**
 * LIGHTING_SETUPS_GLOBAL - Named lighting configurations for consistency
 */
const LIGHTING_SETUPS_GLOBAL = {
  'rembrandt': {
    description: 'Classic dramatic lighting with triangle on shadow side of face',
    keyPosition: '45 degrees from subject, elevated',
    fillRatio: 'Key:Fill 4:1 or higher',
    mood: 'Dramatic, artistic, mysterious',
    colorTemp: 'Warm key, neutral fill'
  },
  'butterfly': {
    description: 'Glamorous Hollywood lighting with shadow under nose',
    keyPosition: 'Directly in front, elevated',
    fillRatio: 'Key:Fill 2:1',
    mood: 'Glamorous, beautiful, classic Hollywood',
    colorTemp: 'Warm overall'
  },
  'split': {
    description: 'Half the face lit, half in shadow',
    keyPosition: '90 degrees from subject',
    fillRatio: 'Key:Fill 8:1 or no fill',
    mood: 'Duality, conflict, mystery, danger',
    colorTemp: 'Cool for menace, warm for complexity'
  },
  'silhouette': {
    description: 'Subject completely backlit, no face detail',
    keyPosition: 'Behind subject',
    fillRatio: 'No fill',
    mood: 'Mystery, power, anonymity, epic',
    colorTemp: 'Based on background light source'
  },
  'practical_naturalism': {
    description: 'Motivated by in-scene light sources',
    keyPosition: 'From visible lamps, windows, screens',
    fillRatio: 'Varies with environment',
    mood: 'Realistic, grounded, authentic',
    colorTemp: 'Mixed based on practicals'
  },
  'high_key': {
    description: 'Bright, even lighting with minimal shadows',
    keyPosition: 'Multiple sources, even coverage',
    fillRatio: 'Key:Fill 1:1',
    mood: 'Happy, safe, comedic, dreamlike',
    colorTemp: 'Bright neutral or slight warmth'
  },
  'chiaroscuro': {
    description: 'Extreme contrast between light and dark',
    keyPosition: 'Single hard source',
    fillRatio: 'Key:Fill 16:1 or higher',
    mood: 'Film noir, danger, psychological depth',
    colorTemp: 'Cool with isolated warm spots'
  },
  'motivated_moonlight': {
    description: 'Night exterior with moon as implied key',
    keyPosition: 'High angle, off camera',
    fillRatio: 'Key:Fill 8:1',
    mood: 'Romantic, mysterious, nocturnal',
    colorTemp: 'Blue key, warm practicals'
  }
};

/**
 * COLOR_GRADING_PRESETS_GLOBAL - Named color grades for visual consistency
 */
const COLOR_GRADING_PRESETS_GLOBAL = {
  'teal_orange': {
    description: 'Complementary blockbuster look',
    shadows: 'Teal/cyan push',
    midtones: 'Slight desaturation except skin',
    highlights: 'Orange/gold warmth',
    contrast: 'Crushed blacks, punchy mids',
    saturation: 'Selective - skin warm, environment cool',
    use: 'Action, sci-fi, modern blockbuster'
  },
  'bleach_bypass': {
    description: 'Desaturated, high contrast gritty look',
    shadows: 'Deep true black',
    midtones: 'Desaturated, silver-gray',
    highlights: 'Blown, harsh',
    contrast: 'Extreme',
    saturation: '40-60% reduction',
    use: 'War, thriller, gritty drama'
  },
  'golden_hour': {
    description: 'Warm romantic magic hour look',
    shadows: 'Warm brown-orange',
    midtones: 'Rich warm tones',
    highlights: 'Golden bloom',
    contrast: 'Soft, lifted blacks',
    saturation: 'Enhanced warm colors',
    use: 'Romance, coming-of-age, nostalgia'
  },
  'noir': {
    description: 'Classic black and white or near-monochrome',
    shadows: 'Deep black, detail preserved',
    midtones: 'Silver gray, sharp',
    highlights: 'Clean white, occasional bloom',
    contrast: 'High with smooth falloff',
    saturation: 'Monochrome or heavily desaturated',
    use: 'Crime, mystery, psychological thriller'
  },
  'vintage_warmth': {
    description: 'Film emulation with lifted blacks',
    shadows: 'Lifted, green-brown tint',
    midtones: 'Faded, warm',
    highlights: 'Soft rolloff, cream tint',
    contrast: 'Reduced, gentle',
    saturation: 'Muted, vintage palette',
    use: 'Period piece, memory sequence, indie'
  },
  'cyberpunk': {
    description: 'Neon-drenched futuristic look',
    shadows: 'Deep purple-blue',
    midtones: 'Magenta-teal split',
    highlights: 'Hot pink or cyan bloom',
    contrast: 'High with crushed mids',
    saturation: 'Hypersaturated neons, desaturated else',
    use: 'Sci-fi, dystopia, club scenes'
  },
  'documentary_natural': {
    description: 'Neutral, truthful representation',
    shadows: 'Natural, detail preserved',
    midtones: 'Neutral, accurate',
    highlights: 'Natural rolloff',
    contrast: 'Moderate, realistic',
    saturation: 'Natural, slight enhancement',
    use: 'Documentary, interview, realism'
  }
};

// =============================================================================
// SECTION 9.0: VISUAL_STYLE_DNA - Comprehensive Visual Style Engine
// =============================================================================
/**
 * VISUAL_STYLE_DNA
 *
 * The definitive visual style engine that controls HOW images are rendered.
 * This is the difference between amateur and jaw-dropping professional output.
 *
 * Each style mode includes:
 * - promptPrefix: Critical keywords added at START of every prompt
 * - cameraLanguage: Technical camera/lens specifications for authenticity
 * - lightingStyle: How lighting should be described per style
 * - textureKeywords: Surface/material quality descriptors
 * - negativeKeywords: Explicit list of what to AVOID
 * - referenceFilms: Style references for AI context
 * - colorGrading: Color treatment approach
 * - skinRendering: How human skin should appear (critical for realism)
 */
const VISUAL_STYLE_DNA = {

  // Available style modes
  STYLE_MODES: {
    PHOTOREALISTIC: 'photorealistic',
    CINEMATIC: 'cinematic',
    STYLIZED_3D: 'stylized_3d',
    ILLUSTRATED: 'illustrated',
    ANIME: 'anime',
    NOIR: 'noir'
  },

  // ========================================
  // PHOTOREALISTIC - Real photography look
  // ========================================
  photorealistic: {
    id: 'photorealistic',
    name: 'Photorealistic',
    description: 'Real photography, documentary realism, actual human beings',
    icon: 'ðŸ“·',

    // CRITICAL: These keywords MUST appear at the START of every prompt
    promptPrefix: 'Professional photograph, real photography, genuine human being, authentic documentary realism, actual physical location, shot on location,',

    // Camera authenticity language
    cameraLanguage: {
      primary: 'Shot on ARRI Alexa Mini with Zeiss Master Prime lenses',
      alternatives: [
        'Canon EOS R5 with RF 85mm f/1.2',
        'Sony A7R IV with GM 24-70mm f/2.8',
        'RED Komodo with Sigma Art primes',
        'Blackmagic URSA with vintage Cooke lenses'
      ],
      sensorDescription: 'full-frame sensor, natural depth of field',
      shutterLanguage: '180-degree shutter, natural motion'
    },

    // Lighting approach for realism
    lightingStyle: {
      approach: 'Natural available light with subtle fill',
      keywords: ['natural window light', 'golden hour sunlight', 'overcast soft light', 'practical lighting from scene elements'],
      avoid: ['dramatic stylized lighting', 'colored gels', 'artificial studio look']
    },

    // Texture and surface rendering
    textureKeywords: [
      'natural skin texture with pores and fine details',
      'real fabric weave visible',
      'authentic material surfaces',
      'environmental textures like weathered wood and worn metal',
      'imperfect real-world surfaces',
      'subsurface skin scattering'
    ],

    // CRITICAL: Comprehensive negative keywords
    negativeKeywords: [
      '3D render', '3D model', 'CGI', 'computer generated', 'digital art',
      'stylized', 'animation', 'cartoon', 'anime', 'illustration',
      'Unreal Engine', 'Unity', 'video game', 'game render', 'game screenshot',
      'synthetic', 'artificial', 'plastic skin', 'waxy skin', 'porcelain skin',
      'hyperreal', 'oversaturated', 'HDR overprocessed',
      'airbrushed', 'smooth skin', 'perfect skin', 'flawless',
      'fantasy lighting', 'impossible lighting', 'rim light everywhere',
      'concept art', 'matte painting', 'digital painting',
      'stock photo', 'generic', 'posed unnaturally'
    ],

    // Film references for style context
    referenceFilms: ['The Revenant', 'No Country for Old Men', 'Zodiac', 'Sicario', 'Hell or High Water'],

    // Color grading approach
    colorGrading: {
      approach: 'Natural color science with subtle grading',
      lut: 'Film emulation - Kodak 5219 or Fuji Eterna',
      saturation: 'Natural, never oversaturated',
      contrast: 'Realistic dynamic range',
      highlights: 'Soft roll-off, no clipping',
      shadows: 'Rich detail, not crushed'
    },

    // Human skin rendering - CRITICAL for realism
    skinRendering: {
      texture: 'visible pores, fine lines, natural imperfections',
      subsurface: 'subtle subsurface scattering in ears and fingertips',
      color: 'natural undertones, blood vessel hints in thin skin areas',
      avoid: 'plastic, waxy, airbrushed, porcelain, unnaturally smooth'
    },

    // Technical quality specs
    technicalSpecs: {
      resolution: '8K RAW quality',
      sharpness: 'Natural sharpness, not oversharpened',
      noise: 'Subtle organic film grain acceptable',
      dof: 'Natural depth of field based on aperture'
    }
  },

  // ========================================
  // CINEMATIC - Hollywood film look
  // ========================================
  cinematic: {
    id: 'cinematic',
    name: 'Cinematic',
    description: 'Hollywood blockbuster look, dramatic but grounded in reality',
    icon: 'ðŸŽ¬',

    promptPrefix: 'Cinematic film still, real human being, actual person, Hollywood production quality, dramatic cinematography, professional movie lighting, genuine actors,',

    cameraLanguage: {
      primary: 'Shot on ARRI Alexa 65 with Panavision anamorphic lenses',
      alternatives: [
        'RED V-Raptor with Cooke Anamorphic/i',
        'Sony Venice with Hawk V-Lite anamorphics',
        'ARRI Alexa Mini LF with vintage Kowa anamorphics'
      ],
      sensorDescription: 'large format sensor, anamorphic lens characteristics',
      shutterLanguage: 'cinematic motion blur, 24fps aesthetic'
    },

    lightingStyle: {
      approach: 'Dramatic three-point lighting with motivated sources',
      keywords: ['key light with character', 'atmospheric fill', 'rim light separation', 'practical motivation', 'volumetric atmosphere'],
      avoid: ['flat lighting', 'amateur on-camera flash', 'unmotivated colors']
    },

    textureKeywords: [
      'cinematic skin rendering',
      'movie-quality costumes and props',
      'high-budget production design',
      'atmospheric depth and haze',
      'anamorphic lens characteristics like oval bokeh and subtle flares'
    ],

    negativeKeywords: [
      'amateur', 'low budget', 'TV movie quality', 'soap opera',
      'video look', 'digital harshness', 'flat lighting',
      'cartoon', 'anime', 'illustration', '3D animated', '3D render', 'CGI character',
      'computer generated', 'Unreal Engine', 'video game', 'Pixar', 'Disney animation',
      'stylized', 'synthetic skin', 'plastic skin', 'waxy skin',
      'oversaturated', 'instagram filter', 'mobile phone quality',
      'stock footage', 'corporate video'
    ],

    referenceFilms: ['Blade Runner 2049', 'Dune', 'Mad Max Fury Road', 'Arrival', 'Interstellar'],

    colorGrading: {
      approach: 'Stylized but grounded color grading',
      lut: 'Custom film LUT with teal/orange split-toning option',
      saturation: 'Controlled saturation with specific color emphasis',
      contrast: 'Rich contrast with preserved details',
      highlights: 'Blooming highlights when motivated',
      shadows: 'Deep shadows with detail'
    },

    skinRendering: {
      texture: 'real human skin with natural texture, visible pores, refined but authentic',
      subsurface: 'natural subsurface scattering like real skin',
      color: 'real human skin tones, natural undertones, authentic complexion',
      avoid: '3D rendered skin, plastic skin, waxy skin, synthetic skin, CGI skin, porcelain doll, airbrushed'
    },

    technicalSpecs: {
      resolution: '6K cinematic quality',
      sharpness: 'Cinema sharpness, slight softness acceptable for mood',
      noise: 'Clean with optional stylistic grain',
      dof: 'Shallow cinematic depth of field'
    }
  },

  // ========================================
  // STYLIZED 3D - Modern 3D animation
  // ========================================
  stylized_3d: {
    id: 'stylized_3d',
    name: 'Stylized 3D',
    description: 'Pixar/Disney quality 3D animation, expressive and polished',
    icon: 'ðŸŽ¨',

    promptPrefix: 'High-end 3D animated film, Pixar animation quality, stylized 3D render, expressive character design, modern animation studio quality,',

    cameraLanguage: {
      primary: 'Virtual camera with cinematic 3D animation cinematography',
      sensorDescription: 'perfect virtual lens, impossible camera moves possible',
      shutterLanguage: 'stylized motion blur for animation'
    },

    lightingStyle: {
      approach: 'Expressive stylized lighting that supports mood',
      keywords: ['global illumination', 'bounce light', 'subsurface scattering', 'volumetric god rays', 'stylized rim lights'],
      avoid: ['flat even lighting', 'harsh shadows without purpose']
    },

    textureKeywords: [
      'stylized but detailed surfaces',
      'appealing material rendering',
      'Pixar-quality subsurface skin',
      'expressive texture work',
      'clean but characterful surfaces'
    ],

    negativeKeywords: [
      'photorealistic', 'live action', 'real photography',
      'low poly', 'mobile game', 'amateur 3D',
      'uncanny valley', 'creepy', 'unsettling',
      'flat shading', 'no lighting', 'PS2 graphics',
      '2D illustration', 'hand drawn', 'sketch'
    ],

    referenceFilms: ['Soul', 'Coco', 'Spider-Verse', 'The Incredibles', 'How to Train Your Dragon'],

    colorGrading: {
      approach: 'Vibrant, appealing, story-driven colors',
      saturation: 'Rich and vibrant but controlled',
      contrast: 'Stylized contrast for mood'
    },

    skinRendering: {
      texture: 'smooth stylized skin with subtle detail',
      subsurface: 'beautiful subsurface scattering, warm and appealing',
      color: 'appealing idealized skin tones',
      avoid: 'realistic pores, harsh imperfections'
    },

    technicalSpecs: {
      resolution: '4K animation render quality',
      sharpness: 'Crisp stylized edges',
      noise: 'No noise, clean render',
      dof: 'Stylized depth of field'
    }
  },

  // ========================================
  // ILLUSTRATED - Artistic/concept art
  // ========================================
  illustrated: {
    id: 'illustrated',
    name: 'Illustrated',
    description: 'Concept art quality, painterly, artistic interpretation',
    icon: 'ðŸ–Œï¸',

    promptPrefix: 'Professional concept art, digital illustration, painterly style, artistic interpretation, high-end illustration,',

    cameraLanguage: {
      primary: 'Artistic composition with illustrative perspective',
      sensorDescription: 'artistic interpretation of camera angles'
    },

    lightingStyle: {
      approach: 'Dramatic artistic lighting for visual impact',
      keywords: ['dramatic light and shadow', 'artistic color lighting', 'atmospheric mood lighting', 'painterly light rendering'],
      avoid: ['flat boring lighting', 'no contrast']
    },

    textureKeywords: [
      'visible brushwork',
      'painterly texture',
      'artistic surface treatment',
      'stylized material rendering',
      'concept art detail level'
    ],

    negativeKeywords: [
      'photorealistic', '3D render', 'photograph',
      'amateur art', 'bad anatomy', 'sketch quality',
      'unfinished', 'rough draft', 'low effort',
      'clip art', 'stock illustration'
    ],

    referenceFilms: ['Into the Spider-Verse (concept art)', 'Arcane', 'Klaus', 'The Art of Studio Ghibli'],

    colorGrading: {
      approach: 'Artistic color choices for emotional impact',
      saturation: 'Artist-controlled, can be stylized',
      contrast: 'Dramatic artistic contrast'
    },

    technicalSpecs: {
      resolution: '4K illustration quality',
      sharpness: 'Artistic sharpness with soft edges where needed',
      noise: 'Artistic texture acceptable'
    }
  },

  // ========================================
  // ANIME - Japanese animation style
  // ========================================
  anime: {
    id: 'anime',
    name: 'Anime',
    description: 'High-quality Japanese animation style, expressive and dynamic',
    icon: 'âœ¨',

    promptPrefix: 'High-quality anime style, Japanese animation, detailed anime art, studio quality anime, expressive anime characters,',

    cameraLanguage: {
      primary: 'Dynamic anime cinematography with dramatic angles'
    },

    lightingStyle: {
      approach: 'Anime-style lighting with cel-shading influence',
      keywords: ['anime lighting', 'dramatic shadows', 'rim light glow', 'atmospheric color'],
      avoid: ['flat lighting', 'no shading']
    },

    textureKeywords: [
      'clean anime linework',
      'cel-shaded surfaces',
      'anime hair rendering',
      'expressive anime eyes',
      'detailed anime backgrounds'
    ],

    negativeKeywords: [
      'photorealistic', '3D render', 'western cartoon',
      'bad anatomy', 'amateur anime', 'chibi (unless requested)',
      'low quality anime', 'rushed animation'
    ],

    referenceFilms: ['Your Name', 'Spirited Away', 'Violet Evergarden', 'Demon Slayer', 'Attack on Titan'],

    colorGrading: {
      approach: 'Vibrant anime color palette',
      saturation: 'Rich anime colors',
      contrast: 'Anime-appropriate contrast'
    },

    technicalSpecs: {
      resolution: '4K anime quality',
      sharpness: 'Crisp anime linework'
    }
  },

  // ========================================
  // NOIR - Dark, high-contrast dramatic
  // ========================================
  noir: {
    id: 'noir',
    name: 'Film Noir',
    description: 'Classic noir aesthetic, high contrast, dramatic shadows',
    icon: 'ðŸŒ‘',

    promptPrefix: 'Film noir style, high contrast black and white with selective color, dramatic shadows, moody atmosphere, classic noir cinematography,',

    cameraLanguage: {
      primary: 'Classic noir camera angles with dutch tilts and low angles',
      sensorDescription: 'noir cinematography style'
    },

    lightingStyle: {
      approach: 'Extreme chiaroscuro lighting with venetian blind shadows',
      keywords: ['hard shadows', 'single source dramatic lighting', 'noir silhouettes', 'smoke and atmosphere', 'neon reflections'],
      avoid: ['flat even lighting', 'bright cheerful lighting']
    },

    textureKeywords: [
      'wet streets reflecting neon',
      'smoky atmospheric haze',
      'gritty urban textures',
      'vintage noir surfaces'
    ],

    negativeKeywords: [
      'bright', 'cheerful', 'colorful', 'cartoon',
      'flat lighting', 'no shadows', 'amateur',
      'low quality', 'blurry'
    ],

    referenceFilms: ['Sin City', 'Blade Runner', 'LA Confidential', 'Chinatown', 'The Third Man'],

    colorGrading: {
      approach: 'Desaturated with selective color accents',
      saturation: 'Very low, almost monochrome',
      contrast: 'Extreme high contrast'
    },

    technicalSpecs: {
      resolution: '4K noir quality',
      sharpness: 'Sharp with grain',
      noise: 'Film noir grain encouraged'
    }
  },

  // ========================================
  // HELPER METHODS
  // ========================================

  /**
   * Get complete style configuration by mode
   * @param {string} styleMode - One of STYLE_MODES values
   * @returns {Object} Complete style configuration
   */
  getStyleConfig(styleMode) {
    const mode = styleMode?.toLowerCase() || 'photorealistic';
    return this[mode] || this.photorealistic;
  },

  /**
   * Generate the prompt prefix for a style
   * @param {string} styleMode - Style mode identifier
   * @returns {string} Prompt prefix to prepend
   */
  getPromptPrefix(styleMode) {
    const config = this.getStyleConfig(styleMode);
    return config.promptPrefix;
  },

  /**
   * Generate comprehensive negative prompt for a style
   * @param {string} styleMode - Style mode identifier
   * @returns {string} Negative prompt string
   */
  getNegativePrompt(styleMode) {
    const config = this.getStyleConfig(styleMode);
    const baseNegative = [
      'blurry', 'low quality', 'distorted', 'amateur',
      'text', 'watermark', 'signature', 'logo',
      'ugly', 'deformed', 'disfigured', 'mutated',
      'bad anatomy', 'extra limbs', 'missing limbs',
      'floating limbs', 'disconnected limbs',
      'malformed hands', 'extra fingers', 'missing fingers',
      'poorly drawn face', 'cloned face'
    ];

    const styleNegative = config.negativeKeywords || [];
    return [...baseNegative, ...styleNegative].join(', ');
  },

  /**
   * Get camera language for prompt enhancement
   * @param {string} styleMode - Style mode identifier
   * @returns {string} Camera technical language
   */
  getCameraLanguage(styleMode) {
    const config = this.getStyleConfig(styleMode);
    const cam = config.cameraLanguage;
    if (!cam) return '';

    return cam.primary + (cam.sensorDescription ? `, ${cam.sensorDescription}` : '');
  },

  /**
   * Get lighting keywords for the style
   * @param {string} styleMode - Style mode identifier
   * @returns {string[]} Array of lighting keywords
   */
  getLightingKeywords(styleMode) {
    const config = this.getStyleConfig(styleMode);
    return config.lightingStyle?.keywords || [];
  },

  /**
   * Get skin rendering guidance (for human subjects)
   * @param {string} styleMode - Style mode identifier
   * @returns {Object} Skin rendering configuration
   */
  getSkinRendering(styleMode) {
    const config = this.getStyleConfig(styleMode);
    return config.skinRendering || {
      texture: 'natural skin texture',
      avoid: 'artificial, plastic'
    };
  },

  /**
   * Build complete style enhancement for image prompt
   * @param {string} styleMode - Style mode identifier
   * @param {boolean} hasHumanSubject - Whether the scene has human characters
   * @returns {Object} { prefix, suffix, negative, camera, lighting }
   */
  buildStyleEnhancement(styleMode, hasHumanSubject = true) {
    const config = this.getStyleConfig(styleMode);

    let prefix = config.promptPrefix;

    // Add camera language
    const camera = this.getCameraLanguage(styleMode);

    // Add skin rendering for human subjects
    let skinText = '';
    if (hasHumanSubject && config.skinRendering) {
      const skin = config.skinRendering;
      skinText = `${skin.texture}, ${skin.color || ''}`.trim();
    }

    // Build technical suffix
    const tech = config.technicalSpecs || {};
    const suffix = [
      tech.resolution || '4K quality',
      tech.sharpness || 'sharp focus',
      config.colorGrading?.approach || ''
    ].filter(Boolean).join(', ');

    // Get negative prompt
    const negative = this.getNegativePrompt(styleMode);

    // Get lighting keywords
    const lighting = this.getLightingKeywords(styleMode).join(', ');

    return {
      prefix,
      suffix,
      negative,
      camera,
      lighting,
      skinText,
      referenceFilms: config.referenceFilms || []
    };
  },

  /**
   * Get all available styles for UI display
   * @returns {Array} Array of style objects for UI
   */
  getAllStyles() {
    return [
      { id: 'photorealistic', name: 'Photorealistic', icon: 'ðŸ“·', description: 'Real photography, documentary realism', recommended: true },
      { id: 'cinematic', name: 'Cinematic', icon: 'ðŸŽ¬', description: 'Hollywood blockbuster look' },
      { id: 'stylized_3d', name: 'Stylized 3D', icon: 'ðŸŽ¨', description: 'Pixar/Disney animation quality' },
      { id: 'illustrated', name: 'Illustrated', icon: 'ðŸ–Œï¸', description: 'Concept art, painterly style' },
      { id: 'anime', name: 'Anime', icon: 'âœ¨', description: 'Japanese animation style' },
      { id: 'noir', name: 'Film Noir', icon: 'ðŸŒ‘', description: 'High contrast, dramatic shadows' }
    ];
  }
};

// =============================================================================
// SECTION 9.1: IMAGE PROMPT GENERATOR - Scene to Image Transformation
// =============================================================================

/**
 * IMAGE PROMPT GENERATOR
 * Transforms scene visual blueprints into precise AI image prompts
 *
 * UPGRADED: Now integrates with VISUAL_STYLE_DNA for style control
 */
const IMAGE_PROMPT_GENERATOR = {

  /**
   * Generate HOLLYWOOD-QUALITY image prompt from scene script
   * Uses BLUEPRINT_EXTRACTOR to enhance simple scene.visual text
   * NOW INTEGRATES WITH VISUAL_STYLE_DNA for comprehensive style control
   *
   * @param {Object} scene - Scene object with visual/action descriptions
   * @param {Object} styleBible - Style guidelines
   * @param {Object} characterBible - Character definitions
   * @param {string} genre - Video genre
   * @param {string} productionMode - Production type
   * @param {string} visualStyleMode - VISUAL_STYLE_DNA mode (photorealistic, cinematic, etc.)
   */
  generateFromScene: (scene, styleBible, characterBible, genre, productionMode, visualStyleMode = 'photorealistic') => {
    // STEP 0: Get VISUAL_STYLE_DNA enhancement for the selected style mode
    // This is the KEY to getting jaw-dropping results - style-specific prompt engineering
    const hasHumanSubject = !!(characterBible && characterBible.length > 0);
    const styleEnhancement = VISUAL_STYLE_DNA.buildStyleEnhancement(visualStyleMode, hasHumanSubject);

    console.log(`[IMAGE_PROMPT_GENERATOR] Using visual style mode: ${visualStyleMode}`);
    console.log(`[IMAGE_PROMPT_GENERATOR] Style prefix: ${styleEnhancement.prefix.substring(0, 50)}...`);

    // STEP 1: Extract blueprint from scene visual description
    // This converts simple text like "[Push in] A workshop..." into structured cinematography data
    const extracted = BLUEPRINT_EXTRACTOR.extractFromScene(scene, genre, productionMode);
    const vb = { ...extracted.visualBlueprint, ...scene.visualBlueprint };
    const ab = { ...extracted.actionBlueprint, ...scene.actionBlueprint };

    // STEP 2: Build subject description (the main visual focus)
    const subject = IMAGE_PROMPT_GENERATOR.buildSubjectDescription(scene, characterBible, vb);

    // STEP 3: Build camera and composition description
    // Now enhanced with style-specific camera language
    const camera = IMAGE_PROMPT_GENERATOR.buildCameraDescription(vb);
    const cameraWithStyle = styleEnhancement.camera
      ? `${camera}, ${styleEnhancement.camera}`
      : camera;

    // STEP 4: Build environment layers (FG/MG/BG)
    const environment = IMAGE_PROMPT_GENERATOR.buildEnvironmentDescription(vb);

    // STEP 5: Build lighting design
    // Now enhanced with style-specific lighting
    const lighting = IMAGE_PROMPT_GENERATOR.buildLightingDescription(vb);
    const lightingWithStyle = styleEnhancement.lighting
      ? `${lighting}, ${styleEnhancement.lighting}`
      : lighting;

    // STEP 6: Build atmosphere and particles
    const atmosphere = IMAGE_PROMPT_GENERATOR.buildAtmosphereDescription(vb);

    // STEP 7: Build style and film look
    const style = IMAGE_PROMPT_GENERATOR.buildStyleDescription(vb, styleBible, genre);

    // STEP 8: Technical quality specs - NOW USING STYLE-SPECIFIC SPECS
    const technical = styleEnhancement.suffix || IMAGE_PROMPT_GENERATOR.buildTechnicalDescription(styleBible);

    // STEP 8.5: Add skin rendering for human subjects
    const skinRendering = hasHumanSubject && styleEnhancement.skinText
      ? styleEnhancement.skinText
      : '';

    // STEP 9: Compile into Hollywood-quality flowing prompt
    // CRITICAL: Style prefix MUST come FIRST for maximum impact
    const basePrompt = IMAGE_PROMPT_GENERATOR.compileHollywoodPrompt({
      subject,
      camera: cameraWithStyle,
      environment,
      lighting: lightingWithStyle,
      atmosphere,
      style,
      technical
    });

    // CRITICAL: Prepend the VISUAL_STYLE_DNA prefix to the prompt
    // This is what makes the difference between stylized 3D and photorealistic output
    const finalPrompt = `${styleEnhancement.prefix} ${skinRendering ? skinRendering + ', ' : ''}${basePrompt}`;

    // Build negative prompt - USE STYLE-SPECIFIC NEGATIVES
    // This is equally important to prevent unwanted rendering styles
    const negative = styleEnhancement.negative ||
      styleBible?.technicalSpecs?.negative ||
      'blurry, low quality, distorted, amateur, text, watermark, signature, ugly, deformed';

    console.log(`[IMAGE_PROMPT_GENERATOR] Final prompt length: ${finalPrompt.length} chars`);
    console.log(`[IMAGE_PROMPT_GENERATOR] Negative prompt length: ${negative.length} chars`);

    return {
      prompt: finalPrompt,
      negativePrompt: negative,
      visualStyleMode: visualStyleMode,
      metadata: {
        shotType: vb.shotType,
        cameraAngle: vb.cameraAngle,
        cameraMovement: vb.cameraMovement,
        mood: vb.mood,
        lightingSetup: vb.lightingSetup,
        filmLook: vb.filmLook,
        styleEnhancement: {
          mode: visualStyleMode,
          hasHumanSubject,
          referenceFilms: styleEnhancement.referenceFilms
        }
      }
    };
  },

  /**
   * Build camera and composition description
   */
  buildCameraDescription: (vb) => {
    const parts = [];

    // Shot type with descriptive text
    const shotDescriptors = {
      'extreme-wide': 'extreme wide shot showing vast scale',
      'wide': 'wide establishing shot',
      'medium-wide': 'medium-wide shot',
      'medium': 'medium shot framing',
      'medium-close': 'medium close-up',
      'close-up': 'intimate close-up',
      'extreme-close-up': 'extreme close-up on fine detail'
    };
    if (vb.shotType && shotDescriptors[vb.shotType]) {
      parts.push(shotDescriptors[vb.shotType]);
    }

    // Camera angle with cinematic impact
    const angleDescriptors = {
      'low-angle': 'low-angle looking up conveying power',
      'high-angle': 'high-angle looking down',
      'dutch-angle': 'dutch angle creating visual tension',
      'eye-level': 'eye-level natural framing',
      'birds-eye': 'birds eye aerial view',
      'worms-eye': 'extreme low worms-eye perspective',
      'over-shoulder': 'over-the-shoulder POV'
    };
    if (vb.cameraAngle && angleDescriptors[vb.cameraAngle]) {
      parts.push(angleDescriptors[vb.cameraAngle]);
    }

    // Camera movement (for context, though image is static)
    const movementContext = {
      'push-in': 'composed as if mid push-in',
      'pull-out': 'framed for pull-out reveal',
      'tracking': 'dynamic tracking composition',
      'handheld': 'handheld energy in framing',
      'crane': 'sweeping crane perspective',
      'static': 'locked-off composed frame'
    };
    if (vb.cameraMovement && movementContext[vb.cameraMovement]) {
      parts.push(movementContext[vb.cameraMovement]);
    }

    // Subject placement
    if (vb.subjectPlacement && vb.subjectPlacement !== 'center') {
      parts.push(`subject positioned ${vb.subjectPlacement.replace(/-/g, ' ')}`);
    }

    return parts.length > 0 ? parts.join(', ') : '';
  },

  /**
   * Build environment layers description
   */
  buildEnvironmentDescription: (vb) => {
    const layers = [];

    if (vb.foreground) {
      layers.push(`foreground: ${vb.foreground}`);
    }
    if (vb.midground && !vb.rawVisual) {
      layers.push(`midground: ${vb.midground}`);
    }
    if (vb.background) {
      layers.push(`background: ${vb.background}`);
    }

    return layers.length > 0 ? layers.join('; ') : '';
  },

  /**
   * Build lighting design description
   */
  buildLightingDescription: (vb) => {
    const parts = [];

    // Lighting setup with detailed description
    const lightingDescriptors = {
      'low-key': 'low-key dramatic lighting with deep shadows and high contrast',
      'high-key': 'bright high-key lighting with even illumination',
      'golden-hour': 'warm golden hour sunlight with soft shadows',
      'blue-hour': 'cool blue hour twilight ambiance',
      'chiaroscuro': 'Renaissance chiaroscuro with extreme light-dark contrast',
      'neon': 'vibrant neon lighting with colorful electric glow',
      'practical': 'realistic practical lighting from visible sources',
      'motivated': 'motivated lighting from natural scene sources',
      'three-point': 'balanced three-point studio lighting'
    };

    if (vb.lightingSetup && lightingDescriptors[vb.lightingSetup]) {
      parts.push(lightingDescriptors[vb.lightingSetup]);
    }

    // Add specific light sources if available
    if (vb.keyLight) parts.push(`key light: ${vb.keyLight}`);
    if (vb.fillLight) parts.push(`fill: ${vb.fillLight}`);
    if (vb.practicalLights?.length) {
      parts.push(`practicals: ${vb.practicalLights.join(', ')}`);
    }

    // Color temperature
    if (vb.colorTemperature) {
      parts.push(`${vb.colorTemperature} color temperature`);
    }

    return parts.length > 0 ? parts.join(', ') : '';
  },

  /**
   * Build atmosphere and particle description
   */
  buildAtmosphereDescription: (vb) => {
    const parts = [];

    // Time of day if available
    if (vb.timeOfDay) parts.push(vb.timeOfDay);

    // Weather conditions
    if (vb.weather && vb.weather !== 'clear') parts.push(vb.weather);

    // Atmospheric particles (critical for cinematic feel)
    if (vb.particles) parts.push(vb.particles);

    // Mood descriptor
    if (vb.mood) {
      const moodDescriptors = {
        'tense': 'tense atmosphere with palpable tension',
        'contemplative': 'contemplative reflective mood',
        'hopeful': 'hopeful uplifting atmosphere',
        'mysterious': 'mysterious enigmatic ambiance',
        'epic': 'epic grand scale feeling',
        'intimate': 'intimate personal atmosphere',
        'melancholic': 'melancholic somber mood',
        'dread': 'atmosphere of creeping dread',
        'authentic': 'authentic grounded realism'
      };
      parts.push(moodDescriptors[vb.mood] || `${vb.mood} mood`);
    }

    return parts.length > 0 ? parts.join(', ') : '';
  },

  /**
   * Build style and film look description
   */
  buildStyleDescription: (vb, styleBible, genre) => {
    const parts = [];

    // Film look from blueprint or style bible
    if (vb.filmLook) parts.push(vb.filmLook);
    if (styleBible?.cinematicReference) parts.push(styleBible.cinematicReference);
    if (styleBible?.visualStyle) parts.push(styleBible.visualStyle);
    if (styleBible?.colorGrade) parts.push(styleBible.colorGrade);

    // Color palette
    if (vb.dominantColors?.length) {
      parts.push(`color palette: ${vb.dominantColors.join(', ')}`);
    }

    // Depth of field (default to shallow for cinematic look)
    if (vb.depthOfField) {
      parts.push(`${vb.depthOfField} depth of field`);
    } else {
      parts.push('shallow depth of field with cinematic bokeh');
    }

    return parts.length > 0 ? parts.join(', ') : '';
  },

  /**
   * Build technical quality specifications
   */
  buildTechnicalDescription: (styleBible) => {
    if (styleBible?.technicalSpecs?.positive) {
      return styleBible.technicalSpecs.positive;
    }
    return '8K resolution, photorealistic, ultra-detailed textures, sharp focus, professional color grading, cinematic aspect ratio';
  },

  /**
   * Compile all parts into HOLLYWOOD-QUALITY flowing prompt
   * Outputs as natural prose, not labeled sections
   */
  compileHollywoodPrompt: (parts) => {
    const segments = [];

    // Subject/scene is always first and most important
    if (parts.subject) segments.push(parts.subject);

    // Environment adds depth
    if (parts.environment) segments.push(parts.environment);

    // Atmosphere for mood
    if (parts.atmosphere) segments.push(parts.atmosphere);

    // Lighting is crucial for cinematic quality
    if (parts.lighting) segments.push(parts.lighting);

    // Camera for composition context
    if (parts.camera) segments.push(parts.camera);

    // Style for film look
    if (parts.style) segments.push(parts.style);

    // Technical specs at the end
    if (parts.technical) segments.push(parts.technical);

    // Join with commas for flowing prose, not newlines
    return segments.filter(s => s && s.trim()).join(', ');
  },

  /**
   * Build subject description from scene - SCENE ACTION FOCUSED
   * CRITICAL FIX: Returns SCENE DESCRIPTION, NOT character physical descriptions
   * Character reference images handle visual consistency - prompts describe WHAT'S HAPPENING
   */
  buildSubjectDescription: (scene, characterBible, vb = {}, locationBible = []) => {
    const parts = [];

    // PRIORITY 1: Use scene.visualPrompt or scene.visual (the cinematographic description from script)
    // This is the PRIMARY source - the script's rich scene description
    const visualPrompt = scene.visualPrompt || scene.visual || '';
    const cleanedVisual = visualPrompt.replace(/\[.*?\]\s*/, '').trim();

    if (cleanedVisual && cleanedVisual.length > 30) {
      // The script's visual prompt should describe the scene action, not just a character
      parts.push(cleanedVisual);
    }

    // PRIORITY 2: Add scene action if available (what's happening)
    if (scene.sceneAction) {
      // Only add if not already covered in visual
      if (!cleanedVisual.toLowerCase().includes(scene.sceneAction.toLowerCase().substring(0, 20))) {
        parts.push(`ACTION: ${scene.sceneAction}`);
      }
    }

    // PRIORITY 3: Build location context if we have location data
    if (scene.location && typeof scene.location === 'object') {
      const loc = scene.location;
      const locationParts = [];

      if (loc.name) {
        // Try to find this location in location bible for richer description
        const bibleLoc = locationBible?.find(l =>
          l.name?.toLowerCase() === loc.name.toLowerCase() ||
          l.normalizedName === loc.name.toLowerCase()
        );

        if (bibleLoc?.keyElements && bibleLoc.keyElements.length > 0) {
          locationParts.push(`SETTING: ${loc.name} - ${bibleLoc.keyElements.slice(0, 3).join(', ')}`);
        } else {
          locationParts.push(`SETTING: ${loc.name}`);
        }
      }

      // Add time/weather/condition modifiers
      if (loc.timeOfDay && loc.timeOfDay !== 'day') {
        locationParts.push(`TIME: ${loc.timeOfDay}`);
      }
      if (loc.weather && loc.weather !== 'clear') {
        locationParts.push(`WEATHER: ${loc.weather}`);
      }
      if (loc.condition && loc.condition !== 'lived_in' && loc.condition !== 'pristine') {
        locationParts.push(`CONDITION: ${loc.condition}${loc.conditionDetails ? ` - ${loc.conditionDetails}` : ''}`);
      }

      if (locationParts.length > 0 && !cleanedVisual.toLowerCase().includes(loc.name?.toLowerCase() || '')) {
        parts.push(locationParts.join('. '));
      }
    }

    // PRIORITY 4: Add character names (NOT descriptions) if in scene
    // Character reference images handle visual consistency - we just need names for context
    if (scene.charactersInScene && scene.charactersInScene.length > 0) {
      const charNames = scene.charactersInScene.join(' and ');
      // Only add if not already mentioned in the visual
      if (!cleanedVisual.toLowerCase().includes(charNames.toLowerCase().split(' ')[0])) {
        parts.push(`FEATURING: ${charNames}`);
      }
    }

    // PRIORITY 5: Use rawVisual from blueprint if nothing else
    if (parts.length === 0 && vb.rawVisual && vb.rawVisual.length > 20) {
      parts.push(vb.rawVisual);
    }

    // PRIORITY 6: Use midground from visual blueprint
    if (parts.length === 0 && vb.midground) {
      parts.push(vb.midground);
    }

    // FALLBACK: Generic scene description
    if (parts.length === 0) {
      return 'A cinematic scene with dramatic composition';
    }

    return parts.join('. ');
  },

  /**
   * Build style prompt from style bible
   */
  buildStylePrompt: (styleBible) => {
    if (!styleBible) return '';

    const parts = [
      styleBible.visualStyle,
      styleBible.colorGrade,
      styleBible.cinematicReference,
      styleBible.mood
    ].filter(Boolean);

    return parts.join(', ');
  },

  /**
   * Get suggested shot composition based on scene type
   * Uses SHOT_COMPOSITION_TEMPLATES for Hollywood-quality framing
   */
  getShotComposition: (scene) => {
    const sceneType = scene.sceneType?.toLowerCase() || '';
    const visualContent = (scene.visual || scene.visualPrompt || '').toLowerCase();

    // Map scene characteristics to shot templates
    if (sceneType.includes('action') || visualContent.includes('fight') || visualContent.includes('battle')) {
      return SHOT_COMPOSITION_TEMPLATES_GLOBAL['action_sequence'];
    }
    if (sceneType.includes('dialogue') || visualContent.includes('conversation') || visualContent.includes('talk')) {
      return SHOT_COMPOSITION_TEMPLATES_GLOBAL['intimate_dialogue'];
    }
    if (sceneType.includes('reveal') || visualContent.includes('discover') || visualContent.includes('realize')) {
      return SHOT_COMPOSITION_TEMPLATES_GLOBAL['revelation'];
    }
    if (sceneType.includes('emotional') || visualContent.includes('cry') || visualContent.includes('breakthrough')) {
      return SHOT_COMPOSITION_TEMPLATES_GLOBAL['emotional_peak'];
    }
    if (sceneType.includes('confrontation') || visualContent.includes('face') || visualContent.includes('showdown')) {
      return SHOT_COMPOSITION_TEMPLATES_GLOBAL['confrontation'];
    }
    if (sceneType.includes('establishing') || sceneType.includes('intro') || sceneType === 'opening') {
      return SHOT_COMPOSITION_TEMPLATES_GLOBAL['establishing_world'];
    }
    if (sceneType.includes('entrance') || visualContent.includes('arrives') || visualContent.includes('appears')) {
      return SHOT_COMPOSITION_TEMPLATES_GLOBAL['hero_entrance'];
    }
    if (sceneType.includes('tension') || visualContent.includes('waiting') || visualContent.includes('suspense')) {
      return SHOT_COMPOSITION_TEMPLATES_GLOBAL['tension_build'];
    }
    if (sceneType.includes('montage') || sceneType.includes('sequence')) {
      return SHOT_COMPOSITION_TEMPLATES_GLOBAL['montage_beat'];
    }
    if (sceneType.includes('resolution') || sceneType.includes('ending') || sceneType === 'finale') {
      return SHOT_COMPOSITION_TEMPLATES_GLOBAL['resolution'];
    }

    // Default to establishing if scene 1, otherwise intimate dialogue
    return scene.id === 1 ? SHOT_COMPOSITION_TEMPLATES_GLOBAL['establishing_world'] : null;
  },

  /**
   * Get suggested lighting setup based on scene mood and time
   * Uses LIGHTING_SETUPS for professional cinematography
   */
  getLightingSetup: (scene, locationState = {}) => {
    const timeOfDay = locationState.timeOfDay || scene.location?.timeOfDay || 'day';
    const mood = scene.mood?.toLowerCase() || '';
    const sceneType = scene.sceneType?.toLowerCase() || '';

    // Night scenes
    if (timeOfDay === 'night' || timeOfDay === 'deep_night') {
      if (mood.includes('romantic') || mood.includes('intimate')) {
        return LIGHTING_SETUPS_GLOBAL['motivated_moonlight'];
      }
      return LIGHTING_SETUPS_GLOBAL['chiaroscuro'];
    }

    // Mood-based lighting
    if (mood.includes('mysterious') || mood.includes('suspense') || mood.includes('danger')) {
      return LIGHTING_SETUPS_GLOBAL['split'];
    }
    if (mood.includes('dramatic') || mood.includes('intense')) {
      return LIGHTING_SETUPS_GLOBAL['rembrandt'];
    }
    if (mood.includes('happy') || mood.includes('joy') || mood.includes('comedy')) {
      return LIGHTING_SETUPS_GLOBAL['high_key'];
    }
    if (mood.includes('glamour') || mood.includes('beauty')) {
      return LIGHTING_SETUPS_GLOBAL['butterfly'];
    }

    // Scene type based
    if (sceneType.includes('action') || sceneType.includes('confrontation')) {
      return LIGHTING_SETUPS_GLOBAL['rembrandt'];
    }
    if (sceneType.includes('establishing') || sceneType.includes('intro')) {
      return LIGHTING_SETUPS_GLOBAL['practical_naturalism'];
    }

    // Default to practical naturalism for realism
    return LIGHTING_SETUPS_GLOBAL['practical_naturalism'];
  },

  /**
   * Get suggested color grading preset based on genre and mood
   * Uses COLOR_GRADING_PRESETS for consistent visual style
   */
  getColorGrading: (scene, genre = '', styleBible = {}) => {
    const mood = scene.mood?.toLowerCase() || '';
    const genreLower = genre.toLowerCase();
    const colorGrade = styleBible?.colorGrade?.toLowerCase() || '';

    // Check style bible first
    if (colorGrade) {
      if (colorGrade.includes('teal') || colorGrade.includes('orange')) {
        return COLOR_GRADING_PRESETS_GLOBAL['teal_orange'];
      }
      if (colorGrade.includes('warm') || colorGrade.includes('golden')) {
        return COLOR_GRADING_PRESETS_GLOBAL['golden_hour'];
      }
      if (colorGrade.includes('noir') || colorGrade.includes('black and white')) {
        return COLOR_GRADING_PRESETS_GLOBAL['noir'];
      }
      if (colorGrade.includes('neon') || colorGrade.includes('cyberpunk')) {
        return COLOR_GRADING_PRESETS_GLOBAL['cyberpunk'];
      }
      if (colorGrade.includes('vintage') || colorGrade.includes('film')) {
        return COLOR_GRADING_PRESETS_GLOBAL['vintage_warmth'];
      }
    }

    // Genre-based
    if (genreLower.includes('action') || genreLower.includes('thriller')) {
      return COLOR_GRADING_PRESETS_GLOBAL['teal_orange'];
    }
    if (genreLower.includes('romance') || genreLower.includes('drama')) {
      return COLOR_GRADING_PRESETS_GLOBAL['golden_hour'];
    }
    if (genreLower.includes('noir') || genreLower.includes('mystery') || genreLower.includes('crime')) {
      return COLOR_GRADING_PRESETS_GLOBAL['noir'];
    }
    if (genreLower.includes('sci-fi') || genreLower.includes('cyberpunk') || genreLower.includes('future')) {
      return COLOR_GRADING_PRESETS_GLOBAL['cyberpunk'];
    }
    if (genreLower.includes('war') || genreLower.includes('historical')) {
      return COLOR_GRADING_PRESETS_GLOBAL['bleach_bypass'];
    }
    if (genreLower.includes('documentary') || genreLower.includes('reality')) {
      return COLOR_GRADING_PRESETS_GLOBAL['documentary_natural'];
    }

    // Mood-based fallback
    if (mood.includes('dark') || mood.includes('gritty')) {
      return COLOR_GRADING_PRESETS_GLOBAL['bleach_bypass'];
    }
    if (mood.includes('nostalgic') || mood.includes('memory')) {
      return COLOR_GRADING_PRESETS_GLOBAL['vintage_warmth'];
    }

    return null;
  },

  /**
   * Detect and suggest visual motifs for thematic consistency
   * Uses VISUAL_MOTIFS for symbolic imagery
   */
  detectVisualMotifs: (scene) => {
    const visual = (scene.visual || scene.visualPrompt || '').toLowerCase();
    const detectedMotifs = [];

    if (visual.includes('light') || visual.includes('shadow') || visual.includes('dark')) {
      detectedMotifs.push(VISUAL_MOTIFS_GLOBAL['light_shadow']);
    }
    if (visual.includes('mirror') || visual.includes('reflection') || visual.includes('water')) {
      detectedMotifs.push(VISUAL_MOTIFS_GLOBAL['reflections']);
    }
    if (visual.includes('door') || visual.includes('threshold') || visual.includes('gate')) {
      detectedMotifs.push(VISUAL_MOTIFS_GLOBAL['doorways_thresholds']);
    }
    if (visual.includes('rain') || visual.includes('ocean') || visual.includes('river') || visual.includes('tears')) {
      detectedMotifs.push(VISUAL_MOTIFS_GLOBAL['water']);
    }
    if (visual.includes('fire') || visual.includes('flame') || visual.includes('burn')) {
      detectedMotifs.push(VISUAL_MOTIFS_GLOBAL['fire']);
    }
    if (visual.includes('clock') || visual.includes('time') || visual.includes('watch')) {
      detectedMotifs.push(VISUAL_MOTIFS_GLOBAL['clocks_time']);
    }

    return detectedMotifs;
  },

  /**
   * Build complete Production Bible context for a scene
   * Combines Shot Composition, Lighting, Color Grading, and Motifs
   */
  buildProductionContext: (scene, genre = '', styleBible = {}, locationState = {}) => {
    const context = {
      shotComposition: IMAGE_PROMPT_GENERATOR.getShotComposition(scene),
      lightingSetup: IMAGE_PROMPT_GENERATOR.getLightingSetup(scene, locationState),
      colorGrading: IMAGE_PROMPT_GENERATOR.getColorGrading(scene, genre, styleBible),
      visualMotifs: IMAGE_PROMPT_GENERATOR.detectVisualMotifs(scene)
    };

    // Build prompt segments from production context
    const segments = [];

    if (context.shotComposition) {
      segments.push(`CAMERA: ${context.shotComposition.composition}`);
      if (context.shotComposition.dof) {
        segments.push(`FOCUS: ${context.shotComposition.dof}`);
      }
    }

    if (context.lightingSetup) {
      segments.push(`LIGHTING: ${context.lightingSetup.description}, ${context.lightingSetup.mood}`);
    }

    if (context.colorGrading) {
      segments.push(`COLOR: ${context.colorGrading.description} - ${context.colorGrading.shadows} shadows, ${context.colorGrading.highlights} highlights`);
    }

    return {
      ...context,
      promptSegments: segments
    };
  },

  /**
   * Compile all prompt parts into final prompt
   */
  compileImagePrompt: (parts) => {
    const sections = [];

    if (parts.subject) sections.push(parts.subject);
    if (parts.composition) sections.push(`Composition: ${parts.composition}`);
    if (parts.environment) sections.push(`Environment: ${parts.environment}`);
    if (parts.lighting) sections.push(`Lighting: ${parts.lighting}`);
    if (parts.atmosphere) sections.push(`Atmosphere: ${parts.atmosphere}`);
    if (parts.colors) sections.push(`Colors: ${parts.colors}`);
    if (parts.style) sections.push(`Style: ${parts.style}`);
    if (parts.technical) sections.push(`Quality: ${parts.technical}`);

    return sections.join('\n\n');
  },

  /**
   * Enhanced prompt generation with genre-specific adjustments
   * UPGRADED: Now passes genre, productionMode, AND visualStyleMode to generateFromScene
   *
   * @param {string} visualStyleMode - VISUAL_STYLE_DNA mode (photorealistic, cinematic, stylized_3d, etc.)
   */
  generateWithGenre: (scene, styleBible, characterBible, genre, productionMode, visualStyleMode = 'photorealistic') => {
    // generateFromScene now uses genre/productionMode for blueprint extraction
    // AND visualStyleMode for VISUAL_STYLE_DNA integration
    const basePrompt = IMAGE_PROMPT_GENERATOR.generateFromScene(scene, styleBible, characterBible, genre, productionMode, visualStyleMode);

    // Apply additional genre-specific enhancements as prefix/suffix
    const genreEnhancements = IMAGE_PROMPT_GENERATOR.getGenreEnhancements(genre, productionMode);

    if (genreEnhancements) {
      // Add genre enhancements to the already Hollywood-quality prompt
      // Note: Style prefix is already at the start from generateFromScene
      basePrompt.prompt = `${basePrompt.prompt}, ${genreEnhancements.suffix}`;
      // Merge negatives, avoiding duplicates
      const existingNegatives = new Set(basePrompt.negativePrompt.split(', ').map(s => s.trim()));
      const genreNegatives = genreEnhancements.negative.split(', ').filter(n => !existingNegatives.has(n.trim()));
      if (genreNegatives.length > 0) {
        basePrompt.negativePrompt = `${basePrompt.negativePrompt}, ${genreNegatives.join(', ')}`;
      }
    }

    return basePrompt;
  },

  /**
   * Get genre-specific prompt enhancements
   */
  getGenreEnhancements: (genre, productionMode) => {
    const enhancements = {
      'documentary-nature': {
        prefix: 'National Geographic quality, nature documentary cinematography',
        suffix: 'David Attenborough style, epic scale, intimate details, reverent treatment',
        negative: 'cartoon, illustration, artificial looking'
      },
      'documentary-true-crime': {
        prefix: 'True crime documentary aesthetic, investigative mood',
        suffix: 'moody lighting, evidence-board style, archival footage feel',
        negative: 'bright, cheerful, colorful'
      },
      'entertainment-horror': {
        prefix: 'Horror film cinematography, unsettling atmosphere',
        suffix: 'deep shadows, negative space, psychological tension, practical effects style',
        negative: 'bright, cheerful, safe, comfortable'
      },
      'entertainment-drama': {
        prefix: 'Prestige television cinematography, cinematic drama',
        suffix: 'Breaking Bad/Succession level visual storytelling, meaningful composition',
        negative: 'flat lighting, boring composition, amateur'
      },
      'educational-explainer': {
        prefix: 'Clean educational visual style, Kurzgesagt/Vox aesthetic',
        suffix: 'clear visual hierarchy, engaging graphics, professional polish',
        negative: 'cluttered, confusing, amateur'
      },
      'cinematic': {
        prefix: 'Feature film cinematography, cinematic excellence',
        suffix: 'Roger Deakins lighting, Denis Villeneuve composition, Hans Zimmer mood',
        negative: 'TV quality, flat, uncinematic, amateur'
      },
      'inspirational': {
        prefix: 'Inspirational visual storytelling, aspirational imagery',
        suffix: 'uplifting mood, golden hour feeling, triumph and hope',
        negative: 'depressing, dark, hopeless'
      }
    };

    return enhancements[genre] || enhancements[productionMode] || null;
  }
};

// =============================================================================
// SECTION 9.3: VIDEO PROMPT GENERATOR - Image + Action to Video Transformation
// =============================================================================

/**
 * VIDEO PROMPT GENERATOR
 * UPGRADED: Produces Hollywood-quality motion prompts
 * The video prompt STARTS with the generated image and ADDS cinematic motion
 */
const VIDEO_PROMPT_GENERATOR = {

  /**
   * Generate HOLLYWOOD-QUALITY video prompt from image and scene
   * Uses BLUEPRINT_EXTRACTOR for enhanced motion descriptions
   */
  generateFromImageAndScene: (generatedImageUrl, scene, styleBible, genre, productionMode) => {
    // Extract blueprint to enhance motion understanding
    const extracted = BLUEPRINT_EXTRACTOR.extractFromScene(scene, genre, productionMode);
    const vb = { ...extracted.visualBlueprint, ...scene.visualBlueprint };
    const ab = { ...extracted.actionBlueprint, ...scene.actionBlueprint };
    const timing = scene.timing || {};

    // Build character motion description
    const characterMotion = VIDEO_PROMPT_GENERATOR.describeCharacterMotion(ab.characterAction, vb);

    // Build camera motion description with cinematic language
    const cameraMotion = VIDEO_PROMPT_GENERATOR.describeCameraMotion(ab.cameraAction, vb);

    // Build environment motion description with atmospheric effects
    const environmentMotion = VIDEO_PROMPT_GENERATOR.describeEnvironmentMotion(ab.environmentAction, vb);

    // Build atmospheric motion (particles, lighting changes)
    const atmosphericMotion = VIDEO_PROMPT_GENERATOR.describeAtmosphericMotion(vb);

    return {
      prompt: VIDEO_PROMPT_GENERATOR.compileHollywoodVideoPrompt({
        startFrame: generatedImageUrl,
        characterMotion,
        cameraMotion,
        environmentMotion,
        atmosphericMotion,
        duration: timing.duration || 6,
        pacing: timing.pacingNote,
        mood: vb.mood,
        filmLook: vb.filmLook
      }),
      metadata: {
        duration: timing.duration,
        hasCharacterMotion: !!ab.characterAction,
        hasCameraMotion: !!ab.cameraAction || !!vb.cameraMovement,
        hasEnvironmentMotion: ab.environmentAction?.length > 0,
        hasAtmosphericMotion: !!vb.particles
      },
      technicalSettings: VIDEO_PROMPT_GENERATOR.getTechnicalSettings(scene, styleBible)
    };
  },

  /**
   * Describe character motion with cinematic detail
   */
  describeCharacterMotion: (characterAction, vb = {}) => {
    if (!characterAction?.action && !vb.rawVisual) {
      return 'subtle natural micro-movements, gentle breathing motion, lifelike presence';
    }

    if (characterAction) {
      const { who, startPose, action, endPose, timing } = characterAction;
      const parts = [];

      if (who) parts.push(who);
      if (action) parts.push(action);
      if (startPose && endPose) {
        parts.push(`transitions smoothly from ${startPose} to ${endPose}`);
      }
      if (timing) parts.push(`paced ${timing}`);

      return parts.join(', ') || 'subtle natural movement';
    }

    // Infer motion from visual description
    return 'subtle movement, natural micro-expressions, lifelike presence';
  },

  /**
   * Describe camera motion with Hollywood cinematography language
   */
  describeCameraMotion: (cameraAction, vb = {}) => {
    const movement = cameraAction?.movement || vb.cameraMovement;
    const speed = cameraAction?.speed || 'slow';

    if (!movement || movement === 'static') {
      return 'static locked-off frame with subtle atmospheric drift';
    }

    // Hollywood camera motion phrases
    const cinematicMovements = {
      'push-in': `${speed} deliberate push-in creating intimacy and focus`,
      'pull-out': `${speed} pull-out revealing wider context`,
      'pan-left': `${speed} pan left scanning the environment`,
      'pan-right': `${speed} pan right following the action`,
      'tilt-up': `${speed} tilt up revealing vertical space`,
      'tilt-down': `${speed} tilt down grounding the scene`,
      'tracking': `${speed} tracking shot moving with the subject`,
      'handheld': 'subtle handheld drift adding organic energy',
      'crane': `${speed} crane movement adding grandeur`,
      'dolly': `${speed} dolly movement creating depth`
    };

    const baseMovement = cinematicMovements[movement] || `${speed} ${movement}`;

    // Add focus information if available
    if (cameraAction?.focus) {
      return `${baseMovement}, focus on ${cameraAction.focus}`;
    }

    return baseMovement;
  },

  /**
   * Describe environment motion for living world feel
   */
  describeEnvironmentMotion: (environmentAction, vb = {}) => {
    const motions = [];

    // Add explicit environment actions
    if (environmentAction?.length > 0) {
      motions.push(...environmentAction);
    }

    // Infer environmental motion from visual blueprint
    if (vb.weather) {
      if (vb.weather.includes('wind')) motions.push('leaves and fabric gently swaying in breeze');
      if (vb.weather.includes('rain')) motions.push('rain continuously falling, drops on surfaces');
      if (vb.weather.includes('storm')) motions.push('dramatic wind movement, shifting light');
    }

    // Light-based environment motion
    if (vb.lightingSetup === 'practical') {
      motions.push('subtle flicker from practical light sources');
    }
    if (vb.lightingSetup === 'neon') {
      motions.push('neon lights gently pulsing with electric energy');
    }

    return motions.length > 0 ? motions.join(', ') : null;
  },

  /**
   * Describe atmospheric particle motion (critical for cinematic feel)
   */
  describeAtmosphericMotion: (vb = {}) => {
    const motions = [];

    if (vb.particles) {
      if (vb.particles.includes('dust')) motions.push('dust motes drifting slowly through light beams');
      if (vb.particles.includes('smoke')) motions.push('wisps of smoke curling and dissipating');
      if (vb.particles.includes('ember')) motions.push('glowing embers floating gently upward');
      if (vb.particles.includes('rain')) motions.push('rain falling in continuous sheets');
      if (vb.particles.includes('mist')) motions.push('mist slowly swirling and drifting');
      if (vb.particles.includes('snow')) motions.push('snowflakes falling gently');
      if (vb.particles.includes('god ray')) motions.push('volumetric light rays shifting subtly');
    }

    // Add mood-based atmospheric motion
    if (vb.mood === 'tense') motions.push('nervous energy in the air');
    if (vb.mood === 'melancholic') motions.push('heavy atmospheric stillness');
    if (vb.mood === 'epic') motions.push('grand atmospheric scale');

    return motions.length > 0 ? motions.join(', ') : null;
  },

  /**
   * Compile HOLLYWOOD-QUALITY video prompt as flowing prose
   */
  compileHollywoodVideoPrompt: (parts) => {
    const promptParts = [];

    // Core motion description
    promptParts.push('Animate this image with cinematic motion:');

    if (parts.characterMotion) {
      promptParts.push(parts.characterMotion);
    }

    if (parts.cameraMotion) {
      promptParts.push(parts.cameraMotion);
    }

    if (parts.environmentMotion) {
      promptParts.push(parts.environmentMotion);
    }

    if (parts.atmosphericMotion) {
      promptParts.push(parts.atmosphericMotion);
    }

    // Add pacing and mood
    if (parts.pacing) {
      promptParts.push(`${parts.pacing} pacing throughout`);
    }

    if (parts.mood) {
      promptParts.push(`maintaining ${parts.mood} atmosphere`);
    }

    // Add film look consistency
    if (parts.filmLook) {
      promptParts.push(parts.filmLook);
    }

    // Critical consistency instruction
    promptParts.push('Maintain exact lighting, colors, and visual style from the starting image');
    promptParts.push(`Duration: ${parts.duration} seconds`);
    promptParts.push('Smooth, cinematic motion quality');

    // Join as flowing prose
    return promptParts.filter(p => p).join('. ') + '.';
  },

  /**
   * Get technical settings for video generation
   */
  getTechnicalSettings: (scene, styleBible) => {
    return {
      maintainConsistency: true,
      preserveLighting: true,
      preserveColors: true,
      styleReference: styleBible?.cinematicReference || null,
      motionIntensity: scene.timing?.pacingNote?.includes('slow') ? 'subtle' :
                       scene.timing?.pacingNote?.includes('fast') ? 'dynamic' : 'moderate'
    };
  },

  /**
   * Generate optimized video prompt for specific AI models
   * UPGRADED: Now passes genre and productionMode for enhanced extraction
   */
  generateForModel: (generatedImageUrl, scene, styleBible, modelType, genre, productionMode) => {
    const basePrompt = VIDEO_PROMPT_GENERATOR.generateFromImageAndScene(generatedImageUrl, scene, styleBible, genre, productionMode);

    // Apply model-specific optimizations
    const optimizers = {
      'minimax': VIDEO_PROMPT_GENERATOR.optimizeForMinimax,
      'runway': VIDEO_PROMPT_GENERATOR.optimizeForRunway,
      'pika': VIDEO_PROMPT_GENERATOR.optimizeForPika,
      'veo': VIDEO_PROMPT_GENERATOR.optimizeForVeo
    };

    const optimizer = optimizers[modelType];
    if (optimizer) {
      return optimizer(basePrompt, scene);
    }

    return basePrompt;
  },

  /**
   * Optimize prompt for Minimax Hailuo
   */
  optimizeForMinimax: (prompt, scene) => {
    // Minimax prefers specific camera movement keywords
    const minimaxCameraKeywords = {
      'push-in': 'slowly push in',
      'pull-out': 'slowly pull out',
      'pan-left': 'smooth pan left',
      'pan-right': 'smooth pan right',
      'tilt-up': 'tilt up',
      'tilt-down': 'tilt down',
      'tracking': 'tracking shot following subject',
      'static': 'static shot'
    };

    const cameraMovement = scene.actionBlueprint?.cameraAction?.movement;
    const minimaxMovement = minimaxCameraKeywords[cameraMovement] || cameraMovement;

    // Add Minimax-specific prefix
    prompt.prompt = `[${minimaxMovement}] ${prompt.prompt}`;
    prompt.modelOptimization = 'minimax';

    return prompt;
  },

  /**
   * Optimize prompt for Runway
   */
  optimizeForRunway: (prompt, scene) => {
    prompt.modelOptimization = 'runway';
    prompt.runwaySettings = {
      motionAmount: scene.timing?.pacingNote?.includes('slow') ? 'low' : 'medium',
      cameraMotion: scene.actionBlueprint?.cameraAction?.movement || 'none'
    };
    return prompt;
  },

  /**
   * Optimize prompt for Pika
   */
  optimizeForPika: (prompt, scene) => {
    prompt.modelOptimization = 'pika';
    return prompt;
  },

  /**
   * Optimize prompt for Google Veo
   */
  optimizeForVeo: (prompt, scene) => {
    prompt.modelOptimization = 'veo';
    prompt.veoSettings = {
      duration: scene.timing?.duration || 6,
      aspectRatio: scene.visualBlueprint?.aspectRatio || '16:9'
    };
    return prompt;
  }
};

// =============================================================================
// SECTION 9.4: PROMPT QUALITY ANALYZER - Validate & Enhance Prompt Quality
// =============================================================================

/**
 * PROMPT_QUALITY_ANALYZER
 * Analyzes prompts to ensure Hollywood-quality output
 * Can detect missing elements and suggest improvements
 */
const PROMPT_QUALITY_ANALYZER = {

  /**
   * Quality thresholds for different production levels
   */
  thresholds: {
    minimum: { length: 80, score: 30, elements: 2 },
    standard: { length: 150, score: 50, elements: 4 },
    hollywood: { length: 250, score: 70, elements: 6 },
    premium: { length: 350, score: 85, elements: 8 }
  },

  /**
   * Required cinematographic elements to detect
   */
  elements: {
    subject: {
      keywords: ['man', 'woman', 'person', 'figure', 'character', 'he', 'she', 'they', 'face', 'hands', 'silhouette'],
      weight: 15
    },
    lighting: {
      keywords: ['light', 'shadow', 'glow', 'illuminat', 'lamp', 'sun', 'moon', 'ambient', 'backlight', 'rim', 'key light', 'fill', 'chiaroscuro', 'golden hour', 'blue hour', 'neon'],
      weight: 15
    },
    camera: {
      keywords: ['shot', 'angle', 'frame', 'close', 'wide', 'medium', 'track', 'pan', 'push', 'pull', 'crane', 'dolly', 'handheld', 'POV', 'low-angle', 'high-angle'],
      weight: 12
    },
    environment: {
      keywords: ['background', 'foreground', 'midground', 'setting', 'location', 'room', 'space', 'city', 'forest', 'interior', 'exterior', 'landscape'],
      weight: 12
    },
    atmosphere: {
      keywords: ['dust', 'smoke', 'mist', 'fog', 'rain', 'particles', 'embers', 'steam', 'haze', 'atmosphere', 'mood', 'ambiance'],
      weight: 10
    },
    color: {
      keywords: ['color', 'palette', 'teal', 'orange', 'warm', 'cool', 'saturate', 'tone', 'grade', 'hue', 'contrast', 'vibrant', 'muted'],
      weight: 8
    },
    style: {
      keywords: ['cinematic', 'film', 'aesthetic', 'photorealistic', '8K', '4K', 'detailed', 'professional', 'blockbuster', 'documentary'],
      weight: 8
    },
    depth: {
      keywords: ['depth of field', 'bokeh', 'focus', 'sharp', 'blur', 'shallow', 'deep focus'],
      weight: 5
    }
  },

  /**
   * Analyze a prompt and return quality metrics
   */
  analyze: (prompt) => {
    if (!prompt || typeof prompt !== 'string') {
      return {
        score: 0,
        quality: 'invalid',
        length: 0,
        elements: {},
        missing: Object.keys(PROMPT_QUALITY_ANALYZER.elements),
        suggestions: ['Prompt is empty or invalid']
      };
    }

    const lowerPrompt = prompt.toLowerCase();
    let score = 0;
    const foundElements = {};
    const missing = [];

    // Length scoring (0-20 points)
    const length = prompt.length;
    if (length >= 400) score += 20;
    else if (length >= 300) score += 16;
    else if (length >= 200) score += 12;
    else if (length >= 100) score += 8;
    else score += 4;

    // Element detection and scoring
    for (const [element, config] of Object.entries(PROMPT_QUALITY_ANALYZER.elements)) {
      const found = config.keywords.some(kw => lowerPrompt.includes(kw));
      foundElements[element] = found;
      if (found) {
        score += config.weight;
      } else {
        missing.push(element);
      }
    }

    // Determine quality tier
    const thresholds = PROMPT_QUALITY_ANALYZER.thresholds;
    let quality = 'poor';
    if (score >= thresholds.premium.score) quality = 'premium';
    else if (score >= thresholds.hollywood.score) quality = 'hollywood';
    else if (score >= thresholds.standard.score) quality = 'standard';
    else if (score >= thresholds.minimum.score) quality = 'minimum';

    // Generate improvement suggestions
    const suggestions = [];
    if (length < 150) suggestions.push('Increase prompt length with more visual detail');
    missing.forEach(element => {
      const suggestionMap = {
        subject: 'Add specific subject description (appearance, clothing, pose, expression)',
        lighting: 'Add lighting design (key light, color temperature, shadow quality)',
        camera: 'Add camera language (shot type, angle, movement)',
        environment: 'Add environment layers (foreground, midground, background)',
        atmosphere: 'Add atmospheric elements (particles, weather, mood)',
        color: 'Add color palette information (dominant colors, temperature)',
        style: 'Add cinematic style reference (film look, quality markers)',
        depth: 'Add depth of field description (shallow/deep, bokeh)'
      };
      if (suggestionMap[element]) suggestions.push(suggestionMap[element]);
    });

    return {
      score: Math.min(100, score),
      quality,
      length,
      elements: foundElements,
      elementCount: Object.values(foundElements).filter(Boolean).length,
      missing,
      suggestions,
      meetsHollywoodStandard: score >= thresholds.hollywood.score
    };
  },

  /**
   * Quick check if prompt meets minimum quality
   */
  meetsMinimum: (prompt) => {
    const analysis = PROMPT_QUALITY_ANALYZER.analyze(prompt);
    return analysis.score >= PROMPT_QUALITY_ANALYZER.thresholds.minimum.score;
  },

  /**
   * Quick check if prompt meets Hollywood quality
   */
  meetsHollywoodStandard: (prompt) => {
    const analysis = PROMPT_QUALITY_ANALYZER.analyze(prompt);
    return analysis.meetsHollywoodStandard;
  },

  /**
   * Get quality tier name
   */
  getQualityTier: (prompt) => {
    return PROMPT_QUALITY_ANALYZER.analyze(prompt).quality;
  },

  /**
   * Generate quality report for logging/debugging
   */
  generateReport: (prompt) => {
    const analysis = PROMPT_QUALITY_ANALYZER.analyze(prompt);
    return `
=== PROMPT QUALITY REPORT ===
Quality Tier: ${analysis.quality.toUpperCase()}
Score: ${analysis.score}/100
Length: ${analysis.length} characters
Elements Found: ${analysis.elementCount}/8

Found Elements:
${Object.entries(analysis.elements).map(([el, found]) => `  ${found ? 'âœ“' : 'âœ—'} ${el}`).join('\n')}

${analysis.suggestions.length > 0 ? `Improvement Suggestions:\n${analysis.suggestions.map(s => `  - ${s}`).join('\n')}` : 'No suggestions - prompt meets Hollywood standard!'}
==============================
    `.trim();
  }
};

// =============================================================================
// SECTION 10: TRANSITION ENGINE - Scene-to-Scene Bridge Intelligence
// =============================================================================

/**
 * TRANSITION ENGINE
 * Intelligent scene-to-scene transitions with visual bridges
 */
const TRANSITION_ENGINE = {

  /**
   * Analyze optimal transition between two scenes
   */
  analyzeTransition: (currentScene, nextScene) => {
    if (!nextScene) {
      return { type: 'fade-to-black', instruction: 'Final scene, fade to black' };
    }

    return {
      cutType: TRANSITION_ENGINE.determineOptimalCut(currentScene, nextScene),
      matchElement: TRANSITION_ENGINE.findVisualBridge(currentScene, nextScene),
      emotionalShift: TRANSITION_ENGINE.calculateEmotionalDelta(currentScene, nextScene),
      continuity: {
        timeDelta: TRANSITION_ENGINE.calculateTimeDelta(currentScene, nextScene),
        locationChange: TRANSITION_ENGINE.detectLocationChange(currentScene, nextScene),
        characterContinuity: TRANSITION_ENGINE.findSharedCharacters(currentScene, nextScene)
      },
      audioTransition: {
        musicBridge: TRANSITION_ENGINE.determineMusicContinuity(currentScene, nextScene),
        ambienceFade: TRANSITION_ENGINE.calculateAmbienceCrossover(currentScene, nextScene)
      }
    };
  },

  /**
   * Determine the optimal cut type between scenes
   */
  determineOptimalCut: (current, next) => {
    // Check for visual bridge (match cut)
    const visualBridge = current.transitionOut?.visualBridge;
    if (visualBridge) {
      return {
        type: 'match-cut',
        element: visualBridge,
        instruction: `Cut on ${visualBridge} matching to similar element in next scene`,
        duration: 0
      };
    }

    // Check for sudden action (hard cut)
    const nextMood = next.visualBlueprint?.mood || next.mood;
    if (nextMood === 'sudden-action' || nextMood === 'shock' || current.transitionOut?.type === 'cut') {
      return {
        type: 'hard-cut',
        instruction: 'Immediate cut, no transition effect',
        duration: 0
      };
    }

    // Check for time passing (dissolve)
    const timePassing = current.transitionOut?.timePassing ||
                        TRANSITION_ENGINE.detectTimePassing(current, next);
    if (timePassing) {
      return {
        type: 'dissolve',
        duration: 1.5,
        instruction: 'Slow dissolve indicating time passage'
      };
    }

    // Check for audio lead (J-cut)
    const nextHasDialogue = next.audioBlueprint?.dialogue || next.dialogue?.length > 0;
    if (nextHasDialogue) {
      return {
        type: 'j-cut',
        audioLead: 0.5,
        instruction: 'Audio from next scene begins before visual cut'
      };
    }

    // Check for audio trail (L-cut)
    const currentHasDialogue = current.audioBlueprint?.dialogue || current.dialogue?.length > 0;
    if (currentHasDialogue && !nextHasDialogue) {
      return {
        type: 'l-cut',
        audioTrail: 0.5,
        instruction: 'Audio from current scene continues into next scene visual'
      };
    }

    // Default to standard cut
    return {
      type: 'cut',
      instruction: 'Standard cut',
      duration: 0
    };
  },

  /**
   * Find visual elements that can bridge two scenes
   */
  findVisualBridge: (current, next) => {
    const bridges = [];

    // Check for color match
    const currentColors = current.visualBlueprint?.dominantColors || [];
    const nextColors = next.visualBlueprint?.dominantColors || [];
    const sharedColors = currentColors.filter(c => nextColors.includes(c));
    if (sharedColors.length > 0) {
      bridges.push({
        type: 'color-match',
        element: sharedColors[0],
        description: `Match on ${sharedColors[0]} color`
      });
    }

    // Check for shape/composition match
    const currentShot = current.visualBlueprint?.shotType;
    const nextShot = next.visualBlueprint?.shotType;
    if (currentShot === nextShot && currentShot === 'extreme-close-up') {
      bridges.push({
        type: 'shape-match',
        description: 'ECU to ECU match cut potential'
      });
    }

    // Check for character continuity
    const currentCharacter = current.actionBlueprint?.characterAction?.who;
    const nextCharacter = next.actionBlueprint?.characterAction?.who;
    if (currentCharacter && currentCharacter === nextCharacter) {
      bridges.push({
        type: 'character-match',
        element: currentCharacter,
        description: `Continue with ${currentCharacter}`
      });
    }

    return bridges.length > 0 ? bridges[0] : null;
  },

  /**
   * Calculate emotional shift between scenes
   */
  calculateEmotionalDelta: (current, next) => {
    const moodIntensity = {
      'peaceful': 1,
      'contemplative': 2,
      'neutral': 3,
      'curious': 4,
      'hopeful': 5,
      'excited': 6,
      'tense': 7,
      'anxious': 8,
      'fear': 9,
      'terror': 10,
      'shock': 10
    };

    const currentMood = current.visualBlueprint?.mood || current.mood || 'neutral';
    const nextMood = next.visualBlueprint?.mood || next.mood || 'neutral';

    const currentIntensity = moodIntensity[currentMood] || 5;
    const nextIntensity = moodIntensity[nextMood] || 5;

    const delta = nextIntensity - currentIntensity;

    return {
      from: currentMood,
      to: nextMood,
      delta: delta,
      direction: delta > 0 ? 'escalating' : delta < 0 ? 'de-escalating' : 'maintaining',
      significance: Math.abs(delta) > 3 ? 'major-shift' : Math.abs(delta) > 1 ? 'moderate-shift' : 'subtle-shift'
    };
  },

  /**
   * Calculate time delta between scenes
   */
  calculateTimeDelta: (current, next) => {
    const currentTime = current.timeContext || 0;
    const nextTime = next.timeContext || 0;
    return nextTime - currentTime;
  },

  /**
   * Detect location change between scenes
   */
  detectLocationChange: (current, next) => {
    const currentLocation = current.location || current.visualBlueprint?.background;
    const nextLocation = next.location || next.visualBlueprint?.background;
    return currentLocation !== nextLocation;
  },

  /**
   * Find shared characters between scenes
   */
  findSharedCharacters: (current, next) => {
    const currentCharacters = [];
    if (current.actionBlueprint?.characterAction?.who) {
      currentCharacters.push(current.actionBlueprint.characterAction.who);
    }

    const nextCharacters = [];
    if (next.actionBlueprint?.characterAction?.who) {
      nextCharacters.push(next.actionBlueprint.characterAction.who);
    }

    return currentCharacters.filter(c => nextCharacters.includes(c));
  },

  /**
   * Detect if time is passing between scenes
   */
  detectTimePassing: (current, next) => {
    // Check for temporal keywords in scene descriptions
    const timeKeywords = ['later', 'next day', 'morning', 'evening', 'night', 'years', 'months', 'weeks'];
    const nextVisual = (next.visualBlueprint?.background || '').toLowerCase();

    return timeKeywords.some(keyword => nextVisual.includes(keyword));
  },

  /**
   * Determine music continuity strategy
   */
  determineMusicContinuity: (current, next) => {
    const currentMusic = current.audioBlueprint?.music;
    const nextMusic = next.audioBlueprint?.music;

    if (currentMusic === nextMusic) {
      return { type: 'continue', description: 'Music continues unchanged' };
    }

    const emotionalShift = TRANSITION_ENGINE.calculateEmotionalDelta(current, next);
    if (emotionalShift.significance === 'major-shift') {
      return { type: 'change', crossfade: 2, description: 'Crossfade to new music over 2 seconds' };
    }

    return { type: 'blend', crossfade: 1, description: 'Quick blend to new music' };
  },

  /**
   * Calculate ambience crossover strategy
   */
  calculateAmbienceCrossover: (current, next) => {
    const locationChange = TRANSITION_ENGINE.detectLocationChange(current, next);

    if (!locationChange) {
      return { type: 'continue', description: 'Ambience continues' };
    }

    return {
      type: 'crossfade',
      duration: 0.5,
      description: 'Quick ambience crossfade for location change'
    };
  },

  /**
   * Visual Bridge Patterns - Templates for match cuts
   */
  visualBridgePatterns: {
    'eye-match': {
      trigger: 'close-up of eye',
      bridge: 'cut to another eye (different character, different context)',
      example: '2001: Space Odyssey - bone to satellite'
    },
    'shape-match': {
      trigger: 'circular/geometric element',
      bridge: 'cut to similar shape in different context',
      example: 'Moon to eyeball, wheel to sun'
    },
    'color-match': {
      trigger: 'dominant color fills frame',
      bridge: 'cut to same color in new scene',
      example: 'Red blood to red sunset'
    },
    'motion-match': {
      trigger: 'direction of movement',
      bridge: 'continue same movement direction in new scene',
      example: 'Character running right, car driving right'
    },
    'audio-match': {
      trigger: 'distinctive sound',
      bridge: 'sound continues but visual context changes',
      example: 'Phone ringing through dream/wake transition'
    }
  },

  /**
   * Get transition recommendation based on genre
   */
  getGenreTransitionStyle: (genre) => {
    const genreStyles = {
      'documentary-nature': ['dissolve', 'fade', 'slow-match-cut'],
      'documentary-true-crime': ['hard-cut', 'j-cut', 'smash-cut'],
      'entertainment-horror': ['hard-cut', 'fade-to-black', 'smash-cut'],
      'entertainment-drama': ['cut', 'dissolve', 'match-cut'],
      'entertainment-comedy': ['hard-cut', 'whip-pan', 'jump-cut'],
      'educational-explainer': ['cut', 'wipe', 'dissolve'],
      'social-viral': ['hard-cut', 'jump-cut', 'smash-cut'],
      'cinematic': ['dissolve', 'match-cut', 'fade']
    };

    return genreStyles[genre] || ['cut', 'dissolve'];
  }
};

// =============================================================================
// SECTION 11: SCENE PROCESSING PIPELINE - Complete Scene Workflow
// =============================================================================

/**
 * SCENE PROCESSING PIPELINE
 * Complete workflow for processing each scene through the chain
 */
const SCENE_PIPELINE = {

  /**
   * Process a single scene through the entire pipeline
   */
  processScene: async (sceneScript, context) => {
    const results = {
      sceneId: sceneScript.sceneId || sceneScript.id,
      steps: [],
      errors: []
    };

    try {
      // STEP 1: Extract Visual Blueprint
      results.steps.push({ step: 'extract_blueprint', status: 'starting' });
      const visualBlueprint = SCENE_PIPELINE.extractVisualBlueprint(sceneScript);
      results.visualBlueprint = visualBlueprint;
      results.steps[results.steps.length - 1].status = 'completed';

      // STEP 2: Generate Image Prompt
      // Now passes visualStyleMode for VISUAL_STYLE_DNA integration
      results.steps.push({ step: 'generate_image_prompt', status: 'starting' });
      const imagePromptResult = IMAGE_PROMPT_GENERATOR.generateWithGenre(
        sceneScript,
        context.styleBible,
        context.characterBible,
        context.genre,
        context.productionMode,
        context.visualStyleMode || 'photorealistic'
      );
      results.imagePrompt = imagePromptResult;
      results.steps[results.steps.length - 1].status = 'completed';

      // STEP 3: Generate Video Prompt Template
      results.steps.push({ step: 'generate_video_prompt', status: 'starting' });
      const videoPromptResult = VIDEO_PROMPT_GENERATOR.generateForModel(
        '[GENERATED_IMAGE_URL]', // Placeholder, replaced after image generation
        sceneScript,
        context.styleBible,
        context.videoModel || 'minimax'
      );
      results.videoPromptTemplate = videoPromptResult;
      results.steps[results.steps.length - 1].status = 'completed';

      // STEP 4: Analyze Transition to Next Scene
      results.steps.push({ step: 'analyze_transition', status: 'starting' });
      const transitionData = TRANSITION_ENGINE.analyzeTransition(
        sceneScript,
        context.nextScene
      );
      results.transition = transitionData;
      results.steps[results.steps.length - 1].status = 'completed';

      // STEP 5: Build Audio Blueprint
      results.steps.push({ step: 'build_audio', status: 'starting' });
      const audioBlueprint = SCENE_PIPELINE.buildAudioBlueprint(sceneScript, context);
      results.audioBlueprint = audioBlueprint;
      results.steps[results.steps.length - 1].status = 'completed';

      results.success = true;

    } catch (error) {
      results.success = false;
      results.errors.push({
        step: results.steps[results.steps.length - 1]?.step || 'unknown',
        error: error.message
      });
      results.steps[results.steps.length - 1].status = 'failed';
    }

    return results;
  },

  /**
   * Extract visual blueprint from scene script
   */
  extractVisualBlueprint: (scene) => {
    // If scene already has visualBlueprint, return it
    if (scene.visualBlueprint) return scene.visualBlueprint;

    // Otherwise, construct from available fields
    return {
      shotType: scene.shotType || 'medium',
      cameraAngle: scene.cameraAngle || 'eye-level',
      cameraMovement: scene.cameraMovement?.[0] || 'static',
      subjectPlacement: 'center',
      mood: scene.mood || 'neutral',
      visualPrompt: scene.visualPrompt || scene.visual || ''
    };
  },

  /**
   * Build audio blueprint from scene
   */
  buildAudioBlueprint: (scene, context) => {
    const audioBlueprint = scene.audioBlueprint || {};

    return {
      ambience: audioBlueprint.ambience || [],
      music: audioBlueprint.music || context.musicStyle || 'ambient',
      sfx: audioBlueprint.sfx || [],
      voiceover: scene.narration || scene.voiceover || null,
      voiceoverDuration: scene.narrationDuration || 0,
      voiceoverOffset: scene.narrationStartTime || 0.5
    };
  },

  /**
   * Process all scenes in a project
   */
  processAllScenes: async (scenes, context) => {
    const results = [];

    for (let i = 0; i < scenes.length; i++) {
      const scene = scenes[i];
      const sceneContext = {
        ...context,
        nextScene: scenes[i + 1] || null,
        prevScene: scenes[i - 1] || null,
        sceneIndex: i,
        totalScenes: scenes.length
      };

      const result = await SCENE_PIPELINE.processScene(scene, sceneContext);
      results.push(result);
    }

    return {
      success: results.every(r => r.success),
      scenes: results,
      summary: {
        total: results.length,
        successful: results.filter(r => r.success).length,
        failed: results.filter(r => !r.success).length
      }
    };
  }
};

// =============================================================================
// SECTION 11.2: QUALITY VALIDATION LAYER
// =============================================================================

/**
 * QUALITY VALIDATION LAYER
 * Validates generated content against script intent
 */
const QUALITY_VALIDATOR = {

  /**
   * Validate image against visual blueprint
   */
  validateImageAgainstScript: (imageAnalysis, visualBlueprint) => {
    const checks = [];
    let score = 0;
    let maxScore = 0;

    // Check shot type
    if (visualBlueprint.shotType) {
      maxScore += 20;
      const shotMatch = QUALITY_VALIDATOR.checkShotType(imageAnalysis, visualBlueprint.shotType);
      checks.push({ check: 'shotType', passed: shotMatch, weight: 20 });
      if (shotMatch) score += 20;
    }

    // Check mood
    if (visualBlueprint.mood) {
      maxScore += 25;
      const moodMatch = QUALITY_VALIDATOR.checkMood(imageAnalysis, visualBlueprint.mood);
      checks.push({ check: 'mood', passed: moodMatch, weight: 25 });
      if (moodMatch) score += 25;
    }

    // Check colors
    if (visualBlueprint.dominantColors?.length > 0) {
      maxScore += 20;
      const colorMatch = QUALITY_VALIDATOR.checkColors(imageAnalysis, visualBlueprint.dominantColors);
      checks.push({ check: 'colors', passed: colorMatch, weight: 20 });
      if (colorMatch) score += 20;
    }

    // Check lighting
    if (visualBlueprint.keyLight) {
      maxScore += 15;
      const lightMatch = QUALITY_VALIDATOR.checkLighting(imageAnalysis, visualBlueprint);
      checks.push({ check: 'lighting', passed: lightMatch, weight: 15 });
      if (lightMatch) score += 15;
    }

    // Check subject presence
    if (visualBlueprint.midground) {
      maxScore += 20;
      const subjectMatch = QUALITY_VALIDATOR.checkSubject(imageAnalysis, visualBlueprint.midground);
      checks.push({ check: 'subject', passed: subjectMatch, weight: 20 });
      if (subjectMatch) score += 20;
    }

    const matchScore = maxScore > 0 ? score / maxScore : 1;

    return {
      passed: matchScore >= 0.7,
      matchScore: matchScore,
      checks: checks,
      suggestions: QUALITY_VALIDATOR.getSuggestions(checks)
    };
  },

  /**
   * Check shot type match
   */
  checkShotType: (imageAnalysis, expectedShotType) => {
    if (!imageAnalysis?.detectedShotType) return true; // Assume pass if no analysis
    return imageAnalysis.detectedShotType === expectedShotType ||
           imageAnalysis.detectedShotType?.includes(expectedShotType);
  },

  /**
   * Check mood match
   */
  checkMood: (imageAnalysis, expectedMood) => {
    if (!imageAnalysis?.detectedMood) return true;
    const moodSimilarity = {
      'tense': ['anxious', 'suspenseful', 'dramatic'],
      'peaceful': ['calm', 'serene', 'tranquil'],
      'mysterious': ['enigmatic', 'dark', 'shadowy'],
      'uplifting': ['hopeful', 'bright', 'optimistic']
    };

    return imageAnalysis.detectedMood === expectedMood ||
           moodSimilarity[expectedMood]?.includes(imageAnalysis.detectedMood);
  },

  /**
   * Check color palette match
   */
  checkColors: (imageAnalysis, expectedColors) => {
    if (!imageAnalysis?.dominantColors) return true;
    const matchCount = expectedColors.filter(c =>
      imageAnalysis.dominantColors.some(dc => dc.includes(c) || c.includes(dc))
    ).length;
    return matchCount >= expectedColors.length * 0.5; // 50% match threshold
  },

  /**
   * Check lighting match
   */
  checkLighting: (imageAnalysis, visualBlueprint) => {
    if (!imageAnalysis?.lightingAnalysis) return true;
    // Simplified check - more sophisticated analysis would be needed
    return true;
  },

  /**
   * Check subject presence
   */
  checkSubject: (imageAnalysis, expectedSubject) => {
    if (!imageAnalysis?.detectedObjects) return true;
    // Check if expected subject keywords are in detected objects
    const subjectKeywords = expectedSubject.toLowerCase().split(/\s+/);
    const detected = imageAnalysis.detectedObjects.map(o => o.toLowerCase()).join(' ');
    return subjectKeywords.some(keyword => detected.includes(keyword));
  },

  /**
   * Get improvement suggestions based on failed checks
   */
  getSuggestions: (checks) => {
    const suggestions = [];

    checks.filter(c => !c.passed).forEach(check => {
      switch (check.check) {
        case 'shotType':
          suggestions.push('Adjust framing - regenerate with more specific shot type instruction');
          break;
        case 'mood':
          suggestions.push('Adjust atmosphere - add more mood-specific lighting and color instructions');
          break;
        case 'colors':
          suggestions.push('Adjust color palette - be more explicit about dominant colors');
          break;
        case 'lighting':
          suggestions.push('Adjust lighting setup - specify light direction and quality more clearly');
          break;
        case 'subject':
          suggestions.push('Adjust subject visibility - ensure main subject is prominently featured');
          break;
      }
    });

    return suggestions;
  },

  /**
   * Validate video against action blueprint
   */
  validateVideoAgainstScript: (videoAnalysis, actionBlueprint) => {
    const checks = [];
    let score = 0;
    let maxScore = 0;

    // Check motion accuracy
    if (actionBlueprint.characterAction) {
      maxScore += 40;
      const motionMatch = QUALITY_VALIDATOR.checkMotion(videoAnalysis, actionBlueprint.characterAction);
      checks.push({ check: 'characterMotion', passed: motionMatch, weight: 40 });
      if (motionMatch) score += 40;
    }

    // Check camera movement
    if (actionBlueprint.cameraAction) {
      maxScore += 30;
      const cameraMatch = QUALITY_VALIDATOR.checkCameraMovement(videoAnalysis, actionBlueprint.cameraAction);
      checks.push({ check: 'cameraMovement', passed: cameraMatch, weight: 30 });
      if (cameraMatch) score += 30;
    }

    // Check style consistency
    maxScore += 30;
    const styleMatch = videoAnalysis?.styleConsistency >= 0.8;
    checks.push({ check: 'styleConsistency', passed: styleMatch, weight: 30 });
    if (styleMatch) score += 30;

    const matchScore = maxScore > 0 ? score / maxScore : 1;

    return {
      passed: matchScore >= 0.7,
      matchScore: matchScore,
      checks: checks,
      motionAccuracy: matchScore,
      styleConsistency: videoAnalysis?.styleConsistency || 1
    };
  },

  /**
   * Check motion matches action blueprint
   */
  checkMotion: (videoAnalysis, characterAction) => {
    if (!videoAnalysis?.detectedMotion) return true;
    // Simplified check
    return true;
  },

  /**
   * Check camera movement matches blueprint
   */
  checkCameraMovement: (videoAnalysis, cameraAction) => {
    if (!videoAnalysis?.detectedCameraMovement) return true;
    return videoAnalysis.detectedCameraMovement === cameraAction.movement;
  }
};

// =============================================================================
// CLOUD FUNCTIONS FOR PROMPT CHAIN ARCHITECTURE
// =============================================================================

/**
 * creationWizardGenerateSceneBlueprint - Generates complete scene blueprint
 * Transforms basic scene data into full visual/action/audio blueprints
 */
exports.creationWizardGenerateSceneBlueprint = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const { scene, genre, productionMode, styleBible, characterBible } = data;

  if (!scene) {
    throw new functions.https.HttpsError('invalid-argument', 'Scene data required');
  }

  try {
    // Build comprehensive scene blueprint using GPT-4o
    const systemPrompt = `You are a Hollywood cinematographer and director creating a complete visual blueprint for a scene.
Your task is to expand a basic scene description into a comprehensive production document.

You must output a JSON object with this EXACT structure:
{
  "visualBlueprint": {
    "shotType": "one of: extreme-wide, wide, medium-wide, medium, medium-close, close-up, extreme-close-up, establishing",
    "cameraAngle": "one of: eye-level, low-angle, high-angle, dutch-angle, birds-eye, worms-eye, over-shoulder",
    "cameraMovement": "one of: static, push-in, pull-out, pan-left, pan-right, tilt-up, tilt-down, tracking, crane, handheld",
    "subjectPlacement": "composition description",
    "foreground": "what's in the foreground",
    "midground": "main subject description",
    "background": "what's in the background",
    "keyLight": "main light description with direction and quality",
    "fillLight": "fill light description",
    "practicalLights": ["array of visible light sources in scene"],
    "weather": "weather conditions or null",
    "particles": "atmospheric particles or null",
    "mood": "single word mood descriptor",
    "dominantColors": ["array of 2-4 dominant colors"],
    "colorTemperature": "warm, cool, neutral, or mixed"
  },
  "actionBlueprint": {
    "characterAction": {
      "who": "character name",
      "startPose": "starting position/pose",
      "action": "what happens during the scene",
      "endPose": "ending position/pose",
      "timing": "timing description"
    },
    "environmentAction": ["array of environment animations"],
    "cameraAction": {
      "movement": "camera movement type",
      "speed": "movement speed description",
      "focus": "focus behavior description"
    }
  },
  "audioBlueprint": {
    "ambience": ["array of ambient sounds"],
    "music": "music style/mood",
    "sfx": ["array of sound effects"],
    "voiceover": "narration text or null"
  },
  "transitionOut": {
    "type": "one of: cut, dissolve, wipe, match-cut, j-cut, l-cut, fade-to-black",
    "timing": "when to transition",
    "visualBridge": "element that connects to next scene or null"
  }
}`;

    const userPrompt = `Create a complete scene blueprint for this scene:

SCENE DATA:
${JSON.stringify(scene, null, 2)}

GENRE: ${genre || 'general'}
PRODUCTION MODE: ${productionMode || 'standard'}
${styleBible ? `STYLE BIBLE: ${JSON.stringify(styleBible)}` : ''}
${characterBible?.length > 0 ? `CHARACTERS: ${JSON.stringify(characterBible)}` : ''}

Generate a comprehensive, production-ready blueprint that a cinematographer could execute immediately.
Be specific and cinematic. This should feel like premium Hollywood production.`;

    const completion = await openai.chat.completions.create({
      model: 'gpt-4o',
      messages: [
        { role: 'system', content: systemPrompt },
        { role: 'user', content: userPrompt }
      ],
      response_format: { type: 'json_object' }, // Force valid JSON output
      temperature: 0.7,
      max_tokens: 2000
    });

    const responseText = completion.choices[0].message.content.trim();

    let blueprint;
    try {
      blueprint = JSON.parse(responseText);
    } catch (parseError) {
      console.error('[creationWizardGenerateSceneBlueprint] Parse error:', parseError);
      throw new functions.https.HttpsError('internal', 'Failed to parse blueprint response');
    }

    return {
      success: true,
      blueprint: {
        sceneId: scene.id,
        ...blueprint
      },
      usage: {
        promptTokens: completion.usage?.prompt_tokens || 0,
        completionTokens: completion.usage?.completion_tokens || 0
      }
    };

  } catch (error) {
    console.error('[creationWizardGenerateSceneBlueprint] Error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to generate blueprint'));
  }
});

/**
 * creationWizardGenerateImagePrompt - Generates optimized image prompt from blueprint
 * Now supports visualStyleMode for VISUAL_STYLE_DNA integration
 */
exports.creationWizardGenerateImagePrompt = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const { scene, styleBible, characterBible, genre, productionMode, visualStyleMode } = data;

  if (!scene) {
    throw new functions.https.HttpsError('invalid-argument', 'Scene data required');
  }

  try {
    // Generate image prompt using the engine with VISUAL_STYLE_DNA
    const promptResult = IMAGE_PROMPT_GENERATOR.generateWithGenre(
      scene,
      styleBible,
      characterBible,
      genre,
      productionMode,
      visualStyleMode || 'photorealistic'
    );

    return {
      success: true,
      imagePrompt: promptResult.prompt,
      negativePrompt: promptResult.negativePrompt,
      visualStyleMode: promptResult.visualStyleMode,
      metadata: promptResult.metadata
    };

  } catch (error) {
    console.error('[creationWizardGenerateImagePrompt] Error:', error);
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to generate image prompt'));
  }
});

/**
 * creationWizardGenerateVideoPrompt - Generates video prompt from image and action
 */
exports.creationWizardGenerateVideoPrompt = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const { imageUrl, scene, styleBible, videoModel } = data;

  if (!scene) {
    throw new functions.https.HttpsError('invalid-argument', 'Scene data required');
  }

  try {
    // Generate video prompt using the engine
    const promptResult = VIDEO_PROMPT_GENERATOR.generateForModel(
      imageUrl || '[GENERATED_IMAGE_URL]',
      scene,
      styleBible,
      videoModel || 'minimax'
    );

    return {
      success: true,
      videoPrompt: promptResult.prompt,
      metadata: promptResult.metadata,
      technicalSettings: promptResult.technicalSettings
    };

  } catch (error) {
    console.error('[creationWizardGenerateVideoPrompt] Error:', error);
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to generate video prompt'));
  }
});

/**
 * creationWizardAnalyzeTransition - Analyzes optimal transition between scenes
 */
exports.creationWizardAnalyzeTransition = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const { currentScene, nextScene, genre } = data;

  if (!currentScene) {
    throw new functions.https.HttpsError('invalid-argument', 'Current scene required');
  }

  try {
    // Analyze transition
    const transitionAnalysis = TRANSITION_ENGINE.analyzeTransition(currentScene, nextScene);

    // Get genre-specific transition styles
    const genreStyles = TRANSITION_ENGINE.getGenreTransitionStyle(genre);

    return {
      success: true,
      transition: transitionAnalysis,
      recommendedStyles: genreStyles,
      visualBridgePatterns: TRANSITION_ENGINE.visualBridgePatterns
    };

  } catch (error) {
    console.error('[creationWizardAnalyzeTransition] Error:', error);
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to analyze transition'));
  }
});

/**
 * creationWizardProcessSceneChain - Processes complete scene through prompt chain
 */
exports.creationWizardProcessSceneChain = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const { scene, nextScene, styleBible, characterBible, genre, productionMode, videoModel, visualStyleMode } = data;

  if (!scene) {
    throw new functions.https.HttpsError('invalid-argument', 'Scene data required');
  }

  // Log visual style mode for debugging
  console.log('[creationWizardProcessSceneChain] Visual style mode:', visualStyleMode || 'photorealistic (default)');

  try {
    // Process scene through the complete pipeline
    const pipelineResult = await SCENE_PIPELINE.processScene(scene, {
      styleBible,
      characterBible,
      genre,
      productionMode,
      videoModel: videoModel || 'minimax',
      nextScene,
      visualStyleMode: visualStyleMode || 'photorealistic'
    });

    return {
      success: pipelineResult.success,
      sceneId: pipelineResult.sceneId,
      visualBlueprint: pipelineResult.visualBlueprint,
      imagePrompt: pipelineResult.imagePrompt,
      videoPromptTemplate: pipelineResult.videoPromptTemplate,
      transition: pipelineResult.transition,
      audioBlueprint: pipelineResult.audioBlueprint,
      steps: pipelineResult.steps,
      errors: pipelineResult.errors
    };

  } catch (error) {
    console.error('[creationWizardProcessSceneChain] Error:', error);
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to process scene chain'));
  }
});

/**
 * creationWizardProcessAllScenes - Batch process all scenes through prompt chain
 */
exports.creationWizardProcessAllScenes = functions.runWith({ timeoutSeconds: 300 }).https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const { scenes, styleBible, characterBible, genre, productionMode, videoModel, projectId, visualStyleMode } = data;

  if (!scenes || !Array.isArray(scenes) || scenes.length === 0) {
    throw new functions.https.HttpsError('invalid-argument', 'Scenes array required');
  }

  // Log visual style mode for debugging
  console.log('[creationWizardProcessAllScenes] Visual style mode:', visualStyleMode || 'photorealistic (default)');

  try {
    // Process all scenes through the pipeline
    const results = await SCENE_PIPELINE.processAllScenes(scenes, {
      styleBible,
      characterBible,
      genre,
      productionMode,
      videoModel: videoModel || 'minimax',
      visualStyleMode: visualStyleMode || 'photorealistic'
    });

    // If projectId provided, save results
    if (projectId) {
      await db.collection('creationProjects').doc(projectId).update({
        'promptChain': {
          processedAt: admin.firestore.FieldValue.serverTimestamp(),
          summary: results.summary,
          scenes: results.scenes.map(s => ({
            sceneId: s.sceneId,
            success: s.success,
            imagePrompt: s.imagePrompt?.prompt?.substring(0, 500) || null, // Truncate for storage
            hasTransition: !!s.transition
          }))
        },
        updatedAt: admin.firestore.FieldValue.serverTimestamp()
      });
    }

    return {
      success: results.success,
      summary: results.summary,
      scenes: results.scenes
    };

  } catch (error) {
    console.error('[creationWizardProcessAllScenes] Error:', error);
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to process scenes'));
  }
});

/**
 * creationWizardGetPromptChainConfig - Gets prompt chain architecture configuration
 */
exports.creationWizardGetPromptChainConfig = functions.https.onCall(async (data, context) => {
  await verifyAuth(context);

  return {
    success: true,
    config: {
      // Scene Script Structure
      shotTypes: Object.keys(SCENE_SCRIPT_STRUCTURE.shotTypes).map(id => ({
        id,
        ...SCENE_SCRIPT_STRUCTURE.shotTypes[id]
      })),
      cameraAngles: Object.keys(SCENE_SCRIPT_STRUCTURE.cameraAngles).map(id => ({
        id,
        ...SCENE_SCRIPT_STRUCTURE.cameraAngles[id]
      })),
      lightingSetups: Object.keys(SCENE_SCRIPT_STRUCTURE.lightingSetups).map(id => ({
        id,
        ...SCENE_SCRIPT_STRUCTURE.lightingSetups[id]
      })),
      // Transition types
      transitionTypes: ['cut', 'dissolve', 'wipe', 'match-cut', 'j-cut', 'l-cut', 'fade-to-black', 'fade-from-black'],
      visualBridgePatterns: TRANSITION_ENGINE.visualBridgePatterns,
      // Video models
      supportedVideoModels: ['minimax', 'runway', 'pika', 'veo']
    }
  };
});

/**
 * creationWizardValidateSceneQuality - Validates generated content against blueprint
 */
exports.creationWizardValidateSceneQuality = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const { imageAnalysis, videoAnalysis, visualBlueprint, actionBlueprint } = data;

  try {
    const results = {
      image: null,
      video: null
    };

    // Validate image if provided
    if (imageAnalysis && visualBlueprint) {
      results.image = QUALITY_VALIDATOR.validateImageAgainstScript(imageAnalysis, visualBlueprint);
    }

    // Validate video if provided
    if (videoAnalysis && actionBlueprint) {
      results.video = QUALITY_VALIDATOR.validateVideoAgainstScript(videoAnalysis, actionBlueprint);
    }

    const overallPassed = (results.image?.passed !== false) && (results.video?.passed !== false);

    return {
      success: true,
      passed: overallPassed,
      results
    };

  } catch (error) {
    console.error('[creationWizardValidateSceneQuality] Error:', error);
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to validate quality'));
  }
});

// Export the engines for potential use in other modules
exports.PROMPT_CHAIN_ARCHITECTURE = {
  SCENE_SCRIPT_STRUCTURE,
  IMAGE_PROMPT_GENERATOR,
  VIDEO_PROMPT_GENERATOR,
  TRANSITION_ENGINE,
  SCENE_PIPELINE,
  QUALITY_VALIDATOR
};

// =============================================================================
// PHASE 12: SHOT DECOMPOSITION ENGINE
// Transforms single scenes into multi-shot cinematic sequences
// =============================================================================

// =============================================================================
// HOLLYWOOD CHOREOGRAPHY ENGINES
// Six specialized engines that transform enriched scene data into detailed
// cinematic prompts with precise 4-beat animation sequences
// =============================================================================

/**
 * BEAT_TIMELINE_GENERATOR
 * Divides 10-second shots into 4 timed beats with frame ranges
 * Beat 1: 0-2s (frames 0-60)   - ESTABLISH
 * Beat 2: 2-5s (frames 60-150) - DEVELOP
 * Beat 3: 5-8s (frames 150-240) - ESCALATE
 * Beat 4: 8-10s (frames 240-300) - RESOLVE
 */
const BEAT_TIMELINE_GENERATOR = {
  /**
   * Generate beat timeline with frame-level timing
   * @param {Array} beatBreakdown - Scene's beatBreakdown array from script
   * @param {number} fps - Frames per second (default 30)
   * @param {number} shotDuration - Duration of shot in seconds (default 10)
   * @returns {Array} Enhanced beats with frame timing
   */
  generateBeatTimeline(beatBreakdown, fps = 30, shotDuration = 10) {
    const defaultBeats = [
      { beat: 1, timing: '0-2s', startFrame: 0, endFrame: 60, duration: 2000, phase: 'ESTABLISH' },
      { beat: 2, timing: '2-5s', startFrame: 60, endFrame: 150, duration: 3000, phase: 'DEVELOP' },
      { beat: 3, timing: '5-8s', startFrame: 150, endFrame: 240, duration: 3000, phase: 'ESCALATE' },
      { beat: 4, timing: '8-10s', startFrame: 240, endFrame: 300, duration: 2000, phase: 'RESOLVE' }
    ];

    // Merge with provided beatBreakdown or use defaults
    return defaultBeats.map((beat, i) => {
      const providedBeat = beatBreakdown?.[i] || {};
      return {
        ...beat,
        action: providedBeat.action || `Beat ${beat.beat} action`,
        characterStates: providedBeat.characterStates || {},
        environmentState: providedBeat.environmentState || '',
        cameraNote: providedBeat.cameraNote || '',
        intensity: providedBeat.intensity ?? (i === 2 ? 0.85 : i === 0 ? 0.5 : 0.65)
      };
    });
  },

  /**
   * Get capture frame recommendation based on beat intensities
   * @param {Array} beats - Processed beat array
   * @returns {Object} Capture recommendation with frame and reason
   */
  getCaptureFrame(beats) {
    // Find peak intensity beat
    const peakBeat = beats.reduce((max, beat) =>
      beat.intensity > max.intensity ? beat : max
    , beats[0]);

    // Capture at 70% through the peak beat for best visual
    const capturePoint = 0.7;
    const frameRange = peakBeat.endFrame - peakBeat.startFrame;
    const captureFrame = Math.floor(peakBeat.startFrame + (frameRange * capturePoint));

    return {
      beat: peakBeat.beat,
      frame: captureFrame,
      timing: `${(captureFrame / 30).toFixed(1)}s`,
      action: peakBeat.action,
      reason: `Peak intensity (${(peakBeat.intensity * 100).toFixed(0)}%) at ${peakBeat.phase} phase`
    };
  }
};

/**
 * ENSEMBLE_BLOCKING_SYSTEM
 * Tracks all characters' positions, movements, and eye-lines across beats
 * Creates a spatial map of the scene for coherent multi-character choreography
 */
const ENSEMBLE_BLOCKING_SYSTEM = {
  /**
   * Generate comprehensive blocking data from spatial info
   * @param {Object} spatialBlocking - Scene's spatialBlocking from script
   * @param {Array} eyelines - Scene's eyelines array from script
   * @param {Array} beatBreakdown - Scene's beatBreakdown array
   * @returns {Object} Complete blocking map
   */
  generateEnsembleBlocking(spatialBlocking, eyelines, beatBreakdown) {
    const blocking = {
      characters: {},
      formations: [],
      eyeContact: [],
      movements: []
    };

    if (!spatialBlocking) {
      return this.getDefaultBlocking();
    }

    // Track each character's journey through the scene
    const allChars = new Set([
      ...Object.keys(spatialBlocking.initialPositions || {}),
      ...Object.keys(spatialBlocking.finalPositions || {})
    ]);

    allChars.forEach(char => {
      const startPos = spatialBlocking.initialPositions?.[char] || {};
      const endPos = spatialBlocking.finalPositions?.[char] || startPos;

      blocking.characters[char] = {
        startPosition: this.formatPosition(startPos),
        endPosition: this.formatPosition(endPos),
        beatPositions: beatBreakdown?.map(b =>
          b.characterStates?.[char]?.position || ''
        ) || [],
        movement: this.calculateMovement(startPos, endPos)
      };
    });

    // Formation evolution
    if (spatialBlocking.formation) {
      const formations = spatialBlocking.formationChange?.split(' â†’ ') || [spatialBlocking.formation];
      blocking.formations = [
        { beat: 1, formation: formations[0] },
        { beat: 4, formation: formations[formations.length - 1] || formations[0] }
      ];
    }

    // Eyeline map with emotional context
    blocking.eyeContact = (eyelines || []).map(eye => ({
      beat: eye.beat,
      from: eye.from,
      to: eye.to,
      emotion: eye.emotion,
      duration: eye.duration,
      description: `${eye.from} looks at ${eye.to} with ${eye.emotion} (${eye.duration})`
    }));

    // Key distances
    blocking.keyDistances = spatialBlocking.keyDistances || [];

    return blocking;
  },

  /**
   * Format position object into readable string
   */
  formatPosition(pos) {
    if (!pos || typeof pos === 'string') return pos || 'center, midground';
    return `${pos.stage || 'center'}, ${pos.depth || 'midground'}, ${pos.elevation || 'standing'}, facing ${pos.facing || 'camera'}`;
  },

  /**
   * Calculate movement type based on position change
   */
  calculateMovement(start, end) {
    if (!start || !end) return 'static';
    const changes = [];
    if (start.stage !== end.stage) changes.push(`stage ${start.stage} â†’ ${end.stage}`);
    if (start.depth !== end.depth) changes.push(`depth ${start.depth} â†’ ${end.depth}`);
    if (start.facing !== end.facing) changes.push(`turns to face ${end.facing}`);
    return changes.length > 0 ? changes.join(', ') : 'static';
  },

  /**
   * Get default blocking for scenes without spatial data
   */
  getDefaultBlocking() {
    return {
      characters: {},
      formations: [{ beat: 1, formation: 'natural' }],
      eyeContact: [],
      keyDistances: [],
      movements: []
    };
  }
};

/**
 * OBJECT_STATE_MACHINE
 * Tracks props and objects through shots with state transitions
 * Handles: hidden â†’ revealed â†’ transferred â†’ displayed sequences
 */
const OBJECT_STATE_MACHINE = {
  /**
   * Generate object states with transition types
   * @param {Array} objectTracking - Scene's objectTracking array
   * @returns {Array} Enhanced object states with transitions
   */
  generateObjectStates(objectTracking) {
    if (!objectTracking || objectTracking.length === 0) {
      return [];
    }

    return objectTracking.map(obj => ({
      object: obj.object,
      transitions: (obj.states || []).map((state, i) => ({
        ...state,
        transitionFrom: i > 0 ? obj.states[i - 1] : null,
        transitionType: i > 0 ? this.determineTransition(obj.states[i - 1], state) : 'initial',
        description: this.describeState(obj.object, state)
      }))
    }));
  },

  /**
   * Determine transition type between two states
   */
  determineTransition(from, to) {
    if (!from || !to) return 'initial';
    if (from.holder !== to.holder) return 'transfer';
    if (from.visibility !== to.visibility) return 'reveal';
    if (from.position !== to.position) return 'reposition';
    return 'maintain';
  },

  /**
   * Create human-readable state description
   */
  describeState(object, state) {
    if (!state) return '';
    const holder = state.holder === 'none' ? 'unattended' : `held by ${state.holder}`;
    return `${object}: ${state.visibility}, ${state.position}, ${holder}`;
  },

  /**
   * Get object continuity requirements for cross-shot matching
   */
  getContinuityRequirements(objectStates) {
    return objectStates.map(obj => {
      const finalState = obj.transitions[obj.transitions.length - 1];
      return {
        object: obj.object,
        exitState: finalState,
        note: `${obj.object} must enter next shot matching: ${this.describeState(obj.object, finalState)}`
      };
    });
  }
};

/**
 * PHYSICS_LAYER
 * Adds body mechanics, weight distribution, breathing, and muscle tension
 * Creates realistic physical presence for characters
 */
const PHYSICS_LAYER = {
  /**
   * Generate physics layer from beat and performance data
   * @param {Array} beatBreakdown - Scene's beatBreakdown array
   * @param {Object} performanceBlueprint - Scene's performanceBlueprint
   * @returns {Array} Physics data per beat
   */
  generatePhysicsLayer(beatBreakdown, performanceBlueprint) {
    if (!beatBreakdown || beatBreakdown.length === 0) {
      return this.getDefaultPhysics();
    }

    return beatBreakdown.map((beat, i) => {
      const intensity = beat.intensity ?? 0.5;
      const characterEnergy = performanceBlueprint?.characterEnergy || {};

      return {
        beat: beat.beat || (i + 1),
        physics: {
          breathing: this.getBreathing(intensity),
          muscleState: this.getMuscleState(intensity),
          weightDistribution: this.getWeight(beat.characterStates, intensity),
          microMovements: this.getMicroMovements(intensity),
          tension: this.getTension(intensity)
        },
        characterPhysics: this.getCharacterPhysics(beat.characterStates, characterEnergy, i)
      };
    });
  },

  /**
   * Get breathing description based on intensity
   */
  getBreathing(intensity) {
    if (intensity > 0.8) return 'rapid, visible chest movement, possible breath holds';
    if (intensity > 0.6) return 'controlled but quickened, visible on exhale';
    if (intensity > 0.4) return 'steady, deep, measured breathing';
    return 'slow, relaxed, barely visible';
  },

  /**
   * Get muscle state based on intensity
   */
  getMuscleState(intensity) {
    if (intensity > 0.8) return 'coiled, ready to spring, visible tension in shoulders and jaw';
    if (intensity > 0.6) return 'engaged, alert posture, prepared for action';
    if (intensity > 0.4) return 'active but relaxed, natural readiness';
    return 'at ease, soft posture, minimal tension';
  },

  /**
   * Get weight distribution from character states
   */
  getWeight(characterStates, intensity) {
    const weights = {};
    if (!characterStates) return weights;

    Object.entries(characterStates).forEach(([char, state]) => {
      const bodyLang = (state.bodyLanguage || '').toLowerCase();
      if (bodyLang.includes('forward') || bodyLang.includes('lean in')) {
        weights[char] = 'balls of feet, leaning forward';
      } else if (bodyLang.includes('back') || bodyLang.includes('retreat')) {
        weights[char] = 'heels, leaning back';
      } else if (intensity > 0.7) {
        weights[char] = 'centered but coiled, ready to move';
      } else {
        weights[char] = 'evenly distributed, grounded';
      }
    });
    return weights;
  },

  /**
   * Get micro-movements based on intensity
   */
  getMicroMovements(intensity) {
    if (intensity > 0.7) return 'fingers flexing, jaw tightening, subtle weight shifts';
    if (intensity > 0.4) return 'occasional blinks, small adjustments, natural fidgets';
    return 'still with natural breathing movement only';
  },

  /**
   * Get tension level
   */
  getTension(intensity) {
    if (intensity > 0.8) return 'high - visible in neck, shoulders, and hands';
    if (intensity > 0.6) return 'moderate - alert and ready';
    if (intensity > 0.4) return 'low - comfortable but aware';
    return 'minimal - fully relaxed';
  },

  /**
   * Get per-character physics from their energy states
   */
  getCharacterPhysics(characterStates, characterEnergy, beatIndex) {
    const physics = {};
    if (!characterStates) return physics;

    Object.keys(characterStates).forEach(char => {
      const energyStates = characterEnergy[char] || [];
      const currentEnergy = energyStates[beatIndex] || 'neutral:50%';
      const [emotion, intensityStr] = currentEnergy.split(':');
      const intensityVal = parseInt(intensityStr) / 100 || 0.5;

      physics[char] = {
        emotion,
        intensity: intensityVal,
        posture: this.getPosture(emotion, intensityVal),
        eyeActivity: this.getEyeActivity(emotion, intensityVal)
      };
    });
    return physics;
  },

  /**
   * Get posture based on emotion and intensity
   */
  getPosture(emotion, intensity) {
    const postures = {
      'hesitant': 'shoulders slightly hunched, weight back',
      'curious': 'head tilted, leaning slightly forward',
      'determined': 'spine straight, shoulders square, chin up',
      'fearful': 'body contracted, ready to retreat',
      'loving': 'open posture, soft shoulders, head inclined',
      'angry': 'shoulders forward, stance wide, fists may clench',
      'sad': 'shoulders dropped, head lowered, collapsed posture',
      'confident': 'chest open, shoulders back, commanding stance'
    };
    return postures[emotion] || 'natural standing posture';
  },

  /**
   * Get eye activity based on emotion
   */
  getEyeActivity(emotion, intensity) {
    if (intensity > 0.7) return 'wide, alert, minimal blinking';
    if (emotion === 'hesitant' || emotion === 'fearful') return 'darting, checking surroundings';
    if (emotion === 'loving' || emotion === 'connected') return 'soft, held gaze, gentle';
    return 'natural movement, regular blinking';
  },

  /**
   * Get default physics for scenes without data
   */
  getDefaultPhysics() {
    return [1, 2, 3, 4].map(beat => ({
      beat,
      physics: {
        breathing: 'natural, steady',
        muscleState: 'relaxed but present',
        weightDistribution: {},
        microMovements: 'natural fidgets',
        tension: 'minimal'
      },
      characterPhysics: {}
    }));
  }
};

// =============================================================================
// CINEMATIC_PHYSICS_ENGINE - Hollywood-Level Force & Momentum Physics
// =============================================================================
/**
 * CINEMATIC_PHYSICS_ENGINE
 *
 * Adds Hollywood-quality physics descriptions to video prompts.
 * Based on 2025 AI filmmaking research: "Stop describing what things look likeâ€”
 * start describing the forces acting on them."
 *
 * Modern AI video models (Runway Gen-4, Sora 2, Kling 2.6) now understand:
 * - Weight and momentum
 * - Cause-and-effect chains
 * - Material physics (fabric, hair, liquid)
 * - Environmental forces
 *
 * This engine generates physics-aware prompt enhancements.
 */
const CINEMATIC_PHYSICS_ENGINE = {

  /**
   * Analyze scene context and generate comprehensive physics layer
   * @param {Object} scene - Scene data with visualPrompt, sceneAction, etc.
   * @param {Object} shot - Shot data with shotType, cameraMovement, etc.
   * @param {number} intensity - Scene intensity (0.0-1.0)
   * @returns {Object} Complete physics analysis for video prompt enhancement
   */
  analyzeScenePhysics(scene, shot, intensity = 0.5) {
    const visualPrompt = scene?.visualPrompt || '';
    const sceneAction = scene?.sceneAction || '';
    const combinedText = `${visualPrompt} ${sceneAction}`.toLowerCase();

    return {
      // Core physics systems
      objectPhysics: this.detectObjectPhysics(combinedText, intensity),
      environmentalForces: this.detectEnvironmentalForces(combinedText),
      materialPhysics: this.detectMaterialPhysics(combinedText),
      contactPhysics: this.detectContactPhysics(combinedText, intensity),
      causeAndEffect: this.generateCauseEffectChains(combinedText, intensity),

      // Physics-aware camera hints
      cameraPhysics: this.getCameraPhysicsHints(shot?.cameraMovement, intensity),

      // Temporal physics (how physics evolves over shot duration)
      temporalPhysics: this.getTemporalPhysicsProgression(intensity)
    };
  },

  /**
   * Detect objects and their physics properties
   */
  detectObjectPhysics(text, intensity) {
    const objects = {
      heavy: [],
      light: [],
      fluid: [],
      particles: []
    };

    // Heavy objects (have weight, momentum, impact)
    const heavyPatterns = [
      { pattern: /sword|blade|weapon/i, physics: 'steel weight, momentum on swing, slicing arc' },
      { pattern: /door|gate/i, physics: 'heavy swing, slow momentum, solid impact' },
      { pattern: /boulder|rock|stone/i, physics: 'massive weight, crushing force, ground shake on impact' },
      { pattern: /body|bodies|corpse/i, physics: 'limp weight, gravity pull, ragdoll physics' },
      { pattern: /armor|shield/i, physics: 'metal weight, clanking on movement, protective mass' },
      { pattern: /staff|pole/i, physics: 'balanced weight, sweeping momentum, whooshing arcs' },
      { pattern: /book|tome|scroll/i, physics: 'paper weight, pages flutter, binding creak' },
      { pattern: /chest|box|crate/i, physics: 'solid mass, scraping on floor, heavy thud' }
    ];

    // Light objects (affected by air, quick movement)
    const lightPatterns = [
      { pattern: /leaf|leaves/i, physics: 'weightless drift, spiral descent, wind-carried' },
      { pattern: /feather/i, physics: 'floating descent, air resistance, gentle landing' },
      { pattern: /paper|parchment/i, physics: 'flutter and fold, caught by air currents' },
      { pattern: /petal|petals|flower/i, physics: 'delicate drift, scatter pattern, soft landing' },
      { pattern: /ash|ashes/i, physics: 'rising thermals, grey drift, settling slowly' },
      { pattern: /snow|snowflake/i, physics: 'lazy descent, wind-swirled, accumulating softly' }
    ];

    // Fluid objects (flow, splash, wave)
    const fluidPatterns = [
      { pattern: /water|river|stream|ocean|sea/i, physics: 'flowing current, wave motion, splash dynamics' },
      { pattern: /blood/i, physics: 'viscous flow, splatter pattern, drip trajectory' },
      { pattern: /rain/i, physics: 'vertical streaks, splash on impact, surface ripples' },
      { pattern: /tear|tears/i, physics: 'welling, slow roll down cheek, drop formation' },
      { pattern: /liquid|potion/i, physics: 'sloshing movement, pour arc, surface tension' },
      { pattern: /lava|magma/i, physics: 'thick slow flow, cooling crust, heat shimmer above' }
    ];

    // Particle objects (many small elements)
    const particlePatterns = [
      { pattern: /dust|motes/i, physics: 'suspended particles, caught in light beams, slow drift' },
      { pattern: /spark|sparks/i, physics: 'quick scatter, fade trajectory, brief illumination' },
      { pattern: /ember|embers/i, physics: 'rising glow, cooling arc, floating upward' },
      { pattern: /smoke/i, physics: 'rising curl, dissipation pattern, air current response' },
      { pattern: /mist|fog/i, physics: 'low-lying blanket, swirl around movement, density variation' },
      { pattern: /sand/i, physics: 'granular flow, wind-blown scatter, settling pattern' },
      { pattern: /debris/i, physics: 'scatter pattern, various weights, settling sequence' }
    ];

    heavyPatterns.forEach(p => { if (p.pattern.test(text)) objects.heavy.push(p.physics); });
    lightPatterns.forEach(p => { if (p.pattern.test(text)) objects.light.push(p.physics); });
    fluidPatterns.forEach(p => { if (p.pattern.test(text)) objects.fluid.push(p.physics); });
    particlePatterns.forEach(p => { if (p.pattern.test(text)) objects.particles.push(p.physics); });

    return objects;
  },

  /**
   * Detect environmental forces acting on the scene
   */
  detectEnvironmentalForces(text) {
    const forces = [];

    // Wind forces
    if (/wind|breeze|gust|storm|gale/i.test(text)) {
      if (/storm|gale|fierce|strong/i.test(text)) {
        forces.push({
          type: 'wind',
          strength: 'strong',
          effect: 'clothing pressed against body, hair streaming, difficult movement'
        });
      } else if (/breeze|gentle|soft/i.test(text)) {
        forces.push({
          type: 'wind',
          strength: 'gentle',
          effect: 'subtle fabric flutter, hair wisps lifting, leaves stirring'
        });
      } else {
        forces.push({
          type: 'wind',
          strength: 'moderate',
          effect: 'clothing rippling, hair flowing, objects may shift'
        });
      }
    }

    // Gravity variations
    if (/fall|falling|drop|descend|plunge/i.test(text)) {
      forces.push({
        type: 'gravity',
        strength: 'normal',
        effect: 'acceleration downward, increasing speed, impact anticipation'
      });
    }
    if (/float|hover|weightless|zero.?g/i.test(text)) {
      forces.push({
        type: 'gravity',
        strength: 'reduced',
        effect: 'slow drift, objects suspended, hair floating upward'
      });
    }

    // Water resistance
    if (/underwater|submerged|swimming|diving/i.test(text)) {
      forces.push({
        type: 'water_resistance',
        strength: 'full',
        effect: 'slowed movement, hair floating, bubbles rising, light refraction'
      });
    }

    // Heat/thermal
    if (/fire|flame|heat|hot|burning/i.test(text)) {
      forces.push({
        type: 'thermal',
        strength: 'high',
        effect: 'rising heat shimmer, sweat forming, light flickering upward'
      });
    }
    if (/cold|ice|frost|frozen|freezing/i.test(text)) {
      forces.push({
        type: 'thermal',
        strength: 'low',
        effect: 'visible breath, condensation, stiff movement, frost spreading'
      });
    }

    // Magical/energy forces
    if (/energy|power|force|magic|glow|aura/i.test(text)) {
      forces.push({
        type: 'energy',
        strength: 'variable',
        effect: 'crackling arcs, pulsing waves, objects responding to invisible force'
      });
    }

    return forces;
  },

  /**
   * Detect materials and their physics behaviors
   */
  detectMaterialPhysics(text) {
    const materials = [];

    // Fabric/clothing
    if (/cloak|cape|robe|coat|dress|gown|fabric|cloth|veil|banner|flag/i.test(text)) {
      materials.push({
        type: 'fabric',
        behavior: 'wave and billow with movement, settle when still, catch wind and light',
        detail: 'fabric ripples follow body motion with slight delay, hem trails behind turns'
      });
    }

    // Hair
    if (/hair|locks|mane|braid|ponytail/i.test(text)) {
      materials.push({
        type: 'hair',
        behavior: 'flows opposite to movement direction, bounces on impact, affected by wind',
        detail: 'strands separate and rejoin, volume responds to humidity/wetness'
      });
    }

    // Metal
    if (/metal|steel|iron|gold|silver|bronze|armor|sword|chain/i.test(text)) {
      materials.push({
        type: 'metal',
        behavior: 'rigid movement, reflects light sharply, rings or clanks on contact',
        detail: 'catches light as angle changes, slight flex on thin pieces, weight affects swing'
      });
    }

    // Glass/crystal
    if (/glass|crystal|gem|jewel|mirror/i.test(text)) {
      materials.push({
        type: 'glass',
        behavior: 'refracts light, casts prismatic reflections, shatters dramatically',
        detail: 'internal light play, rainbow edges, sharp fragmentation pattern'
      });
    }

    // Leather
    if (/leather|hide|strap|belt|boot/i.test(text)) {
      materials.push({
        type: 'leather',
        behavior: 'creaks with movement, flexes but holds shape, ages with wear',
        detail: 'subtle sound on flex, worn areas lighter, stiff until warmed'
      });
    }

    // Wood
    if (/wood|wooden|timber|branch|tree/i.test(text)) {
      materials.push({
        type: 'wood',
        behavior: 'solid but can splinter, grain visible, creaks under stress',
        detail: 'flex before break, splinters scatter on impact, hollow vs solid sounds'
      });
    }

    return materials;
  },

  /**
   * Detect and describe contact/impact physics
   */
  detectContactPhysics(text, intensity) {
    const contacts = [];

    // Footsteps
    if (/walk|step|run|stride|pace/i.test(text)) {
      const weight = intensity > 0.7 ? 'heavy, purposeful' : 'measured, deliberate';
      contacts.push({
        type: 'footsteps',
        description: `${weight} footfalls, ground compression, subtle dust rise`,
        timing: 'rhythmic, matching movement pace'
      });
    }

    // Combat/strikes
    if (/hit|strike|punch|kick|slash|stab|clash|fight/i.test(text)) {
      contacts.push({
        type: 'combat_impact',
        description: 'impact force transfer, target recoil, attacker follow-through',
        timing: 'sharp impact moment, brief freeze, momentum continuation'
      });
    }

    // Landing/falling
    if (/land|jump|leap|fall|drop/i.test(text)) {
      contacts.push({
        type: 'landing',
        description: 'leg absorption, body compression, balance recovery',
        timing: 'impact â†’ absorption â†’ stabilization (0.3s â†’ 0.5s â†’ settle)'
      });
    }

    // Touch/grasp
    if (/touch|grab|grasp|hold|reach|hand/i.test(text)) {
      contacts.push({
        type: 'touch',
        description: 'finger contact pressure, object response to grip',
        timing: 'approach â†’ contact â†’ grip adjustment'
      });
    }

    // Collision
    if (/crash|collide|smash|shatter|break/i.test(text)) {
      contacts.push({
        type: 'collision',
        description: 'sudden momentum transfer, debris scatter, settling aftermath',
        timing: 'impact spike â†’ scatter â†’ settle (instant â†’ 1s â†’ 2s)'
      });
    }

    return contacts;
  },

  /**
   * Generate cause-and-effect physics chains
   */
  generateCauseEffectChains(text, intensity) {
    const chains = [];

    // Movement â†’ environment reaction
    if (/walk|run|move|stride/i.test(text)) {
      chains.push({
        cause: 'character movement',
        effects: [
          'clothing follows body with 0.1s delay',
          'hair trails movement direction',
          'nearby light particles scatter',
          'fabric settles after stopping'
        ]
      });
    }

    // Turn â†’ cascade effect
    if (/turn|spin|pivot|rotate/i.test(text)) {
      chains.push({
        cause: 'character rotation',
        effects: [
          'cape/coat swings outward (centrifugal)',
          'hair arcs in rotation direction',
          'weight shifts to outer foot',
          'eyes lead the turn'
        ]
      });
    }

    // Impact â†’ reaction chain
    if (/hit|strike|impact|clash/i.test(text)) {
      chains.push({
        cause: 'physical impact',
        effects: [
          'shock wave through contact point',
          'receiver body deformation/recoil',
          'attacker follow-through momentum',
          'environmental particle scatter',
          'settling return to equilibrium'
        ]
      });
    }

    // Emotional â†’ physical manifestation
    if (intensity > 0.7) {
      chains.push({
        cause: 'high emotional intensity',
        effects: [
          'breathing visible and quickened',
          'micro-tremors in extremities',
          'posture tension visible',
          'environment seems to respond (pathetic fallacy)'
        ]
      });
    }

    return chains;
  },

  /**
   * Get camera physics hints (how camera movement affects perception)
   */
  getCameraPhysicsHints(cameraMovement, intensity) {
    const movement = (cameraMovement || '').toLowerCase();

    if (movement.includes('handheld') || movement.includes('shaky')) {
      return {
        style: 'handheld',
        physics: 'organic micro-movements, breathing rhythm, human imperfection',
        effect: 'visceral, immediate, documentary feel'
      };
    }

    if (movement.includes('dolly') || movement.includes('track')) {
      return {
        style: 'tracking',
        physics: 'smooth glide, parallax on background layers, steady pursuit',
        effect: 'professional, deliberate, following action'
      };
    }

    if (movement.includes('crane') || movement.includes('jib')) {
      return {
        style: 'crane',
        physics: 'sweeping arc, reveal physics, grandeur movement',
        effect: 'epic, establishing, god-like perspective'
      };
    }

    if (movement.includes('zoom')) {
      return {
        style: 'zoom',
        physics: 'optical compression, background squeeze, no parallax',
        effect: 'emotional punch, isolation, focus shift'
      };
    }

    if (movement.includes('static') || movement.includes('locked')) {
      return {
        style: 'static',
        physics: 'stable frame, motion exists only in subjects',
        effect: 'theatrical, observational, tension through stillness'
      };
    }

    return {
      style: 'neutral',
      physics: 'subtle stabilization, natural float',
      effect: 'cinematic, professional, unobtrusive'
    };
  },

  /**
   * Get temporal physics progression (how physics evolves over the shot)
   */
  getTemporalPhysicsProgression(intensity) {
    // Physics changes over a 10-second shot
    return {
      seconds_0_2: {
        phase: 'initiation',
        description: 'Forces begin to act, momentum building, initial states',
        physicsNote: 'Objects at rest or in established motion, forces accumulating'
      },
      seconds_2_5: {
        phase: 'development',
        description: 'Physics in full effect, cause-effect chains playing out',
        physicsNote: 'Peak momentum, maximum force expression, cascading effects'
      },
      seconds_5_8: {
        phase: 'peak_or_transfer',
        description: intensity > 0.7
          ? 'Maximum energy expression, potential energy release'
          : 'Energy transfer between elements, momentum shifts',
        physicsNote: 'Key physics moments - impacts, releases, transformations'
      },
      seconds_8_10: {
        phase: 'resolution',
        description: 'Energy dissipation, settling, new equilibrium forming',
        physicsNote: 'Motion dampening, particles settling, fabric falling still'
      }
    };
  },

  /**
   * Generate complete physics enhancement for a video prompt
   * @param {Object} scene - Scene data
   * @param {Object} shot - Shot data
   * @param {number} intensity - Intensity value 0.0-1.0
   * @returns {string} Physics enhancement text to append to video prompt
   */
  generatePhysicsEnhancement(scene, shot, intensity = 0.5) {
    const physics = this.analyzeScenePhysics(scene, shot, intensity);
    const lines = [];

    lines.push('[PHYSICS LAYER - Force & Momentum]');

    // Object physics
    const allObjects = [
      ...physics.objectPhysics.heavy,
      ...physics.objectPhysics.light,
      ...physics.objectPhysics.fluid,
      ...physics.objectPhysics.particles
    ];
    if (allObjects.length > 0) {
      lines.push(`Objects: ${allObjects.slice(0, 3).join('; ')}`);
    }

    // Environmental forces
    if (physics.environmentalForces.length > 0) {
      const forceDesc = physics.environmentalForces
        .map(f => `${f.type}: ${f.effect}`)
        .slice(0, 2)
        .join('. ');
      lines.push(`Forces: ${forceDesc}`);
    }

    // Material physics
    if (physics.materialPhysics.length > 0) {
      const matDesc = physics.materialPhysics
        .map(m => m.detail)
        .slice(0, 2)
        .join('. ');
      lines.push(`Materials: ${matDesc}`);
    }

    // Contact physics
    if (physics.contactPhysics.length > 0) {
      const contactDesc = physics.contactPhysics
        .map(c => c.description)
        .slice(0, 2)
        .join('. ');
      lines.push(`Contact: ${contactDesc}`);
    }

    // Cause-effect chains (most important for modern AI video)
    if (physics.causeAndEffect.length > 0) {
      const chain = physics.causeAndEffect[0];
      lines.push(`Causeâ†’Effect: ${chain.cause} triggers ${chain.effects.slice(0, 2).join(', ')}`);
    }

    // Camera physics
    lines.push(`Camera: ${physics.cameraPhysics.physics}`);

    return lines.join('\n');
  },

  /**
   * Get physics summary for metadata
   */
  getPhysicsSummary(scene, shot, intensity) {
    const physics = this.analyzeScenePhysics(scene, shot, intensity);

    return {
      hasObjectPhysics: Object.values(physics.objectPhysics).some(arr => arr.length > 0),
      hasEnvironmentalForces: physics.environmentalForces.length > 0,
      hasMaterialPhysics: physics.materialPhysics.length > 0,
      hasContactPhysics: physics.contactPhysics.length > 0,
      causeEffectChains: physics.causeAndEffect.length,
      cameraStyle: physics.cameraPhysics.style,
      temporalPhases: Object.keys(physics.temporalPhysics)
    };
  },

  /**
   * Generate CONDENSED physics hint for video prompt (max 120 chars)
   * Full enhancement stored separately as metadata
   */
  generateCondensedHint(scene, shot, intensity = 0.5) {
    const physics = this.analyzeScenePhysics(scene, shot, intensity);
    const hints = [];

    // Most important: materials and their motion
    if (physics.materialPhysics.length > 0) {
      const materials = physics.materialPhysics.slice(0, 2).map(m => m.type).join(', ');
      hints.push(materials + ' motion');
    }

    // Environmental forces
    if (physics.environmentalForces.length > 0) {
      hints.push(physics.environmentalForces[0].type);
    }

    // Camera style
    hints.push(physics.cameraPhysics.style);

    return hints.length > 0 ? `[Physics: ${hints.join(', ')}]` : '';
  }
};

// =============================================================================
// CHARACTER_REFERENCE_ENGINE - Visual Consistency Across Shots
// =============================================================================
/**
 * CHARACTER_REFERENCE_ENGINE
 *
 * Maintains character visual consistency across shots using anchor images and
 * reference sheets. Based on 2025 AI filmmaking best practices:
 * - LoRA training concepts (20-50 images with angle variety)
 * - Character sheets (front, side, expression views)
 * - First-shot anchoring (use first appearance as reference)
 *
 * The engine:
 * 1. Extracts character visual anchors from concept/bible data
 * 2. Generates pose/expression reference sheets
 * 3. Provides per-shot character consistency hints
 * 4. Tracks character appearances across scenes
 */
const CHARACTER_REFERENCE_ENGINE = {

  /**
   * Extract character anchors from scene/project data
   * @param {Object} characterBible - Character bible array
   * @param {Object} sceneMemory - Scene memory with character descriptions
   * @param {Object} enrichmentData - Concept enrichment data
   * @returns {Object} Character anchor data
   */
  extractCharacterAnchors(characterBible, sceneMemory, enrichmentData) {
    const anchors = {};

    // Extract from character bible (primary source)
    if (characterBible && Array.isArray(characterBible)) {
      characterBible.forEach((char, idx) => {
        const name = char.name || char.archetype || `Character_${idx + 1}`;
        anchors[name] = {
          id: `char_${idx}`,
          name: name,
          source: 'characterBible',
          // Visual description
          visualDescription: char.visualDescription || char.appearance || null,
          archetype: char.archetype || null,
          role: char.role || 'supporting',
          // Physical attributes (extracted or inferred)
          physicalAttributes: this.extractPhysicalAttributes(char),
          // Clothing/costume
          costume: this.extractCostume(char),
          // Distinguishing features
          distinguishingFeatures: this.extractDistinguishingFeatures(char),
          // Reference images (if available)
          referenceImages: {
            primary: char.referenceImage || char.imageUrl || null,
            poses: [],
            expressions: []
          },
          // Scene appearances (to be populated)
          appearances: [],
          firstAppearanceShot: null
        };
      });
    }

    // Enrich from scene memory
    if (sceneMemory?.characterDescriptions) {
      sceneMemory.characterDescriptions.forEach(desc => {
        const name = desc.name || desc.character;
        if (name && anchors[name]) {
          anchors[name].visualDescription = anchors[name].visualDescription || desc.description;
          if (desc.sceneId) {
            anchors[name].appearances.push(desc.sceneId);
          }
        }
      });
    }

    // Enrich from concept data
    if (enrichmentData?.characters) {
      enrichmentData.characters.forEach(char => {
        const name = char.name || char.archetype;
        if (name) {
          if (!anchors[name]) {
            anchors[name] = {
              id: `char_enriched_${Object.keys(anchors).length}`,
              name: name,
              source: 'enrichment',
              visualDescription: null,
              archetype: char.archetype,
              role: char.role,
              physicalAttributes: {},
              costume: {},
              distinguishingFeatures: [],
              referenceImages: { primary: null, poses: [], expressions: [] },
              appearances: [],
              firstAppearanceShot: null
            };
          }
          // Merge enrichment data
          anchors[name].archetype = anchors[name].archetype || char.archetype;
          anchors[name].role = anchors[name].role || char.role;
        }
      });
    }

    return anchors;
  },

  /**
   * Extract physical attributes from character data
   */
  extractPhysicalAttributes(char) {
    const desc = (char.visualDescription || char.appearance || '').toLowerCase();
    const attributes = {
      gender: null,
      age: null,
      build: null,
      height: null,
      hairColor: null,
      hairStyle: null,
      eyeColor: null,
      skinTone: null
    };

    // Gender detection
    if (/\b(woman|female|she|her|girl|lady)\b/i.test(desc)) attributes.gender = 'female';
    else if (/\b(man|male|he|his|boy|guy)\b/i.test(desc)) attributes.gender = 'male';

    // Age detection
    if (/\b(young|youth|teen|adolescent)\b/i.test(desc)) attributes.age = 'young';
    else if (/\b(middle.?aged?|mature)\b/i.test(desc)) attributes.age = 'middle-aged';
    else if (/\b(old|elderly|aged|ancient|wizened)\b/i.test(desc)) attributes.age = 'elderly';
    else if (/\b(child|kid|little)\b/i.test(desc)) attributes.age = 'child';

    // Build detection
    if (/\b(muscular|athletic|strong|powerful|built)\b/i.test(desc)) attributes.build = 'athletic';
    else if (/\b(slim|slender|thin|lean|lithe)\b/i.test(desc)) attributes.build = 'slim';
    else if (/\b(large|heavy|stocky|broad)\b/i.test(desc)) attributes.build = 'large';

    // Hair color
    const hairColorMatch = desc.match(/\b(black|dark|brown|blonde|blond|red|auburn|grey|gray|white|silver|golden)\s*hair\b/i);
    if (hairColorMatch) attributes.hairColor = hairColorMatch[1];

    // Hair style
    const hairStyleMatch = desc.match(/\b(long|short|cropped|braided|curly|straight|wavy|shaved|bald)\s*hair\b/i);
    if (hairStyleMatch) attributes.hairStyle = hairStyleMatch[1];

    // Eye color
    const eyeColorMatch = desc.match(/\b(blue|green|brown|hazel|grey|gray|golden|amber|dark|black)\s*eyes?\b/i);
    if (eyeColorMatch) attributes.eyeColor = eyeColorMatch[1];

    return attributes;
  },

  /**
   * Extract costume/clothing details
   */
  extractCostume(char) {
    const desc = (char.visualDescription || char.appearance || '').toLowerCase();
    const costume = {
      type: null,
      colors: [],
      material: null,
      accessories: []
    };

    // Clothing type
    if (/\b(armor|armour|plate)\b/i.test(desc)) costume.type = 'armor';
    else if (/\b(robe|robes|cloak|cape)\b/i.test(desc)) costume.type = 'robes';
    else if (/\b(dress|gown)\b/i.test(desc)) costume.type = 'dress';
    else if (/\b(suit|formal|tuxedo)\b/i.test(desc)) costume.type = 'formal';
    else if (/\b(casual|jeans|t-?shirt)\b/i.test(desc)) costume.type = 'casual';
    else if (/\b(uniform|military)\b/i.test(desc)) costume.type = 'uniform';
    else if (/\b(traditional|kimono|hanbok|sari)\b/i.test(desc)) costume.type = 'traditional';

    // Colors in clothing
    const colorMatches = desc.match(/\b(red|blue|green|black|white|gold|silver|purple|crimson|azure|emerald|onyx|ivory)\b/gi);
    if (colorMatches) costume.colors = [...new Set(colorMatches.map(c => c.toLowerCase()))];

    // Materials
    if (/\b(leather)\b/i.test(desc)) costume.material = 'leather';
    else if (/\b(silk|satin)\b/i.test(desc)) costume.material = 'silk';
    else if (/\b(wool|woolen)\b/i.test(desc)) costume.material = 'wool';
    else if (/\b(metal|steel|iron)\b/i.test(desc)) costume.material = 'metal';

    // Accessories
    const accessoryPatterns = [
      /\b(sword|blade|weapon)\b/i,
      /\b(staff|wand)\b/i,
      /\b(crown|tiara|circlet)\b/i,
      /\b(necklace|pendant|amulet)\b/i,
      /\b(ring|rings)\b/i,
      /\b(gloves|gauntlets)\b/i,
      /\b(boots|sandals)\b/i,
      /\b(mask)\b/i,
      /\b(glasses|spectacles)\b/i,
      /\b(hat|hood|helm|helmet)\b/i
    ];
    accessoryPatterns.forEach(pattern => {
      const match = desc.match(pattern);
      if (match) costume.accessories.push(match[1].toLowerCase());
    });

    return costume;
  },

  /**
   * Extract distinguishing features
   */
  extractDistinguishingFeatures(char) {
    const desc = (char.visualDescription || char.appearance || '').toLowerCase();
    const features = [];

    // Scars, tattoos, marks
    if (/\bscar\b/i.test(desc)) features.push('scar');
    if (/\btattoo\b/i.test(desc)) features.push('tattoo');
    if (/\bmark|birthmark\b/i.test(desc)) features.push('distinctive mark');

    // Facial features
    if (/\bbeard\b/i.test(desc)) features.push('beard');
    if (/\bmustache\b/i.test(desc)) features.push('mustache');
    if (/\bfreckles\b/i.test(desc)) features.push('freckles');

    // Physical traits
    if (/\beye.?patch\b/i.test(desc)) features.push('eyepatch');
    if (/\bprosthetic|mechanical|cybernetic\b/i.test(desc)) features.push('prosthetic');
    if (/\bwings\b/i.test(desc)) features.push('wings');
    if (/\btail\b/i.test(desc)) features.push('tail');
    if (/\bhorns\b/i.test(desc)) features.push('horns');
    if (/\bpointed ears|elf ears\b/i.test(desc)) features.push('pointed ears');

    return features;
  },

  /**
   * Generate character reference sheet for a specific character
   * Returns structured data for AI image/video consistency
   */
  generateReferenceSheet(characterAnchor) {
    if (!characterAnchor) return null;

    const sheet = {
      characterId: characterAnchor.id,
      characterName: characterAnchor.name,

      // Core visual identity (MUST be consistent)
      coreIdentity: {
        physicalBuild: this.buildPhysicalDescription(characterAnchor.physicalAttributes),
        facialFeatures: this.buildFacialDescription(characterAnchor.physicalAttributes),
        distinguishingMarks: characterAnchor.distinguishingFeatures.join(', ') || 'none',
        primaryCostume: this.buildCostumeDescription(characterAnchor.costume)
      },

      // Pose reference positions (for AI video consistency)
      poseGuide: {
        neutral: 'Standing straight, arms relaxed at sides, weight evenly distributed',
        walking: 'Mid-stride, arms swinging naturally, looking ahead',
        action: 'Dynamic pose, weight forward, ready for movement',
        emotional: 'Posture reflects emotional state while maintaining physical characteristics'
      },

      // Expression range (for AI video consistency)
      expressionGuide: {
        neutral: 'Relaxed face, natural expression, characteristic features visible',
        focused: 'Slight brow furrow, determined eyes, jaw set',
        emotional: 'Clear expression while maintaining facial structure',
        speaking: 'Natural mouth movement, characteristic expressions'
      },

      // AI video prompt helper
      videoPromptTemplate: this.buildVideoPromptTemplate(characterAnchor),

      // Consistency checklist
      consistencyChecklist: [
        `Hair: ${characterAnchor.physicalAttributes.hairColor || 'as established'} ${characterAnchor.physicalAttributes.hairStyle || ''} hair`,
        `Eyes: ${characterAnchor.physicalAttributes.eyeColor || 'as established'} eyes`,
        `Build: ${characterAnchor.physicalAttributes.build || 'as established'} build`,
        `Costume: ${characterAnchor.costume.type || 'as established'} in ${characterAnchor.costume.colors.join('/') || 'established colors'}`,
        ...characterAnchor.distinguishingFeatures.map(f => `Feature: ${f} visible when relevant`)
      ]
    };

    return sheet;
  },

  /**
   * Build physical description string
   */
  buildPhysicalDescription(attrs) {
    const parts = [];
    if (attrs.gender) parts.push(attrs.gender);
    if (attrs.age) parts.push(attrs.age);
    if (attrs.build) parts.push(`${attrs.build} build`);
    if (attrs.height) parts.push(attrs.height);
    return parts.join(', ') || 'as established in reference';
  },

  /**
   * Build facial description string
   */
  buildFacialDescription(attrs) {
    const parts = [];
    if (attrs.hairColor || attrs.hairStyle) {
      parts.push(`${attrs.hairColor || ''} ${attrs.hairStyle || ''} hair`.trim());
    }
    if (attrs.eyeColor) parts.push(`${attrs.eyeColor} eyes`);
    if (attrs.skinTone) parts.push(`${attrs.skinTone} skin`);
    return parts.join(', ') || 'as established in reference';
  },

  /**
   * Build costume description string
   */
  buildCostumeDescription(costume) {
    const parts = [];
    if (costume.colors.length > 0) parts.push(costume.colors.join(' and '));
    if (costume.material) parts.push(costume.material);
    if (costume.type) parts.push(costume.type);
    if (costume.accessories.length > 0) parts.push(`with ${costume.accessories.join(', ')}`);
    return parts.join(' ') || 'as established in reference';
  },

  /**
   * Build video prompt template for character
   */
  buildVideoPromptTemplate(anchor) {
    const parts = [];

    // Core identity
    if (anchor.name) parts.push(`[${anchor.name}]`);

    // Physical description
    const physical = [];
    if (anchor.physicalAttributes.gender) physical.push(anchor.physicalAttributes.gender);
    if (anchor.physicalAttributes.age) physical.push(anchor.physicalAttributes.age);
    if (anchor.physicalAttributes.build) physical.push(`${anchor.physicalAttributes.build} build`);
    if (physical.length > 0) parts.push(physical.join(' '));

    // Hair and face
    const face = [];
    if (anchor.physicalAttributes.hairColor) {
      face.push(`${anchor.physicalAttributes.hairColor} ${anchor.physicalAttributes.hairStyle || ''} hair`.trim());
    }
    if (anchor.physicalAttributes.eyeColor) face.push(`${anchor.physicalAttributes.eyeColor} eyes`);
    if (face.length > 0) parts.push(face.join(', '));

    // Costume
    if (anchor.costume.type || anchor.costume.colors.length > 0) {
      const costumeStr = this.buildCostumeDescription(anchor.costume);
      if (costumeStr) parts.push(`wearing ${costumeStr}`);
    }

    // Distinguishing features
    if (anchor.distinguishingFeatures.length > 0) {
      parts.push(`with ${anchor.distinguishingFeatures.join(', ')}`);
    }

    return parts.join(', ') || anchor.visualDescription || 'character as established';
  },

  /**
   * Get per-shot character reference hints
   * @param {Object} shot - Shot data
   * @param {Object} scene - Scene data
   * @param {Object} characterAnchors - All character anchors
   * @returns {Object} Character reference hints for this shot
   */
  getShotCharacterHints(shot, scene, characterAnchors) {
    const hints = {
      charactersInShot: [],
      consistencyRequirements: [],
      referenceNotes: [],
      promptEnhancement: ''
    };

    // Detect characters from scene/shot data
    const charactersInScene = scene?.charactersInScene || [];
    const shotPrompt = (shot?.videoPrompt || shot?.prompt || '').toLowerCase();

    // Check each anchor against the shot
    Object.values(characterAnchors).forEach(anchor => {
      const name = anchor.name.toLowerCase();
      const isInScene = charactersInScene.some(c =>
        (c.toLowerCase && c.toLowerCase().includes(name)) || c === anchor.name
      );
      const isInPrompt = shotPrompt.includes(name);

      if (isInScene || isInPrompt) {
        hints.charactersInShot.push({
          name: anchor.name,
          referenceSheet: this.generateReferenceSheet(anchor),
          promptTemplate: this.buildVideoPromptTemplate(anchor)
        });

        // Add consistency requirements
        hints.consistencyRequirements.push(
          `${anchor.name}: Maintain exact visual appearance as established`
        );

        if (anchor.distinguishingFeatures.length > 0) {
          hints.consistencyRequirements.push(
            `${anchor.name} features: ${anchor.distinguishingFeatures.join(', ')} must be visible when character is shown`
          );
        }
      }
    });

    // Build prompt enhancement
    if (hints.charactersInShot.length > 0) {
      const charDescriptions = hints.charactersInShot
        .map(c => c.promptTemplate)
        .join('; ');
      hints.promptEnhancement = `[CHARACTER CONSISTENCY] ${charDescriptions}`;
    }

    return hints;
  },

  /**
   * Register first appearance of a character
   */
  registerFirstAppearance(characterName, shotId, imageUrl, anchors) {
    if (anchors[characterName]) {
      if (!anchors[characterName].firstAppearanceShot) {
        anchors[characterName].firstAppearanceShot = shotId;
      }
      if (imageUrl && !anchors[characterName].referenceImages.primary) {
        anchors[characterName].referenceImages.primary = imageUrl;
      }
    }
    return anchors;
  },

  /**
   * Generate character consistency enhancement for video prompt
   * @param {Object} scene - Scene data
   * @param {Object} shot - Shot data
   * @param {Object} characterBible - Character bible data
   * @returns {string} Character consistency text to append to video prompt
   */
  generateCharacterEnhancement(scene, shot, characterBible) {
    // Extract anchors
    const anchors = this.extractCharacterAnchors(characterBible, null, null);

    // Get shot-specific hints
    const hints = this.getShotCharacterHints(shot, scene, anchors);

    if (hints.charactersInShot.length === 0) {
      return '';
    }

    const lines = [];
    lines.push('[CHARACTER CONSISTENCY]');

    hints.charactersInShot.forEach(char => {
      const sheet = char.referenceSheet;
      if (sheet) {
        lines.push(`${char.name}: ${sheet.coreIdentity.physicalBuild}, ${sheet.coreIdentity.facialFeatures}`);
        if (sheet.coreIdentity.primaryCostume) {
          lines.push(`  Costume: ${sheet.coreIdentity.primaryCostume}`);
        }
        if (sheet.coreIdentity.distinguishingMarks !== 'none') {
          lines.push(`  Features: ${sheet.coreIdentity.distinguishingMarks}`);
        }
      }
    });

    lines.push('MAINTAIN: Same face, same build, same costume across all shots');

    return lines.join('\n');
  },

  /**
   * Get character reference summary for metadata
   */
  getCharacterSummary(characterBible, scene, shot) {
    const anchors = this.extractCharacterAnchors(characterBible, null, null);
    const hints = this.getShotCharacterHints(shot, scene, anchors);

    return {
      totalCharacters: Object.keys(anchors).length,
      charactersInShot: hints.charactersInShot.length,
      characterNames: hints.charactersInShot.map(c => c.name),
      hasConsistencyRequirements: hints.consistencyRequirements.length > 0,
      anchorsWithImages: Object.values(anchors).filter(a => a.referenceImages.primary).length
    };
  },

  /**
   * Generate CONDENSED character hint for video prompt (max 100 chars)
   * Full enhancement stored separately as metadata
   */
  generateCondensedHint(scene, shot, characterBible) {
    const anchors = this.extractCharacterAnchors(characterBible, null, null);
    const hints = this.getShotCharacterHints(shot, scene, anchors);

    if (hints.charactersInShot.length === 0) return '';

    // Just character names and key visual feature
    const charSummary = hints.charactersInShot.slice(0, 2).map(c => {
      const anchor = anchors[c.name];
      const keyFeature = anchor?.physical?.hair || anchor?.physical?.build || '';
      return keyFeature ? `${c.name} (${keyFeature})` : c.name;
    }).join(', ');

    return `[Characters: ${charSummary}]`;
  }
};

// =============================================================================
// AUDIO_BEAT_ENGINE - Sound Design Mapping for AI Video
// =============================================================================
/**
 * AUDIO_BEAT_ENGINE
 *
 * Maps audio cues to video beats for enhanced AI video generation.
 * Based on 2025 AI filmmaking: Native audio generation is now expected.
 *
 * Modern AI video models (Veo 3, WAN 2.6, Seedance 1.5) generate:
 * - Synchronized dialogue with lip-sync
 * - Ambient sound effects
 * - Environmental audio
 * - Impact/action sounds
 *
 * This engine generates audio-aware prompt enhancements.
 */
const AUDIO_BEAT_ENGINE = {

  /**
   * Analyze scene for audio cues
   * @param {Object} scene - Scene data
   * @param {Object} shot - Shot data
   * @param {number} beatIndex - Current beat index (0-3 for 4-beat system)
   * @returns {Object} Audio mapping for this beat
   */
  analyzeAudioCues(scene, shot, beatIndex = 0) {
    const visualPrompt = scene?.visualPrompt || '';
    const sceneAction = scene?.sceneAction || '';
    const narration = scene?.narration || '';
    const combinedText = `${visualPrompt} ${sceneAction} ${narration}`.toLowerCase();

    return {
      // Dialogue/speech cues
      dialogue: this.detectDialogueCues(combinedText, narration),
      // Ambient sound environment
      ambience: this.detectAmbienceCues(combinedText),
      // Action/impact sounds
      actionSounds: this.detectActionSounds(combinedText),
      // Music/score suggestions
      musicCues: this.detectMusicCues(combinedText, scene),
      // Beat-specific timing
      beatTiming: this.getBeatAudioTiming(beatIndex),
      // Emotional audio atmosphere
      emotionalAudio: this.getEmotionalAudioAtmosphere(scene, shot)
    };
  },

  /**
   * Detect dialogue and speech cues
   */
  detectDialogueCues(text, narration) {
    const cues = {
      hasDialogue: false,
      dialogueType: null,
      lipSyncRequired: false,
      speechPatterns: [],
      voiceNotes: []
    };

    // Check for dialogue indicators
    if (/["'].+["']|says|speaks|whispers|shouts|calls|replies|asks|exclaims/i.test(text)) {
      cues.hasDialogue = true;
      cues.lipSyncRequired = true;

      // Dialogue type
      if (/whispers?|quiet|soft voice|murmur/i.test(text)) {
        cues.dialogueType = 'whisper';
        cues.voiceNotes.push('soft, intimate delivery, close-mic sound');
      } else if (/shouts?|screams?|yells?|calls out/i.test(text)) {
        cues.dialogueType = 'shout';
        cues.voiceNotes.push('loud, projected voice, slight echo possible');
      } else if (/chant|incant|spell/i.test(text)) {
        cues.dialogueType = 'chant';
        cues.voiceNotes.push('rhythmic, ceremonial delivery');
      } else {
        cues.dialogueType = 'normal';
        cues.voiceNotes.push('conversational tone, natural delivery');
      }
    }

    // Check for narration/voiceover
    if (narration && narration.length > 20) {
      cues.hasDialogue = true;
      if (!cues.dialogueType) {
        cues.dialogueType = 'voiceover';
        cues.voiceNotes.push('narrative voiceover, not synced to visible character');
      }
    }

    // Speech patterns for lip-sync hints
    if (cues.hasDialogue) {
      if (/question|asks|\?/i.test(text)) {
        cues.speechPatterns.push('questioning intonation, rising pitch');
      }
      if (/exclaims?|!|\bwow\b|\bno\b/i.test(text)) {
        cues.speechPatterns.push('emphatic delivery, wide mouth movements');
      }
      if (/pause|hesitat|trail/i.test(text)) {
        cues.speechPatterns.push('pauses and hesitations in speech');
      }
    }

    return cues;
  },

  /**
   * Detect ambient environment sounds
   */
  detectAmbienceCues(text) {
    const ambience = {
      primary: null,
      secondary: [],
      intensity: 'normal',
      layers: []
    };

    // Natural environments
    if (/forest|woods|jungle|trees/i.test(text)) {
      ambience.primary = 'forest';
      ambience.layers.push('bird calls', 'rustling leaves', 'distant wildlife');
    }
    if (/ocean|sea|beach|waves|shore/i.test(text)) {
      ambience.primary = 'ocean';
      ambience.layers.push('crashing waves', 'seabirds', 'wind over water');
    }
    if (/mountain|peak|cliff|heights/i.test(text)) {
      ambience.primary = 'mountain';
      ambience.layers.push('wind gusts', 'distant echoes', 'sparse wildlife');
    }
    if (/desert|sand|dune/i.test(text)) {
      ambience.primary = 'desert';
      ambience.layers.push('wind over sand', 'heat shimmer hum', 'sparse sounds');
    }
    if (/river|stream|waterfall|creek/i.test(text)) {
      ambience.secondary.push('flowing water');
      ambience.layers.push('water movement', 'splashing');
    }

    // Weather
    if (/rain|storm|thunder/i.test(text)) {
      ambience.secondary.push('rain');
      ambience.layers.push('rainfall', 'thunder rumbles', 'water dripping');
      ambience.intensity = 'high';
    }
    if (/wind|breeze|gust/i.test(text)) {
      ambience.secondary.push('wind');
      ambience.layers.push('wind howling or whistling');
    }
    if (/snow|blizzard|frost/i.test(text)) {
      ambience.secondary.push('cold');
      ambience.layers.push('muffled silence', 'crunching snow', 'icy wind');
    }

    // Urban/interior
    if (/city|urban|street|market/i.test(text)) {
      ambience.primary = 'urban';
      ambience.layers.push('crowd murmur', 'distant traffic', 'footsteps');
    }
    if (/temple|church|cathedral|shrine/i.test(text)) {
      ambience.primary = 'sacred';
      ambience.layers.push('echoing space', 'reverberant silence', 'distant chanting');
    }
    if (/cave|cavern|underground/i.test(text)) {
      ambience.primary = 'cave';
      ambience.layers.push('dripping water', 'echoing footsteps', 'distant rumbles');
    }
    if (/castle|palace|throne/i.test(text)) {
      ambience.primary = 'castle';
      ambience.layers.push('stone echo', 'distant voices', 'torches crackling');
    }
    if (/tavern|inn|bar/i.test(text)) {
      ambience.primary = 'tavern';
      ambience.layers.push('crowd chatter', 'clinking glasses', 'fireplace crackle');
    }

    // Time of day
    if (/night|midnight|dark/i.test(text)) {
      ambience.secondary.push('night');
      ambience.layers.push('crickets', 'owl hoots', 'nocturnal sounds');
    }
    if (/dawn|sunrise|morning/i.test(text)) {
      ambience.secondary.push('dawn');
      ambience.layers.push('birdsong', 'rooster crow', 'awakening sounds');
    }

    // Silence/tension
    if (/silent|quiet|still|eerie|tense/i.test(text)) {
      ambience.intensity = 'minimal';
      ambience.layers.push('oppressive silence', 'subtle tension');
    }

    return ambience;
  },

  /**
   * Detect action and impact sounds
   */
  detectActionSounds(text) {
    const sounds = [];

    // Combat sounds
    if (/sword|blade|slash|cut/i.test(text)) {
      sounds.push({
        type: 'weapon',
        sound: 'sword slash - metal singing through air, impact clang',
        timing: 'on action'
      });
    }
    if (/punch|hit|strike|blow/i.test(text)) {
      sounds.push({
        type: 'impact',
        sound: 'physical impact - thud, grunt, body reaction',
        timing: 'on contact'
      });
    }
    if (/kick|stomp/i.test(text)) {
      sounds.push({
        type: 'impact',
        sound: 'kick impact - whoosh, thud, target reaction',
        timing: 'on contact'
      });
    }
    if (/block|shield|parry/i.test(text)) {
      sounds.push({
        type: 'defense',
        sound: 'blocking impact - metal clang, wood thunk, grunt of effort',
        timing: 'on block'
      });
    }
    if (/arrow|bow|shot|projectile/i.test(text)) {
      sounds.push({
        type: 'projectile',
        sound: 'arrow flight - whoosh, thunk on impact',
        timing: 'release to impact'
      });
    }

    // Movement sounds
    if (/run|sprint|dash/i.test(text)) {
      sounds.push({
        type: 'movement',
        sound: 'running footsteps - rapid rhythm, breathing',
        timing: 'continuous'
      });
    }
    if (/walk|step|stride/i.test(text)) {
      sounds.push({
        type: 'movement',
        sound: 'walking footsteps - measured pace on surface',
        timing: 'rhythmic'
      });
    }
    if (/jump|leap|vault/i.test(text)) {
      sounds.push({
        type: 'movement',
        sound: 'jump - push off, air, landing impact',
        timing: 'takeoff to landing'
      });
    }
    if (/fall|drop|crash/i.test(text)) {
      sounds.push({
        type: 'impact',
        sound: 'falling impact - air rush, heavy landing, aftermath',
        timing: 'acceleration to impact'
      });
    }

    // Object sounds
    if (/door|gate/i.test(text)) {
      sounds.push({
        type: 'object',
        sound: 'door movement - creak, latch, closing thud',
        timing: 'on action'
      });
    }
    if (/break|shatter|crack|smash/i.test(text)) {
      sounds.push({
        type: 'destruction',
        sound: 'breaking - crack, shatter, debris scatter',
        timing: 'on impact'
      });
    }
    if (/explosion|blast|boom/i.test(text)) {
      sounds.push({
        type: 'explosion',
        sound: 'explosion - boom, debris, shockwave echo',
        timing: 'immediate then aftermath'
      });
    }

    // Magic/energy sounds
    if (/magic|spell|energy|power|glow/i.test(text)) {
      sounds.push({
        type: 'magic',
        sound: 'magical energy - ethereal hum, crackle, whoosh',
        timing: 'build to release'
      });
    }
    if (/fire|flame|burn/i.test(text)) {
      sounds.push({
        type: 'element',
        sound: 'fire - crackling, roaring, heat shimmer',
        timing: 'continuous with flare-ups'
      });
    }
    if (/lightning|electricity|spark/i.test(text)) {
      sounds.push({
        type: 'element',
        sound: 'electricity - crackle, zap, thunder',
        timing: 'sudden bursts'
      });
    }

    // Emotional sounds
    if (/cry|sob|weep/i.test(text)) {
      sounds.push({
        type: 'emotional',
        sound: 'crying - sobs, sniffles, unsteady breathing',
        timing: 'intermittent'
      });
    }
    if (/laugh|chuckle/i.test(text)) {
      sounds.push({
        type: 'emotional',
        sound: 'laughter - genuine, character-appropriate',
        timing: 'on emotion'
      });
    }
    if (/scream|shriek/i.test(text)) {
      sounds.push({
        type: 'emotional',
        sound: 'scream - intense vocalization, echo',
        timing: 'sudden'
      });
    }

    return sounds;
  },

  /**
   * Detect music/score cues
   */
  detectMusicCues(text, scene) {
    const music = {
      mood: null,
      tempo: null,
      intensity: null,
      instruments: [],
      notes: []
    };

    // Determine mood from scene content
    if (/battle|fight|combat|war|clash/i.test(text)) {
      music.mood = 'action';
      music.tempo = 'fast';
      music.intensity = 'high';
      music.instruments.push('percussion', 'brass', 'strings (intense)');
      music.notes.push('driving rhythm, building tension');
    } else if (/love|romance|tender|gentle|embrace/i.test(text)) {
      music.mood = 'romantic';
      music.tempo = 'slow';
      music.intensity = 'low';
      music.instruments.push('strings', 'piano', 'soft winds');
      music.notes.push('warm, emotional, intimate');
    } else if (/fear|terror|horror|creep|sinister/i.test(text)) {
      music.mood = 'tension';
      music.tempo = 'varied';
      music.intensity = 'building';
      music.instruments.push('low strings', 'dissonant tones', 'sparse percussion');
      music.notes.push('unsettling, building dread');
    } else if (/triumph|victory|hero|glory|success/i.test(text)) {
      music.mood = 'triumphant';
      music.tempo = 'moderate-fast';
      music.intensity = 'high';
      music.instruments.push('full orchestra', 'brass fanfare', 'choral');
      music.notes.push('soaring, uplifting, celebratory');
    } else if (/sad|grief|loss|mourn|tragedy/i.test(text)) {
      music.mood = 'melancholic';
      music.tempo = 'slow';
      music.intensity = 'low';
      music.instruments.push('solo strings', 'piano', 'minimal');
      music.notes.push('somber, reflective, emotional weight');
    } else if (/mystery|discover|reveal|secret/i.test(text)) {
      music.mood = 'mysterious';
      music.tempo = 'slow-moderate';
      music.intensity = 'medium';
      music.instruments.push('winds', 'sparse strings', 'ethereal pads');
      music.notes.push('curious, anticipatory, unfolding');
    } else if (/peace|calm|serene|tranquil/i.test(text)) {
      music.mood = 'peaceful';
      music.tempo = 'slow';
      music.intensity = 'minimal';
      music.instruments.push('ambient pads', 'nature sounds', 'soft melody');
      music.notes.push('restful, contemplative, still');
    }

    // Scene position affects music
    const sceneIndex = scene?.sceneIndex || 0;
    if (sceneIndex === 0) {
      music.notes.push('opening - establish world theme');
    }

    return music;
  },

  /**
   * Get beat-specific audio timing (for 4-beat system)
   */
  getBeatAudioTiming(beatIndex) {
    const beatTimings = [
      {
        beat: 1,
        timing: '0-2s',
        audioFocus: 'establish',
        description: 'Establish audio atmosphere, ambient foundation',
        notes: 'Fade in ambience, set sonic palette'
      },
      {
        beat: 2,
        timing: '2-5s',
        audioFocus: 'develop',
        description: 'Develop audio layers, introduce action sounds',
        notes: 'Add specific sounds, dialogue begins if present'
      },
      {
        beat: 3,
        timing: '5-8s',
        audioFocus: 'peak',
        description: 'Audio climax - loudest/most intense moment',
        notes: 'Impact sounds, emotional peaks, music swells'
      },
      {
        beat: 4,
        timing: '8-10s',
        audioFocus: 'resolve',
        description: 'Audio resolution - settle or transition',
        notes: 'Sounds fade or transform to next scene'
      }
    ];

    return beatTimings[beatIndex] || beatTimings[0];
  },

  /**
   * Get emotional audio atmosphere
   */
  getEmotionalAudioAtmosphere(scene, shot) {
    const choreography = scene?.choreography || {};
    const intensity = choreography.intensityProgression?.[0] || 0.5;
    const sceneArc = choreography.sceneArc || 'steady_build';

    let atmosphere = {
      intensity: 'moderate',
      dynamic: 'stable',
      suggestion: ''
    };

    if (intensity > 0.8) {
      atmosphere.intensity = 'intense';
      atmosphere.dynamic = 'dramatic peaks';
      atmosphere.suggestion = 'loud, impactful, emotionally charged audio';
    } else if (intensity > 0.6) {
      atmosphere.intensity = 'heightened';
      atmosphere.dynamic = 'building';
      atmosphere.suggestion = 'energetic, forward-moving audio';
    } else if (intensity > 0.4) {
      atmosphere.intensity = 'moderate';
      atmosphere.dynamic = 'steady';
      atmosphere.suggestion = 'balanced audio, clear and present';
    } else {
      atmosphere.intensity = 'subtle';
      atmosphere.dynamic = 'restrained';
      atmosphere.suggestion = 'quiet, intimate, careful audio';
    }

    // Arc-specific adjustments
    if (sceneArc === 'build_to_climax') {
      atmosphere.suggestion += ' - audio should crescendo';
    } else if (sceneArc === 'tension_release') {
      atmosphere.suggestion += ' - build tension then release';
    } else if (sceneArc === 'peak_then_calm') {
      atmosphere.suggestion += ' - start intense, gradually calm';
    }

    return atmosphere;
  },

  /**
   * Generate audio enhancement for video prompt
   * @param {Object} scene - Scene data
   * @param {Object} shot - Shot data
   * @param {number} beatIndex - Beat index for timing
   * @returns {string} Audio enhancement text for video prompt
   */
  generateAudioEnhancement(scene, shot, beatIndex = 0) {
    const audio = this.analyzeAudioCues(scene, shot, beatIndex);
    const lines = [];

    lines.push('[AUDIO DESIGN]');

    // Dialogue cues
    if (audio.dialogue.hasDialogue) {
      lines.push(`Dialogue: ${audio.dialogue.dialogueType} - ${audio.dialogue.voiceNotes.join(', ')}`);
      if (audio.dialogue.lipSyncRequired) {
        lines.push('Lip-sync: Required - match mouth movements to speech');
      }
    }

    // Ambience
    if (audio.ambience.primary) {
      lines.push(`Ambience: ${audio.ambience.primary} - ${audio.ambience.layers.slice(0, 3).join(', ')}`);
    }

    // Action sounds (most important for modern AI video)
    if (audio.actionSounds.length > 0) {
      const soundsStr = audio.actionSounds
        .slice(0, 3)
        .map(s => `${s.type}: ${s.sound}`)
        .join('; ');
      lines.push(`SFX: ${soundsStr}`);
    }

    // Music mood
    if (audio.musicCues.mood) {
      lines.push(`Music: ${audio.musicCues.mood} mood, ${audio.musicCues.tempo} tempo`);
    }

    // Beat timing
    lines.push(`Beat ${audio.beatTiming.beat} (${audio.beatTiming.timing}): ${audio.beatTiming.audioFocus} - ${audio.beatTiming.notes}`);

    return lines.join('\n');
  },

  /**
   * Get audio summary for metadata
   */
  getAudioSummary(scene, shot, beatIndex) {
    const audio = this.analyzeAudioCues(scene, shot, beatIndex);

    return {
      hasDialogue: audio.dialogue.hasDialogue,
      dialogueType: audio.dialogue.dialogueType,
      lipSyncRequired: audio.dialogue.lipSyncRequired,
      ambienceType: audio.ambience.primary,
      actionSoundCount: audio.actionSounds.length,
      musicMood: audio.musicCues.mood,
      emotionalIntensity: audio.emotionalAudio.intensity,
      beatPhase: audio.beatTiming.audioFocus
    };
  },

  /**
   * Generate CONDENSED audio hint for video prompt (max 80 chars)
   * Full enhancement stored separately as metadata
   */
  generateCondensedHint(scene, shot, beatIndex = 0) {
    const audio = this.analyzeAudioCues(scene, shot, beatIndex);
    const hints = [];

    // Most important: dialogue type if present
    if (audio.dialogue.hasDialogue) {
      hints.push(audio.dialogue.dialogueType);
    }

    // Mood/atmosphere
    if (audio.musicCues.mood && audio.musicCues.mood !== 'neutral') {
      hints.push(audio.musicCues.mood + ' mood');
    }

    // Ambience type
    if (audio.ambience.primary && audio.ambience.primary !== 'general') {
      hints.push(audio.ambience.primary);
    }

    return hints.length > 0 ? `[Audio: ${hints.join(', ')}]` : '';
  }
};

// =============================================================================
// BEAT_TIMELINE_ENGINE - Temporal Shot Choreography
// =============================================================================
/**
 * BEAT_TIMELINE_ENGINE
 *
 * Creates precise TEMPORAL choreography for each shot - the key differentiator
 * between amateur and Hollywood-quality video generation.
 *
 * Instead of just describing WHAT is in the shot, this engine describes
 * WHEN things happen during the shot's duration.
 *
 * Uses the 4-beat system:
 * - ESTABLISH (0-2s): Set the stage, initial state
 * - DEVELOP (2-5s): Action begins, momentum builds
 * - ESCALATE (5-8s): Peak action/emotion, highest intensity
 * - RESOLVE (8-10s): Landing, transition setup, breath
 *
 * Synthesizes data from:
 * - Script narrative beat (what story moment)
 * - Physics engine (how materials move)
 * - Character engine (who and emotional state)
 * - Audio engine (intensity curve)
 */
const BEAT_TIMELINE_ENGINE = {

  // Beat phases with timing
  BEAT_PHASES: {
    ESTABLISH: { start: 0, end: 2, purpose: 'Set the stage' },
    DEVELOP: { start: 2, end: 5, purpose: 'Build momentum' },
    ESCALATE: { start: 5, end: 8, purpose: 'Peak intensity' },
    RESOLVE: { start: 8, end: 10, purpose: 'Land and transition' }
  },

  // Action verbs for different intensities
  ACTION_VERBS: {
    low: ['rests', 'waits', 'observes', 'breathes', 'stands', 'settles'],
    medium: ['moves', 'turns', 'reaches', 'walks', 'speaks', 'gestures'],
    high: ['rushes', 'strikes', 'leaps', 'spins', 'clashes', 'charges'],
    transitional: ['shifts', 'pauses', 'steadies', 'prepares', 'focuses']
  },

  // Material behavior templates (physics integration)
  MATERIAL_BEHAVIORS: {
    fabric: {
      ESTABLISH: 'fabric settles into place',
      DEVELOP: 'fabric begins to sway',
      ESCALATE: 'fabric billows dramatically',
      RESOLVE: 'fabric flows with momentum'
    },
    hair: {
      ESTABLISH: 'hair rests naturally',
      DEVELOP: 'strands catch movement',
      ESCALATE: 'hair sweeps with motion',
      RESOLVE: 'hair settles softly'
    },
    environment: {
      ESTABLISH: 'atmosphere hangs still',
      DEVELOP: 'particles drift gently',
      ESCALATE: 'elements swirl intensely',
      RESOLVE: 'environment calms'
    }
  },

  // Emotional progression patterns
  EMOTION_PROGRESSIONS: {
    tension_build: ['watchful stillness', 'growing unease', 'mounting tension', 'coiled readiness'],
    revelation: ['unknowing calm', 'dawning realization', 'full recognition', 'emotional impact'],
    action_peak: ['poised stance', 'explosive motion', 'full commitment', 'follow-through'],
    quiet_moment: ['peaceful state', 'subtle shift', 'gentle movement', 'serene resolution'],
    confrontation: ['tense standoff', 'first move', 'engaged clash', 'aftermath']
  },

  /**
   * Generate beat timeline from shot data and all engine outputs
   * @param {Object} shot - Shot data with narrative beat, action, etc.
   * @param {Object} scene - Scene data for context
   * @param {Object} physicsData - Output from CINEMATIC_PHYSICS_ENGINE
   * @param {Object} characterData - Output from CHARACTER_REFERENCE_ENGINE
   * @param {Object} audioData - Output from AUDIO_BEAT_ENGINE
   * @returns {Object} Timeline with beat descriptions and metadata
   */
  generateTimeline(shot, scene, physicsData = {}, characterData = {}, audioData = {}) {
    // Extract narrative context
    const narrativeBeat = shot.narrativeBeat || {};
    const action = narrativeBeat.action || shot.action || scene.action || '';
    const emotion = narrativeBeat.emotion || shot.emotion || scene.emotion || 'neutral';
    const intensity = narrativeBeat.intensity || shot.intensity || 0.5;

    // Determine emotion progression pattern
    const progressionType = this._detectProgressionType(action, emotion, intensity);
    const emotionProgression = this.EMOTION_PROGRESSIONS[progressionType] || this.EMOTION_PROGRESSIONS.quiet_moment;

    // Get primary character
    const primaryCharacter = characterData.characterNames?.[0] ||
                              this._extractCharacterFromText(shot.prompt || scene.description || '') ||
                              'the figure';

    // Get primary material for physics
    const primaryMaterial = this._detectPrimaryMaterial(physicsData, shot, scene);

    // Get camera movement
    const cameraMovement = shot.cameraMovement || 'static';

    // Build each beat
    const timeline = {
      duration: shot.duration || 10,
      beats: {
        ESTABLISH: this._buildBeat('ESTABLISH', primaryCharacter, emotionProgression[0], primaryMaterial, cameraMovement, intensity * 0.4),
        DEVELOP: this._buildBeat('DEVELOP', primaryCharacter, emotionProgression[1], primaryMaterial, cameraMovement, intensity * 0.6),
        ESCALATE: this._buildBeat('ESCALATE', primaryCharacter, emotionProgression[2], primaryMaterial, cameraMovement, intensity),
        RESOLVE: this._buildBeat('RESOLVE', primaryCharacter, emotionProgression[3], primaryMaterial, cameraMovement, intensity * 0.7)
      },
      metadata: {
        progressionType,
        primaryCharacter,
        primaryMaterial,
        cameraMovement,
        peakIntensity: intensity
      }
    };

    return timeline;
  },

  /**
   * Build a single beat description
   */
  _buildBeat(beatName, character, emotionState, material, camera, intensity) {
    const phase = this.BEAT_PHASES[beatName];
    const materialBehavior = this.MATERIAL_BEHAVIORS[material]?.[beatName] || '';

    // Select action verb based on intensity
    const verbCategory = intensity > 0.7 ? 'high' : intensity > 0.4 ? 'medium' : 'low';
    const verbs = this.ACTION_VERBS[beatName === 'RESOLVE' ? 'transitional' : verbCategory];
    const verb = verbs[Math.floor(Math.random() * verbs.length)];

    return {
      timing: `${phase.start}-${phase.end}s`,
      purpose: phase.purpose,
      description: `${character} ${verb}, ${emotionState}`,
      material: materialBehavior,
      intensity: Math.round(intensity * 100)
    };
  },

  /**
   * Detect progression type from action and emotion
   */
  _detectProgressionType(action, emotion, intensity) {
    const a = action.toLowerCase();
    const e = emotion.toLowerCase();

    if (/fight|battle|strike|attack|defend|clash/.test(a)) return 'action_peak';
    if (/realize|discover|understand|reveal|see|notice/.test(a)) return 'revelation';
    if (/confront|face|challenge|stand.*against/.test(a)) return 'confrontation';
    if (/wait|watch|observe|prepare|tense/.test(a) || e === 'tense') return 'tension_build';
    if (intensity > 0.7) return 'action_peak';
    if (intensity < 0.3) return 'quiet_moment';
    return 'tension_build';
  },

  /**
   * Detect primary material from physics data
   */
  _detectPrimaryMaterial(physicsData, shot, scene) {
    const text = (shot.prompt || '') + (scene.description || '');

    if (/cape|cloak|dress|robe|coat|fabric|cloth/i.test(text)) return 'fabric';
    if (/hair|strand|locks/i.test(text)) return 'hair';
    return 'environment';
  },

  /**
   * Extract character name from text
   */
  _extractCharacterFromText(text) {
    // Look for capitalized names
    const nameMatch = text.match(/\b([A-Z][a-z]+)\b(?:\s+[a-z]+\s+|,|\s+(?:stands|walks|moves|looks|turns))/);
    if (nameMatch) return nameMatch[1];

    // Look for "the [role]" patterns
    const roleMatch = text.match(/the\s+(hero|warrior|woman|man|figure|protagonist|character)/i);
    if (roleMatch) return 'the ' + roleMatch[1].toLowerCase();

    return null;
  },

  /**
   * Generate the timeline prompt text for video generation
   * This is the MAIN output - a structured temporal description
   * @returns {string} ~350-400 char timeline description
   */
  generateTimelinePrompt(shot, scene, physicsData = {}, characterData = {}, audioData = {}) {
    const timeline = this.generateTimeline(shot, scene, physicsData, characterData, audioData);

    const lines = [`[BEAT TIMELINE - ${timeline.duration}s]`];

    for (const [beatName, beat] of Object.entries(timeline.beats)) {
      let line = `${beat.timing} ${beatName}: ${beat.description}`;
      if (beat.material) {
        line += `, ${beat.material}`;
      }
      lines.push(line);
    }

    return lines.join('\n');
  },

  /**
   * Generate condensed timeline hint (for when space is limited)
   * @returns {string} ~150 char condensed timeline
   */
  generateCondensedHint(shot, scene, physicsData = {}, characterData = {}, audioData = {}) {
    const timeline = this.generateTimeline(shot, scene, physicsData, characterData, audioData);

    // Just the key moments
    const establish = timeline.beats.ESTABLISH.description.split(',')[0];
    const escalate = timeline.beats.ESCALATE.description.split(',')[0];
    const resolve = timeline.beats.RESOLVE.description.split(',')[0];

    return `[Timeline: 0-2s ${establish} â†’ 5-8s ${escalate} â†’ 8-10s ${resolve}]`;
  }
};

// =============================================================================
// SHOT_SEQUENCE_VALIDATOR - Cinematographic Progression Validation
// =============================================================================
/**
 * SHOT_SEQUENCE_VALIDATOR
 *
 * Validates that shot sequences follow cinematographic best practices.
 * Ensures camera progressions are logical and visually coherent.
 *
 * Validates:
 * - Shot type progression (wide â†’ medium â†’ close-up pattern)
 * - Camera movement variety
 * - Transition logic between shots
 * - Visual storytelling flow
 */
const SHOT_SEQUENCE_VALIDATOR = {

  // Valid shot type progressions (cinematographic rules)
  VALID_PROGRESSIONS: {
    'establishing': ['wide', 'medium_wide', 'medium'],
    'wide': ['medium_wide', 'medium', 'establishing'],
    'medium_wide': ['medium', 'close_up', 'wide'],
    'medium': ['close_up', 'medium_wide', 'extreme_close_up'],
    'close_up': ['extreme_close_up', 'medium', 'cutaway'],
    'extreme_close_up': ['close_up', 'medium', 'wide'],
    'cutaway': ['medium', 'close_up', 'wide'],
    'over_shoulder': ['close_up', 'medium', 'over_shoulder'],
    'two_shot': ['close_up', 'medium', 'over_shoulder'],
    'pov': ['close_up', 'medium', 'reaction']
  },

  // Camera movement variety rules
  MOVEMENT_CATEGORIES: {
    static: ['static', 'locked', 'tripod'],
    pan: ['pan_left', 'pan_right', 'pan'],
    tilt: ['tilt_up', 'tilt_down', 'tilt'],
    dolly: ['dolly_in', 'dolly_out', 'push', 'pull'],
    track: ['track_left', 'track_right', 'tracking'],
    crane: ['crane_up', 'crane_down', 'crane', 'jib'],
    handheld: ['handheld', 'shaky'],
    steadicam: ['steadicam', 'gimbal', 'smooth'],
    drone: ['drone', 'aerial']
  },

  /**
   * Validate a sequence of shots
   * @param {Array} shots - Array of shot objects
   * @returns {Object} Validation result with issues and suggestions
   */
  validateSequence(shots) {
    if (!shots || shots.length === 0) {
      return {
        valid: true,
        score: 100,
        issues: [],
        suggestions: []
      };
    }

    const issues = [];
    const suggestions = [];
    let score = 100;

    // Run all validation checks
    const progressionResult = this.validateShotProgression(shots);
    issues.push(...progressionResult.issues);
    suggestions.push(...progressionResult.suggestions);
    score -= progressionResult.penalty;

    const movementResult = this.validateMovementVariety(shots);
    issues.push(...movementResult.issues);
    suggestions.push(...movementResult.suggestions);
    score -= movementResult.penalty;

    const transitionResult = this.validateTransitions(shots);
    issues.push(...transitionResult.issues);
    suggestions.push(...transitionResult.suggestions);
    score -= transitionResult.penalty;

    const flowResult = this.validateVisualFlow(shots);
    issues.push(...flowResult.issues);
    suggestions.push(...flowResult.suggestions);
    score -= flowResult.penalty;

    return {
      valid: score >= 70,
      score: Math.max(0, score),
      issues,
      suggestions,
      progressionValid: progressionResult.issues.length === 0,
      movementVariety: movementResult.variety,
      transitionsSmooth: transitionResult.issues.length === 0
    };
  },

  /**
   * Validate shot type progression
   */
  validateShotProgression(shots) {
    const issues = [];
    const suggestions = [];
    let penalty = 0;

    for (let i = 0; i < shots.length - 1; i++) {
      const current = this.normalizeShotType(shots[i].shotType);
      const next = this.normalizeShotType(shots[i + 1].shotType);

      // Check if progression is valid
      const validNextShots = this.VALID_PROGRESSIONS[current] || [];

      // Jump detection (e.g., establishing directly to extreme close-up)
      if (this.isJumpCut(current, next)) {
        issues.push({
          type: 'jump_cut',
          location: `Shot ${i + 1} â†’ ${i + 2}`,
          message: `Jump cut detected: ${current} â†’ ${next}. Consider adding intermediate shot.`
        });
        penalty += 5;
        suggestions.push(`Insert a ${this.suggestIntermediateShot(current, next)} between shots ${i + 1} and ${i + 2}`);
      }
    }

    // Check for shot type variety
    const shotTypes = shots.map(s => this.normalizeShotType(s.shotType));
    const uniqueTypes = new Set(shotTypes);
    if (uniqueTypes.size < Math.min(3, shots.length)) {
      issues.push({
        type: 'low_variety',
        message: `Low shot type variety. Only ${uniqueTypes.size} unique types in ${shots.length} shots.`
      });
      penalty += 5;
      suggestions.push('Add more variety in shot types for visual interest');
    }

    return { issues, suggestions, penalty };
  },

  /**
   * Validate camera movement variety
   */
  validateMovementVariety(shots) {
    const issues = [];
    const suggestions = [];
    let penalty = 0;

    const movements = shots.map(s => this.normalizeMovement(s.cameraMovement));
    const categories = movements.map(m => this.categorizeMovement(m));

    // Check for consecutive same movements
    let sameMovementCount = 1;
    for (let i = 1; i < categories.length; i++) {
      if (categories[i] === categories[i - 1]) {
        sameMovementCount++;
        if (sameMovementCount >= 3) {
          issues.push({
            type: 'repetitive_movement',
            location: `Shots ${i - 1} to ${i + 1}`,
            message: `Repetitive camera movement: ${sameMovementCount} consecutive ${categories[i]} movements`
          });
          penalty += 3;
        }
      } else {
        sameMovementCount = 1;
      }
    }

    // Calculate movement variety
    const uniqueCategories = new Set(categories);
    const variety = (uniqueCategories.size / Object.keys(this.MOVEMENT_CATEGORIES).length) * 100;

    if (variety < 30 && shots.length >= 4) {
      suggestions.push('Consider adding more variety in camera movements (pan, dolly, crane, etc.)');
    }

    return { issues, suggestions, penalty, variety };
  },

  /**
   * Validate transitions between shots
   */
  validateTransitions(shots) {
    const issues = [];
    const suggestions = [];
    let penalty = 0;

    // Check for 180-degree rule violations (if we had spatial data)
    // Check for matching action (if action data available)

    for (let i = 0; i < shots.length - 1; i++) {
      const current = shots[i];
      const next = shots[i + 1];

      // Check for jarring transitions
      const currentIntensity = current.cinematicPhysics?.intensity || 0.5;
      const nextIntensity = next.cinematicPhysics?.intensity || 0.5;

      // Sudden intensity jump without buildup
      if (Math.abs(nextIntensity - currentIntensity) > 0.5) {
        suggestions.push(`Shot ${i + 1} â†’ ${i + 2}: Large intensity change (${(currentIntensity * 100).toFixed(0)}% â†’ ${(nextIntensity * 100).toFixed(0)}%). Consider smoother transition.`);
      }
    }

    return { issues, suggestions, penalty };
  },

  /**
   * Validate visual storytelling flow
   */
  validateVisualFlow(shots) {
    const issues = [];
    const suggestions = [];
    let penalty = 0;

    // Opening shot check
    if (shots.length > 0) {
      const firstShot = this.normalizeShotType(shots[0].shotType);
      if (!['establishing', 'wide', 'medium_wide'].includes(firstShot)) {
        suggestions.push(`Consider starting with an establishing or wide shot instead of ${firstShot}`);
      }
    }

    // Closing shot check
    if (shots.length > 1) {
      const lastShot = this.normalizeShotType(shots[shots.length - 1].shotType);
      // Good endings: close-up (emotional), medium (balanced), wide (context)
      if (lastShot === 'extreme_close_up') {
        suggestions.push('Consider pulling back slightly for the final shot to give context');
      }
    }

    // Pacing check (shot duration variance)
    const durations = shots.map(s => s.duration || 10);
    const avgDuration = durations.reduce((a, b) => a + b, 0) / durations.length;
    const variance = durations.reduce((sum, d) => sum + Math.pow(d - avgDuration, 2), 0) / durations.length;

    if (variance < 0.5 && shots.length >= 3) {
      suggestions.push('Consider varying shot durations for better pacing and rhythm');
    }

    return { issues, suggestions, penalty };
  },

  /**
   * Normalize shot type string
   */
  normalizeShotType(shotType) {
    if (!shotType) return 'medium';
    const type = shotType.toLowerCase().replace(/[\s-]/g, '_');

    if (type.includes('establish')) return 'establishing';
    if (type.includes('extreme') && type.includes('close')) return 'extreme_close_up';
    if (type.includes('close')) return 'close_up';
    if (type.includes('medium') && type.includes('wide')) return 'medium_wide';
    if (type.includes('medium')) return 'medium';
    if (type.includes('wide')) return 'wide';
    if (type.includes('over') && type.includes('shoulder')) return 'over_shoulder';
    if (type.includes('two')) return 'two_shot';
    if (type.includes('pov') || type.includes('point')) return 'pov';
    if (type.includes('cutaway')) return 'cutaway';

    return 'medium';
  },

  /**
   * Normalize camera movement
   */
  normalizeMovement(movement) {
    if (!movement) return 'static';
    return movement.toLowerCase().replace(/[\s-]/g, '_');
  },

  /**
   * Categorize movement type
   */
  categorizeMovement(movement) {
    const m = movement.toLowerCase();
    for (const [category, keywords] of Object.entries(this.MOVEMENT_CATEGORIES)) {
      if (keywords.some(k => m.includes(k))) {
        return category;
      }
    }
    return 'static';
  },

  /**
   * Check if this is a jarring jump cut
   */
  isJumpCut(current, next) {
    const jumpPairs = [
      ['establishing', 'extreme_close_up'],
      ['establishing', 'close_up'],
      ['wide', 'extreme_close_up'],
      ['extreme_close_up', 'establishing'],
      ['extreme_close_up', 'wide']
    ];

    return jumpPairs.some(([a, b]) =>
      (current === a && next === b) || (current === b && next === a)
    );
  },

  /**
   * Suggest intermediate shot for jump cuts
   */
  suggestIntermediateShot(from, to) {
    if (from === 'establishing' || from === 'wide') {
      return 'medium or medium_wide';
    }
    if (to === 'extreme_close_up') {
      return 'close_up';
    }
    return 'medium';
  },

  /**
   * Get sequence summary for metadata
   */
  getSequenceSummary(shots) {
    const validation = this.validateSequence(shots);

    return {
      valid: validation.valid,
      score: validation.score,
      issueCount: validation.issues.length,
      suggestionCount: validation.suggestions.length,
      progressionValid: validation.progressionValid,
      movementVariety: validation.movementVariety?.toFixed(1) + '%',
      transitionsSmooth: validation.transitionsSmooth
    };
  }
};

// =============================================================================
// KEYFRAME_QUALITY_ENGINE - Image Quality Validation for Video Generation
// =============================================================================
/**
 * KEYFRAME_QUALITY_ENGINE
 *
 * Validates keyframe images before video generation.
 * Based on 2025 AI filmmaking: "The cleaner the keyframe, the less
 * the video model needs to inventâ€”and the more stable your motion becomes."
 *
 * Validates:
 * - Subject clarity (faces unobscured, clear focus)
 * - Motion-readiness (pose suitable for animation)
 * - Technical quality (composition, lighting, framing)
 * - AI-specific requirements (avoid problematic elements)
 */
const KEYFRAME_QUALITY_ENGINE = {

  // Quality thresholds
  THRESHOLDS: {
    minSubjectVisibility: 0.3,  // At least 30% of frame
    maxCrowding: 5,             // Max characters before quality drops
    minContrast: 0.4,           // Minimum contrast for clear motion
    optimalAspectRatios: ['16:9', '4:3', '1:1', '9:16']
  },

  /**
   * Analyze image prompt for keyframe quality indicators
   * @param {Object} shot - Shot data with imagePrompt
   * @param {Object} scene - Scene context
   * @returns {Object} Quality assessment
   */
  analyzeKeyframeQuality(shot, scene) {
    const imagePrompt = shot?.imagePrompt || shot?.prompt || '';
    const videoPrompt = shot?.videoPrompt || '';
    const shotType = shot?.shotType || 'medium';

    return {
      subjectClarity: this.assessSubjectClarity(imagePrompt, shotType),
      motionReadiness: this.assessMotionReadiness(imagePrompt, videoPrompt),
      technicalQuality: this.assessTechnicalQuality(imagePrompt),
      aiCompatibility: this.assessAICompatibility(imagePrompt),
      recommendations: this.generateRecommendations(imagePrompt, shotType)
    };
  },

  /**
   * Assess subject clarity in the prompt
   */
  assessSubjectClarity(prompt, shotType) {
    const lowerPrompt = prompt.toLowerCase();
    const issues = [];
    let score = 100;

    // Face visibility issues
    if (/back\s+view|from\s+behind|rear\s+view/i.test(prompt)) {
      issues.push('Subject shown from behind - face not visible');
      score -= 15;
    }
    if (/obscured|hidden|covered\s+face|mask|silhouette/i.test(prompt)) {
      issues.push('Face may be obscured or hidden');
      score -= 20;
    }
    if (/crowd|group|many\s+people|multiple\s+characters/i.test(prompt)) {
      issues.push('Multiple subjects may reduce individual clarity');
      score -= 10;
    }

    // Focus issues
    if (/blur|blurry|out\s+of\s+focus|soft\s+focus/i.test(prompt)) {
      issues.push('Blur mentioned - may affect video generation');
      score -= 15;
    }
    if (/motion\s+blur/i.test(prompt)) {
      issues.push('Motion blur in keyframe can cause video artifacts');
      score -= 20;
    }

    // Good indicators
    if (/clear\s+view|facing\s+camera|front\s+view|visible\s+face/i.test(prompt)) {
      score += 10;
    }
    if (/sharp|crisp|detailed|high\s+definition/i.test(prompt)) {
      score += 5;
    }

    // Shot type specific
    if ((shotType.includes('close') || shotType.includes('medium')) &&
        !/(face|portrait|expression|eyes)/i.test(prompt)) {
      issues.push('Close/medium shot without clear face description');
      score -= 10;
    }

    return {
      score: Math.max(0, Math.min(100, score)),
      issues,
      suitable: score >= 70
    };
  },

  /**
   * Assess motion readiness of the keyframe
   */
  assessMotionReadiness(imagePrompt, videoPrompt) {
    const combined = `${imagePrompt} ${videoPrompt}`.toLowerCase();
    const issues = [];
    let score = 100;

    // Static poses that are hard to animate
    if (/perfectly\s+still|frozen|statue|completely\s+motionless/i.test(combined)) {
      issues.push('Extremely static pose - may create unnatural video motion');
      score -= 10;
    }

    // Problematic positions
    if (/lying\s+down|on\s+the\s+ground|prone|supine/i.test(combined)) {
      issues.push('Lying position can be difficult for video models');
      score -= 15;
    }
    if (/extreme\s+angle|dutch\s+angle|tilted/i.test(combined)) {
      issues.push('Extreme camera angle may complicate motion');
      score -= 5;
    }

    // Good motion indicators
    if (/ready\s+to|about\s+to|beginning\s+to|starting\s+to/i.test(combined)) {
      score += 10;
    }
    if (/dynamic|in\s+motion|moving|walking|running/i.test(combined)) {
      score += 5;
    }
    if (/natural\s+pose|relaxed\s+stance|balanced/i.test(combined)) {
      score += 5;
    }

    // Action potential
    const hasActionPotential = /arms|legs|hands|body|torso|movement/i.test(combined);
    if (!hasActionPotential) {
      issues.push('Limited motion description - add body/movement details');
      score -= 5;
    }

    return {
      score: Math.max(0, Math.min(100, score)),
      issues,
      motionReady: score >= 70
    };
  },

  /**
   * Assess technical quality indicators
   */
  assessTechnicalQuality(prompt) {
    const lowerPrompt = prompt.toLowerCase();
    const issues = [];
    let score = 100;

    // Lighting issues
    if (/harsh\s+shadows|extreme\s+contrast|backlit\s+only/i.test(prompt)) {
      issues.push('Challenging lighting may affect video generation');
      score -= 10;
    }
    if (/dark|underexposed|low\s+light|shadows/i.test(prompt) &&
        !/dramatic|cinematic|moody/i.test(prompt)) {
      issues.push('Low light conditions may reduce video quality');
      score -= 5;
    }

    // Composition issues
    if (/cropped|cut\s+off|edge\s+of\s+frame/i.test(prompt)) {
      issues.push('Subject cropping may cause issues in video');
      score -= 10;
    }

    // Quality indicators (positive)
    if (/cinematic|professional|high\s+quality|4K|8K/i.test(prompt)) {
      score += 10;
    }
    if (/well\s+lit|balanced\s+lighting|natural\s+light/i.test(prompt)) {
      score += 5;
    }
    if (/sharp\s+focus|detailed|crisp/i.test(prompt)) {
      score += 5;
    }

    return {
      score: Math.max(0, Math.min(100, score)),
      issues,
      technicallySound: score >= 70
    };
  },

  /**
   * Assess AI video generation compatibility
   */
  assessAICompatibility(prompt) {
    const lowerPrompt = prompt.toLowerCase();
    const issues = [];
    const warnings = [];
    let score = 100;

    // Known problematic elements for AI video
    if (/text|words|letters|writing|sign\s+with|banner\s+saying/i.test(prompt)) {
      issues.push('Text in image can cause artifacts in video');
      score -= 20;
    }
    if (/mirror|reflection|glass/i.test(prompt)) {
      warnings.push('Reflections can be challenging for AI video');
      score -= 5;
    }
    if (/water\s+surface|transparent|translucent/i.test(prompt)) {
      warnings.push('Transparent/water surfaces may have artifacts');
      score -= 5;
    }
    if (/fingers|hands\s+close|detailed\s+hands/i.test(prompt)) {
      warnings.push('Hand details can be problematic in AI video');
      score -= 5;
    }
    if (/crowd|many\s+faces|group\s+of\s+people/i.test(prompt)) {
      warnings.push('Multiple faces increase consistency challenges');
      score -= 10;
    }

    // Positive AI-friendly elements
    if (/simple\s+background|clean\s+composition|uncluttered/i.test(prompt)) {
      score += 5;
    }
    if (/single\s+subject|one\s+person|lone\s+figure/i.test(prompt)) {
      score += 5;
    }

    return {
      score: Math.max(0, Math.min(100, score)),
      issues,
      warnings,
      aiCompatible: score >= 70
    };
  },

  /**
   * Generate recommendations for improving keyframe quality
   */
  generateRecommendations(prompt, shotType) {
    const recommendations = [];
    const lowerPrompt = prompt.toLowerCase();

    // Subject recommendations
    if (!/(face|portrait|expression|looking)/i.test(prompt)) {
      recommendations.push('Add clear face/expression description for character shots');
    }

    // Composition recommendations
    if (!/(centered|framing|composition|rule\s+of\s+thirds)/i.test(prompt)) {
      recommendations.push('Specify composition (centered, rule of thirds, etc.)');
    }

    // Lighting recommendations
    if (!/(light|lit|lighting|sun|shadow)/i.test(prompt)) {
      recommendations.push('Add lighting description for more controlled output');
    }

    // Quality recommendations
    if (!/(quality|resolution|4K|cinematic|professional)/i.test(prompt)) {
      recommendations.push('Add quality indicators (cinematic, 4K, professional)');
    }

    // Shot-type specific
    if (shotType.includes('close') && !/(expression|emotion|eyes|face)/i.test(prompt)) {
      recommendations.push('Close-up shots should emphasize facial expression/emotion');
    }
    if (shotType.includes('wide') && !/(environment|setting|location|landscape)/i.test(prompt)) {
      recommendations.push('Wide shots should describe the environment/setting');
    }

    return recommendations;
  },

  /**
   * Validate a shot's keyframe quality
   * @param {Object} shot - Shot data
   * @param {Object} scene - Scene context
   * @returns {Object} Validation result with score and recommendations
   */
  validateKeyframe(shot, scene) {
    const analysis = this.analyzeKeyframeQuality(shot, scene);

    // Calculate overall score
    const overallScore = Math.round(
      (analysis.subjectClarity.score * 0.3) +
      (analysis.motionReadiness.score * 0.3) +
      (analysis.technicalQuality.score * 0.2) +
      (analysis.aiCompatibility.score * 0.2)
    );

    // Collect all issues
    const allIssues = [
      ...analysis.subjectClarity.issues,
      ...analysis.motionReadiness.issues,
      ...analysis.technicalQuality.issues,
      ...analysis.aiCompatibility.issues
    ];

    return {
      valid: overallScore >= 70,
      score: overallScore,
      grade: this.scoreToGrade(overallScore),
      issues: allIssues,
      warnings: analysis.aiCompatibility.warnings || [],
      recommendations: analysis.recommendations,
      details: {
        subjectClarity: analysis.subjectClarity.score,
        motionReadiness: analysis.motionReadiness.score,
        technicalQuality: analysis.technicalQuality.score,
        aiCompatibility: analysis.aiCompatibility.score
      }
    };
  },

  /**
   * Convert score to letter grade
   */
  scoreToGrade(score) {
    if (score >= 90) return 'A';
    if (score >= 80) return 'B';
    if (score >= 70) return 'C';
    if (score >= 60) return 'D';
    return 'F';
  },

  /**
   * Get keyframe quality summary for metadata
   */
  getKeyframeSummary(shot, scene) {
    const validation = this.validateKeyframe(shot, scene);

    return {
      valid: validation.valid,
      score: validation.score,
      grade: validation.grade,
      issueCount: validation.issues.length,
      warningCount: validation.warnings.length,
      recommendationCount: validation.recommendations.length,
      subjectClarity: validation.details.subjectClarity,
      motionReadiness: validation.details.motionReadiness
    };
  }
};

/**
 * ENVIRONMENT_RESPONSE_SYSTEM
 * Environment reacts to character actions and emotions
 * Syncs atmospheric elements to story beats
 */
const ENVIRONMENT_RESPONSE_SYSTEM = {
  /**
   * Generate environment responses from cues
   * @param {Array} environmentalCues - Scene's environmentalCues array
   * @param {Array} beatBreakdown - Scene's beatBreakdown array
   * @returns {Array} Environment state per beat
   */
  generateEnvironmentResponses(environmentalCues, beatBreakdown) {
    return [1, 2, 3, 4].map(beatNum => {
      const beat = beatBreakdown?.find(b => b.beat === beatNum) || {};
      const cues = (environmentalCues || []).filter(c => c.beat === beatNum);

      return {
        beat: beatNum,
        baseEnvironment: beat.environmentState || this.getDefaultEnvironment(beatNum),
        reactiveElements: cues.map(cue => ({
          element: cue.element,
          state: cue.state,
          trigger: cue.syncedTo,
          description: `${cue.element} ${cue.state} (synced to: ${cue.syncedTo})`
        })),
        atmosphericMood: this.getAtmosphericMood(beat.intensity || 0.5)
      };
    });
  },

  /**
   * Get default environment based on beat position
   */
  getDefaultEnvironment(beatNum) {
    const defaults = {
      1: 'Ambient atmosphere established, lighting set, particles drift gently',
      2: 'Environment begins responding to action, subtle shifts in lighting',
      3: 'Peak environmental response - dramatic lighting, active particles',
      4: 'Environment settles, atmosphere resolves to new state'
    };
    return defaults[beatNum] || 'Standard atmospheric conditions';
  },

  /**
   * Get atmospheric mood from intensity
   */
  getAtmosphericMood(intensity) {
    if (intensity > 0.8) return 'charged, dramatic, elements at peak activity';
    if (intensity > 0.6) return 'building, responsive, heightened awareness';
    if (intensity > 0.4) return 'active, present, supporting the action';
    return 'calm, ambient, establishing mood';
  },

  /**
   * Generate continuity notes for environment
   */
  getContinuityNotes(environmentResponses) {
    const finalBeat = environmentResponses[3];
    return {
      exitState: finalBeat.baseEnvironment,
      mood: finalBeat.atmosphericMood,
      activeElements: finalBeat.reactiveElements.map(e => e.element),
      note: `Next shot should maintain: ${finalBeat.atmosphericMood}`
    };
  }
};

/**
 * CROSS_SHOT_CALLBACKS
 * Ensures visual continuity between shots
 * Tracks: position matches, gaze follow-through, object handoffs
 */
const CROSS_SHOT_CALLBACKS = {
  /**
   * Generate cross-shot continuity callbacks
   * @param {Object} currentScene - Current scene data
   * @param {Object} previousScene - Previous scene data (if any)
   * @param {Object} nextScene - Next scene data (if any)
   * @returns {Object} Continuity callbacks
   */
  generateCrossCallbacks(currentScene, previousScene, nextScene) {
    const callbacks = {
      fromPrevious: [],
      toNext: [],
      warnings: []
    };

    // Match positions from previous shot's end
    if (previousScene?.spatialBlocking?.finalPositions) {
      const prevPositions = previousScene.spatialBlocking.finalPositions;
      const currPositions = currentScene?.spatialBlocking?.initialPositions || {};

      Object.keys(prevPositions).forEach(char => {
        if (currPositions[char]) {
          callbacks.fromPrevious.push({
            type: 'position_match',
            character: char,
            expectedPosition: prevPositions[char],
            note: `${char} should enter matching exit from previous shot`
          });
        }
      });

      // Check for object handoffs
      if (previousScene.objectTracking) {
        previousScene.objectTracking.forEach(obj => {
          const lastState = obj.states?.[obj.states.length - 1];
          if (lastState) {
            callbacks.fromPrevious.push({
              type: 'object_continuity',
              object: obj.object,
              expectedState: lastState,
              note: `${obj.object} should match: ${lastState.visibility}, ${lastState.position}`
            });
          }
        });
      }
    }

    // Setup continuity for next shot
    if (nextScene) {
      const lastBeat = currentScene?.beatBreakdown?.[3] || {};
      callbacks.toNext.push({
        type: 'intensity_handoff',
        intensity: lastBeat.intensity || 0.5,
        note: `Next scene should acknowledge intensity level of ${((lastBeat.intensity || 0.5) * 100).toFixed(0)}%`
      });

      // Environment handoff
      if (currentScene?.environmentalCues) {
        const lastEnvCue = currentScene.environmentalCues.find(c => c.beat === 4);
        if (lastEnvCue) {
          callbacks.toNext.push({
            type: 'environment_continuity',
            element: lastEnvCue.element,
            state: lastEnvCue.state,
            note: `${lastEnvCue.element} should continue at ${lastEnvCue.state} state`
          });
        }
      }
    }

    // Check for potential continuity issues
    callbacks.warnings = this.checkContinuityIssues(currentScene, previousScene);

    return callbacks;
  },

  /**
   * Check for potential continuity issues
   */
  checkContinuityIssues(current, previous) {
    const warnings = [];

    if (previous && current) {
      // Check for sudden position jumps
      const prevFinal = previous.spatialBlocking?.finalPositions || {};
      const currInitial = current.spatialBlocking?.initialPositions || {};

      Object.keys(prevFinal).forEach(char => {
        if (currInitial[char]) {
          const prev = prevFinal[char];
          const curr = currInitial[char];
          if (prev.stage !== curr.stage && prev.depth !== curr.depth) {
            warnings.push({
              type: 'position_jump',
              character: char,
              message: `${char} jumps from ${prev.stage}/${prev.depth} to ${curr.stage}/${curr.depth}`
            });
          }
        }
      });
    }

    return warnings;
  },

  /**
   * Format callbacks for prompt inclusion
   */
  formatForPrompt(callbacks) {
    const lines = [];

    if (callbacks.fromPrevious.length > 0) {
      lines.push('[CONTINUITY FROM PREVIOUS SHOT]');
      callbacks.fromPrevious.forEach(cb => {
        lines.push(`- ${cb.note}`);
      });
      lines.push('');
    }

    if (callbacks.toNext.length > 0) {
      lines.push('[CONTINUITY TO NEXT SHOT]');
      callbacks.toNext.forEach(cb => {
        lines.push(`- ${cb.note}`);
      });
      lines.push('');
    }

    if (callbacks.warnings.length > 0) {
      lines.push('[CONTINUITY WARNINGS]');
      callbacks.warnings.forEach(w => {
        lines.push(`âš ï¸ ${w.message}`);
      });
      lines.push('');
    }

    return lines.join('\n');
  }
};

// =============================================================================
// OPENING_SCENE_HANDLER - Hollywood World-First Structure Enforcer
// Ensures opening scenes follow cinematic conventions: Environment â†’ Character
// =============================================================================

/**
 * OPENING_SCENE_HANDLER
 * Enforces Hollywood "World-First" structure for opening scenes.
 *
 * The Rule: In cinema, opening scenes establish the WORLD before the CHARACTER.
 * - Shot 1: Pure environment (no character visible)
 * - Shot 2: World details (character as tiny element if any)
 * - Shot 3: Character REVEALED (audience meets protagonist)
 * - Shot 4: Character context (what drives them, story hook)
 */
const OPENING_SCENE_HANDLER = {
  /**
   * Check if a scene is an opening scene
   */
  isOpeningScene(scene, sceneIndex) {
    if (sceneIndex === 0) return true;
    if (scene?.sceneType === 'opening_narration') return true;
    if (scene?.sceneRole === 'setup' && sceneIndex < 2) return true;
    return false;
  },

  /**
   * Get World-First shot structure for opening scenes
   * @param {number} shotCount - Number of shots to generate
   * @param {Object} scene - Scene data
   * @returns {Array} Shot structure with World-First requirements
   */
  getOpeningShotStructure(shotCount, scene) {
    const characters = scene?.charactersInScene || [];
    const protagonist = characters[0] || 'the protagonist';
    const location = scene?.location?.name || 'the world';

    // Define World-First shot roles based on shot count
    if (shotCount <= 2) {
      return [
        {
          role: 'WORLD_THEN_REVEAL',
          focus: 'environment_to_character',
          requirement: `Start with wide environment of ${location}, then reveal ${protagonist}`,
          characterVisibility: 'none_to_full',
          cameraNote: 'Sweeping/crane movement, environment dominant then finds character'
        },
        {
          role: 'CHARACTER_CONTEXT',
          focus: 'character_in_world',
          requirement: `${protagonist} established in their world, hint at journey`,
          characterVisibility: 'full',
          cameraNote: 'Medium shot, character with world context visible'
        }
      ];
    }

    if (shotCount === 3) {
      return [
        {
          role: 'WORLD_ESTABLISHMENT',
          focus: 'environment_only',
          requirement: `Pure environment shot of ${location}. NO CHARACTER VISIBLE. World is the subject.`,
          characterVisibility: 'none',
          cameraNote: 'Extreme wide/aerial, sweeping camera, pure world-building'
        },
        {
          role: 'CHARACTER_REVEAL',
          focus: 'character_discovered',
          requirement: `Camera discovers ${protagonist} in the environment. First glimpse of our hero.`,
          characterVisibility: 'revealed',
          cameraNote: 'Camera finds/lands on character, moment of discovery'
        },
        {
          role: 'CHARACTER_CONTEXT',
          focus: 'character_motivation',
          requirement: `${protagonist} in context - what draws them, hint at the journey ahead`,
          characterVisibility: 'full',
          cameraNote: 'Medium shot, establish character purpose and story hook'
        }
      ];
    }

    // 4+ shots - full Hollywood structure
    return [
      {
        role: 'WORLD_ESTABLISHMENT',
        focus: 'environment_only',
        requirement: `Pure environment shot of ${location}. NO CHARACTER VISIBLE. Establish the scale and uniqueness of this world.`,
        characterVisibility: 'none',
        cameraNote: 'Extreme wide/aerial/crane, sweeping movement, world is the star'
      },
      {
        role: 'WORLD_DETAIL',
        focus: 'environment_with_hint',
        requirement: `Continue exploring ${location}. Character may appear as TINY SILHOUETTE only - not the focus.`,
        characterVisibility: 'silhouette_only',
        cameraNote: 'Wide shot, descending/moving through world, 90% environment'
      },
      {
        role: 'CHARACTER_REVEAL',
        focus: 'character_discovered',
        requirement: `Camera discovers ${protagonist}. This is THE MOMENT the audience meets our hero. Make it count.`,
        characterVisibility: 'revealed',
        cameraNote: 'Camera arrives at character, reveal moment, medium-wide to medium'
      },
      {
        role: 'CHARACTER_CONTEXT',
        focus: 'character_hook',
        requirement: `${protagonist} in their world with purpose. Show what calls to them. Plant the story hook.`,
        characterVisibility: 'full',
        cameraNote: 'Medium shot, character looking toward destiny/goal, anticipation'
      }
    ].slice(0, shotCount);
  },

  /**
   * Apply World-First structure to shot prompts
   * @param {Array} shots - Array of shot objects
   * @param {Object} scene - Scene data
   * @param {number} sceneIndex - Scene index (0 = first scene)
   * @returns {Array} Enhanced shots with World-First structure
   */
  applyWorldFirstStructure(shots, scene, sceneIndex) {
    if (!this.isOpeningScene(scene, sceneIndex)) {
      return shots;
    }

    console.log('[OPENING_SCENE_HANDLER] Applying World-First structure to opening scene');

    const openingStructure = this.getOpeningShotStructure(shots.length, scene);
    const location = scene?.location?.name || scene?.visualPrompt?.match(/in (?:the |a )?([^,\.]+)/i)?.[1] || 'the world';
    const characters = scene?.charactersInScene || [];
    const protagonist = characters[0] || 'the protagonist';

    return shots.map((shot, idx) => {
      const structure = openingStructure[idx];
      if (!structure) return shot;

      // Build World-First enhanced prompt
      const worldFirstPrefix = this.buildWorldFirstPrefix(structure, location, protagonist, idx);
      const originalPrompt = shot.videoPrompt || shot.prompt || '';

      // Enhance the prompt with World-First requirements
      const enhancedPrompt = `[OPENING SCENE - ${structure.role}]
${structure.requirement}

Camera: ${structure.cameraNote}
Character Visibility: ${structure.characterVisibility}

${worldFirstPrefix}

${originalPrompt}`;

      return {
        ...shot,
        videoPrompt: enhancedPrompt,
        openingStructure: {
          role: structure.role,
          focus: structure.focus,
          characterVisibility: structure.characterVisibility,
          isWorldFirst: idx < 2
        }
      };
    });
  },

  /**
   * Build World-First prefix based on shot role
   */
  buildWorldFirstPrefix(structure, location, protagonist, shotIndex) {
    switch (structure.role) {
      case 'WORLD_ESTABLISHMENT':
        return `CRITICAL: This shot must show ONLY the environment. ${protagonist} should NOT be visible.
Focus entirely on ${location} - its scale, atmosphere, unique elements.
The audience needs to understand THIS WORLD before meeting the character.
Think: Opening of Blade Runner, Lord of the Rings, Avatar - world first, always.`;

      case 'WORLD_DETAIL':
        return `Continue building the world of ${location}.
If ${protagonist} appears, they should be a TINY figure - barely noticeable.
The environment remains 90% of the visual focus.
We're still in world-building mode.`;

      case 'WORLD_THEN_REVEAL':
        return `Start with environment, then REVEAL ${protagonist}.
First half: Pure ${location} establishment.
Second half: Camera discovers ${protagonist}.
The reveal should feel like a discovery, not a cut-to.`;

      case 'CHARACTER_REVEAL':
        return `THE REVEAL MOMENT: Camera discovers ${protagonist}.
This is when the audience MEETS our hero for the first time.
Make this moment visually memorable - good framing, good lighting.
${protagonist} should be doing something that defines them.`;

      case 'CHARACTER_CONTEXT':
        return `${protagonist} is now established. Show them in their world.
What calls to them? What's their purpose? Plant the story hook.
They might look toward something in the distance - their destiny.
End this shot with anticipation of the journey ahead.`;

      default:
        return '';
    }
  },

  /**
   * Validate that opening scene follows World-First rules
   */
  validateWorldFirst(shots, sceneIndex) {
    if (sceneIndex !== 0) return { valid: true, warnings: [] };

    const warnings = [];

    if (shots.length > 0) {
      const firstShot = shots[0];
      const prompt = (firstShot.videoPrompt || firstShot.prompt || '').toLowerCase();

      // Check if first shot mentions character prominently
      const characterPatterns = [
        /^(?!\[opening).*\b(stands?|walks?|runs?|looks?|watches?|sees?)\b/i,
        /\bclose-?up\b.*\bface\b/i,
        /\b(he|she|they)\s+(is|are)\s+/i
      ];

      for (const pattern of characterPatterns) {
        if (pattern.test(prompt) && !prompt.includes('no character') && !prompt.includes('environment')) {
          warnings.push({
            type: 'character_too_early',
            message: 'Shot 1 may focus on character too early. Opening should establish world first.',
            suggestion: 'Consider starting with pure environment shot before introducing character.'
          });
          break;
        }
      }
    }

    return {
      valid: warnings.length === 0,
      warnings
    };
  },

  /**
   * Get opening scene choreography beats
   */
  getOpeningChoreographyBeats(scene) {
    const characters = scene?.charactersInScene || [];
    const protagonist = characters[0] || 'the protagonist';
    const location = scene?.location?.name || 'the world';

    return [
      {
        beat: 1,
        timing: '0-2s',
        phase: 'WORLD_ONLY',
        action: `Aerial/wide view of ${location}. No character visible. Pure environment establishment.`,
        characterStates: {},
        environmentState: `${location} in full glory - scale, atmosphere, unique visual elements`,
        cameraNote: 'Sweeping crane/drone, extreme wide, world is the subject',
        intensity: 0.3
      },
      {
        beat: 2,
        timing: '2-5s',
        phase: 'WORLD_EXPLORE',
        action: `Camera moves through ${location}, revealing details. Tiny silhouette of ${protagonist} may be visible in distance.`,
        characterStates: {
          [protagonist]: {
            position: 'distant, barely visible if at all',
            gesture: 'N/A - too far',
            expression: 'N/A - too far',
            bodyLanguage: 'silhouette only'
          }
        },
        environmentState: 'World details: architecture, atmosphere, mood, unique elements',
        cameraNote: 'Descending/tracking through world, maintaining wide perspective',
        intensity: 0.5
      },
      {
        beat: 3,
        timing: '5-8s',
        phase: 'CHARACTER_REVEAL',
        action: `Camera discovers ${protagonist}. The reveal moment - audience meets our hero.`,
        characterStates: {
          [protagonist]: {
            position: 'center frame, properly revealed',
            gesture: 'doing something character-defining',
            expression: 'focused, in their element',
            bodyLanguage: 'confident, purposeful'
          }
        },
        environmentState: 'World frames character, provides context',
        cameraNote: 'Camera arrives at/finds character, medium-wide to medium',
        intensity: 0.7
      },
      {
        beat: 4,
        timing: '8-10s',
        phase: 'STORY_HOOK',
        action: `${protagonist} in context. They sense/see something that will drive the story. The hook is planted.`,
        characterStates: {
          [protagonist]: {
            position: 'medium shot, character dominant',
            gesture: 'looking toward destiny/goal',
            expression: 'anticipation, determination, or concern',
            bodyLanguage: 'ready for what comes next'
          }
        },
        environmentState: 'Something in the distance calls - the story hook visible',
        cameraNote: 'Hold on character with goal/destiny visible in frame',
        intensity: 0.6
      }
    ];
  }
};

// =============================================================================
// HOLLYWOOD_CHOREOGRAPHER - Master Orchestrator
// Combines all 6 engines into complete choreographed video prompts
// =============================================================================

/**
 * HOLLYWOOD_CHOREOGRAPHER
 * The master orchestrator that combines all choreography engines to produce
 * rich, detailed prompts for AI video generation.
 *
 * Input: Enriched scene data with choreography fields
 * Output: Complete 4-beat choreographed prompt ready for video generation
 */
const HOLLYWOOD_CHOREOGRAPHER = {
  /**
   * Generate complete Hollywood choreography from enriched scene data
   * @param {Object} enrichedScene - Scene with all choreography fields
   * @param {Object} previousScene - Previous scene for continuity (optional)
   * @param {Object} nextScene - Next scene for continuity (optional)
   * @param {string} shotType - Camera shot type for this scene
   * @param {Object} styleBible - Style bible for visual consistency
   * @returns {Object} Complete choreographed data with prompt
   */
  generateChoreography(enrichedScene, previousScene = null, nextScene = null, shotType = 'MEDIUM', styleBible = {}) {
    console.log('[HOLLYWOOD_CHOREOGRAPHER] Generating choreography for scene:', enrichedScene?.sceneNumber || 'unknown');

    // Check if scene has enriched choreography data
    const hasEnrichment = this.hasChoreographyData(enrichedScene);
    if (!hasEnrichment) {
      console.log('[HOLLYWOOD_CHOREOGRAPHER] Scene lacks choreography data, using fallback');
      return this.generateFallbackChoreography(enrichedScene, shotType, styleBible);
    }

    // Expand simplified choreography format if needed
    const expandedScene = enrichedScene.choreography
      ? this.expandSimplifiedChoreography(enrichedScene)
      : enrichedScene;

    console.log('[HOLLYWOOD_CHOREOGRAPHER] Using', expandedScene.choreography ? 'simplified' : 'expanded', 'choreography format');

    // Generate all component data from engines
    const beatTimeline = BEAT_TIMELINE_GENERATOR.generateBeatTimeline(expandedScene.beatBreakdown);
    const captureRecommendation = BEAT_TIMELINE_GENERATOR.getCaptureFrame(beatTimeline);

    const ensembleBlocking = ENSEMBLE_BLOCKING_SYSTEM.generateEnsembleBlocking(
      expandedScene.spatialBlocking,
      expandedScene.eyelines,
      expandedScene.beatBreakdown
    );

    const objectStates = OBJECT_STATE_MACHINE.generateObjectStates(expandedScene.objectTracking || []);

    const physicsLayer = PHYSICS_LAYER.generatePhysicsLayer(
      expandedScene.beatBreakdown,
      expandedScene.performanceBlueprint
    );

    const environmentResponses = ENVIRONMENT_RESPONSE_SYSTEM.generateEnvironmentResponses(
      expandedScene.environmentalCues || [],
      expandedScene.beatBreakdown
    );

    const crossCallbacks = CROSS_SHOT_CALLBACKS.generateCrossCallbacks(
      expandedScene,
      previousScene,
      nextScene
    );

    // Combine all data into choreography structure
    const choreographyData = {
      sceneInfo: {
        number: expandedScene.sceneNumber || expandedScene.id || 1,
        title: expandedScene.sceneTitle || `Scene ${expandedScene.sceneNumber || 1}`,
        shotType: shotType,
        sceneType: expandedScene.sceneType || 'dialogue',
        sceneArc: expandedScene.performanceBlueprint?.sceneArc || 'build_to_climax'
      },
      spatial: ensembleBlocking,
      beats: beatTimeline.map((beat, i) => ({
        ...beat,
        physics: physicsLayer[i]?.physics || {},
        characterPhysics: physicsLayer[i]?.characterPhysics || {},
        environment: environmentResponses[i] || {},
        objects: objectStates.map(obj => obj.transitions[i]).filter(Boolean),
        eyeline: ensembleBlocking.eyeContact.find(e => e.beat === beat.beat) || null
      })),
      continuity: crossCallbacks,
      capture: captureRecommendation,
      styleBible: styleBible
    };

    // Build the formatted prompt
    const prompt = this.buildChoreographedPrompt(choreographyData);

    return {
      choreographyData,
      prompt,
      captureFrame: captureRecommendation,
      hasEnrichment: true
    };
  },

  /**
   * Check if scene has choreography enrichment data
   * Supports both simplified format (choreography object) and expanded format
   */
  hasChoreographyData(scene) {
    if (!scene) return false;
    // Check for simplified choreography format (new)
    if (scene.choreography?.keyBeats?.length > 0 || scene.choreography?.sceneArc) {
      return true;
    }
    // Check for expanded format (legacy)
    return !!(
      scene.beatBreakdown?.length > 0 ||
      scene.spatialBlocking?.initialPositions ||
      scene.performanceBlueprint?.sceneArc ||
      scene.eyelines?.length > 0
    );
  },

  /**
   * Expand simplified choreography format into full format for processing
   */
  expandSimplifiedChoreography(scene) {
    const choreo = scene.choreography;
    if (!choreo) return scene;

    // Parse keyBeats into beatBreakdown
    const beatBreakdown = (choreo.keyBeats || []).map((beatText, i) => {
      // Extract action from "Beat X (timing): action" format
      const actionMatch = beatText.match(/Beat \d.*?:\s*(.+)$/i);
      const action = actionMatch ? actionMatch[1] : beatText;

      return {
        beat: i + 1,
        timing: ['0-2s', '2-5s', '5-8s', '8-10s'][i],
        action: action,
        characterStates: {},
        environmentState: '',
        cameraNote: '',
        intensity: choreo.intensityProgression?.[i] || [0.5, 0.65, 0.85, 0.7][i]
      };
    });

    // Ensure 4 beats
    while (beatBreakdown.length < 4) {
      const i = beatBreakdown.length;
      beatBreakdown.push({
        beat: i + 1,
        timing: ['0-2s', '2-5s', '5-8s', '8-10s'][i],
        action: 'Action continues',
        characterStates: {},
        environmentState: '',
        cameraNote: '',
        intensity: [0.5, 0.65, 0.85, 0.7][i]
      });
    }

    // Build expanded scene data
    return {
      ...scene,
      beatBreakdown: beatBreakdown,
      performanceBlueprint: {
        sceneArc: choreo.sceneArc || 'steady_build',
        overallIntensity: {
          start: choreo.intensityProgression?.[0] || 0.5,
          peak: Math.max(...(choreo.intensityProgression || [0.85])),
          end: choreo.intensityProgression?.[3] || 0.7
        },
        characterEnergy: {}
      },
      spatialBlocking: {
        initialPositions: {},
        finalPositions: {},
        formation: 'natural',
        formationChange: 'static',
        keyDistances: [],
        blockingDescription: choreo.blocking || ''
      },
      eyelines: [],
      eyeContactDescription: choreo.eyeContact || '',
      physicsNote: choreo.physicsNote || '',
      environmentSyncDescription: choreo.environmentSync || '',
      environmentalCues: []
    };
  },

  /**
   * Generate fallback choreography when enrichment data is missing
   */
  generateFallbackChoreography(scene, shotType, styleBible) {
    const sceneAction = scene?.sceneAction || scene?.action || '';
    const characters = scene?.charactersInScene || [];

    // Create basic beat structure from sceneAction
    const beats = this.inferBeatsFromAction(sceneAction, characters);

    const choreographyData = {
      sceneInfo: {
        number: scene?.sceneNumber || scene?.id || 1,
        title: scene?.sceneTitle || 'Scene',
        shotType: shotType,
        sceneType: scene?.sceneType || 'dialogue',
        sceneArc: 'steady_build'
      },
      spatial: ENSEMBLE_BLOCKING_SYSTEM.getDefaultBlocking(),
      beats: beats,
      continuity: { fromPrevious: [], toNext: [], warnings: [] },
      capture: { beat: 3, frame: 210, timing: '7.0s', action: 'Peak moment', reason: 'Default capture at beat 3' },
      styleBible: styleBible
    };

    return {
      choreographyData,
      prompt: this.buildChoreographedPrompt(choreographyData),
      captureFrame: choreographyData.capture,
      hasEnrichment: false
    };
  },

  /**
   * Infer basic beats from sceneAction text
   */
  inferBeatsFromAction(sceneAction, characters) {
    const sentences = sceneAction.split(/[.!?]+/).filter(s => s.trim().length > 10);
    const defaultPhysics = PHYSICS_LAYER.getDefaultPhysics();

    return [1, 2, 3, 4].map((beatNum, i) => {
      const sentence = sentences[i] || sentences[0] || 'Action continues';
      return {
        beat: beatNum,
        timing: ['0-2s', '2-5s', '5-8s', '8-10s'][i],
        phase: ['ESTABLISH', 'DEVELOP', 'ESCALATE', 'RESOLVE'][i],
        action: sentence.trim(),
        characterStates: {},
        physics: defaultPhysics[i]?.physics || {},
        characterPhysics: {},
        environment: {
          baseEnvironment: ENVIRONMENT_RESPONSE_SYSTEM.getDefaultEnvironment(beatNum),
          reactiveElements: [],
          atmosphericMood: ENVIRONMENT_RESPONSE_SYSTEM.getAtmosphericMood(i === 2 ? 0.85 : 0.5)
        },
        objects: [],
        eyeline: null,
        intensity: i === 2 ? 0.85 : i === 0 ? 0.5 : 0.65
      };
    });
  },

  /**
   * Build the complete choreographed prompt from data
   */
  buildChoreographedPrompt(data) {
    const lines = [];
    const { sceneInfo, spatial, beats, continuity, capture, styleBible } = data;

    // Header
    lines.push(`SHOT ${sceneInfo.number} | ${sceneInfo.shotType.toUpperCase()} | ${sceneInfo.title}`);
    lines.push(`Scene Type: ${sceneInfo.sceneType} | Arc: ${sceneInfo.sceneArc}`);
    lines.push('');

    // Style reference if available
    if (styleBible?.visualStyle) {
      lines.push(`[STYLE] ${styleBible.visualStyle}`);
      if (styleBible.colorGrade) lines.push(`[COLOR] ${styleBible.colorGrade}`);
      if (styleBible.lighting) lines.push(`[LIGHTING] ${styleBible.lighting}`);
      lines.push('');
    }

    // Spatial blocking
    lines.push('[SPATIAL BLOCKING]');
    if (Object.keys(spatial.characters).length > 0) {
      Object.entries(spatial.characters).forEach(([char, positions]) => {
        lines.push(`${char}: ${positions.startPosition}`);
        if (positions.movement !== 'static') {
          lines.push(`  Movement: ${positions.movement}`);
        }
      });
    }
    if (spatial.formations.length > 0) {
      lines.push(`Formation: ${spatial.formations.map(f => f.formation).join(' â†’ ')}`);
    }
    if (spatial.keyDistances.length > 0) {
      lines.push(`Distances: ${spatial.keyDistances.join(', ')}`);
    }
    lines.push('');

    // Each beat with full detail
    beats.forEach(beat => {
      lines.push(`[BEAT ${beat.beat}: ${beat.timing}] ${beat.phase}`);
      lines.push(`Action: ${beat.action}`);

      // Character states
      if (Object.keys(beat.characterStates).length > 0) {
        Object.entries(beat.characterStates).forEach(([char, state]) => {
          lines.push(`${char}:`);
          if (state.position) lines.push(`  Position: ${state.position}`);
          if (state.gesture) lines.push(`  Gesture: ${state.gesture}`);
          if (state.expression) lines.push(`  Expression: ${state.expression}`);
          if (state.bodyLanguage) lines.push(`  Body: ${state.bodyLanguage}`);
        });
      }

      // Character physics
      if (Object.keys(beat.characterPhysics || {}).length > 0) {
        Object.entries(beat.characterPhysics).forEach(([char, phys]) => {
          if (phys.posture) lines.push(`${char} Physics: ${phys.posture}, eyes ${phys.eyeActivity}`);
        });
      }

      // General physics
      if (beat.physics) {
        const phys = beat.physics;
        lines.push(`Physics: ${phys.breathing}, ${phys.muscleState}`);
        if (phys.tension) lines.push(`Tension: ${phys.tension}`);
      }

      // Eyeline
      if (beat.eyeline) {
        lines.push(`Eyeline: ${beat.eyeline.description}`);
      }

      // Environment
      if (beat.environment) {
        lines.push(`Environment: ${beat.environment.baseEnvironment}`);
        if (beat.environment.reactiveElements?.length > 0) {
          beat.environment.reactiveElements.forEach(elem => {
            lines.push(`  ${elem.description}`);
          });
        }
        lines.push(`Atmosphere: ${beat.environment.atmosphericMood}`);
      }

      // Objects
      if (beat.objects?.length > 0) {
        beat.objects.forEach(obj => {
          if (obj.description) {
            lines.push(`Object: ${obj.description}`);
          }
        });
      }

      lines.push(`Intensity: ${(beat.intensity * 100).toFixed(0)}%`);
      lines.push('');
    });

    // Continuity section
    if (continuity.fromPrevious?.length > 0 || continuity.toNext?.length > 0) {
      lines.push(CROSS_SHOT_CALLBACKS.formatForPrompt(continuity));
    }

    // Capture frame
    lines.push(`>>> CAPTURE FRAME: Beat ${capture.beat} at ${capture.timing} <<<`);
    lines.push(`>>> ${capture.action} <<<`);
    lines.push(`>>> ${capture.reason} <<<`);

    return lines.join('\n');
  },

  /**
   * Generate choreography for multiple shots from a single scene
   * Used when a scene is decomposed into multiple shots
   */
  generateMultiShotChoreography(enrichedScene, shotCount, previousScene = null, nextScene = null, styleBible = {}) {
    const shots = [];
    const shotTypes = this.getShotTypeSequence(enrichedScene.sceneType, shotCount);

    for (let i = 0; i < shotCount; i++) {
      // Create a variant of the scene data for each shot
      const shotScene = this.createShotVariant(enrichedScene, i, shotCount);

      // Determine context for continuity
      const prevContext = i === 0 ? previousScene : shots[i - 1]?.choreographyData;
      const nextContext = i === shotCount - 1 ? nextScene : null;

      const choreography = this.generateChoreography(
        shotScene,
        prevContext,
        nextContext,
        shotTypes[i],
        styleBible
      );

      choreography.shotIndex = i + 1;
      choreography.totalShots = shotCount;
      shots.push(choreography);
    }

    return shots;
  },

  /**
   * Get appropriate shot type sequence for scene
   */
  getShotTypeSequence(sceneType, shotCount) {
    const sequences = {
      dialogue: ['MEDIUM TWO-SHOT', 'CLOSE-UP', 'OVER-SHOULDER', 'MEDIUM'],
      action: ['WIDE', 'MEDIUM', 'CLOSE-UP', 'WIDE'],
      emotional: ['MEDIUM', 'CLOSE-UP', 'EXTREME CLOSE-UP', 'MEDIUM'],
      establishing: ['EXTREME WIDE', 'WIDE', 'MEDIUM', 'WIDE'],
      revelation: ['MEDIUM', 'PUSH-IN', 'CLOSE-UP', 'REACTION'],
      default: ['WIDE', 'MEDIUM', 'CLOSE-UP', 'MEDIUM']
    };

    const sequence = sequences[sceneType] || sequences.default;
    return Array.from({ length: shotCount }, (_, i) => sequence[i % sequence.length]);
  },

  /**
   * Create a variant of scene data focused on a specific shot
   */
  createShotVariant(scene, shotIndex, totalShots) {
    if (!scene.beatBreakdown || scene.beatBreakdown.length < 4) {
      return scene;
    }

    // Distribute beats across shots
    const beatsPerShot = Math.ceil(4 / totalShots);
    const startBeat = shotIndex * beatsPerShot;
    const endBeat = Math.min(startBeat + beatsPerShot, 4);

    // Extract relevant beats for this shot
    const shotBeats = scene.beatBreakdown.slice(startBeat, endBeat);

    // Renumber beats to be 1-4 within the shot
    const renumberedBeats = shotBeats.map((beat, i) => ({
      ...beat,
      beat: i + 1,
      timing: ['0-2s', '2-5s', '5-8s', '8-10s'][i]
    }));

    // Pad to 4 beats if needed
    while (renumberedBeats.length < 4) {
      const lastBeat = renumberedBeats[renumberedBeats.length - 1] || {};
      renumberedBeats.push({
        ...lastBeat,
        beat: renumberedBeats.length + 1,
        timing: ['0-2s', '2-5s', '5-8s', '8-10s'][renumberedBeats.length],
        action: 'Continuation of previous action'
      });
    }

    return {
      ...scene,
      beatBreakdown: renumberedBeats,
      sceneTitle: `${scene.sceneTitle || 'Scene'} - Shot ${shotIndex + 1}/${totalShots}`
    };
  },

  /**
   * Quick check to determine if choreography should be used
   */
  shouldUseChoreography(scene) {
    // Use choreography if scene has any enrichment data
    return this.hasChoreographyData(scene);
  }
};

// =============================================================================
// STORY_BEAT_DECOMPOSER - Frame-Chain Video Prompt Generation
// =============================================================================
/**
 * STORY_BEAT_DECOMPOSER - Decomposes sceneAction into connected shot beats
 *
 * The KEY insight: Each shot must have START â†’ ACTION â†’ END structure.
 * - END STATE of Shot N becomes START STATE of Shot N+1 (frame chain)
 * - Each shot describes 10 seconds of rich, dense action
 * - Prompts must guide Minimax to animate TOWARD the end state
 *
 * Example for sceneAction: "Kai surveys the city, turns to companions, they leap"
 * - Shot 1: START: Rooftop view â†’ ACTION: Kai surveys, turns, speaks â†’ END: Facing companions mid-gesture
 * - Shot 2: START: Kai facing companions â†’ ACTION: They respond, gather, prepare â†’ END: Three at rooftop edge
 * - Shot 3: START: Three at edge â†’ ACTION: Run, leap, descend â†’ END: Mid-air with city below
 */

/**
 * ACTION_LIBRARY (Upgrade 4)
 * Pre-built action templates for common scene types
 * Used by AI decomposition as reference examples for rich, cinematic action sequences
 */
const ACTION_LIBRARY = {
  // Scene type templates with example action sequences
  templates: {
    action: {
      name: 'Action Sequence',
      description: 'High-energy physical action, fights, chases, stunts',
      examples: [
        {
          scenario: 'Combat Confrontation',
          shots: [
            { action: 'Character takes defensive stance, eyes locked on opponent, muscles tensing as they circle', endState: 'Crouched in fighting stance, fists raised' },
            { action: 'Explosive first strike - lunge forward, punch/kick delivered with full body rotation', endState: 'Follow-through pose, extended limb, weight shifted forward' },
            { action: 'Counter-attack sequence - block, dodge, return strike with increasing intensity', endState: 'Both fighters locked in close combat, gripping each other' }
          ],
          captureNotes: 'Capture at peak extension moments or brief pauses between exchanges'
        },
        {
          scenario: 'Chase/Pursuit',
          shots: [
            { action: 'Character bursts into sprint, environment blurring past, dodging obstacles', endState: 'Mid-stride, leaning forward into run' },
            { action: 'Vaulting over obstacle, sliding under barrier, sharp direction change', endState: 'Landing from jump or emerging from slide' },
            { action: 'Close call - near miss, last-second dodge, desperate final push', endState: 'Reaching safety or catching target' }
          ],
          captureNotes: 'Capture at landing moments, direction changes, or obstacle clearance'
        },
        {
          scenario: 'Heroic Leap/Stunt',
          shots: [
            { action: 'Character surveys the gap, backs up for running start, determined expression', endState: 'At edge, coiled to jump, arms back' },
            { action: 'Explosive leap into the air, body extended, defying gravity', endState: 'Peak of jump, silhouetted against sky/background' },
            { action: 'Descent and landing - controlled fall, impact, recovery roll', endState: 'Landed in heroic pose, dust settling' }
          ],
          captureNotes: 'Capture at peak height or stable landing pose'
        }
      ]
    },
    dialogue: {
      name: 'Dialogue Scene',
      description: 'Conversation, negotiation, verbal confrontation',
      examples: [
        {
          scenario: 'Important Conversation',
          shots: [
            { action: 'Speaker approaches, settles into position, makes eye contact, begins speaking with hand gestures', endState: 'Mid-gesture, leaning in, engaged expression' },
            { action: 'Listener reacts - nodding, frowning, shifting weight, responding with body language', endState: 'Contemplative pose, hand on chin or crossed arms' },
            { action: 'Speaker concludes point with emphatic gesture, steps back, awaits response', endState: 'Arms open in question or statement pose' }
          ],
          captureNotes: 'Capture during pauses between dialogue beats, at gesture completion'
        },
        {
          scenario: 'Confrontation/Argument',
          shots: [
            { action: 'Characters face off, tension building, one steps forward with accusatory gesture', endState: 'Pointing finger, body leaning forward aggressively' },
            { action: 'Defensive response - stepping back, raising hands, shaking head in denial', endState: 'Defensive posture, palms out or arms crossed' },
            { action: 'Emotional peak - voice raised, dramatic gesture, turning away in frustration', endState: 'Turned away, shoulders tense, or storming off' }
          ],
          captureNotes: 'Capture at emotional peaks or moment before/after turn'
        }
      ]
    },
    emotional: {
      name: 'Emotional Moment',
      description: 'Deep feelings, grief, joy, love, revelation of emotion',
      examples: [
        {
          scenario: 'Grief/Loss',
          shots: [
            { action: 'Character receives news, face falls, body begins to crumble, hand reaches for support', endState: 'Collapsed against wall or into chair, head bowed' },
            { action: 'Tears flow, shoulders shake, hand covers face or reaches out to lost memory', endState: 'Head in hands, or reaching toward photograph/memento' },
            { action: 'Slow composure gathering, wiping eyes, taking deep breath, steeling themselves', endState: 'Looking up with wet eyes but determined expression' }
          ],
          captureNotes: 'Capture during stillness of grief or moment of composure'
        },
        {
          scenario: 'Joy/Celebration',
          shots: [
            { action: 'Realization dawns, smile spreads, eyes widen, body straightens with excitement', endState: 'Beaming smile, hands coming together or raising' },
            { action: 'Celebratory action - jumping, embracing others, pumping fist, spinning', endState: 'Arms raised in victory or mid-embrace' },
            { action: 'Sharing the moment - looking to loved ones, inclusive gesture, group celebration', endState: 'Connected with others, shared joy visible' }
          ],
          captureNotes: 'Capture at peak of joy or in embrace/connection moment'
        },
        {
          scenario: 'Tender/Romantic',
          shots: [
            { action: 'Characters draw closer, tentative touches, eyes meeting with meaning', endState: 'Faces close, hand on cheek or holding hands' },
            { action: 'The moment builds, breath held, leaning in, world fading away', endState: 'Foreheads touching or on verge of kiss' },
            { action: 'Connection made - embrace, kiss, or holding each other close', endState: 'Embracing, peaceful expressions, intimate closeness' }
          ],
          captureNotes: 'Capture in tender held moments, not during movement'
        }
      ]
    },
    establishing: {
      name: 'Establishing Scene',
      description: 'Setting the scene, arrival, location reveal, time of day',
      examples: [
        {
          scenario: 'Location Reveal',
          shots: [
            { action: 'Wide view of location, camera slowly revealing scope, ambient movement (birds, vehicles, people)', endState: 'Full location visible, establishing geography' },
            { action: 'Moving closer, picking out details, signs of life or atmosphere', endState: 'Medium view showing key location feature' }
          ],
          captureNotes: 'Capture when camera has settled on clean composition'
        },
        {
          scenario: 'Character Arrival',
          shots: [
            { action: 'Character approaches in distance, environment framing them, purpose in stride', endState: 'Character centered in frame, destination visible' },
            { action: 'Reaching destination, pausing to take in surroundings, orienting themselves', endState: 'Standing at threshold, looking at destination' },
            { action: 'First step into new space, crossing threshold, environment reacting to presence', endState: 'Just inside, surveying new space' }
          ],
          captureNotes: 'Capture at threshold moments or survey pauses'
        }
      ]
    },
    revelation: {
      name: 'Revelation/Discovery',
      description: 'Finding something, realizing truth, dramatic discovery',
      examples: [
        {
          scenario: 'Object Discovery',
          shots: [
            { action: 'Searching, scanning environment, attention caught by something, moving toward it', endState: 'Reaching toward or crouching near discovery' },
            { action: 'Picking up/examining object, turning it over, recognition dawning', endState: 'Holding object at eye level, studying it' },
            { action: 'Reaction to discovery - shock, understanding, determination to act', endState: 'Standing with purpose, object clutched, decision made' }
          ],
          captureNotes: 'Capture at moment of recognition or decision'
        },
        {
          scenario: 'Truth Revealed',
          shots: [
            { action: 'Evidence presented or witnessed, character processing information', endState: 'Frozen in realization, eyes fixed on proof' },
            { action: 'Mind racing - flashbacks implied, puzzle pieces connecting, understanding growing', endState: 'Eyes widening, mouth opening, body stiffening' },
            { action: 'Full realization hits - emotional response, decision forming, action initiated', endState: 'Transformed by truth, new resolve visible' }
          ],
          captureNotes: 'Capture at peak of realization or moment of transformation'
        }
      ]
    },
    contemplative: {
      name: 'Contemplative/Reflective',
      description: 'Quiet reflection, decision-making, solitary moments',
      examples: [
        {
          scenario: 'Solitary Reflection',
          shots: [
            { action: 'Character in thoughtful pose, gazing at vista/object, slight movements showing internal processing', endState: 'Still, gazing into distance, weight of thoughts visible' },
            { action: 'Small action reveals thought - touching memento, adjusting clothing, deep breath', endState: 'Settled after small action, more contemplative' }
          ],
          captureNotes: 'Capture in stillness, when character is most composed'
        },
        {
          scenario: 'Decision Making',
          shots: [
            { action: 'Weighing options - looking between choices, internal debate visible in expression', endState: 'Focused on one direction, consideration evident' },
            { action: 'Resolution forming - jaw sets, shoulders square, small nod of self-agreement', endState: 'Decided, body language committed' },
            { action: 'First move toward decision - turning, reaching, stepping with purpose', endState: 'In motion toward chosen path' }
          ],
          captureNotes: 'Capture at moment of decision or just before movement'
        }
      ]
    },
    montage: {
      name: 'Montage Sequence',
      description: 'Rapid progression, training, preparation, time passing',
      examples: [
        {
          scenario: 'Training/Preparation',
          shots: [
            { action: 'First attempt - struggling, imperfect form, determination despite difficulty', endState: 'Catching breath, minor failure visible' },
            { action: 'Improvement showing - faster, stronger, more precise movements', endState: 'Better pose, confidence growing' },
            { action: 'Mastery moment - perfect execution, power and grace combined', endState: 'Peak form, triumphant expression' }
          ],
          captureNotes: 'Capture at completion of each training beat'
        },
        {
          scenario: 'Assembly/Creation',
          shots: [
            { action: 'Gathering materials, laying out components, preparation visible', endState: 'Components arranged, ready to begin' },
            { action: 'Building/creating - hands working, focus intense, progress visible', endState: 'Mid-creation, work taking shape' },
            { action: 'Completion - final touch, stepping back, admiring work', endState: 'Creation complete, creator satisfied' }
          ],
          captureNotes: 'Capture at completion of each assembly stage'
        }
      ]
    }
  },

  /**
   * Get relevant examples for a scene type
   * @param {string} sceneType - The type of scene
   * @param {string} sceneAction - The actual scene action (for scenario matching)
   * @returns {object} Matching template with examples
   */
  getTemplateForType(sceneType, sceneAction = '') {
    const template = this.templates[sceneType] || this.templates.dialogue;

    // Try to find most relevant example based on sceneAction keywords
    if (sceneAction) {
      const actionLower = sceneAction.toLowerCase();
      const scoredExamples = template.examples.map(example => {
        const scenarioLower = example.scenario.toLowerCase();
        let score = 0;
        // Simple keyword matching
        if (actionLower.includes('fight') || actionLower.includes('combat')) score += scenarioLower.includes('combat') ? 10 : 0;
        if (actionLower.includes('chase') || actionLower.includes('run')) score += scenarioLower.includes('chase') ? 10 : 0;
        if (actionLower.includes('talk') || actionLower.includes('speak')) score += scenarioLower.includes('conversation') ? 10 : 0;
        if (actionLower.includes('argue') || actionLower.includes('confront')) score += scenarioLower.includes('confrontation') ? 10 : 0;
        if (actionLower.includes('cry') || actionLower.includes('grief')) score += scenarioLower.includes('grief') ? 10 : 0;
        if (actionLower.includes('joy') || actionLower.includes('celebrate')) score += scenarioLower.includes('joy') ? 10 : 0;
        if (actionLower.includes('love') || actionLower.includes('kiss')) score += scenarioLower.includes('romantic') ? 10 : 0;
        if (actionLower.includes('discover') || actionLower.includes('find')) score += scenarioLower.includes('discovery') ? 10 : 0;
        if (actionLower.includes('think') || actionLower.includes('decide')) score += scenarioLower.includes('decision') ? 10 : 0;
        if (actionLower.includes('train') || actionLower.includes('practice')) score += scenarioLower.includes('training') ? 10 : 0;
        if (actionLower.includes('arrive') || actionLower.includes('enter')) score += scenarioLower.includes('arrival') ? 10 : 0;
        if (actionLower.includes('leap') || actionLower.includes('jump')) score += scenarioLower.includes('leap') ? 10 : 0;
        return { example, score };
      });

      const bestMatch = scoredExamples.sort((a, b) => b.score - a.score)[0];
      if (bestMatch.score > 0) {
        return {
          ...template,
          bestExample: bestMatch.example,
          allExamples: template.examples
        };
      }
    }

    return {
      ...template,
      bestExample: template.examples[0],
      allExamples: template.examples
    };
  },

  /**
   * Format examples for AI prompt inclusion
   * @param {string} sceneType - The scene type
   * @param {string} sceneAction - The scene action for matching
   * @param {number} shotCount - Target number of shots
   * @returns {string} Formatted examples for prompt
   */
  formatForPrompt(sceneType, sceneAction, shotCount) {
    const template = this.getTemplateForType(sceneType, sceneAction);
    const example = template.bestExample;

    let formatted = `
ACTION LIBRARY REFERENCE (${template.name}):
Scenario: "${example.scenario}"
${example.captureNotes}

Example shot structure:`;

    example.shots.forEach((shot, idx) => {
      formatted += `
Shot ${idx + 1}: ${shot.action}
  â†’ ENDS WITH: ${shot.endState}`;
    });

    formatted += `

Adapt this structure to your scene's specific action. Match the energy and pacing of "${template.name}" scenes.`;

    return formatted;
  }
};

const STORY_BEAT_DECOMPOSER = {
  /**
   * AI-Powered Motion-Dense Decomposition
   * Creates rich 10-second motion sequences from scene action
   * Each shot describes CONTINUOUS MOVEMENT over time, not static frames
   * @param {string} sceneAction - Rich action sequence for the scene
   * @param {number} shotCount - Number of shots to create
   * @param {number} clipDuration - Duration per shot (6 or 10 seconds)
   * @param {object} sceneContext - { visualPrompt, narration, characters, sceneId, sceneIndex, totalScenes }
   * @param {object} openai - OpenAI client instance
   * @param {object} crossSceneContext - { previousSceneEndState, previousSceneId, nextSceneHint } for continuity
   * @returns {Promise<Array>} Array of motion-dense shot prompts
   */
  async decomposeWithAI(sceneAction, shotCount, clipDuration, sceneContext, openai, crossSceneContext = null) {
    // Validate sceneAction is rich enough for AI decomposition
    if (!sceneAction || sceneAction.trim().length < 50) {
      console.log('[STORY_BEAT_DECOMPOSER] sceneAction too brief, using rule-based fallback');
      return this.decomposeIntoShots(sceneAction, shotCount, sceneContext);
    }

    // Determine scene type for action library (Upgrade 4)
    const sceneType = this.detectSceneType(sceneAction);
    const actionLibraryReference = ACTION_LIBRARY.formatForPrompt(sceneType, sceneAction, shotCount);
    console.log(`[STORY_BEAT_DECOMPOSER] Scene type detected: ${sceneType}, using action library template`);

    // Build cross-scene continuity section if available
    const crossSceneSection = crossSceneContext?.previousSceneEndState ? `
CROSS-SCENE CONTINUITY:
This is Scene ${sceneContext.sceneIndex || '?'} of ${sceneContext.totalScenes || '?'}.
PREVIOUS SCENE ended with: "${crossSceneContext.previousSceneEndState}"
The FIRST SHOT must flow naturally from this ending.
` : '';

    // Extract characters and environment from visual context
    const visualContext = sceneContext.visualPrompt || '';
    const narration = sceneContext.narration || '';

    const prompt = `You are a cinematic AI video director creating MOTION-DENSE shot sequences for Minimax AI video generation.

CRITICAL: Each shot is ${clipDuration} SECONDS of continuous motion - NOT a static frame!
${clipDuration} seconds is substantial. Characters breathe, shift, gesture. Environment moves. Camera flows.

=== SCENE DATA ===
SCENE ACTION: "${sceneAction}"

VISUAL CONTEXT: ${visualContext.substring(0, 400)}

NARRATION: "${narration.substring(0, 300)}"
${crossSceneSection}
${actionLibraryReference}

=== YOUR TASK ===
Create ${shotCount} MOTION-DENSE video prompts. Each prompt describes ${clipDuration} seconds of CONTINUOUS ACTION.

=== MOTION-DENSE REQUIREMENTS ===

1. CHARACTER MOTION (required in every shot):
   - Breathing patterns (slow/quickening/deep)
   - Body shifts (weight transfer, posture changes)
   - Micro-gestures (fingers, shoulders, head tilts)
   - Facial progression (expressions evolving)
   - Eye movement (scanning, focusing, meeting)
   Example: "Kai breathes slowly, tension visible in shoulders. His eyes scan the room, brow furrowing with concentration."

2. ENVIRONMENT ANIMATION (required in every shot):
   - Light changes (flickering, pulsing, shifting)
   - Atmospheric motion (smoke drifting, rain falling, dust floating)
   - Tech elements (displays scrolling, holograms rotating)
   - Natural elements (flames dancing, water rippling, leaves rustling)
   Example: "Holographic displays pulse and rotate around them. Rain streaks down windows, city lights blurring."

3. CAMERA MOVEMENT (choose appropriate):
   - "Slow push in" - building intensity/focus
   - "Smooth tracking" - following action
   - "Gradual pull back" - revealing/concluding
   - "Static with subtle drift" - contemplative
   Include the camera direction at START of each prompt.

4. EMOTIONAL PROGRESSION (each shot has an arc):
   - Beginning state â†’ development â†’ end state
   - Tension builds OR releases within each shot
   Example: Shot moves from "concentration" through "realization" to "shock"

5. CONTINUITY PHRASES (shots 2+ must include):
   - "Continuing seamlessly from previous frame"
   - References to previous shot's end position
   - Smooth transitions in action and camera

=== OUTPUT FORMAT ===
Return ONLY valid JSON:
{
  "shots": [
    {
      "shotIndex": 0,
      "cameraMovement": "Slow push in",
      "motionPrompt": "Full motion-dense prompt for Minimax. Include camera, character actions, environment animation, quality keywords. ${clipDuration} seconds of continuous motion.",
      "characterActions": "Brief: what characters physically do",
      "environmentAnimation": "Brief: what moves in environment",
      "emotionalArc": "starting emotion â†’ ending emotion",
      "endState": "Exact pose/position for frame capture",
      "captureStability": "high/medium/low"
    }
  ],
  "sceneEndState": "Final state of last shot - passed to next scene",
  "chainLogic": "How the shots connect narratively"
}

=== EXAMPLE MOTION PROMPT ===
"Slow push in. Kai's eyes scan left to right across holographic data, brow furrowing with concentration. His breathing becomes visible, quickening slightly. Fingers tap against thigh in unconscious rhythm. Holographic displays rotate and pulse around him, data streams flowing. Rain streaks down windows in background, city lights blurring. His posture gradually stiffens as realization begins. Expression shifts from focus to dawning recognition. Cinematic, photorealistic, smooth fluid motion, natural lifelike movement."

=== QUALITY KEYWORDS (append to each prompt) ===
"Cinematic, photorealistic, smooth fluid motion, natural lifelike movement, seamless animation, professional cinematography quality."

Create ${shotCount} shots now:`;

    try {
      console.log(`[STORY_BEAT_DECOMPOSER] Using AI decomposition for ${shotCount} motion-dense shots`);

      const completion = await openai.chat.completions.create({
        model: 'gpt-4o-mini',
        messages: [
          {
            role: 'system',
            content: 'You are a cinematic AI video director. You create MOTION-DENSE shot sequences where every second is filled with character movement, environmental animation, and camera flow. Never describe static frames - describe continuous action over time. Always return valid JSON.'
          },
          { role: 'user', content: prompt }
        ],
        temperature: 0.7,
        max_tokens: 3000
      });

      const responseText = completion.choices[0].message.content.trim();

      // Parse JSON response
      let parsed;
      try {
        const jsonMatch = responseText.match(/```(?:json)?\s*([\s\S]*?)```/);
        const jsonStr = jsonMatch ? jsonMatch[1].trim() : responseText;
        parsed = JSON.parse(jsonStr);
      } catch (parseError) {
        console.error('[STORY_BEAT_DECOMPOSER] Failed to parse AI response, using fallback');
        return this.decomposeIntoShots(sceneAction, shotCount, sceneContext);
      }

      if (!parsed.shots || !Array.isArray(parsed.shots) || parsed.shots.length === 0) {
        console.error('[STORY_BEAT_DECOMPOSER] AI returned invalid structure, using fallback');
        return this.decomposeIntoShots(sceneAction, shotCount, sceneContext);
      }

      // Convert AI response to standard shot format with motion-dense prompts
      const shots = parsed.shots.map((shot, idx) => {
        const isFirst = idx === 0;
        const isLast = idx === parsed.shots.length - 1;

        // Build the final video prompt for Minimax
        let videoPrompt = shot.motionPrompt || '';

        // Ensure continuity phrase for shots after first
        if (!isFirst && !videoPrompt.toLowerCase().includes('continuing')) {
          videoPrompt = `Continuing seamlessly from previous frame. ${videoPrompt}`;
        }

        // Ensure quality keywords are present
        if (!videoPrompt.toLowerCase().includes('cinematic')) {
          videoPrompt += ' Cinematic, photorealistic, smooth fluid motion, natural lifelike movement.';
        }

        return {
          shotIndex: idx,
          isFirst,
          isLast,
          cameraMovement: shot.cameraMovement || 'Smooth tracking',
          videoPrompt,
          characterActions: shot.characterActions || '',
          environmentAnimation: shot.environmentAnimation || '',
          emotionalArc: shot.emotionalArc || '',
          endState: shot.endState || '',
          captureDescription: shot.endState,
          aiGenerated: true,
          motionDense: true,
          captureSuggestion: {
            timing: `${clipDuration - 2}-${clipDuration} seconds`,
            reason: 'End of motion sequence at stable pose',
            stability: shot.captureStability || 'medium',
            timingSeconds: { start: clipDuration - 2, end: clipDuration, recommended: clipDuration - 1 }
          },
          crossSceneTransition: isFirst && crossSceneContext?.previousSceneEndState ? {
            fromPreviousScene: true,
            previousSceneEndState: crossSceneContext.previousSceneEndState,
            transitionType: crossSceneContext.transitionType || 'cut'
          } : null
        };
      });

      const lastShot = parsed.shots[parsed.shots.length - 1];
      const sceneEndState = parsed.sceneEndState || lastShot?.endState;
      const suggestedNextSceneTransition = parsed.suggestedNextSceneTransition || 'cut';

      console.log(`[STORY_BEAT_DECOMPOSER] Motion-dense decomposition successful: ${shots.length} shots`);
      console.log(`[STORY_BEAT_DECOMPOSER] Chain logic: ${parsed.chainLogic || 'Not provided'}`);

      return {
        shots,
        sceneEndState,
        suggestedNextSceneTransition,
        chainLogic: parsed.chainLogic
      };

    } catch (error) {
      console.error('[STORY_BEAT_DECOMPOSER] AI decomposition failed:', error.message);
      return this.decomposeIntoShots(sceneAction, shotCount, sceneContext);
    }
  },

  /**
   * Build video prompt from AI-generated content (legacy support)
   */
  buildVideoPromptFromAI(startState, action, endState, meta) {
    const parts = [];

    if (meta.isFirst) {
      parts.push(`OPENING: ${startState}`);
    } else {
      parts.push(startState);
    }

    parts.push(`ACTION: ${action}`);
    parts.push(endState);

    // 4. Animation guidance
    const guidance = meta.isLast
      ? 'Animate to final resting pose, hold for conclusion.'
      : 'Animate smoothly toward capture point. End in stable, capturable position.';
    parts.push(guidance);

    return parts.join(' ');
  },

  /**
   * Parse capture timing string to get start/end seconds
   * @param {string} timingStr - e.g., "8-10 seconds" or "9-10 seconds"
   * @param {number} clipDuration - Shot duration (6 or 10)
   * @returns {object} { start, end, recommended }
   */
  parseCaptureTiming(timingStr, clipDuration) {
    if (!timingStr) {
      return { start: clipDuration - 2, end: clipDuration, recommended: clipDuration - 1 };
    }

    // Extract numbers from timing string
    const numbers = timingStr.match(/(\d+)/g);
    if (numbers && numbers.length >= 2) {
      const start = parseInt(numbers[0], 10);
      const end = parseInt(numbers[1], 10);
      return {
        start: Math.min(start, clipDuration),
        end: Math.min(end, clipDuration),
        recommended: Math.min(Math.round((start + end) / 2), clipDuration)
      };
    } else if (numbers && numbers.length === 1) {
      const time = parseInt(numbers[0], 10);
      return {
        start: Math.max(0, time - 1),
        end: Math.min(time, clipDuration),
        recommended: Math.min(time, clipDuration)
      };
    }

    // Default to last 2 seconds
    return { start: clipDuration - 2, end: clipDuration, recommended: clipDuration - 1 };
  },

  /**
   * Extract key verbs for motion summary
   */
  extractKeyVerbs(text) {
    const verbMatch = text.match(/\b(walks?|runs?|leaps?|jumps?|turns?|speaks?|reaches?|grabs?|stands?|sits?|looks?|moves?|steps?|enters?|exits?|falls?|rises?|opens?|closes?)\b/gi);
    if (verbMatch && verbMatch.length > 0) {
      const uniqueVerbs = [...new Set(verbMatch.map(v => v.toLowerCase()))];
      return uniqueVerbs.slice(0, 3).join(', ');
    }
    return 'progressive action';
  },

  /**
   * Detect scene type from sceneAction text (Upgrade 4)
   * Used to select appropriate action library templates
   * @param {string} sceneAction - The scene action text
   * @returns {string} Scene type: action, dialogue, emotional, establishing, revelation, contemplative, montage
   */
  detectSceneType(sceneAction) {
    if (!sceneAction) return 'dialogue';
    const text = sceneAction.toLowerCase();

    // Action keywords - fights, chases, physical action
    if (text.match(/fight|combat|battle|chase|run|sprint|dodge|attack|defend|punch|kick|strike|leap|jump|vault/)) {
      return 'action';
    }
    // Emotional keywords - feelings, reactions
    if (text.match(/cry|tear|grief|mourn|sob|embrace|hug|kiss|love|joy|celebrate|laugh|smile|heartbreak|tender/)) {
      return 'emotional';
    }
    // Revelation keywords - discovery, realization
    if (text.match(/discover|find|realize|reveal|uncover|truth|secret|evidence|understand|recognize|piece together/)) {
      return 'revelation';
    }
    // Establishing keywords - locations, arrivals
    if (text.match(/arrive|enter|approach|survey|vista|landscape|morning|sunset|establishing|location|city|building/)) {
      return 'establishing';
    }
    // Contemplative keywords - thinking, reflecting
    if (text.match(/think|ponder|contemplate|reflect|gaze|stare|decide|consider|weight|alone|silent|quiet/)) {
      return 'contemplative';
    }
    // Montage keywords - training, preparation, progression
    if (text.match(/train|practice|prepare|assemble|build|create|improve|progress|montage|sequence of/)) {
      return 'montage';
    }
    // Dialogue keywords (default) - conversation, speaking
    if (text.match(/speak|talk|say|tell|explain|discuss|argue|negotiate|convince|respond|reply/)) {
      return 'dialogue';
    }

    // Default to dialogue for unknown
    return 'dialogue';
  },

  /**
   * Decompose sceneAction into shot beats with END STATES (RULE-BASED FALLBACK)
   * @param {string} sceneAction - Rich action sequence for the scene
   * @param {number} shotCount - Number of shots to create
   * @param {object} sceneContext - { visualPrompt, narration, characters }
   * @returns {Array} Array of shot beats with start/action/end
   */
  decomposeIntoShots(sceneAction, shotCount, sceneContext = {}) {
    if (!sceneAction || sceneAction.trim().length < 20) {
      // Fallback: use visualPrompt if sceneAction is missing/brief
      return this.fallbackDecomposition(sceneContext.visualPrompt, shotCount, sceneContext);
    }

    // Parse sceneAction into sentences/action segments
    const actionSegments = this.parseActionSegments(sceneAction);

    // Distribute segments across shots
    const shotBeats = this.distributeToShots(actionSegments, shotCount);

    // Define END STATES for each shot (capture points)
    const shotsWithEndStates = this.defineEndStates(shotBeats, sceneContext);

    // Generate video prompts with START â†’ ACTION â†’ END structure
    return this.generateShotPrompts(shotsWithEndStates, sceneContext);
  },

  /**
   * Parse sceneAction into individual action segments
   */
  parseActionSegments(sceneAction) {
    // Split by sentence-ending punctuation, keeping the punctuation
    const sentences = sceneAction
      .split(/(?<=[.!?])\s+/)
      .filter(s => s.trim().length > 0);

    // Further split long sentences at natural break points
    const segments = [];
    for (const sentence of sentences) {
      if (sentence.length > 150) {
        // Split at commas or "and"/"then"/"as" connectors
        const parts = sentence.split(/,\s+(?:and|then|as|while)\s+|,\s+/);
        segments.push(...parts.filter(p => p.trim().length > 10));
      } else {
        segments.push(sentence);
      }
    }

    return segments.map(s => s.trim());
  },

  /**
   * Distribute action segments across shots
   */
  distributeToShots(segments, shotCount) {
    const shots = Array(shotCount).fill(null).map(() => ({ segments: [] }));

    if (segments.length === 0) {
      return shots;
    }

    // Calculate segments per shot
    const segmentsPerShot = Math.ceil(segments.length / shotCount);

    let segmentIndex = 0;
    for (let shotIndex = 0; shotIndex < shotCount; shotIndex++) {
      const endIndex = Math.min(segmentIndex + segmentsPerShot, segments.length);
      shots[shotIndex].segments = segments.slice(segmentIndex, endIndex);
      segmentIndex = endIndex;
    }

    // Ensure last shot gets any remaining segments
    if (segmentIndex < segments.length) {
      shots[shotCount - 1].segments.push(...segments.slice(segmentIndex));
    }

    return shots;
  },

  /**
   * Define END STATES (capture points) for each shot
   */
  defineEndStates(shotBeats, sceneContext) {
    return shotBeats.map((shot, index) => {
      const isFirst = index === 0;
      const isLast = index === shotBeats.length - 1;
      const segments = shot.segments;

      // Determine what this shot's action describes
      const actionText = segments.join(' ');

      // Extract the END STATE from the last segment
      const lastSegment = segments[segments.length - 1] || '';
      const endState = this.extractEndState(lastSegment, actionText, isLast);

      // Determine START STATE (from previous shot's end, or scene opening)
      const startState = isFirst
        ? this.extractOpeningState(sceneContext.visualPrompt, actionText)
        : null; // Will be filled in from previous shot's end

      return {
        shotIndex: index,
        isFirst,
        isLast,
        segments,
        actionText,
        startState,
        endState
      };
    });
  },

  /**
   * Extract end state from action text
   */
  extractEndState(lastSegment, fullAction, isLast) {
    // Look for position/pose indicators
    const positionPatterns = [
      /(?:stands?|standing)\s+(?:at|by|near|on)\s+[^,.]+/i,
      /(?:sits?|sitting)\s+(?:down|in|on)\s+[^,.]+/i,
      /(?:facing|turned toward|looking at)\s+[^,.]+/i,
      /(?:reaches?|reaching|extends?|extending)\s+(?:toward|for|out)/i,
      /(?:leaps?|leaping|jumps?|jumping|descends?|descending)/i,
      /(?:runs?|running|walks?|walking)\s+(?:toward|into|through)/i,
      /(?:holds?|holding|grips?|gripping)\s+[^,.]+/i
    ];

    // Try to find a specific pose/position
    for (const pattern of positionPatterns) {
      const match = fullAction.match(pattern);
      if (match) {
        return this.formatEndState(match[0], isLast);
      }
    }

    // Fallback: use the last meaningful phrase
    const lastPhrase = lastSegment.replace(/[.!?]+$/, '').trim();
    return this.formatEndState(lastPhrase, isLast);
  },

  /**
   * Format end state as a capturable moment
   */
  formatEndState(rawState, isLast) {
    if (isLast) {
      return `Scene concludes with: ${rawState}. Hold for final frame.`;
    }
    return `CAPTURE POINT: ${rawState}. This frame transitions to next shot.`;
  },

  /**
   * Extract opening state from visual prompt
   */
  extractOpeningState(visualPrompt, actionText) {
    if (!visualPrompt) {
      return 'Scene opens';
    }

    // Look for character positions in visual prompt
    const positionMatch = visualPrompt.match(
      /(?:stands?|sits?|positions?|located)\s+(?:at|by|near|on|in)\s+[^,.]+/i
    );

    if (positionMatch) {
      return `Opening: ${positionMatch[0]}`;
    }

    // Look for character + location
    const charMatch = visualPrompt.match(
      /([A-Z][a-z]+)\s+(?:is|are|stands?|sits?)\s+[^,.]+/i
    );

    if (charMatch) {
      return `Opening: ${charMatch[0]}`;
    }

    return 'Scene opens with established visual';
  },

  /**
   * Generate final shot prompts with START â†’ ACTION â†’ END structure
   */
  generateShotPrompts(shotsWithEndStates, sceneContext) {
    // First pass: set start states from previous end states
    for (let i = 1; i < shotsWithEndStates.length; i++) {
      const prevEndState = shotsWithEndStates[i - 1].endState;
      shotsWithEndStates[i].startState = `Continuing from: ${prevEndState.replace('CAPTURE POINT: ', '').replace('. This frame transitions to next shot.', '')}`;
    }

    // Second pass: generate video prompts
    return shotsWithEndStates.map(shot => {
      const videoPrompt = this.buildVideoPrompt(shot, sceneContext);
      return {
        ...shot,
        videoPrompt,
        motionDescription: this.createMotionSummary(shot)
      };
    });
  },

  /**
   * Build MOTION-DENSE video prompt
   * Each prompt describes continuous action over the full shot duration
   */
  buildVideoPrompt(shot, sceneContext) {
    const parts = [];

    // 1. CAMERA MOVEMENT (choose based on shot position)
    const cameraMovement = shot.isFirst
      ? 'Slow push in to establish the scene.'
      : shot.isLast
        ? 'Gradual pull back for the closing moment.'
        : 'Smooth tracking.';
    parts.push(cameraMovement);

    // 2. CHARACTER MOTION (extract and enhance with micro-actions)
    const actionContent = shot.actionText || shot.segments.join('. ');
    if (actionContent) {
      // Add breathing and subtle motion indicators
      const enhancedAction = this.enhanceWithMotion(actionContent, shot.isFirst, shot.isLast);
      parts.push(enhancedAction);
    }

    // 3. ENVIRONMENT ANIMATION (add based on context)
    const envAnimation = this.getEnvironmentAnimation(sceneContext.visualPrompt, shot.isLast);
    if (envAnimation) {
      parts.push(envAnimation);
    }

    // 4. CONTINUITY PHRASE (for shots after first)
    if (!shot.isFirst) {
      parts.push('Continuing seamlessly from previous frame.');
    }

    // 5. QUALITY KEYWORDS
    parts.push('Cinematic, photorealistic, smooth fluid motion, natural lifelike movement.');

    return parts.join(' ');
  },

  /**
   * Enhance action text with motion-dense details
   */
  enhanceWithMotion(actionText, isFirst, isLast) {
    let enhanced = actionText;

    // Add breathing cues if character actions are present
    if (/\b(stands?|sits?|waits?|watches?|looks?)\b/i.test(actionText)) {
      if (isFirst) {
        enhanced = enhanced.replace(/^(\w+)/, '$1 breathes slowly, tension visible.');
      } else if (isLast) {
        enhanced += ', breath deepening as the moment settles.';
      } else {
        enhanced += ', subtle shifts in posture.';
      }
    }

    // Add eye movement cues
    if (/\b(notices?|sees?|observes?|watches?)\b/i.test(actionText)) {
      enhanced = enhanced.replace(/\b(notices?|sees?|observes?|watches?)\b/gi, (match) => {
        return `eyes ${match.toLowerCase()}, attention focusing`;
      });
    }

    return enhanced;
  },

  /**
   * Get environment animation based on visual context
   */
  getEnvironmentAnimation(visualPrompt, isLast) {
    if (!visualPrompt) return '';

    const prompt = visualPrompt.toLowerCase();
    const animations = [];

    // Detect environment types and add appropriate animation
    if (/tech|lab|digital|hologram|display|screen/i.test(prompt)) {
      animations.push('Displays pulse and flicker with data');
    }
    if (/fire|flame|torch|candle/i.test(prompt)) {
      animations.push(isLast ? 'Softly flames flicker' : 'Steadily flames flicker and dance');
    }
    if (/rain|storm|weather/i.test(prompt)) {
      animations.push('Rain streaks down windows, lights blurring');
    }
    if (/city|urban|street|neon/i.test(prompt)) {
      animations.push('City lights pulse in the background');
    }
    if (/nature|forest|trees|leaves/i.test(prompt)) {
      animations.push('Leaves rustle gently, light filtering through');
    }
    if (/water|ocean|river|waves/i.test(prompt)) {
      animations.push('Water ripples and reflects light');
    }

    // Default atmospheric motion if nothing specific found
    if (animations.length === 0) {
      animations.push('Ambient light shifts subtly in the environment');
    }

    return animations.slice(0, 2).join('. ') + '.';
  },

  /**
   * Create a brief motion summary for UI display
   */
  createMotionSummary(shot) {
    const action = shot.actionText || '';
    // Extract key verbs/actions
    const verbMatch = action.match(/\b(walks?|runs?|leaps?|turns?|speaks?|reaches?|stands?|sits?|looks?|moves?)\b/gi);
    if (verbMatch && verbMatch.length > 0) {
      const uniqueVerbs = [...new Set(verbMatch.map(v => v.toLowerCase()))];
      return `Shot ${shot.shotIndex + 1}: ${uniqueVerbs.slice(0, 3).join(', ')}`;
    }
    return `Shot ${shot.shotIndex + 1}: Progressive action`;
  },

  /**
   * Fallback decomposition when sceneAction is missing
   * Uses visualPrompt to generate basic progressive shots
   */
  fallbackDecomposition(visualPrompt, shotCount, sceneContext) {
    const prompt = visualPrompt || '';
    const shots = [];

    // Extract any characters mentioned
    const characters = this.extractCharacters(prompt);
    const environment = this.extractEnvironment(prompt);

    // Create progressive shots based on typical scene structure
    const phases = ['establish', 'develop', 'build', 'peak', 'resolve'];

    for (let i = 0; i < shotCount; i++) {
      const phase = phases[Math.min(i, phases.length - 1)];
      const isFirst = i === 0;
      const isLast = i === shotCount - 1;

      const shot = this.createFallbackShot(
        i, phase, characters, environment,
        { isFirst, isLast, shotCount }
      );
      shots.push(shot);
    }

    return shots;
  },

  /**
   * Create a fallback shot when no sceneAction exists
   */
  createFallbackShot(index, phase, characters, environment, meta) {
    const char = characters[0] || 'Subject';
    const env = environment || 'the scene';

    const phaseActions = {
      establish: {
        action: `${char} is revealed in ${env}, establishing the setting with subtle environmental motion`,
        end: `${char} positioned in frame, environment established`
      },
      develop: {
        action: `${char} shows subtle movement and expression change, engaging with the surroundings`,
        end: `${char} in developed pose, ready for next action`
      },
      build: {
        action: `Energy increases as ${char} moves with purpose, tension building in the scene`,
        end: `${char} at heightened state, movement intensifying`
      },
      peak: {
        action: `${char} reaches the dramatic high point, maximum visual impact and motion`,
        end: `${char} at peak dramatic moment`
      },
      resolve: {
        action: `${char} settles into resolution, motion slowing to satisfying conclusion`,
        end: `${char} in final resting state, scene complete`
      }
    };

    const phaseData = phaseActions[phase] || phaseActions.develop;

    return {
      shotIndex: index,
      isFirst: meta.isFirst,
      isLast: meta.isLast,
      segments: [phaseData.action],
      actionText: phaseData.action,
      startState: meta.isFirst ? `Opening on ${env}` : 'Continuing from previous frame',
      endState: meta.isLast
        ? `Scene concludes: ${phaseData.end}. Hold for final frame.`
        : `CAPTURE POINT: ${phaseData.end}. This frame transitions to next shot.`,
      videoPrompt: this.buildVideoPrompt({
        isFirst: meta.isFirst,
        isLast: meta.isLast,
        startState: meta.isFirst ? `Opening on ${env}` : 'Continuing from previous frame',
        actionText: phaseData.action,
        endState: meta.isLast
          ? `Scene concludes: ${phaseData.end}. Hold for final frame.`
          : `CAPTURE POINT: ${phaseData.end}. This frame transitions to next shot.`
      }, {}),
      motionDescription: `Shot ${index + 1}: ${phase} phase`
    };
  },

  /**
   * Extract character names from text
   */
  extractCharacters(text) {
    const charPatterns = [
      /([A-Z][a-z]+)\s+(?:stands?|sits?|walks?|looks?|gazes?|holds?)/g,
      /(?:Foreground|Midground|Background):\s*([A-Z][a-z]+)/g
    ];

    const characters = new Set();
    for (const pattern of charPatterns) {
      let match;
      while ((match = pattern.exec(text)) !== null) {
        if (match[1] && match[1].length > 2) {
          characters.add(match[1]);
        }
      }
    }

    return [...characters];
  },

  /**
   * Extract environment description
   */
  extractEnvironment(text) {
    const envMatch = text.match(
      /(?:in|at|on)\s+(?:a|the|an)?\s*([^,.]{10,50})/i
    );
    return envMatch ? envMatch[1].trim() : null;
  }
};

// =============================================================================
// ENHANCED_VIDEO_PROMPT_BUILDER - Hollywood-Quality 10-Second Video Prompts
// =============================================================================
/**
 * ENHANCED_VIDEO_PROMPT_BUILDER
 *
 * Generates temporally-progressive, motion-dense video prompts optimized for
 * 10-second AI video generation. Based on professional cinematography principles
 * and Minimax Hailuo best practices.
 *
 * Key principles:
 * 1. TEMPORAL SEGMENTATION - Break 10s into 3-4 phases with distinct actions
 * 2. MULTI-LAYER MOTION - Primary, secondary, and background action layers
 * 3. PROFESSIONAL CAMERA LANGUAGE - Dolly, crane, steadicam, rack focus
 * 4. MICRO-MOTION DETAILS - Breathing, blinking, weight shifts, hand movements
 * 5. ENVIRONMENT ANIMATION - Continuous background motion throughout
 */
const ENHANCED_VIDEO_PROMPT_BUILDER = {

  // =========================================================================
  // PROFESSIONAL CAMERA MOVEMENT LIBRARY
  // =========================================================================
  cameraMovements: {
    establishing: [
      'Camera slowly dollies forward, gradually revealing',
      'Crane shot descends smoothly to establish',
      'Wide establishing shot with gentle push-in toward',
      'Steadicam glides in from wide to medium on',
      'Slow zoom through atmospheric depth toward'
    ],
    developing: [
      'Camera tracks laterally, following the action as',
      'Smooth dolly circles around to capture',
      'Steadicam follows movement, maintaining focus on',
      'Subtle push-in intensifies focus as',
      'Camera arcs gracefully to reveal'
    ],
    building: [
      'Dynamic tracking shot accelerates alongside',
      'Camera pushes in with purpose toward',
      'Handheld energy follows the increasing tension as',
      'Quick dolly-in emphasizes the moment when',
      'Camera cranes up to capture the scale of'
    ],
    peak: [
      'Camera holds steady on the crucial moment as',
      'Dramatic push to close-up captures',
      'Camera circles to capture all angles of',
      'Slow-motion emphasis on the peak action where',
      'Tight tracking maintains intensity as'
    ],
    resolve: [
      'Camera slowly pulls back, settling into stillness as',
      'Gentle crane up provides closure while',
      'Steadicam retreats to wide, framing the resolution where',
      'Slow pull-out reveals the full aftermath as',
      'Camera settles into final composition as'
    ]
  },

  // =========================================================================
  // MICRO-ACTION LIBRARY - Human details that fill time naturally
  // =========================================================================
  microActions: {
    breathing: {
      calm: 'chest rises and falls with slow, measured breaths',
      tense: 'breathing quickens, shoulders tight with held tension',
      emotional: 'breath catches, then releases with visible emotion',
      relaxing: 'deep breath releases built-up tension, shoulders dropping',
      focused: 'controlled breathing, stillness of concentration'
    },
    eyeMovement: {
      scanning: 'eyes scan the surroundings, taking in details',
      focused: 'gaze locked intently on a single point',
      shifting: 'eyes dart between focal points with uncertainty',
      emotional: 'eyes glisten with emotion, blinking slowly',
      realization: 'eyes widen slightly with dawning understanding',
      softening: 'gaze softens, warmth entering the expression'
    },
    weight: {
      shifting: 'weight shifts subtly from foot to foot',
      settling: 'weight settles firmly, grounding into stance',
      leaning: 'body leans slightly toward point of interest',
      bracing: 'weight centers, body bracing for what comes',
      relaxing: 'weight releases, posture becoming more open'
    },
    hands: {
      idle: 'fingers move with subtle nervous energy',
      purposeful: 'hands move with deliberate intention',
      emotional: 'hands clench and release with feeling',
      reaching: 'hand extends slowly, almost tentatively',
      resting: 'hands settle into stillness, finding rest',
      gesturing: 'hands gesture naturally while communicating'
    },
    facial: {
      neutral: 'subtle muscle movements animate the expression',
      thinking: 'brow furrows slightly in concentration',
      emotional: 'expression shifts through complex emotions',
      reacting: 'face responds naturally to the moment',
      resolving: 'expression settles into peaceful resolution'
    }
  },

  // =========================================================================
  // ENVIRONMENT ANIMATION PATTERNS - Continuous background motion
  // =========================================================================
  environmentAnimations: {
    urban: {
      continuous: [
        'city lights pulse and flicker in the background',
        'traffic flows in soft bokeh behind the subject',
        'neon signs cast shifting colored reflections',
        'pedestrians move as blurred silhouettes in distance'
      ],
      atmospheric: [
        'heat haze ripples above warm pavement',
        'steam rises from vents in the street',
        'rain streaks catch and scatter city lights',
        'wind stirs litter and leaves along the street'
      ]
    },
    nature: {
      continuous: [
        'leaves rustle and sway in gentle breeze',
        'grass ripples in waves across the field',
        'branches bob and shift with air currents',
        'flowers nod gently in the moving air'
      ],
      atmospheric: [
        'light filters through moving canopy above',
        'shadows dance across the ground',
        'dust motes drift through shafts of light',
        'clouds drift slowly, changing the light quality'
      ]
    },
    interior: {
      continuous: [
        'curtains sway gently from unseen air movement',
        'light shifts as clouds pass outside windows',
        'dust particles drift through ambient light',
        'shadows lengthen almost imperceptibly'
      ],
      atmospheric: [
        'candle flames flicker and dance',
        'firelight casts moving shadows on walls',
        'soft ambient light pulses subtly',
        'reflections shimmer on glass surfaces'
      ]
    },
    scifi: {
      continuous: [
        'holographic displays pulse with data streams',
        'ambient lighting cycles through subtle color shifts',
        'screens flicker with scrolling information',
        'indicators and lights blink in rhythmic patterns'
      ],
      atmospheric: [
        'energy fields ripple with contained power',
        'particle effects drift through the air',
        'digital interfaces respond to nearby movement',
        'atmospheric processors hum with visible output'
      ]
    },
    dojo: {
      continuous: [
        'incense smoke curls and drifts upward',
        'candle flames flicker, casting dancing shadows',
        'light plays across polished wooden floors',
        'fabric banners sway in the still air'
      ],
      atmospheric: [
        'dust motes drift through shafts of light',
        'shadows shift with the moving flames',
        'ambient light transitions with passing time',
        'gentle air currents stir loose fabric'
      ]
    }
  },

  // =========================================================================
  // TEMPORAL PHASE STRUCTURES for 10-second videos
  // =========================================================================
  temporalStructures: {
    // Single subject with emotional journey
    emotionalArc: {
      phases: [
        { start: 0, end: 3, name: 'opening', intensity: 0.3 },
        { start: 3, end: 6, name: 'development', intensity: 0.6 },
        { start: 6, end: 10, name: 'culmination', intensity: 1.0 }
      ],
      description: 'Emotional journey from introduction to resolution'
    },
    // Action or movement focused
    actionSequence: {
      phases: [
        { start: 0, end: 2, name: 'setup', intensity: 0.2 },
        { start: 2, end: 5, name: 'action', intensity: 0.7 },
        { start: 5, end: 8, name: 'peak', intensity: 1.0 },
        { start: 8, end: 10, name: 'settle', intensity: 0.4 }
      ],
      description: 'Physical action with clear beginning and end'
    },
    // Dialogue or interaction focused
    dialogueInteraction: {
      phases: [
        { start: 0, end: 3, name: 'engage', intensity: 0.4 },
        { start: 3, end: 7, name: 'exchange', intensity: 0.7 },
        { start: 7, end: 10, name: 'respond', intensity: 0.8 }
      ],
      description: 'Conversation or non-verbal exchange between subjects'
    },
    // Reveal or discovery
    revealMoment: {
      phases: [
        { start: 0, end: 4, name: 'anticipation', intensity: 0.3 },
        { start: 4, end: 6, name: 'reveal', intensity: 1.0 },
        { start: 6, end: 10, name: 'reaction', intensity: 0.7 }
      ],
      description: 'Building to a moment of revelation or discovery'
    },
    // Contemplative or atmospheric
    contemplative: {
      phases: [
        { start: 0, end: 4, name: 'stillness', intensity: 0.2 },
        { start: 4, end: 7, name: 'shift', intensity: 0.4 },
        { start: 7, end: 10, name: 'settle', intensity: 0.3 }
      ],
      description: 'Meditative, slow-paced scene with subtle changes'
    }
  },

  // =========================================================================
  // MAIN PROMPT GENERATION METHODS
  // =========================================================================

  /**
   * Generate an enhanced 10-second video prompt
   * @param {Object} shotData - Shot information from decomposition
   * @param {Object} sceneContext - Scene visual/narrative context
   * @param {number} videoDuration - Video duration in seconds (6 or 10)
   * @returns {string} Enhanced video prompt
   */
  generateEnhancedPrompt(shotData, sceneContext, videoDuration = 10) {
    const is10Second = videoDuration >= 10;

    // Extract narrative elements
    const narrative = this.extractNarrativeContext(sceneContext);

    // Determine temporal structure based on shot type
    const structure = this.selectTemporalStructure(shotData, narrative);

    // Build the layered prompt
    const promptLayers = [];

    // 1. CAMERA LAYER - Professional movement
    const cameraLayer = this.buildCameraLayer(shotData, structure, is10Second);
    promptLayers.push(cameraLayer);

    // 2. PRIMARY ACTION LAYER - Main subject motion
    const primaryLayer = this.buildPrimaryActionLayer(shotData, narrative, structure, is10Second);
    promptLayers.push(primaryLayer);

    // 3. SECONDARY ACTION LAYER - Supporting details (for 10s only)
    if (is10Second) {
      const secondaryLayer = this.buildSecondaryActionLayer(shotData, narrative, structure);
      if (secondaryLayer) promptLayers.push(secondaryLayer);
    }

    // 4. MICRO-MOTION LAYER - Human details
    const microLayer = this.buildMicroMotionLayer(narrative, structure, is10Second);
    if (microLayer) promptLayers.push(microLayer);

    // 5. ENVIRONMENT LAYER - Background animation
    const envLayer = this.buildEnvironmentLayer(narrative, structure, is10Second);
    if (envLayer) promptLayers.push(envLayer);

    // 6. CONTINUITY LAYER - Frame chaining
    if (!shotData.isFirst) {
      promptLayers.push('Continuing seamlessly from previous frame, maintaining visual consistency.');
    }

    // 7. QUALITY LAYER - Technical keywords
    const qualityLayer = this.buildQualityLayer(narrative.mood);
    promptLayers.push(qualityLayer);

    return promptLayers.join(' ');
  },

  /**
   * Extract narrative context from scene data
   */
  extractNarrativeContext(sceneContext) {
    const visualPrompt = sceneContext?.visualPrompt || '';
    const promptLower = visualPrompt.toLowerCase();

    // Detect characters
    const characters = [];
    const charPatterns = [
      /([A-Z][a-z]+)\s+(?:sits?|stands?|walks?|looks?|gazes?|holds?|moves?|reaches?|turns?)/gi
    ];
    const seenNames = new Set();
    for (const pattern of charPatterns) {
      let match;
      while ((match = pattern.exec(visualPrompt)) !== null) {
        const name = match[1];
        if (!seenNames.has(name.toLowerCase()) && name.length > 2) {
          seenNames.add(name.toLowerCase());
          characters.push(name);
        }
      }
    }

    // Detect environment type
    const envType = /dojo|temple|shrine|meditation/.test(promptLower) ? 'dojo' :
                    /city|urban|street|building|neon/.test(promptLower) ? 'urban' :
                    /forest|nature|outdoor|garden|tree|field/.test(promptLower) ? 'nature' :
                    /space|ship|station|futuristic|tech|cyber/.test(promptLower) ? 'scifi' :
                    /room|interior|inside|office|home/.test(promptLower) ? 'interior' : 'interior';

    // Detect mood
    const mood = /troubled|tense|worried|anxious|conflict|struggle/.test(promptLower) ? 'tense' :
                 /peaceful|calm|serene|gentle|quiet/.test(promptLower) ? 'peaceful' :
                 /hopeful|warm|tender|love|comfort/.test(promptLower) ? 'warm' :
                 /mysterious|eerie|dark|ominous|unknown/.test(promptLower) ? 'mysterious' :
                 /energetic|action|fast|intense|dynamic/.test(promptLower) ? 'dynamic' : 'neutral';

    // Detect action type
    const actionType = /walk|run|move|approach|step|enter|exit/.test(promptLower) ? 'movement' :
                       /sit|stand|wait|contemplate|think|meditate/.test(promptLower) ? 'contemplative' :
                       /talk|speak|discuss|converse|explain/.test(promptLower) ? 'dialogue' :
                       /reveal|discover|realize|understand|see/.test(promptLower) ? 'revelation' :
                       /fight|struggle|conflict|battle|clash/.test(promptLower) ? 'action' : 'general';

    return {
      characters,
      envType,
      mood,
      actionType,
      visualPrompt
    };
  },

  /**
   * Select appropriate temporal structure based on context
   */
  selectTemporalStructure(shotData, narrative) {
    // Map action types to structures
    const structureMap = {
      'contemplative': this.temporalStructures.contemplative,
      'dialogue': this.temporalStructures.dialogueInteraction,
      'revelation': this.temporalStructures.revealMoment,
      'action': this.temporalStructures.actionSequence,
      'movement': this.temporalStructures.actionSequence,
      'general': this.temporalStructures.emotionalArc
    };

    let structure = structureMap[narrative.actionType] || this.temporalStructures.emotionalArc;

    // Adjust for shot position
    if (shotData.isFirst) {
      // First shots should emphasize establishment
      structure = { ...structure };
    } else if (shotData.isLast) {
      // Last shots should emphasize resolution
      structure = { ...structure };
    }

    return structure;
  },

  /**
   * Build camera movement layer with temporal awareness
   */
  buildCameraLayer(shotData, structure, is10Second) {
    const phase = shotData.isFirst ? 'establishing' :
                  shotData.isLast ? 'resolve' :
                  shotData.arcPosition === 'peak' ? 'peak' :
                  shotData.arcPosition === 'build' ? 'building' : 'developing';

    const movements = this.cameraMovements[phase] || this.cameraMovements.developing;
    const baseMovement = movements[Math.floor(Math.random() * movements.length)];

    if (is10Second) {
      // For 10-second videos, describe camera evolution over time
      if (shotData.isFirst) {
        return `${baseMovement} the scene, then settling into stable framing for the action.`;
      } else if (shotData.isLast) {
        return `${baseMovement} the scene reaches its conclusion, widening to capture the final moment.`;
      } else {
        return `${baseMovement} the unfolding action, maintaining focus throughout the continuous movement.`;
      }
    }

    return baseMovement;
  },

  /**
   * Build primary action layer with temporal segments
   */
  buildPrimaryActionLayer(shotData, narrative, structure, is10Second) {
    const actionText = shotData.actionText || shotData.segments?.join('. ') || '';
    const character = narrative.characters[0] || 'The subject';

    if (!is10Second) {
      // For 6-second videos, simpler action description
      return actionText || `${character} moves through the moment with natural motion.`;
    }

    // For 10-second videos, create temporal segments
    const phases = structure.phases;
    const segmentDescriptions = [];

    for (let i = 0; i < phases.length; i++) {
      const phase = phases[i];
      const timeRange = `[${phase.start}s-${phase.end}s]`;

      let segmentAction;
      if (actionText && i === 0) {
        // Use provided action for first segment
        segmentAction = actionText;
      } else {
        // Generate continuation for subsequent segments
        segmentAction = this.generatePhaseAction(phase.name, character, narrative.mood, phase.intensity);
      }

      segmentDescriptions.push(`${timeRange} ${segmentAction}`);
    }

    return segmentDescriptions.join(' ');
  },

  /**
   * Generate action description for a specific phase
   */
  generatePhaseAction(phaseName, character, mood, intensity) {
    const phaseActions = {
      opening: {
        tense: `${character} enters with visible tension, movements measured and alert`,
        peaceful: `${character} appears in calm stillness, serene presence establishing`,
        warm: `${character} is revealed with gentle warmth, soft energy radiating`,
        mysterious: `${character} emerges from shadow, presence gradually manifesting`,
        dynamic: `${character} enters with energy, immediate sense of motion`,
        neutral: `${character} comes into view, presence naturally establishing`
      },
      development: {
        tense: `tension builds in ${character}'s posture, movements becoming more deliberate`,
        peaceful: `${character} continues in flowing calm, gentle progression of motion`,
        warm: `connection deepens as ${character} moves with emotional warmth`,
        mysterious: `${character}'s intention becomes clearer, mystery developing`,
        dynamic: `${character}'s energy increases, motion accelerating purposefully`,
        neutral: `${character} progresses through the moment, natural development`
      },
      culmination: {
        tense: `${character} reaches peak tension, decisive moment arriving`,
        peaceful: `${character} settles into deep tranquility, peace fully realized`,
        warm: `emotional peak as ${character}'s warmth reaches its fullest expression`,
        mysterious: `revelation moment as ${character}'s purpose becomes clear`,
        dynamic: `${character}'s motion reaches maximum intensity, peak action`,
        neutral: `${character} arrives at the natural conclusion of the moment`
      },
      setup: {
        tense: `${character} prepares, tension coiling for what comes`,
        neutral: `${character} positions for the action ahead`
      },
      action: {
        dynamic: `${character} executes the primary action with full commitment`,
        neutral: `${character} moves through the main action sequence`
      },
      peak: {
        dynamic: `${character} at maximum intensity, the crucial moment`,
        neutral: `${character} reaches the height of the action`
      },
      settle: {
        peaceful: `${character} comes to rest, motion settling`,
        neutral: `${character}'s movement concludes, finding stillness`
      },
      engage: {
        warm: `${character} engages with warmth, connection initiating`,
        neutral: `${character} begins the interaction`
      },
      exchange: {
        warm: `the exchange deepens between ${character} and the other`,
        neutral: `${character} continues through the interaction`
      },
      respond: {
        warm: `${character} responds with emotional depth`,
        neutral: `${character} reacts to complete the exchange`
      },
      anticipation: {
        mysterious: `${character} waits with building anticipation`,
        neutral: `${character} moves toward the coming moment`
      },
      reveal: {
        mysterious: `the revelation strikes ${character}, moment of impact`,
        neutral: `${character} experiences the key moment`
      },
      reaction: {
        emotional: `${character}'s response unfolds with visible impact`,
        neutral: `${character} processes and responds to what occurred`
      },
      stillness: {
        peaceful: `${character} exists in meditative stillness, barely moving`,
        neutral: `${character} remains in quiet contemplation`
      },
      shift: {
        peaceful: `subtle internal shift occurs within ${character}`,
        neutral: `${character} experiences gentle transition`
      }
    };

    const moodActions = phaseActions[phaseName];
    if (!moodActions) return `${character} continues through this moment`;

    return moodActions[mood] || moodActions.neutral || `${character} progresses naturally`;
  },

  /**
   * Build secondary action layer for supporting details
   */
  buildSecondaryActionLayer(shotData, narrative, structure) {
    if (narrative.characters.length < 2) {
      // No secondary character, add environmental interaction
      return null;
    }

    const secondaryChar = narrative.characters[1];
    const mood = narrative.mood;

    const secondaryActions = {
      tense: `Meanwhile, ${secondaryChar} responds with matching tension, body language reflecting the mood.`,
      peaceful: `${secondaryChar} mirrors the calm energy, movements harmoniously aligned.`,
      warm: `${secondaryChar} responds with equal warmth, connection visible in subtle gestures.`,
      dynamic: `${secondaryChar} matches the energy, complementary motion adding to the scene.`,
      neutral: `${secondaryChar} responds naturally, supporting the primary action.`
    };

    return secondaryActions[mood] || secondaryActions.neutral;
  },

  /**
   * Build micro-motion layer for human details
   */
  buildMicroMotionLayer(narrative, structure, is10Second) {
    const mood = narrative.mood;
    const character = narrative.characters[0] || 'The subject';

    // Select appropriate micro-actions based on mood
    const breathingType = mood === 'tense' ? 'tense' :
                          mood === 'peaceful' ? 'calm' :
                          mood === 'warm' ? 'emotional' : 'calm';

    const eyeType = mood === 'mysterious' ? 'scanning' :
                    mood === 'tense' ? 'shifting' :
                    mood === 'warm' ? 'softening' : 'focused';

    const breathing = this.microActions.breathing[breathingType];
    const eyes = this.microActions.eyeMovement[eyeType];

    if (is10Second) {
      // For 10s, include multiple micro-action types
      const weight = this.microActions.weight[mood === 'tense' ? 'bracing' : 'settling'];
      const hands = this.microActions.hands[mood === 'tense' ? 'emotional' : 'idle'];

      return `Throughout, ${character}'s ${breathing}, ${eyes}. ${character}'s ${weight}, while ${hands}.`;
    }

    return `${character}'s ${breathing}, ${eyes}.`;
  },

  /**
   * Build environment animation layer
   */
  buildEnvironmentLayer(narrative, structure, is10Second) {
    const envType = narrative.envType;
    const envAnimations = this.environmentAnimations[envType] || this.environmentAnimations.interior;

    // Select continuous and atmospheric animations
    const continuous = envAnimations.continuous[Math.floor(Math.random() * envAnimations.continuous.length)];

    if (is10Second) {
      // For 10s, add atmospheric details too
      const atmospheric = envAnimations.atmospheric[Math.floor(Math.random() * envAnimations.atmospheric.length)];
      return `In the background, ${continuous}. ${atmospheric}.`;
    }

    return `Background: ${continuous}.`;
  },

  /**
   * Build quality/technical layer
   */
  buildQualityLayer(mood) {
    const moodQualities = {
      tense: 'Cinematic tension, dramatic lighting, photorealistic detail, fluid natural motion.',
      peaceful: 'Serene cinematography, soft natural lighting, smooth dreamlike motion, photorealistic.',
      warm: 'Warm cinematic tones, intimate framing, gentle fluid motion, photorealistic emotion.',
      mysterious: 'Atmospheric cinematography, chiaroscuro lighting, subtle ethereal motion, photorealistic.',
      dynamic: 'Dynamic cinematography, energetic framing, powerful fluid motion, photorealistic action.',
      neutral: 'Professional cinematography, balanced lighting, smooth natural motion, photorealistic quality.'
    };

    return moodQualities[mood] || moodQualities.neutral;
  },

  // =========================================================================
  // INTEGRATION METHODS - Connect to existing systems
  // =========================================================================

  /**
   * Replace the basic video prompt from SHOT_DECOMPOSITION_ENGINE
   * Called during shot processing
   */
  enhanceExistingPrompt(basicPrompt, shotData, sceneContext, videoDuration = 10) {
    // If we have good scene context, generate fully enhanced prompt
    if (sceneContext && sceneContext.visualPrompt) {
      return this.generateEnhancedPrompt(shotData, sceneContext, videoDuration);
    }

    // Otherwise enhance the basic prompt with temporal structure
    return this.expandBasicPrompt(basicPrompt, shotData, videoDuration);
  },

  /**
   * Expand a basic prompt into motion-dense format
   */
  expandBasicPrompt(basicPrompt, shotData, videoDuration) {
    const is10Second = videoDuration >= 10;
    const parts = [];

    // Parse basic elements from prompt
    const hasCameraMove = /push|pull|track|pan|crane|dolly/i.test(basicPrompt);
    const hasCharacter = /\b[A-Z][a-z]+\b/.test(basicPrompt);

    // 1. Enhance camera if not present
    if (!hasCameraMove) {
      const phase = shotData.isFirst ? 'establishing' :
                    shotData.isLast ? 'resolve' : 'developing';
      const movements = this.cameraMovements[phase];
      parts.push(movements[0]);
    }

    // 2. Add the basic prompt content
    parts.push(basicPrompt);

    // 3. For 10s, add temporal expansion
    if (is10Second) {
      parts.push('The action continues to develop over the full duration with natural progression.');
      parts.push('Subtle breathing, weight shifts, and micro-expressions maintain constant motion.');
    }

    // 4. Add environment motion
    parts.push('Background elements animate subtly throughout.');

    // 5. Quality keywords
    parts.push('Cinematic, photorealistic, smooth fluid motion throughout the entire shot.');

    return parts.join(' ');
  }
};

// =============================================================================
// SCENE_CLOSURE_ENGINE - Hollywood-Quality Scene Endings
// =============================================================================
/**
 * SCENE_CLOSURE_ENGINE
 *
 * Ensures every scene ends with meaningful narrative resolution, not abrupt cuts.
 * Based on Hollywood principles of scene closure:
 *
 * 1. EMOTIONAL RESOLUTION - Character reaches a moment of clarity/change
 * 2. VISUAL FINALITY - Camera and framing signal conclusion
 * 3. TEMPORAL DECELERATION - Motion slows to a satisfying end
 * 4. NARRATIVE COMPLETION - The scene's "question" is answered
 *
 * Closure Types:
 * - RESOLUTION: Conflict resolved, peace found (most common)
 * - CLIFFHANGER: Tension held for next scene (dramatic beats)
 * - TRANSITION: Smooth handoff to next scene
 * - FINALITY: Chapter/act conclusion with full closure
 */
const SCENE_CLOSURE_ENGINE = {

  // =========================================================================
  // CLOSURE TYPE PATTERNS - Hollywood scene ending archetypes
  // =========================================================================
  closureTypes: {
    resolution: {
      name: 'Resolution',
      description: 'Conflict or tension resolves, character finds peace/understanding',
      cameraMovements: [
        'Camera slowly pulls back, widening to capture the moment of resolution',
        'Gentle crane up provides closure, rising to show the completed tableau',
        'Slow dolly out, settling into a wide shot that frames the resolution',
        'Camera holds steady as the moment breathes, then subtly pulls back'
      ],
      emotionalBeats: [
        'tension releases from the body, posture softening',
        'expression transforms from conflict to acceptance',
        'breathing deepens, shoulders drop with release',
        'eyes close briefly in acknowledgment, then open with clarity',
        'a small but meaningful smile emerges'
      ],
      temporalPatterns: [
        '[Final 3s] Movement gradually slows, finding stillness',
        '[Final 3s] Action decelerates to a moment of quiet peace',
        '[Final 3s] Motion settles like water becoming still'
      ],
      qualityKeywords: 'Peaceful resolution, satisfying conclusion, moment of clarity, emotional release.'
    },

    cliffhanger: {
      name: 'Cliffhanger',
      description: 'Tension maintained or amplified for continuation',
      cameraMovements: [
        'Camera pushes in slowly on the unresolved moment, holding tension',
        'Slow zoom into eyes or key element that holds the suspense',
        'Camera freezes on the pivotal instant, future uncertain',
        'Tracking shot halts abruptly, capturing the suspended moment'
      ],
      emotionalBeats: [
        'expression freezes in realization or shock',
        'body tenses, caught in the moment before action',
        'eyes widen as understanding dawns',
        'breath catches, held in anticipation',
        'hand reaches but does not complete the gesture'
      ],
      temporalPatterns: [
        '[Final 3s] Motion slows to near-stillness, tension palpable',
        '[Final 3s] Action suspends at the critical moment',
        '[Final 3s] Time seems to stretch as the moment hangs'
      ],
      qualityKeywords: 'Suspended tension, pivotal moment, unresolved anticipation, dramatic pause.'
    },

    transition: {
      name: 'Transition',
      description: 'Scene flows naturally into the next, maintaining momentum',
      cameraMovements: [
        'Camera continues its movement, preparing to hand off to next scene',
        'Smooth dolly that could seamlessly connect to following shot',
        'Camera arcs toward the direction of the next narrative beat',
        'Pull back that reveals the path to what comes next'
      ],
      emotionalBeats: [
        'movement continues with purpose toward next objective',
        'attention shifts toward what awaits',
        'body orients toward the next challenge or destination',
        'expression holds determination for what comes',
        'first steps toward the next scene begin'
      ],
      temporalPatterns: [
        '[Final 3s] Motion maintains momentum, flowing forward',
        '[Final 3s] Action continues seamlessly, energy preserved',
        '[Final 3s] Movement carries through to the transition point'
      ],
      qualityKeywords: 'Flowing transition, continuous momentum, narrative bridge, seamless handoff.'
    },

    finality: {
      name: 'Finality',
      description: 'Complete conclusion, chapter/act ending, definitive closure',
      cameraMovements: [
        'Grand pull back to extreme wide, showing the completed story moment',
        'Slow crane up and back, rising above the scene in conclusion',
        'Camera gradually retreats, framing becomes symmetrical and balanced',
        'Majestic wide shot settles into perfect compositional balance'
      ],
      emotionalBeats: [
        'complete stillness achieved, journey concluded',
        'final pose held with dignity and completion',
        'ultimate expression of resolution and acceptance',
        'body language signals "the end" of this chapter',
        'eyes look toward the horizon or close in final peace'
      ],
      temporalPatterns: [
        '[Final 3s] All motion comes to complete rest, perfect stillness',
        '[Final 3s] The world settles into its final configuration',
        '[Final 3s] Time seems to pause in acknowledgment of completion'
      ],
      qualityKeywords: 'Definitive ending, complete closure, final moment, epic conclusion.'
    },

    contemplative: {
      name: 'Contemplative',
      description: 'Reflective ending, character processing what occurred',
      cameraMovements: [
        'Camera holds in intimate framing as character reflects',
        'Slow, almost imperceptible push in on thoughtful expression',
        'Static shot with world moving around the still, contemplating figure',
        'Gentle orbit around the figure lost in thought'
      ],
      emotionalBeats: [
        'eyes gaze into middle distance, processing',
        'subtle expressions cross the face like passing clouds',
        'breathing becomes deep and measured in reflection',
        'hand rises to touch face or object of significance',
        'posture settles into contemplative stillness'
      ],
      temporalPatterns: [
        '[Final 3s] Internal processing visible in subtle facial changes',
        '[Final 3s] Stillness of body, activity of mind visible in eyes',
        '[Final 3s] The moment stretches as understanding deepens'
      ],
      qualityKeywords: 'Deep reflection, internal processing, meaningful pause, contemplative stillness.'
    },

    emotional_peak: {
      name: 'Emotional Peak',
      description: 'Scene ends at height of emotion, impact before resolution',
      cameraMovements: [
        'Camera holds tight on the emotional apex, not retreating',
        'Push in reaches its closest point at peak emotion',
        'Multiple angle suggestion through slight camera drift at climax',
        'Intimate framing captures every nuance of the peak moment'
      ],
      emotionalBeats: [
        'tears fall or are bravely held back',
        'embrace tightens at fullest expression',
        'voice would crack if speaking, emotion visible in throat',
        'the kiss, the touch, the connection at its deepest',
        'joy or sorrow expressed without restraint'
      ],
      temporalPatterns: [
        '[Final 3s] Emotion reaches its fullest expression',
        '[Final 3s] The peak is held, savored, fully experienced',
        '[Final 3s] Maximum emotional impact sustained through the end'
      ],
      qualityKeywords: 'Emotional climax, peak intensity, raw feeling, powerful conclusion.'
    }
  },

  // =========================================================================
  // CLOSURE DETECTION - Analyze scene to determine appropriate ending
  // =========================================================================

  /**
   * Detect the appropriate closure type based on scene context
   * @param {Object} sceneContext - Scene information
   * @param {Object} scenePosition - Position in overall story (sceneIndex, totalScenes)
   * @returns {string} Closure type key
   */
  detectClosureType(sceneContext, scenePosition = {}) {
    const visualPrompt = (sceneContext?.visualPrompt || '').toLowerCase();
    const narration = (sceneContext?.narration || '').toLowerCase();
    const sceneAction = (sceneContext?.sceneAction || '').toLowerCase();
    const combined = `${visualPrompt} ${narration} ${sceneAction}`;

    const { sceneIndex = 0, totalScenes = 1 } = scenePosition;
    const isLastScene = sceneIndex === totalScenes - 1;
    const isNearEnd = sceneIndex >= totalScenes - 2;

    // Check for explicit finality indicators (last scene, ending words)
    if (isLastScene || /\b(finally|ultimate|last|ending|conclusion|the end)\b/.test(combined)) {
      return 'finality';
    }

    // Check for cliffhanger indicators
    if (/\b(suddenly|shock|reveal|discover|realize|but then|however|twist)\b/.test(combined)) {
      return 'cliffhanger';
    }

    // Check for emotional peak indicators
    if (/\b(tears|crying|embrace|kiss|love|grief|joy|overwhelming|burst)\b/.test(combined)) {
      return 'emotional_peak';
    }

    // Check for contemplative indicators
    if (/\b(think|ponder|reflect|remember|consider|gaze|stare|wonder|meditate)\b/.test(combined)) {
      return 'contemplative';
    }

    // Check for resolution indicators
    if (/\b(peace|calm|accept|understand|resolve|forgive|release|let go|relief)\b/.test(combined)) {
      return 'resolution';
    }

    // Check for transition indicators
    if (/\b(next|continue|move|toward|begin|start|then|after)\b/.test(combined)) {
      return 'transition';
    }

    // Default based on position
    if (isNearEnd) {
      return 'resolution';
    }

    return 'transition'; // Default for mid-story scenes
  },

  // =========================================================================
  // CLOSURE PROMPT GENERATION
  // =========================================================================

  /**
   * Generate closure-enhanced prompt for the final shot
   * @param {Object} shotData - Final shot information
   * @param {Object} sceneContext - Scene context
   * @param {Object} scenePosition - Position in story
   * @param {number} videoDuration - Video duration (6 or 10)
   * @returns {string} Enhanced closure prompt
   */
  generateClosurePrompt(shotData, sceneContext, scenePosition = {}, videoDuration = 10) {
    const closureType = this.detectClosureType(sceneContext, scenePosition);
    const closure = this.closureTypes[closureType];

    if (!closure) {
      console.warn(`[SCENE_CLOSURE_ENGINE] Unknown closure type: ${closureType}, using resolution`);
      return this.generateClosurePrompt(shotData, sceneContext, scenePosition, videoDuration);
    }

    console.log(`[SCENE_CLOSURE_ENGINE] Detected closure type: ${closureType}`);

    const parts = [];

    // 1. CLOSURE CAMERA - Specific to ending type
    const cameraMove = closure.cameraMovements[Math.floor(Math.random() * closure.cameraMovements.length)];
    parts.push(cameraMove);

    // 2. CHARACTER CLOSURE - Emotional beat for the ending
    const character = this.extractPrimaryCharacter(sceneContext);
    if (character) {
      const emotionalBeat = closure.emotionalBeats[Math.floor(Math.random() * closure.emotionalBeats.length)];
      parts.push(`${character}'s ${emotionalBeat}.`);
    }

    // 3. TEMPORAL PATTERN - How time should feel at the end
    if (videoDuration >= 10) {
      const temporalPattern = closure.temporalPatterns[Math.floor(Math.random() * closure.temporalPatterns.length)];
      parts.push(temporalPattern);
    }

    // 4. EXISTING ACTION - Preserve what was in the shot
    const existingAction = shotData.actionText || shotData.narrativeBeat?.action || '';
    if (existingAction) {
      parts.push(existingAction);
    }

    // 5. CLOSURE ENVIRONMENT - World responds to the ending
    const envClosure = this.generateEnvironmentClosure(sceneContext, closureType);
    if (envClosure) {
      parts.push(envClosure);
    }

    // 6. QUALITY KEYWORDS - Closure-specific
    parts.push(closure.qualityKeywords);
    parts.push('Cinematic scene conclusion, photorealistic, smooth deceleration to stillness.');

    return parts.join(' ');
  },

  /**
   * Extract primary character name from context
   */
  extractPrimaryCharacter(sceneContext) {
    const prompt = sceneContext?.visualPrompt || '';
    const match = prompt.match(/([A-Z][a-z]+)\s+(?:sits?|stands?|walks?|looks?|gazes?)/);
    return match ? match[1] : 'The subject';
  },

  /**
   * Generate environment closure description
   */
  generateEnvironmentClosure(sceneContext, closureType) {
    const envType = this.detectEnvironmentType(sceneContext);

    const environmentClosures = {
      urban: {
        resolution: 'City lights seem to soften, the urban pulse settling into evening calm.',
        cliffhanger: 'City sounds cut to silence, the metropolis holding its breath.',
        transition: 'The city continues its rhythm as the focus shifts to what comes next.',
        finality: 'The cityscape becomes a backdrop to the completed moment, lights twinkling like stars.',
        contemplative: 'Urban noise fades to white noise, the city becoming mere backdrop to inner reflection.',
        emotional_peak: 'Even the city seems to pause, honoring the emotional moment.'
      },
      nature: {
        resolution: 'A gentle breeze stirs the leaves one last time, then settles into peace.',
        cliffhanger: 'Nature falls silent, as if waiting for what comes next.',
        transition: 'The wind picks up slightly, carrying toward the next moment.',
        finality: 'The sun breaks through clouds, bathing the scene in golden completion.',
        contemplative: 'Nature breathes softly around the still figure, patient and eternal.',
        emotional_peak: 'Even the wind seems to hold its breath for this moment.'
      },
      interior: {
        resolution: 'The room settles into comfortable stillness, dust motes drifting lazily.',
        cliffhanger: 'Shadows deepen in the corners, the room thick with unresolved tension.',
        transition: 'Light shifts toward the door, suggesting the path forward.',
        finality: 'The space achieves perfect balance, every element in its final place.',
        contemplative: 'The room becomes a sanctuary of quiet reflection.',
        emotional_peak: 'The intimate space amplifies every emotion in the silence.'
      },
      scifi: {
        resolution: 'Holographic displays fade to standby, technology yielding to the human moment.',
        cliffhanger: 'Screens flash with unread data, the future hanging in digital suspension.',
        transition: 'Systems hum with preparation for what comes next.',
        finality: 'All displays show completion, the mission concluded in synchronized stillness.',
        contemplative: 'Technology dims respectfully, allowing space for human reflection.',
        emotional_peak: 'Even the machines seem to acknowledge the weight of the moment.'
      },
      dojo: {
        resolution: 'Incense smoke rises in a straight line, inner and outer peace aligned.',
        cliffhanger: 'Candle flames freeze mid-flicker, the ancient space suspended in time.',
        transition: 'Energy flows toward the next practice, the path continuing.',
        finality: 'Perfect stillness achieved, the space resonating with completed purpose.',
        contemplative: 'The sacred space holds the moment in eternal meditation.',
        emotional_peak: 'The spiritual atmosphere intensifies, honoring the emotional truth.'
      }
    };

    const envClosures = environmentClosures[envType] || environmentClosures.interior;
    return envClosures[closureType] || envClosures.resolution;
  },

  /**
   * Detect environment type from context
   */
  detectEnvironmentType(sceneContext) {
    const prompt = (sceneContext?.visualPrompt || '').toLowerCase();

    if (/city|urban|street|neon|building|metropolis/.test(prompt)) return 'urban';
    if (/forest|nature|outdoor|garden|tree|field|mountain/.test(prompt)) return 'nature';
    if (/dojo|temple|shrine|meditation|spiritual/.test(prompt)) return 'dojo';
    if (/space|ship|station|futuristic|tech|cyber|hologram/.test(prompt)) return 'scifi';
    return 'interior';
  },

  // =========================================================================
  // INTEGRATION - Apply closure to final shot
  // =========================================================================

  /**
   * Enhance the final shot with proper closure
   * Called during shot processing to ensure meaningful scene endings
   */
  enhanceFinalShot(finalShot, sceneContext, scenePosition, videoDuration) {
    if (!finalShot.isLast) {
      console.warn('[SCENE_CLOSURE_ENGINE] enhanceFinalShot called on non-final shot');
      return finalShot;
    }

    const closurePrompt = this.generateClosurePrompt(
      finalShot,
      sceneContext,
      scenePosition,
      videoDuration
    );

    // Merge closure with existing prompt
    const existingPrompt = finalShot.videoPrompt || '';

    // If existing prompt is basic, replace entirely
    if (existingPrompt.length < 100) {
      finalShot.videoPrompt = closurePrompt;
    } else {
      // Enhance existing prompt with closure elements
      finalShot.videoPrompt = `${existingPrompt} [SCENE CLOSURE] ${closurePrompt}`;
    }

    // Add closure metadata
    finalShot.closureType = this.detectClosureType(sceneContext, scenePosition);
    finalShot.hasHollywoodClosure = true;

    console.log(`[SCENE_CLOSURE_ENGINE] Enhanced final shot with ${finalShot.closureType} closure`);

    return finalShot;
  },

  /**
   * Get a human-readable summary of the closure for UI display
   */
  getClosureSummary(closureType) {
    const summaries = {
      resolution: 'Scene resolves with emotional release and peace',
      cliffhanger: 'Scene ends with suspended tension, anticipation for what comes',
      transition: 'Scene flows naturally toward the next narrative beat',
      finality: 'Scene concludes with complete, definitive closure',
      contemplative: 'Scene ends in reflective stillness and inner processing',
      emotional_peak: 'Scene ends at the height of emotional intensity'
    };
    return summaries[closureType] || 'Scene ends with cinematic closure';
  }
};

// =============================================================================
// CROSS_SHOT_INTELLIGENCE - Shot-to-Shot Narrative Continuity
// =============================================================================
/**
 * CROSS_SHOT_INTELLIGENCE
 *
 * Ensures shots within a scene connect narratively, emotionally, and visually.
 * Creates progressive storytelling where each shot builds on the previous.
 *
 * Key principles:
 * 1. STATE PROPAGATION - Previous shot's end informs next shot's start
 * 2. EMOTIONAL MOMENTUM - Intensity builds progressively across shots
 * 3. ACTION CONTINUITY - Physical actions flow naturally between shots
 * 4. VISUAL RHYTHM - Pacing varies to create dynamic viewing experience
 * 5. NARRATIVE HOOKS - Each shot creates anticipation for the next
 */
const CROSS_SHOT_INTELLIGENCE = {

  // =========================================================================
  // EMOTIONAL INTENSITY CURVES - How emotion builds across shots
  // =========================================================================
  intensityCurves: {
    // Standard dramatic arc
    dramatic: {
      name: 'Dramatic Arc',
      description: 'Classic rising action to climax',
      getIntensity: (shotIndex, totalShots) => {
        const progress = shotIndex / (totalShots - 1);
        // Sigmoid-like curve peaking before the end
        if (progress < 0.7) {
          return 0.2 + (progress / 0.7) * 0.7; // 0.2 â†’ 0.9
        } else {
          return 0.9 - ((progress - 0.7) / 0.3) * 0.2; // 0.9 â†’ 0.7 (slight resolution)
        }
      }
    },
    // Building tension
    tension: {
      name: 'Rising Tension',
      description: 'Continuous escalation',
      getIntensity: (shotIndex, totalShots) => {
        const progress = shotIndex / (totalShots - 1);
        return 0.3 + progress * 0.7; // 0.3 â†’ 1.0
      }
    },
    // Contemplative, even pacing
    contemplative: {
      name: 'Contemplative',
      description: 'Even, meditative pacing',
      getIntensity: (shotIndex, totalShots) => {
        return 0.4 + Math.sin(shotIndex * 0.5) * 0.15; // Gentle wave around 0.4
      }
    },
    // Action sequence with peaks
    action: {
      name: 'Action Peaks',
      description: 'Multiple intensity peaks',
      getIntensity: (shotIndex, totalShots) => {
        const progress = shotIndex / (totalShots - 1);
        // Multiple peaks
        const wave = Math.sin(progress * Math.PI * 2) * 0.3;
        return 0.5 + wave + progress * 0.2;
      }
    },
    // Emotional release pattern
    emotional: {
      name: 'Emotional Journey',
      description: 'Build to emotional release',
      getIntensity: (shotIndex, totalShots) => {
        const progress = shotIndex / (totalShots - 1);
        if (progress < 0.6) {
          return 0.3 + (progress / 0.6) * 0.6; // Build
        } else {
          return 0.9 + Math.sin((progress - 0.6) * Math.PI * 2.5) * 0.1; // Emotional plateau with variation
        }
      }
    }
  },

  // =========================================================================
  // TRANSITION PHRASES - How shots connect linguistically
  // =========================================================================
  transitionPhrases: {
    // When previous shot ends with action
    action_to_action: [
      'The motion continues as',
      'Following through from the previous movement,',
      'The action flows forward as',
      'Building on the momentum,'
    ],
    // When previous shot ends statically
    static_to_action: [
      'Breaking from stillness,',
      'The moment of quiet ends as',
      'Energy returns as',
      'From the pause,'
    ],
    // When transitioning to slower moment
    action_to_static: [
      'The movement settles into',
      'Motion gives way to stillness as',
      'The energy calms into',
      'Settling from the action,'
    ],
    // When intensity increases
    build_intensity: [
      'Intensity heightens as',
      'The tension rises further as',
      'Energy amplifies as',
      'Building toward the peak,'
    ],
    // When intensity decreases
    release_intensity: [
      'The intensity begins to release as',
      'Tension eases slightly as',
      'The peak passes, giving way to',
      'Energy softens as'
    ],
    // Emotional transitions
    emotional_shift: [
      'The emotional landscape shifts as',
      'Feeling deepens as',
      'The inner state transforms as',
      'Emotion evolves as'
    ]
  },

  // =========================================================================
  // MOMENTUM MODIFIERS - Add progressive energy to prompts
  // =========================================================================
  momentumModifiers: {
    building: {
      low: ['subtle energy emerging', 'first hints of momentum', 'gentle stirring begins'],
      medium: ['growing intensity', 'building momentum', 'energy accumulating'],
      high: ['powerful forward motion', 'unstoppable momentum', 'peak energy driving forward']
    },
    sustaining: {
      low: ['maintaining gentle pace', 'steady quiet energy', 'even flow continues'],
      medium: ['consistent momentum held', 'stable energy maintained', 'rhythm sustained'],
      high: ['peak intensity sustained', 'maximum energy held', 'climactic power maintained']
    },
    releasing: {
      low: ['energy gently fading', 'soft release beginning', 'quiet settling starts'],
      medium: ['momentum easing', 'energy finding rest', 'gradual deceleration'],
      high: ['dramatic release', 'powerful resolution', 'cathartic settling']
    }
  },

  // =========================================================================
  // SHOT STATE TRACKING - Track what connects shots
  // =========================================================================

  /**
   * Analyze a shot's end state for continuity
   * @param {Object} shot - Shot data
   * @returns {Object} End state analysis
   */
  analyzeEndState(shot) {
    const endState = shot.endState || shot.narrativeBeat?.endState || '';
    const videoPrompt = shot.videoPrompt || '';
    const combined = `${endState} ${videoPrompt}`.toLowerCase();

    // Detect motion state
    const isInMotion = /moving|walking|running|reaching|turning|approaching|stepping/.test(combined);
    const isStatic = /still|frozen|paused|holding|resting|waiting|standing|sitting/.test(combined);

    // Detect emotional state
    const emotionalState = this.detectEmotionalState(combined);

    // Detect physical position
    const position = this.detectPosition(combined);

    // Detect attention/focus
    const focus = this.detectFocus(combined);

    return {
      motionState: isInMotion ? 'in_motion' : isStatic ? 'static' : 'transitional',
      emotionalState,
      position,
      focus,
      rawEndState: endState
    };
  },

  /**
   * Detect emotional state from text
   */
  detectEmotionalState(text) {
    if (/tense|anxious|worried|nervous|stressed/.test(text)) return 'tense';
    if (/calm|peaceful|serene|relaxed|at ease/.test(text)) return 'calm';
    if (/happy|joyful|excited|elated|thrilled/.test(text)) return 'joyful';
    if (/sad|sorrowful|melancholy|grieving|tearful/.test(text)) return 'sad';
    if (/angry|furious|enraged|frustrated/.test(text)) return 'angry';
    if (/determined|resolute|focused|driven/.test(text)) return 'determined';
    if (/curious|intrigued|wondering|questioning/.test(text)) return 'curious';
    if (/loving|tender|affectionate|warm/.test(text)) return 'loving';
    return 'neutral';
  },

  /**
   * Detect physical position from text
   */
  detectPosition(text) {
    if (/sitting|seated|cross-legged/.test(text)) return 'seated';
    if (/standing|upright|on feet/.test(text)) return 'standing';
    if (/lying|prone|supine|horizontal/.test(text)) return 'lying';
    if (/kneeling|crouching|bent/.test(text)) return 'kneeling';
    if (/leaning|tilted|angled/.test(text)) return 'leaning';
    return 'undefined';
  },

  /**
   * Detect attention/focus from text
   */
  detectFocus(text) {
    if (/looking at|staring at|watching|gazing at|focused on/.test(text)) {
      const match = text.match(/(?:looking at|staring at|watching|gazing at|focused on)\s+(\w+)/);
      return match ? match[1] : 'something';
    }
    if (/eyes closed|looking inward|internal/.test(text)) return 'internal';
    if (/looking away|averting|turned away/.test(text)) return 'away';
    return 'forward';
  },

  // =========================================================================
  // MAIN INTELLIGENCE METHODS
  // =========================================================================

  /**
   * Generate continuity context for a shot based on previous shot
   * @param {Object} currentShot - Current shot data
   * @param {Object} previousShot - Previous shot data (null for first shot)
   * @param {number} shotIndex - Current shot index
   * @param {number} totalShots - Total shots in scene
   * @param {Object} sceneContext - Overall scene context
   * @returns {Object} Continuity enhancement data
   */
  generateContinuityContext(currentShot, previousShot, shotIndex, totalShots, sceneContext) {
    const isFirst = shotIndex === 0;
    const isLast = shotIndex === totalShots - 1;

    // Determine intensity curve based on scene mood
    const curve = this.selectIntensityCurve(sceneContext);
    const currentIntensity = curve.getIntensity(shotIndex, totalShots);
    const previousIntensity = isFirst ? 0.2 : curve.getIntensity(shotIndex - 1, totalShots);

    // Analyze previous shot's end state
    const previousEndState = previousShot ? this.analyzeEndState(previousShot) : null;

    // Determine momentum direction
    const momentumDirection = currentIntensity > previousIntensity ? 'building' :
                              currentIntensity < previousIntensity ? 'releasing' : 'sustaining';

    // Get intensity level (low/medium/high)
    const intensityLevel = currentIntensity < 0.4 ? 'low' :
                          currentIntensity < 0.7 ? 'medium' : 'high';

    // Generate transition phrase
    const transitionPhrase = this.generateTransitionPhrase(
      previousEndState,
      momentumDirection,
      isFirst
    );

    // Generate momentum modifier
    const momentumModifier = this.momentumModifiers[momentumDirection][intensityLevel]
      [Math.floor(Math.random() * 3)];

    // Generate "from previous" context
    const fromPreviousContext = previousShot ? this.generateFromPreviousContext(previousEndState) : null;

    // Generate anticipation for next shot (if not last)
    const anticipationHook = !isLast ? this.generateAnticipationHook(currentShot, shotIndex, totalShots) : null;

    return {
      shotIndex,
      isFirst,
      isLast,
      intensity: currentIntensity,
      intensityLevel,
      momentumDirection,
      transitionPhrase,
      momentumModifier,
      fromPreviousContext,
      anticipationHook,
      previousEndState: previousEndState?.rawEndState || null,
      narrativePosition: this.getNarrativePosition(shotIndex, totalShots)
    };
  },

  /**
   * Select appropriate intensity curve based on scene context
   */
  selectIntensityCurve(sceneContext) {
    const combined = `${sceneContext?.visualPrompt || ''} ${sceneContext?.narration || ''} ${sceneContext?.sceneAction || ''}`.toLowerCase();

    if (/fight|battle|chase|action|explosive/.test(combined)) {
      return this.intensityCurves.action;
    }
    if (/meditat|contemplat|reflect|peace|calm|quiet/.test(combined)) {
      return this.intensityCurves.contemplative;
    }
    if (/emotion|tear|love|grief|joy|sorrow/.test(combined)) {
      return this.intensityCurves.emotional;
    }
    if (/tense|suspens|thriller|danger|threat/.test(combined)) {
      return this.intensityCurves.tension;
    }
    return this.intensityCurves.dramatic;
  },

  /**
   * Generate transition phrase based on previous state
   */
  generateTransitionPhrase(previousEndState, momentumDirection, isFirst) {
    if (isFirst) {
      return 'Opening the scene,';
    }

    const prevMotion = previousEndState?.motionState || 'transitional';

    // Select phrase category
    let category;
    if (momentumDirection === 'building') {
      category = 'build_intensity';
    } else if (momentumDirection === 'releasing') {
      category = 'release_intensity';
    } else if (prevMotion === 'in_motion') {
      category = 'action_to_action';
    } else if (prevMotion === 'static') {
      category = 'static_to_action';
    } else {
      category = 'emotional_shift';
    }

    const phrases = this.transitionPhrases[category];
    return phrases[Math.floor(Math.random() * phrases.length)];
  },

  /**
   * Generate context from previous shot's end state
   */
  generateFromPreviousContext(previousEndState) {
    if (!previousEndState) return null;

    const parts = [];

    // Position continuity
    if (previousEndState.position !== 'undefined') {
      parts.push(`continuing from ${previousEndState.position} position`);
    }

    // Emotional continuity
    if (previousEndState.emotionalState !== 'neutral') {
      parts.push(`carrying the ${previousEndState.emotionalState} energy`);
    }

    // Motion continuity
    if (previousEndState.motionState === 'in_motion') {
      parts.push('momentum carrying forward');
    }

    // Focus continuity
    if (previousEndState.focus && previousEndState.focus !== 'forward') {
      parts.push(`attention still ${previousEndState.focus}`);
    }

    return parts.length > 0 ? parts.join(', ') : null;
  },

  /**
   * Generate anticipation hook for next shot
   */
  generateAnticipationHook(currentShot, shotIndex, totalShots) {
    const progress = shotIndex / (totalShots - 1);

    if (progress < 0.3) {
      return 'building toward what comes next';
    } else if (progress < 0.6) {
      return 'setting up the approaching climax';
    } else if (progress < 0.85) {
      return 'tension peaks, preparing for resolution';
    } else {
      return 'final moments before closure';
    }
  },

  /**
   * Get narrative position description
   */
  getNarrativePosition(shotIndex, totalShots) {
    const progress = shotIndex / (totalShots - 1);

    if (progress === 0) return 'opening';
    if (progress < 0.25) return 'early_development';
    if (progress < 0.5) return 'rising_action';
    if (progress < 0.75) return 'approaching_peak';
    if (progress < 0.95) return 'peak_to_resolution';
    return 'closing';
  },

  // =========================================================================
  // PROMPT ENHANCEMENT - Apply intelligence to video prompts
  // =========================================================================

  /**
   * Enhance a shot's video prompt with cross-shot intelligence
   * @param {Object} shot - Shot to enhance
   * @param {Object} continuityContext - Context from generateContinuityContext
   * @returns {string} Enhanced video prompt
   */
  enhancePromptWithContinuity(shot, continuityContext) {
    const existingPrompt = shot.videoPrompt || '';
    const parts = [];

    // 1. Add transition phrase (if not first)
    if (!continuityContext.isFirst) {
      parts.push(continuityContext.transitionPhrase);
    }

    // 2. Add "from previous" context if available
    if (continuityContext.fromPreviousContext) {
      parts.push(`[From previous: ${continuityContext.fromPreviousContext}]`);
    }

    // 3. Add the existing prompt content
    parts.push(existingPrompt);

    // 4. Add momentum modifier
    parts.push(`[Momentum: ${continuityContext.momentumModifier}]`);

    // 5. Add intensity marker
    const intensityPercent = Math.round(continuityContext.intensity * 100);
    parts.push(`[Intensity: ${intensityPercent}%]`);

    // 6. Add narrative position context
    parts.push(`[Position: ${continuityContext.narrativePosition.replace(/_/g, ' ')}]`);

    // 7. Add anticipation hook if not last
    if (continuityContext.anticipationHook) {
      parts.push(`[Narrative: ${continuityContext.anticipationHook}]`);
    }

    return parts.join(' ');
  },

  // =========================================================================
  // BATCH PROCESSING - Process all shots in a scene
  // =========================================================================

  /**
   * Process all shots in a scene with cross-shot intelligence
   * @param {Array} shots - Array of shots
   * @param {Object} sceneContext - Scene context
   * @returns {Array} Enhanced shots with continuity
   */
  processSceneShots(shots, sceneContext) {
    if (!shots || shots.length === 0) return shots;

    const totalShots = shots.length;
    const enhancedShots = [];

    for (let i = 0; i < totalShots; i++) {
      const currentShot = { ...shots[i] };
      const previousShot = i > 0 ? enhancedShots[i - 1] : null;

      // Generate continuity context
      const continuityContext = this.generateContinuityContext(
        currentShot,
        previousShot,
        i,
        totalShots,
        sceneContext
      );

      // Enhance the prompt
      currentShot.videoPrompt = this.enhancePromptWithContinuity(currentShot, continuityContext);

      // Add continuity metadata to shot
      currentShot.crossShotIntelligence = {
        intensity: continuityContext.intensity,
        intensityLevel: continuityContext.intensityLevel,
        momentumDirection: continuityContext.momentumDirection,
        narrativePosition: continuityContext.narrativePosition,
        fromPrevious: continuityContext.fromPreviousContext,
        toNext: continuityContext.anticipationHook
      };

      enhancedShots.push(currentShot);
    }

    console.log(`[CROSS_SHOT_INTELLIGENCE] Processed ${totalShots} shots with narrative continuity`);

    return enhancedShots;
  },

  /**
   * Get a human-readable summary of the narrative flow
   */
  getNarrativeFlowSummary(shots) {
    if (!shots || shots.length === 0) return 'No shots to analyze';

    const positions = shots.map(s => s.crossShotIntelligence?.narrativePosition || 'unknown');
    const momentums = shots.map(s => s.crossShotIntelligence?.momentumDirection || 'sustaining');

    const buildingCount = momentums.filter(m => m === 'building').length;
    const sustainingCount = momentums.filter(m => m === 'sustaining').length;
    const releasingCount = momentums.filter(m => m === 'releasing').length;

    let arcType = 'balanced';
    if (buildingCount > releasingCount * 1.5) arcType = 'rising action dominant';
    else if (releasingCount > buildingCount * 1.5) arcType = 'resolution dominant';
    else if (sustainingCount > buildingCount + releasingCount) arcType = 'sustained intensity';

    return `Narrative arc: ${arcType}. Flow: ${positions.join(' â†’ ')}`;
  }
};

// =============================================================================
// VISUAL_CONTINUITY_ENGINE - Shot-to-Shot Visual Coherence (Inspired by Qwen workflow)
// =============================================================================
/**
 * VISUAL_CONTINUITY_ENGINE
 *
 * Ensures visual coherence across shots by implementing key principles from
 * professional video generation workflows:
 *
 * 1. VISUAL ANCHOR EXTRACTION - Lock core visual elements (character, wardrobe, lighting)
 * 2. OUTPUT â†’ INPUT CHAINING - Previous shot's output informs next shot's generation
 * 3. LAST FRAME REFERENCE - Capture last frame for video-to-video continuity
 * 4. PROMPT PARTITIONING - Separate locked elements from variable action/camera
 *
 * Based on Qwen Image Edit workflow principle:
 * "Preserve core identifiers (names, outfit, props) and lighting terms;
 *  vary only the action and camera language between scenes."
 */
const VISUAL_CONTINUITY_ENGINE = {

  // =========================================================================
  // VISUAL ANCHOR CATEGORIES - Elements that must stay consistent
  // =========================================================================
  anchorCategories: {
    character: {
      name: 'Character Identity',
      extractors: [
        /([A-Z][a-z]+(?:\s+[A-Z][a-z]+)?)\s+(?:is|stands|sits|walks|looks)/gi,
        /(?:the\s+)?(man|woman|person|figure|character)\s+(?:with|wearing|in)/gi
      ],
      preservePattern: 'Maintain exact character appearance from reference:'
    },
    wardrobe: {
      name: 'Wardrobe & Props',
      extractors: [
        /wearing\s+([^,.]+)/gi,
        /dressed\s+in\s+([^,.]+)/gi,
        /(?:with|carrying|holding)\s+(?:a\s+)?([^,.]+(?:coat|jacket|shirt|dress|suit|robe|armor|hat|glasses|bag|sword|weapon)[^,.]*)/gi
      ],
      preservePattern: 'Same clothing and props as reference:'
    },
    lighting: {
      name: 'Lighting Setup',
      extractors: [
        /((?:soft|hard|dramatic|natural|ambient|neon|golden|blue|warm|cool|dim|bright)\s+(?:light|lighting|glow|illumination))/gi,
        /(backlit|sidelit|frontlit|rim\s+light|key\s+light)/gi,
        /((?:morning|evening|sunset|sunrise|night|day)\s+light)/gi
      ],
      preservePattern: 'Maintain lighting from reference:'
    },
    colorPalette: {
      name: 'Color Palette',
      extractors: [
        /((?:warm|cool|muted|vibrant|desaturated|saturated)\s+(?:colors?|palette|tones?))/gi,
        /((?:orange|teal|blue|green|red|purple|golden|silver)\s+(?:hues?|tint|cast))/gi
      ],
      preservePattern: 'Match color palette from reference:'
    },
    environment: {
      name: 'Environment',
      extractors: [
        /(?:in|at|inside|within)\s+(?:a|the|an)?\s*([^,.]{5,40}(?:room|office|street|forest|city|building|space|station|dojo|temple|home|studio)[^,.]*)/gi,
        /((?:urban|rural|futuristic|ancient|modern|industrial|natural)\s+(?:setting|environment|backdrop))/gi
      ],
      preservePattern: 'Same environment as reference:'
    },
    style: {
      name: 'Visual Style',
      extractors: [
        /(photorealistic|cinematic|film\s+noir|anime|illustration|hyper-?realistic)/gi,
        /((?:35mm|50mm|85mm)\s+(?:film|lens|photography))/gi,
        /((?:Arri|RED|Blackmagic|IMAX)\s+(?:camera|cinematography)?)/gi
      ],
      preservePattern: 'Maintain visual style from reference:'
    }
  },

  // =========================================================================
  // ANCHOR EXTRACTION - Pull consistent elements from scene description
  // =========================================================================

  /**
   * Extract all visual anchors from a scene/shot description
   * @param {string} sceneDescription - The visual prompt or scene description
   * @param {Object} styleBible - Optional style bible for additional anchors
   * @returns {Object} Extracted anchors by category
   */
  extractVisualAnchors(sceneDescription, styleBible = null) {
    const anchors = {};
    const text = sceneDescription || '';

    // Extract from each category
    for (const [category, config] of Object.entries(this.anchorCategories)) {
      const matches = [];

      for (const extractor of config.extractors) {
        let match;
        const regex = new RegExp(extractor.source, extractor.flags);
        while ((match = regex.exec(text)) !== null) {
          const extracted = match[1] || match[0];
          if (extracted && extracted.length > 2) {
            matches.push(extracted.trim());
          }
        }
      }

      if (matches.length > 0) {
        anchors[category] = {
          values: [...new Set(matches)], // Deduplicate
          preservePattern: config.preservePattern
        };
      }
    }

    // Add style bible elements if available
    if (styleBible) {
      if (styleBible.style && !anchors.style) {
        anchors.style = {
          values: [styleBible.style],
          preservePattern: this.anchorCategories.style.preservePattern
        };
      }
      if (styleBible.colorGrade && !anchors.colorPalette) {
        anchors.colorPalette = {
          values: [styleBible.colorGrade],
          preservePattern: this.anchorCategories.colorPalette.preservePattern
        };
      }
      if (styleBible.lighting && !anchors.lighting) {
        anchors.lighting = {
          values: [styleBible.lighting],
          preservePattern: this.anchorCategories.lighting.preservePattern
        };
      }
    }

    return anchors;
  },

  /**
   * Build a locked anchor string that should be prepended to all shot prompts
   * @param {Object} anchors - Extracted anchors
   * @returns {string} Anchor preservation string
   */
  buildAnchorLockString(anchors) {
    const parts = ['[VISUAL CONTINUITY LOCK]'];

    for (const [category, data] of Object.entries(anchors)) {
      if (data.values && data.values.length > 0) {
        parts.push(`${data.preservePattern} ${data.values.join(', ')}`);
      }
    }

    parts.push('[END LOCK - Only action and camera may vary below]');

    return parts.join(' ');
  },

  // =========================================================================
  // SHOT CHAINING - Connect shot outputs to subsequent inputs
  // =========================================================================

  /**
   * Generate "from previous shot" visual context
   * This simulates the Qwen workflow's image-to-image conditioning
   * @param {Object} previousShot - Previous shot data (including generated image URL)
   * @returns {Object} Chaining context
   */
  generateChainingContext(previousShot) {
    if (!previousShot) return null;

    const context = {
      hasImageReference: !!previousShot.imageUrl,
      imageUrl: previousShot.imageUrl || null,
      endState: previousShot.endState || previousShot.narrativeBeat?.endState || null,
      visualElements: this.extractEndFrameElements(previousShot)
    };

    // Build textual description of what to maintain
    const maintainParts = [];

    if (context.visualElements.characterPosition) {
      maintainParts.push(`Character positioned ${context.visualElements.characterPosition}`);
    }
    if (context.visualElements.cameraAngle) {
      maintainParts.push(`Camera angle: ${context.visualElements.cameraAngle}`);
    }
    if (context.visualElements.lightingState) {
      maintainParts.push(`Lighting: ${context.visualElements.lightingState}`);
    }

    context.chainingPrompt = maintainParts.length > 0
      ? `[FROM PREVIOUS FRAME: ${maintainParts.join('. ')}]`
      : '[CONTINUING FROM PREVIOUS SHOT]';

    return context;
  },

  /**
   * Extract visual elements from end of previous shot
   */
  extractEndFrameElements(shot) {
    const endState = (shot.endState || shot.narrativeBeat?.endState || '').toLowerCase();
    const prompt = (shot.videoPrompt || '').toLowerCase();
    const combined = `${endState} ${prompt}`;

    return {
      characterPosition: this.detectPosition(combined),
      cameraAngle: this.detectCameraAngle(combined),
      lightingState: this.detectLightingState(combined),
      emotionalState: this.detectEmotionalState(combined)
    };
  },

  detectPosition(text) {
    if (/center|middle|foreground/.test(text)) return 'center frame';
    if (/left/.test(text)) return 'frame left';
    if (/right/.test(text)) return 'frame right';
    if (/background|distant/.test(text)) return 'background';
    return null;
  },

  detectCameraAngle(text) {
    if (/close-?up|closeup|tight/.test(text)) return 'close-up';
    if (/wide|establishing/.test(text)) return 'wide';
    if (/medium/.test(text)) return 'medium';
    if (/over.?shoulder|ots/.test(text)) return 'over-shoulder';
    return null;
  },

  detectLightingState(text) {
    if (/bright|lit|illuminate/.test(text)) return 'well-lit';
    if (/dark|shadow|dim/.test(text)) return 'low-light';
    if (/dramatic|contrast/.test(text)) return 'dramatic contrast';
    return null;
  },

  detectEmotionalState(text) {
    if (/tense|anxious|nervous/.test(text)) return 'tense';
    if (/calm|peaceful|serene/.test(text)) return 'calm';
    if (/happy|joyful|excited/.test(text)) return 'joyful';
    if (/sad|sorrowful/.test(text)) return 'sad';
    return 'neutral';
  },

  // =========================================================================
  // LAST FRAME CAPTURE INSTRUCTIONS - For video-to-video continuity
  // =========================================================================

  /**
   * Generate instructions for capturing and using last frame
   * This is metadata for the video generation process
   */
  generateLastFrameInstructions(shotIndex, totalShots, isLast) {
    if (isLast) {
      return {
        captureLastFrame: false, // No need for final shot
        reason: 'Final shot of scene - no subsequent shot to chain'
      };
    }

    return {
      captureLastFrame: true,
      captureTimingMs: 'final_frame', // Capture at end of video
      useFor: 'next_shot_reference',
      reason: 'Chain visual continuity to next shot',
      instructions: [
        'Capture the final rendered frame of this video',
        'Store as reference image for Shot ' + (shotIndex + 2),
        'Use as first_frame_image OR as visual conditioning context'
      ]
    };
  },

  // =========================================================================
  // PROMPT PARTITIONING - Separate locked vs variable elements
  // =========================================================================

  /**
   * Partition a prompt into locked (consistent) and variable (action/camera) parts
   * @param {string} fullPrompt - Complete shot prompt
   * @param {Object} anchors - Visual anchors to preserve
   * @returns {Object} Partitioned prompt
   */
  partitionPrompt(fullPrompt, anchors) {
    // The locked part comes from anchors
    const lockedPart = this.buildAnchorLockString(anchors);

    // The variable part is action and camera description
    // Try to extract just the action/camera portions
    const actionPatterns = [
      /(?:camera|shot)\s+(?:pushes?|pulls?|tracks?|pans?|tilts?|cranes?|dollies?)[^.]+/gi,
      /\[\d+s-\d+s\][^[]+/g, // Temporal segments
      /(?:walks?|runs?|sits?|stands?|moves?|reaches?|turns?|looks?)[^.]+/gi
    ];

    let variablePart = fullPrompt;

    // Remove anchor content from variable part to avoid duplication
    for (const [category, data] of Object.entries(anchors)) {
      if (data.values) {
        for (const value of data.values) {
          variablePart = variablePart.replace(new RegExp(value, 'gi'), '');
        }
      }
    }

    return {
      locked: lockedPart,
      variable: variablePart.trim(),
      combined: `${lockedPart} ${variablePart.trim()}`
    };
  },

  // =========================================================================
  // SCENE PROCESSING - Apply continuity to all shots in a scene
  // =========================================================================

  /**
   * Process all shots in a scene with visual continuity
   * @param {Array} shots - Array of shot objects
   * @param {Object} sceneContext - Scene context (visualPrompt, styleBible, etc.)
   * @returns {Array} Enhanced shots with continuity data
   */
  processSceneWithContinuity(shots, sceneContext) {
    if (!shots || shots.length === 0) return shots;

    // Extract global anchors from scene description
    const globalAnchors = this.extractVisualAnchors(
      sceneContext?.visualPrompt || '',
      sceneContext?.styleBible || null
    );

    const anchorLockString = this.buildAnchorLockString(globalAnchors);

    console.log(`[VISUAL_CONTINUITY_ENGINE] Extracted ${Object.keys(globalAnchors).length} anchor categories`);

    const enhancedShots = [];

    for (let i = 0; i < shots.length; i++) {
      const shot = { ...shots[i] };
      const previousShot = i > 0 ? enhancedShots[i - 1] : null;
      const isLast = i === shots.length - 1;

      // 1. Add chaining context from previous shot
      const chainingContext = this.generateChainingContext(previousShot);

      // 2. Add last frame capture instructions
      const lastFrameInstructions = this.generateLastFrameInstructions(i, shots.length, isLast);

      // 3. Enhance the video prompt with continuity elements
      let enhancedPrompt = shot.videoPrompt || '';

      // Prepend anchor lock (if not already present)
      if (!enhancedPrompt.includes('[VISUAL CONTINUITY LOCK]')) {
        enhancedPrompt = `${anchorLockString} ${enhancedPrompt}`;
      }

      // Add chaining context (if not first shot)
      if (chainingContext && !shot.isFirst) {
        enhancedPrompt = `${chainingContext.chainingPrompt} ${enhancedPrompt}`;
      }

      shot.videoPrompt = enhancedPrompt;

      // 4. Add continuity metadata
      shot.visualContinuity = {
        globalAnchors: i === 0 ? globalAnchors : null, // Only store on first shot
        chainingContext: chainingContext,
        lastFrameCapture: lastFrameInstructions,
        anchorCategories: Object.keys(globalAnchors)
      };

      enhancedShots.push(shot);
    }

    console.log(`[VISUAL_CONTINUITY_ENGINE] Processed ${shots.length} shots with visual continuity`);

    return enhancedShots;
  },

  // =========================================================================
  // UTILITIES - Helper methods
  // =========================================================================

  /**
   * Get a summary of visual continuity for logging/UI
   */
  getContinuitySummary(shots) {
    if (!shots || shots.length === 0) return 'No shots to analyze';

    const firstShot = shots[0];
    const anchors = firstShot?.visualContinuity?.globalAnchors || {};
    const categories = Object.keys(anchors);

    const chainedCount = shots.filter(s => s.visualContinuity?.chainingContext).length;
    const captureCount = shots.filter(s => s.visualContinuity?.lastFrameCapture?.captureLastFrame).length;

    return `Visual continuity: ${categories.length} anchor categories locked, ${chainedCount} shots chained, ${captureCount} frames to capture`;
  }
};

// =============================================================================
// NARRATIVE_BEAT_GENERATOR - Legacy compatibility wrapper
// =============================================================================
/**
 * NARRATIVE_BEAT_GENERATOR - Now wraps STORY_BEAT_DECOMPOSER for backward compatibility
 */
const NARRATIVE_BEAT_GENERATOR = {
  /**
   * Extract characters, their states, and possible actions from scene prompt
   */
  extractNarrativeElements(visualPrompt) {
    if (!visualPrompt) return this.getDefaultNarrativeElements();

    const prompt = visualPrompt;
    const promptLower = prompt.toLowerCase();

    // Extract character names (capitalized words that appear with verbs)
    const characterPatterns = [
      /([A-Z][a-z]+)\s+(?:sits|stands|walks|looks|gazes|stares|holds|reaches|moves|turns|watches|waits|breathes|leans)/gi,
      /(?:Foreground|Midground|Background):\s*([A-Z][a-z]+)/gi,
      /([A-Z][a-z]+)'s?\s+(?:face|eyes|hands|expression|posture|body)/gi
    ];

    const characters = [];
    const seenNames = new Set();
    for (const pattern of characterPatterns) {
      let match;
      while ((match = pattern.exec(prompt)) !== null) {
        const name = match[1];
        if (!seenNames.has(name.toLowerCase()) && name.length > 2) {
          seenNames.add(name.toLowerCase());
          characters.push({ name, rawContext: match[0] });
        }
      }
    }

    // Extract character states/poses
    const statePatterns = [
      { pattern: /sits?\s+(?:cross-legged|down|quietly|still)/gi, state: 'sitting', motion: 'subtle breathing, slight shifts' },
      { pattern: /stands?\s+(?:nearby|still|tall|watching)/gi, state: 'standing', motion: 'weight shifts, subtle movement' },
      { pattern: /look(?:s|ing)\s+(?:troubled|worried|concerned|thoughtful)/gi, state: 'troubled', motion: 'tension in shoulders, furrowed brow' },
      { pattern: /look(?:s|ing)\s+(?:hopeful|happy|relieved|peaceful)/gi, state: 'hopeful', motion: 'expression softening, relaxing posture' },
      { pattern: /walk(?:s|ing)|approach(?:es|ing)|step(?:s|ping)/gi, state: 'moving', motion: 'steps forward, approaching' },
      { pattern: /reach(?:es|ing)|extend(?:s|ing)\s+(?:hand|arm)/gi, state: 'reaching', motion: 'hand extends toward' },
      { pattern: /gaz(?:es|ing)|star(?:es|ing)/gi, state: 'gazing', motion: 'eyes focused, contemplative' },
      { pattern: /hold(?:s|ing)|grip(?:s|ping)/gi, state: 'holding', motion: 'grip tightens or loosens' },
      { pattern: /turn(?:s|ing)|pivot(?:s|ing)/gi, state: 'turning', motion: 'body rotates, head turns' }
    ];

    const characterStates = [];
    for (const char of characters) {
      const states = [];
      const possibleMotions = [];
      for (const { pattern, state, motion } of statePatterns) {
        if (pattern.test(char.rawContext) || pattern.test(prompt)) {
          states.push(state);
          possibleMotions.push(motion);
        }
      }
      characterStates.push({
        name: char.name,
        states: states.length > 0 ? states : ['present'],
        possibleMotions: possibleMotions.length > 0 ? possibleMotions : ['subtle movement']
      });
    }

    // Extract environment elements that can animate
    const environmentElements = [];
    const envPatterns = [
      { pattern: /candle|flame|fire/gi, element: 'candles', motion: 'flames flicker and dance' },
      { pattern: /hologram|holographic|display/gi, element: 'holograms', motion: 'displays pulse and shift' },
      { pattern: /light|glow|illuminate/gi, element: 'lighting', motion: 'light shifts and plays across surfaces' },
      { pattern: /shadow/gi, element: 'shadows', motion: 'shadows lengthen and move' },
      { pattern: /dust|particle|mote/gi, element: 'particles', motion: 'particles drift through the air' },
      { pattern: /smoke|mist|fog/gi, element: 'atmosphere', motion: 'wisps curl and drift' },
      { pattern: /water|rain|ripple/gi, element: 'water', motion: 'water ripples and flows' },
      { pattern: /wind|breeze/gi, element: 'wind', motion: 'air currents stir the scene' },
      { pattern: /curtain|fabric|cloth/gi, element: 'fabric', motion: 'fabric sways gently' },
      { pattern: /tree|leaf|plant|foliage/gi, element: 'vegetation', motion: 'leaves rustle softly' }
    ];

    for (const { pattern, element, motion } of envPatterns) {
      if (pattern.test(prompt)) {
        environmentElements.push({ element, motion });
      }
    }

    // Detect scene mood/tone
    const mood = /troubled|tense|worried|anxious|conflict/.test(promptLower) ? 'tense' :
                 /peaceful|calm|serene|gentle/.test(promptLower) ? 'peaceful' :
                 /hopeful|warm|tender|love/.test(promptLower) ? 'warm' :
                 /mysterious|eerie|dark|ominous/.test(promptLower) ? 'mysterious' :
                 /energetic|action|fast|intense/.test(promptLower) ? 'dynamic' : 'neutral';

    // Detect environment type
    const envType = /dojo|temple|shrine/.test(promptLower) ? 'dojo' :
                    /city|urban|street|building/.test(promptLower) ? 'urban' :
                    /forest|nature|outdoor|garden/.test(promptLower) ? 'nature' :
                    /space|ship|station|futuristic/.test(promptLower) ? 'scifi' :
                    /room|interior|inside|office/.test(promptLower) ? 'interior' : 'generic';

    return {
      characters: characterStates,
      environmentElements,
      mood,
      envType,
      originalPrompt: prompt
    };
  },

  getDefaultNarrativeElements() {
    return {
      characters: [],
      environmentElements: [],
      mood: 'neutral',
      envType: 'generic',
      originalPrompt: ''
    };
  },

  /**
   * Generate STORY-DRIVEN action prompts for each shot
   * Each prompt describes SPECIFIC motion that happens in that shot
   */
  generateProgressivePrompts(shotCount, sceneData, shotSequence) {
    const { visualPrompt, narration } = sceneData;
    const narrative = this.extractNarrativeElements(visualPrompt);
    const prompts = [];

    // Generate specific actions for each shot based on the narrative
    const shotActions = this.generateShotActions(shotCount, narrative);

    // Camera movement descriptions
    const cameraDescriptions = {
      'static': 'Steady shot',
      'slow_push': 'Slowly pushing in',
      'slow_pull': 'Gradually pulling back',
      'tracking': 'Smooth tracking',
      'pan': 'Panning across',
      'tilt': 'Tilting',
      'crane_up': 'Rising crane shot',
      'crane_down': 'Descending crane shot',
      'handheld': 'Organic handheld movement',
      'orbit': 'Orbiting around'
    };

    for (let i = 0; i < shotCount; i++) {
      const shot = shotSequence[i] || {};
      const isFirst = i === 0;
      const isLast = i === shotCount - 1;
      const action = shotActions[i];

      // Get camera movement
      const cameraMove = cameraDescriptions[shot.cameraMovement] || 'Smooth movement';

      // Build the specific video prompt
      const videoPrompt = this.buildStoryDrivenPrompt(
        action,
        cameraMove,
        narrative,
        { isFirst, isLast, shotIndex: i, totalShots: shotCount }
      );

      prompts.push({
        videoPrompt,
        arcPosition: action.arcPosition,
        shotIndex: i,
        motionDescription: action.summary
      });
    }

    return prompts;
  },

  /**
   * Generate specific actions for each shot based on extracted narrative
   */
  generateShotActions(shotCount, narrative) {
    const { characters, environmentElements, mood } = narrative;
    const actions = [];

    // Define progression phases
    const getPhase = (index, total) => {
      if (total === 1) return 'complete';
      if (index === 0) return 'establish';
      if (index === total - 1) return 'resolve';
      const progress = index / (total - 1);
      if (progress < 0.4) return 'develop';
      if (progress < 0.7) return 'build';
      return 'peak';
    };

    for (let i = 0; i < shotCount; i++) {
      const phase = getPhase(i, shotCount);

      // Generate character-specific actions based on phase
      let characterAction = '';
      let envAction = '';

      if (characters.length > 0) {
        characterAction = this.getCharacterActionForPhase(characters, phase, i, shotCount);
      }

      if (environmentElements.length > 0) {
        envAction = this.getEnvironmentActionForPhase(environmentElements, phase);
      }

      // Create summary for UI
      const summary = this.createActionSummary(phase, characterAction, characters);

      actions.push({
        arcPosition: phase,
        characterAction,
        envAction,
        summary,
        mood
      });
    }

    return actions;
  },

  /**
   * Get specific character action for the current phase
   */
  getCharacterActionForPhase(characters, phase, shotIndex, totalShots) {
    const primaryChar = characters[0];
    const secondaryChar = characters[1];

    // Phase-specific character actions
    const phaseActions = {
      establish: {
        sitting: (char) => `${char.name} breathes slowly, slight tension visible in the shoulders`,
        standing: (char) => `${char.name} stands still, gaze fixed, subtle weight shift`,
        troubled: (char) => `${char.name}'s expression shows concern, brow slightly furrowed`,
        moving: (char) => `${char.name} enters the frame with measured steps`,
        default: (char) => `${char.name} is present in the scene, subtle breathing motion`
      },
      develop: {
        sitting: (char) => `${char.name}'s posture shifts slightly, hands move subtly`,
        standing: (char) => `${char.name} takes a small step, body language opening up`,
        troubled: (char) => `${char.name}'s tension increases, hands clench slightly`,
        reaching: (char) => `${char.name} begins to extend a hand forward`,
        default: (char) => `${char.name} shows subtle movement, energy building`
      },
      build: {
        sitting: (char) => `${char.name} leans forward with growing attention`,
        standing: (char) => `${char.name} moves closer, purpose in each step`,
        troubled: (char) => `${char.name}'s internal conflict visible in expression`,
        reaching: (char) => `${char.name}'s hand extends further, almost touching`,
        default: (char) => `${char.name}'s movement intensifies`
      },
      peak: {
        sitting: (char) => `${char.name} looks up, a moment of connection`,
        standing: (char) => `${char.name} makes definitive contact, hand on shoulder`,
        troubled: (char) => `${char.name}'s expression shifts, breakthrough moment`,
        reaching: (char) => `${char.name} makes contact, the gesture complete`,
        default: (char) => `${char.name} in the moment of greatest intensity`
      },
      resolve: {
        sitting: (char) => `${char.name}'s shoulders relax, breath deepens`,
        standing: (char) => `${char.name} settles into stillness, moment of peace`,
        troubled: (char) => `${char.name}'s expression softens, tension releasing`,
        default: (char) => `${char.name} finds a moment of resolution`
      },
      complete: {
        default: (char) => `${char.name} moves through the complete emotional arc`
      }
    };

    // Get primary character action
    let action = '';
    if (primaryChar) {
      const phaseAction = phaseActions[phase] || phaseActions.establish;
      const state = primaryChar.states[0] || 'default';
      const actionFn = phaseAction[state] || phaseAction.default;
      action = actionFn(primaryChar);
    }

    // Add secondary character action for interaction shots
    if (secondaryChar && (phase === 'develop' || phase === 'build' || phase === 'peak')) {
      const secondaryState = secondaryChar.states[0] || 'standing';
      if (secondaryState === 'standing' || secondaryState === 'moving') {
        action += `. ${secondaryChar.name} responds with subtle movement`;
      } else if (secondaryState === 'reaching') {
        action += `. ${secondaryChar.name}'s hand extends in support`;
      }
    }

    return action;
  },

  /**
   * Get environment animation for the current phase
   */
  getEnvironmentActionForPhase(elements, phase) {
    const intensity = {
      establish: 'gently',
      develop: 'steadily',
      build: 'increasingly',
      peak: 'dramatically',
      resolve: 'softly',
      complete: 'continuously'
    };

    const adverb = intensity[phase] || 'subtly';
    const envActions = elements.slice(0, 2).map(e => {
      const motion = e.motion.replace(/^[a-z]+/, adverb);
      return motion;
    });

    return envActions.join(', ');
  },

  /**
   * Create a summary for UI display
   */
  createActionSummary(phase, characterAction, characters) {
    if (characters.length === 0) {
      const summaries = {
        establish: 'Scene opens with establishing motion',
        develop: 'Scene develops with progressive action',
        build: 'Tension and movement intensify',
        peak: 'Scene reaches dramatic peak',
        resolve: 'Scene resolves to stillness',
        complete: 'Complete scene in one shot'
      };
      return summaries[phase] || 'Scene progresses';
    }

    const char = characters[0].name;
    const summaries = {
      establish: `${char} - opening moment`,
      develop: `${char} - action develops`,
      build: `${char} - intensity builds`,
      peak: `${char} - dramatic moment`,
      resolve: `${char} - resolution`,
      complete: `${char} - complete arc`
    };
    return summaries[phase] || `${char} in motion`;
  },

  /**
   * Build the final story-driven video prompt
   */
  buildStoryDrivenPrompt(action, cameraMove, narrative, meta) {
    const parts = [];

    // 1. Camera movement with context
    if (meta.isFirst) {
      parts.push(`${cameraMove} to establish the scene`);
    } else if (meta.isLast) {
      parts.push(`${cameraMove} for the closing moment`);
    } else {
      parts.push(cameraMove);
    }

    // 2. The main character action (THE MOST IMPORTANT PART)
    if (action.characterAction) {
      parts.push(action.characterAction);
    }

    // 3. Environment animation
    if (action.envAction) {
      parts.push(action.envAction);
    }

    // 4. Mood-appropriate pacing
    const pacingByMood = {
      tense: 'Measured, deliberate movement with underlying tension',
      peaceful: 'Gentle, flowing motion with calm energy',
      warm: 'Soft, inviting movement with emotional warmth',
      mysterious: 'Subtle, enigmatic motion with anticipation',
      dynamic: 'Energetic, purposeful movement',
      neutral: 'Smooth, natural motion'
    };
    parts.push(pacingByMood[action.mood] || pacingByMood.neutral);

    // 5. Continuity hint for frame-chained shots (not first shot)
    if (!meta.isFirst) {
      parts.push('Continuing seamlessly from previous frame');
    }

    return parts.join('. ') + '.';
  },

  // ============================================
  // LEGACY COMPATIBILITY METHODS
  // ============================================

  /**
   * Legacy method - now uses new story-driven system
   */
  generateBeats(shotCount, sceneData) {
    const dummySequence = Array(shotCount).fill(null).map((_, i) => ({
      shotType: i === 0 ? 'establishing_wide' : i === shotCount - 1 ? 'medium_closeup' : 'medium',
      cameraMovement: i === 0 ? 'slow_push' : 'tracking'
    }));

    const progressivePrompts = this.generateProgressivePrompts(shotCount, sceneData, dummySequence);

    return progressivePrompts.map((p, idx) => ({
      shotIndex: idx,
      isFirst: idx === 0,
      isLast: idx === shotCount - 1,
      progress: shotCount > 1 ? idx / (shotCount - 1) : 0.5,
      characterMotion: { motionDescription: p.motionDescription },
      cameraMotion: { movement: dummySequence[idx].cameraMovement },
      environmentMotion: [],
      narrativeContext: { mood: 'neutral' },
      nextShotHook: idx < shotCount - 1 ? 'continues to next moment' : null,
      videoPrompt: p.videoPrompt,
      arcPosition: p.arcPosition
    }));
  },

  /**
   * Legacy method - now uses story-driven system
   */
  buildVideoPrompt(beat, shot, styleBible, visualPrompt = '') {
    if (beat.videoPrompt) {
      return beat.videoPrompt;
    }

    // Generate on the fly using new system
    const narrative = this.extractNarrativeElements(visualPrompt);
    const phase = beat.isFirst ? 'establish' :
                  beat.isLast ? 'resolve' :
                  beat.progress < 0.4 ? 'develop' :
                  beat.progress < 0.7 ? 'build' : 'peak';

    const action = {
      arcPosition: phase,
      characterAction: narrative.characters.length > 0 ?
        this.getCharacterActionForPhase(narrative.characters, phase, beat.shotIndex || 0, 1) : '',
      envAction: narrative.environmentElements.length > 0 ?
        this.getEnvironmentActionForPhase(narrative.environmentElements, phase) : '',
      mood: narrative.mood
    };

    const cameraDescriptions = {
      'static': 'Steady shot',
      'slow_push': 'Slowly pushing in',
      'slow_pull': 'Gradually pulling back',
      'tracking': 'Smooth tracking'
    };
    const cameraMove = cameraDescriptions[shot.cameraMovement] || 'Smooth movement';

    return this.buildStoryDrivenPrompt(
      action,
      cameraMove,
      narrative,
      { isFirst: beat.isFirst, isLast: beat.isLast, shotIndex: beat.shotIndex || 0, totalShots: 1 }
    );
  }
};

/**
 * SHOT_DECOMPOSITION_ENGINE
 *
 * Professional cinematography breaks scenes into multiple shots for visual interest.
 * A 30-second scene typically contains 4-8 different shots that when combined
 * create a cohesive, dynamic viewing experience.
 *
 * This engine decomposes scene descriptions into shot sequences while maintaining
 * visual consistency across all shots in the scene.
 */
const SHOT_DECOMPOSITION_ENGINE = {
  // Shot type library with cinematographic purpose
  shotTypes: {
    'establishing_wide': {
      name: 'Establishing Wide',
      description: 'Sets the scene, shows full environment',
      cameraDistance: 'far',
      typicalDuration: { min: 2, max: 4 },
      promptPrefix: '[Extreme wide establishing shot]',
      useCases: ['scene_opening', 'location_change', 'time_passage'],
      psychology: 'Context, scale, isolation'
    },
    'wide': {
      name: 'Wide Shot',
      description: 'Full body visible with environment context',
      cameraDistance: 'far',
      typicalDuration: { min: 2, max: 4 },
      promptPrefix: '[Wide shot, full body]',
      useCases: ['action', 'movement', 'group_interaction'],
      psychology: 'Action clarity, spatial awareness'
    },
    'medium_wide': {
      name: 'Medium Wide',
      description: 'Waist up, balances subject and environment',
      cameraDistance: 'medium',
      typicalDuration: { min: 2, max: 3 },
      promptPrefix: '[Medium wide shot, waist up]',
      useCases: ['walking', 'dialogue_with_context', 'transitions'],
      psychology: 'Balance, natural conversation distance'
    },
    'medium': {
      name: 'Medium Shot',
      description: 'Standard conversational framing',
      cameraDistance: 'medium',
      typicalDuration: { min: 1.5, max: 3 },
      promptPrefix: '[Medium shot, chest to head]',
      useCases: ['dialogue', 'reactions', 'standard_coverage'],
      psychology: 'Engagement, intimacy without intrusion'
    },
    'medium_closeup': {
      name: 'Medium Close-up',
      description: 'Shoulders and head, emotional focus',
      cameraDistance: 'close',
      typicalDuration: { min: 1.5, max: 2.5 },
      promptPrefix: '[Medium close-up, shoulders and head]',
      useCases: ['emotional_dialogue', 'important_moments', 'character_focus'],
      psychology: 'Emotional connection, importance'
    },
    'closeup': {
      name: 'Close-up',
      description: 'Face fills frame, maximum emotion',
      cameraDistance: 'close',
      typicalDuration: { min: 1, max: 2 },
      promptPrefix: '[Close-up shot, face fills frame]',
      useCases: ['emotional_peaks', 'reveals', 'reactions'],
      psychology: 'Intense emotion, vulnerability, truth'
    },
    'extreme_closeup': {
      name: 'Extreme Close-up',
      description: 'Single feature (eye, hand, object)',
      cameraDistance: 'very_close',
      typicalDuration: { min: 0.5, max: 1.5 },
      promptPrefix: '[Extreme close-up, detail shot]',
      useCases: ['symbolic_moments', 'tension', 'detail_reveals'],
      psychology: 'Symbolism, tension, obsession'
    },
    'insert': {
      name: 'Insert Shot',
      description: 'Object or detail relevant to action',
      cameraDistance: 'close',
      typicalDuration: { min: 0.5, max: 1.5 },
      promptPrefix: '[Insert shot, object detail]',
      useCases: ['props', 'hands', 'meaningful_objects'],
      psychology: 'Narrative importance, foreshadowing'
    },
    'over_shoulder': {
      name: 'Over-the-Shoulder',
      description: 'POV from behind one character looking at another',
      cameraDistance: 'medium',
      typicalDuration: { min: 1.5, max: 2.5 },
      promptPrefix: '[Over-the-shoulder shot]',
      useCases: ['dialogue', 'confrontation', 'connection'],
      psychology: 'Perspective, dialogue intimacy'
    },
    'reaction': {
      name: 'Reaction Shot',
      description: 'Character responding to something',
      cameraDistance: 'close',
      typicalDuration: { min: 1, max: 2 },
      promptPrefix: '[Reaction shot, close-up]',
      useCases: ['responses', 'reveals', 'emotional_beats'],
      psychology: 'Audience identification, emotional mirror'
    },
    'pov': {
      name: 'Point of View',
      description: 'What the character sees',
      cameraDistance: 'varies',
      typicalDuration: { min: 1, max: 3 },
      promptPrefix: '[POV shot, first-person perspective]',
      useCases: ['discovery', 'reveals', 'immersion'],
      psychology: 'Immersion, identification, discovery'
    }
  },

  // Camera movements for each shot type
  cameraMovements: {
    'static': { name: 'Static', description: 'Camera locked in place', energy: 'calm' },
    'slow_push': { name: 'Slow Push In', description: 'Gradual move toward subject', energy: 'building' },
    'slow_pull': { name: 'Slow Pull Out', description: 'Gradual move away from subject', energy: 'releasing' },
    'tracking': { name: 'Tracking', description: 'Camera follows subject movement', energy: 'dynamic' },
    'pan': { name: 'Pan', description: 'Camera rotates horizontally', energy: 'revealing' },
    'tilt': { name: 'Tilt', description: 'Camera rotates vertically', energy: 'revealing' },
    'crane_up': { name: 'Crane Up', description: 'Camera rises vertically', energy: 'epic' },
    'crane_down': { name: 'Crane Down', description: 'Camera descends vertically', energy: 'grounding' },
    'handheld': { name: 'Handheld', description: 'Subtle organic movement', energy: 'intimate' },
    'orbit': { name: 'Orbit', description: 'Camera circles subject', energy: 'dramatic' }
  },

  // Scene type patterns - what shots work well for different scene types
  scenePatterns: {
    'action': {
      name: 'Action Sequence',
      recommendedShots: ['wide', 'medium', 'closeup', 'insert'],
      pacing: 'fast',
      avgShotsPerScene: 5,
      shotDurationMultiplier: 0.7
    },
    'dialogue': {
      name: 'Dialogue Scene',
      recommendedShots: ['medium', 'over_shoulder', 'closeup', 'reaction'],
      pacing: 'moderate',
      avgShotsPerScene: 4,
      shotDurationMultiplier: 1.0
    },
    'emotional': {
      name: 'Emotional Moment',
      recommendedShots: ['medium_closeup', 'closeup', 'extreme_closeup', 'reaction'],
      pacing: 'slow',
      avgShotsPerScene: 3,
      shotDurationMultiplier: 1.3
    },
    'establishing': {
      name: 'Establishing Scene',
      recommendedShots: ['establishing_wide', 'wide', 'medium_wide'],
      pacing: 'slow',
      avgShotsPerScene: 2,
      shotDurationMultiplier: 1.5
    },
    'revelation': {
      name: 'Reveal/Discovery',
      recommendedShots: ['medium', 'pov', 'reaction', 'closeup'],
      pacing: 'building',
      avgShotsPerScene: 4,
      shotDurationMultiplier: 1.1
    },
    'montage': {
      name: 'Montage Sequence',
      recommendedShots: ['wide', 'medium', 'closeup', 'insert'],
      pacing: 'fast',
      avgShotsPerScene: 6,
      shotDurationMultiplier: 0.6
    },
    'contemplative': {
      name: 'Contemplative/Reflective',
      recommendedShots: ['wide', 'medium_closeup', 'closeup'],
      pacing: 'very_slow',
      avgShotsPerScene: 2,
      shotDurationMultiplier: 1.5
    }
  },

  // Transition types between shots
  shotTransitions: {
    'cut': { name: 'Cut', description: 'Instant transition', energy: 'neutral' },
    'match_cut': { name: 'Match Cut', description: 'Visual element matches next shot', energy: 'connected' },
    'dissolve': { name: 'Dissolve', description: 'Gradual blend', energy: 'dreamy' },
    'l_cut': { name: 'L-Cut', description: 'Audio continues over visual cut', energy: 'continuous' },
    'j_cut': { name: 'J-Cut', description: 'Next audio starts before visual cut', energy: 'anticipatory' }
  },

  /**
   * Analyze scene type from description
   */
  analyzeSceneType(sceneDescription) {
    const desc = sceneDescription.toLowerCase();

    // Action keywords
    if (desc.match(/fight|chase|run|escape|battle|action|attack|defend|sprint/)) {
      return 'action';
    }
    // Dialogue keywords
    if (desc.match(/talk|speak|conversation|discuss|argue|negotiate|explain/)) {
      return 'dialogue';
    }
    // Emotional keywords
    if (desc.match(/cry|mourn|grieve|love|embrace|tender|emotional|heartbreak|joy/)) {
      return 'emotional';
    }
    // Establishing keywords
    if (desc.match(/arrive|enter|morning|location|city|landscape|establish/)) {
      return 'establishing';
    }
    // Revelation keywords
    if (desc.match(/discover|reveal|find|realize|truth|secret|uncover/)) {
      return 'revelation';
    }
    // Contemplative keywords
    if (desc.match(/think|ponder|contemplate|reflect|alone|silent|stare/)) {
      return 'contemplative';
    }
    // Default to dialogue
    return 'dialogue';
  },

  /**
   * Calculate optimal number of shots based on scene duration
   */
  calculateShotCount(sceneDuration, sceneType) {
    const pattern = this.scenePatterns[sceneType];
    const baseShotCount = pattern.avgShotsPerScene;

    // Adjust based on duration (assume 5s per shot baseline)
    const durationFactor = sceneDuration / (baseShotCount * 5);

    // Calculate shot count, clamped between 2 and 8
    return Math.max(2, Math.min(8, Math.round(baseShotCount * durationFactor)));
  },

  /**
   * Generate shot sequence for a scene
   * IMPORTANT: Minimax only supports 6s or 10s video clips
   */
  generateShotSequence(sceneType, shotCount, sceneDuration, clipDuration = 10) {
    const pattern = this.scenePatterns[sceneType];
    const recommendedShots = pattern.recommendedShots;
    const sequence = [];

    // MINIMAX CONSTRAINT: Each shot must be exactly 6s or 10s
    // The clipDuration parameter should be 6 or 10 - DEFAULT TO 10s
    const validClipDuration = clipDuration === 6 ? 6 : 10;

    for (let i = 0; i < shotCount; i++) {
      // Cycle through recommended shots
      const shotTypeKey = recommendedShots[i % recommendedShots.length];
      const shotType = this.shotTypes[shotTypeKey];

      // FIXED: Shot duration is ALWAYS the Minimax clip duration (6s or 10s)
      const shotDuration = validClipDuration;

      // Determine camera movement based on shot type and position
      const isLastShot = (i === shotCount - 1);
      let cameraMovement = 'static';
      if (i === 0) cameraMovement = 'slow_push'; // Opening shot
      else if (isLastShot) cameraMovement = 'slow_pull'; // Closing shot
      else if (pattern.pacing === 'fast') cameraMovement = 'tracking';
      else if (shotTypeKey.includes('closeup')) cameraMovement = 'static';

      sequence.push({
        shotIndex: i + 1,
        shotType: shotTypeKey,
        shotTypeName: shotType.name,
        duration: shotDuration, // ALWAYS 6 or 10 seconds
        cameraMovement: cameraMovement,
        promptPrefix: shotType.promptPrefix,
        purpose: shotType.useCases[0],
        transition: isLastShot ? 'cut' : 'cut'
      });
    }

    // Log the Hollywood math
    console.log(`[HollywoodDecomposition] Scene: ${sceneDuration}s â†’ ${shotCount} shots Ã— ${validClipDuration}s = ${shotCount * validClipDuration}s total`);

    return sequence;
  },

  /**
   * LOCAL shot prompt generation - no AI required
   * Uses smart mathematical decomposition of the scene prompt
   *
   * NEW: Generates SEPARATE prompts for IMAGE and VIDEO generation:
   * - imagePrompt: Visual composition (environment, lighting, framing) for Imagen
   * - videoPrompt: ACTION and MOTION focused prompt for Minimax video
   *
   * @param {string} scenePrompt - Visual description of the scene
   * @param {string} narration - Narration text for this scene
   * @param {Array} shotSequence - Array of shot objects from generateShotSequence
   * @param {object} styleBible - Style settings
   * @param {Array} characterBible - Character definitions (MINIMAL use for video - let S2V-01 handle faces)
   * @param {object} sceneActionData - { sceneAction, actionBlueprint } for motion generation
   */
  generateLocalShotPrompts(scenePrompt, narration, shotSequence, styleBible, characterBible, sceneActionData = {}) {
    // Extract key elements from scene prompt for IMAGE generation
    const extractedElements = this.extractSceneElements(scenePrompt);

    // ================================================================
    // NEW: Use STORY_BEAT_DECOMPOSER for video prompts
    // This decomposes sceneAction into connected shots with END STATES
    // Each shot has: START â†’ ACTION â†’ END (capture point) structure
    // ================================================================
    const storyBeats = STORY_BEAT_DECOMPOSER.decomposeIntoShots(
      sceneActionData.sceneAction,  // Primary source: rich action sequence
      shotSequence.length,
      {
        visualPrompt: scenePrompt,
        narration: narration,
        characters: characterBible
      }
    );

    // Convert to legacy beat format for backward compatibility with the rest of the code
    const narrativeBeats = storyBeats.map((beat, idx) => ({
      shotIndex: idx,
      isFirst: beat.isFirst,
      isLast: beat.isLast,
      progress: shotSequence.length > 1 ? idx / (shotSequence.length - 1) : 0.5,
      characterMotion: { motionDescription: beat.motionDescription },
      cameraMotion: { movement: shotSequence[idx]?.cameraMovement || 'tracking' },
      environmentMotion: [],
      narrativeContext: { mood: 'neutral' },
      nextShotHook: beat.isLast ? null : 'Frame chains to next shot',
      videoPrompt: beat.videoPrompt,  // START â†’ ACTION â†’ END structured prompt
      arcPosition: beat.isFirst ? 'establish' : beat.isLast ? 'resolve' : 'develop',
      // NEW: Expose end state for UI display
      endState: beat.endState,
      startState: beat.startState
    }));

    // Generate prompts for each shot
    return shotSequence.map((shot, idx) => {
      const isFirst = idx === 0;
      const isLast = idx === shotSequence.length - 1;
      const progress = idx / (shotSequence.length - 1 || 1); // 0 to 1
      const beat = narrativeBeats[idx] || {};

      // ========================================
      // IMAGE PROMPT (for Imagen - visual composition)
      // Focus: Environment, lighting, composition, framing
      // Minimal character description - let Character Bible handle it
      // ========================================
      let imagePrompt = shot.promptPrefix + ' ';

      // Build the scene description based on shot type
      if (shot.shotType.includes('establishing') || shot.shotType === 'wide' || shot.shotType === 'medium_wide') {
        imagePrompt += extractedElements.environment + ' ';
        imagePrompt += extractedElements.atmosphere + ' ';
        if (extractedElements.subject) {
          imagePrompt += `${extractedElements.subject} shown FULL BODY from head to feet, standing in the scene. `;
          imagePrompt += 'Ensure entire body is visible, no cropping of heads or limbs. ';
        }
      } else if (shot.shotType.includes('closeup') || shot.shotType === 'extreme_closeup') {
        if (extractedElements.subject) {
          imagePrompt += `Close-up view of ${extractedElements.subject}, face and upper body clearly visible. `;
        }
        imagePrompt += extractedElements.emotionalContext + ' ';
        imagePrompt += 'Sharp focus on facial features and expression, head fully in frame. ';
      } else {
        if (extractedElements.subject) {
          imagePrompt += extractedElements.subject + ', shown from waist up with head clearly visible. ';
        }
        imagePrompt += `with ${extractedElements.environment} visible in background. `;
        imagePrompt += 'Character fully framed, no cropping of head or shoulders. ';
      }

      // Add lighting and atmosphere
      imagePrompt += extractedElements.lighting + ' ';

      // Add style bible elements for visual consistency
      if (styleBible && styleBible.enabled) {
        if (styleBible.style) imagePrompt += styleBible.style + ' ';
        if (styleBible.colorGrade) imagePrompt += styleBible.colorGrade + ' ';
        if (styleBible.lighting) imagePrompt += 'Lighting: ' + styleBible.lighting + ' ';
        if (styleBible.atmosphere) imagePrompt += styleBible.atmosphere + ' ';
      }

      // MINIMAL character info for image - just name reference, not full description
      // The Character Bible portraits + S2V-01 subject_reference handles face consistency
      if (characterBible && characterBible.length > 0 && beat.characterMotion?.who) {
        const charName = beat.characterMotion.who;
        imagePrompt += `${charName} clearly visible in frame. `;
      }

      // Technical quality for images
      imagePrompt += 'Cinematic quality, 4K resolution, professional lighting, shallow depth of field. ';

      // Emotional context from narration
      if (narration && beat.narrativeContext?.mood && beat.narrativeContext.mood !== 'neutral') {
        imagePrompt += `Mood: ${beat.narrativeContext.mood}. `;
      }

      // ========================================
      // VIDEO PROMPT (for Minimax - ACTION focused)
      // Focus: What MOVES, what CHANGES, what HAPPENS
      // NO character appearance details - S2V-01 uses subject_reference
      // Pass scenePrompt as fallback in case no action data exists
      // ========================================
      const videoPrompt = NARRATIVE_BEAT_GENERATOR.buildVideoPrompt(beat, shot, styleBible, scenePrompt);

      // ========================================
      // Legacy prompt (combined, for backward compatibility)
      // ========================================
      const legacyPrompt = imagePrompt.trim();

      return {
        ...shot,
        // NEW: Separate prompts for image and video
        imagePrompt: imagePrompt.trim(),
        videoPrompt: videoPrompt,
        // Legacy: Keep 'prompt' for backward compatibility
        prompt: legacyPrompt,
        // Narrative beat info for UI display
        narrativeBeat: {
          action: beat.characterMotion?.action || null,
          motionDescription: beat.characterMotion?.motionDescription || null,
          startState: beat.characterMotion?.startState || null,
          endState: beat.characterMotion?.endState || null,
          transitionHook: beat.nextShotHook || null,
          mood: beat.narrativeContext?.mood || 'neutral',
          progress: beat.progress
        },
        purpose: this.getShotPurpose(shot.shotType, isFirst, isLast),
        focusElement: this.getFocusElement(shot.shotType, extractedElements)
      };
    });
  },

  /**
   * Extract key visual elements from scene description
   */
  extractSceneElements(scenePrompt) {
    const prompt = scenePrompt || '';

    // Extract subject (person/character)
    const subjectPatterns = [
      /([A-Z][a-z]+(?:\s+[A-Z][a-z]+)?)\s+(?:stands|sits|walks|runs|looks|gazes|stares)/i,
      /(?:a|the|an)\s+(man|woman|person|figure|character|protagonist|hero)\s/i,
      /([A-Z][a-z]+)\s+is\s/i
    ];
    let subject = '';
    for (const pattern of subjectPatterns) {
      const match = prompt.match(pattern);
      if (match) {
        subject = match[1] || match[0];
        break;
      }
    }

    // Extract environment
    const envPatterns = [
      /(?:in|at|on|inside|within|outside)\s+(?:a|the|an)?\s*([^,.]+(?:room|city|street|forest|building|office|space|landscape|metropolis|rooftop|window|chamber)[^,.]*)/i,
      /(?:sprawling|vast|dark|bright|neon|urban|rural)\s+([^,.]+)/i
    ];
    let environment = 'the scene';
    for (const pattern of envPatterns) {
      const match = prompt.match(pattern);
      if (match) {
        environment = match[1] || match[0];
        break;
      }
    }

    // Extract atmosphere/mood
    const atmospherePatterns = [
      /(dramatic|moody|serene|tense|peaceful|chaotic|mysterious|ethereal|gritty|cinematic)\s*(?:lighting|atmosphere|mood|tone)?/i,
      /(?:lighting|atmosphere|mood):\s*([^,.]+)/i
    ];
    let atmosphere = 'cinematic atmosphere';
    for (const pattern of atmospherePatterns) {
      const match = prompt.match(pattern);
      if (match) {
        atmosphere = match[1] || match[0];
        break;
      }
    }

    // Extract lighting
    const lightingPatterns = [
      /((?:neon|natural|harsh|soft|dramatic|golden|blue|warm|cold|dim|bright)\s*(?:light|lighting|lit|glow))/i,
      /(?:lit by|illuminated by|bathed in)\s+([^,.]+)/i
    ];
    let lighting = 'cinematic lighting';
    for (const pattern of lightingPatterns) {
      const match = prompt.match(pattern);
      if (match) {
        lighting = match[1] || match[0];
        break;
      }
    }

    // Extract action/movement
    const actionPatterns = [
      /(\w+ing)\s+(?:through|across|toward|into|out|over)/i,
      /(?:stands|sits|walks|gazes|looks|stares|moves|runs)\s+([^,.]+)/i
    ];
    let action = '';
    for (const pattern of actionPatterns) {
      const match = prompt.match(pattern);
      if (match) {
        action = match[0];
        break;
      }
    }

    // Extract emotional context
    const emotionPatterns = [
      /(contemplative|intense|peaceful|anxious|determined|hopeful|sad|joyful|focused|reflective)/i,
      /(?:expression|face|eyes)\s+(?:showing|revealing|filled with)?\s*([^,.]+)/i
    ];
    let emotionalContext = 'focused expression';
    for (const pattern of emotionPatterns) {
      const match = prompt.match(pattern);
      if (match) {
        emotionalContext = match[1] || match[0];
        break;
      }
    }

    return {
      subject: subject || 'the main subject',
      environment,
      atmosphere,
      lighting,
      action: action || 'in the scene',
      emotionalContext
    };
  },

  getShotPurpose(shotType, isFirst, isLast) {
    if (isFirst) return 'Establish scene and context';
    if (isLast) return 'Conclude scene with emotional emphasis';

    const purposes = {
      'establishing_wide': 'Set the scene and show full environment',
      'wide': 'Show action and spatial relationships',
      'medium_wide': 'Balance subject and environment',
      'medium': 'Standard coverage for dialogue and action',
      'medium_closeup': 'Emotional engagement with subject',
      'closeup': 'Capture emotion and detail',
      'extreme_closeup': 'Intense emotional moment or detail',
      'over_shoulder': 'Create connection between characters',
      'pov': 'Subjective viewer experience',
      'insert': 'Highlight important detail or object',
      'two_shot': 'Show relationship between two subjects',
      'reaction': 'Capture response to action'
    };
    return purposes[shotType] || 'Scene coverage';
  },

  getFocusElement(shotType, elements) {
    if (shotType.includes('closeup')) return elements.subject || 'subject face';
    if (shotType.includes('wide') || shotType.includes('establishing')) return elements.environment;
    if (shotType === 'insert') return 'key detail';
    return elements.subject || 'main subject';
  }
};

/**
 * creationWizardDecomposeSceneToShots - Decomposes a scene into multiple cinematic shots
 *
 * Uses AI-POWERED decomposition when sceneAction is rich enough, with rule-based fallback.
 * - AI mode: GPT intelligently divides sceneAction into shots with capture points
 * - Fallback mode: Mathematical decomposition using cinematographic formulas
 *
 * Each shot has: START â†’ ACTION â†’ END structure for frame-chain continuity
 */
exports.creationWizardDecomposeSceneToShots = functions
  .runWith({
    timeoutSeconds: 60, // Increased for AI decomposition
    memory: '512MB'
  })
  .https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const {
    scene,           // { id, visualPrompt, narration, duration, ... }
    targetShotCount, // Optional: override automatic shot count
    clipDuration: inputClipDuration, // Video clip duration (6 or 10 seconds from Minimax)
    genre,
    productionMode,
    styleBible,      // For visual consistency
    characterBible,  // For character consistency
    // NEW: Cross-scene continuity (Upgrade 3)
    previousSceneContext, // { endState, sceneId, transitionType } from previous scene
    sceneIndex,      // Current scene index (0-based)
    totalScenes      // Total number of scenes in video
  } = data;

  if (!scene || !scene.visualPrompt) {
    throw new functions.https.HttpsError('invalid-argument', 'Scene with visualPrompt required');
  }

  // Scene duration - now supports Hollywood-style longer scenes (30-60s)
  const sceneDuration = scene.duration || scene.visualDuration || 35;
  const sceneDescription = scene.visualPrompt || scene.visual || '';
  const narration = scene.narration || '';

  // Clip duration from Minimax (6s or 10s) - DEFAULT TO 10s for more complex actions
  const clipDuration = inputClipDuration || 10;

  // NEW: Extract action data for video prompt generation
  // This includes sceneAction (high-level what happens) and actionBlueprint (detailed motion)
  const sceneActionData = {
    sceneAction: scene.sceneAction || scene.action || null,
    actionBlueprint: scene.actionBlueprint || null
  };

  try {
    // STEP 1: Analyze scene type based on keywords
    const sceneType = SHOT_DECOMPOSITION_ENGINE.analyzeSceneType(sceneDescription);

    // STEP 2: Calculate optimal shot count using HOLLYWOOD FORMULA
    // Formula: shots = ceil(sceneDuration / clipDuration)
    // This ensures each shot matches Minimax video clip duration
    const hollywoodShotCount = Math.ceil(sceneDuration / clipDuration);

    // Also consider scene type for fine-tuning
    const sceneTypeAdjustment = SHOT_DECOMPOSITION_ENGINE.calculateShotCount(sceneDuration, sceneType);

    // Use targetShotCount if provided, otherwise use Hollywood calculation
    // Clamp between 2 and 10 shots (for Hollywood-style scenes)
    const shotCount = Math.min(Math.max(targetShotCount || hollywoodShotCount, 2), 10);

    console.log(`[creationWizardDecomposeSceneToShots] Hollywood Shot Math:
      Scene Duration: ${sceneDuration}s
      Clip Duration: ${clipDuration}s
      Hollywood Shot Count: ${hollywoodShotCount}
      Scene Type Adjustment: ${sceneTypeAdjustment}
      Final Shot Count: ${shotCount}`);

    // STEP 3: Generate shot sequence with EXACT Minimax clip duration
    // Each shot duration = clipDuration (exactly 6s or 10s from Minimax)
    const baseSequence = SHOT_DECOMPOSITION_ENGINE.generateShotSequence(sceneType, shotCount, sceneDuration, clipDuration);

    console.log(`[creationWizardDecomposeSceneToShots] Scene ${scene.id}: ${sceneType} type, ${shotCount} shots, ${sceneDuration}s duration`);

    // ================================================================
    // STEP 4: AI-POWERED or RULE-BASED decomposition
    // Use AI when sceneAction is rich enough (> 50 chars)
    // ================================================================
    const sceneAction = sceneActionData.sceneAction;
    const useAI = sceneAction && sceneAction.trim().length >= 50;
    let storyBeats = null;
    let decompositionMethod = 'rule-based';

    // Track scene-level continuity data for return
    let sceneEndState = null;
    let suggestedNextSceneTransition = 'cut';

    if (useAI) {
      console.log(`[creationWizardDecomposeSceneToShots] Using AI decomposition for rich sceneAction (${sceneAction.length} chars)`);
      try {
        // Build cross-scene context for AI (Upgrade 3)
        const crossSceneContext = previousSceneContext?.endState ? {
          previousSceneEndState: previousSceneContext.endState,
          previousSceneId: previousSceneContext.sceneId,
          transitionType: previousSceneContext.transitionType || scene.transition?.type || 'cut'
        } : null;

        if (crossSceneContext) {
          console.log(`[creationWizardDecomposeSceneToShots] Cross-scene context: transitioning from scene ${crossSceneContext.previousSceneId}`);
        }

        const aiResult = await STORY_BEAT_DECOMPOSER.decomposeWithAI(
          sceneAction,
          shotCount,
          clipDuration,
          {
            visualPrompt: sceneDescription,
            narration: narration,
            characters: characterBible,
            sceneId: scene.id,
            sceneIndex: sceneIndex,
            totalScenes: totalScenes
          },
          openai,
          crossSceneContext  // Pass cross-scene context (Upgrade 3)
        );

        // Handle new return format (object with shots array)
        if (aiResult && aiResult.shots) {
          storyBeats = aiResult.shots;
          sceneEndState = aiResult.sceneEndState;
          suggestedNextSceneTransition = aiResult.suggestedNextSceneTransition || 'cut';
          decompositionMethod = storyBeats[0]?.aiGenerated ? 'ai-powered' : 'rule-based-fallback';
        } else if (Array.isArray(aiResult)) {
          // Backward compatibility: if it returns just an array
          storyBeats = aiResult;
          decompositionMethod = storyBeats[0]?.aiGenerated ? 'ai-powered' : 'rule-based-fallback';
        }

        console.log(`[creationWizardDecomposeSceneToShots] AI decomposition result: ${storyBeats?.length || 0} shots (method: ${decompositionMethod})`);
      } catch (aiError) {
        console.error(`[creationWizardDecomposeSceneToShots] AI decomposition failed:`, aiError.message);
        storyBeats = null; // Will trigger fallback below
      }
    }

    // FALLBACK: Generate LOCAL prompts using rule-based decomposition
    const shotsWithPrompts = storyBeats
      ? baseSequence.map((shot, idx) => {
          // Merge base shot data with AI-generated story beats
          const beat = storyBeats[idx] || {};
          return {
            ...shot,
            videoPrompt: beat.videoPrompt || shot.videoPrompt,
            narrativeBeat: {
              action: beat.actionText,
              startState: beat.startState,
              endState: beat.endState,
              captureDescription: beat.captureDescription,
              motionDescription: beat.motionDescription,
              aiGenerated: beat.aiGenerated || false
            }
          };
        })
      : SHOT_DECOMPOSITION_ENGINE.generateLocalShotPrompts(
          sceneDescription,
          narration,
          baseSequence,
          styleBible,
          characterBible,
          sceneActionData
        );

    console.log(`[creationWizardDecomposeSceneToShots] Final decomposition method: ${decompositionMethod}`);

    // STEP 5: Extract consistency anchors from original prompt
    const extractedElements = SHOT_DECOMPOSITION_ENGINE.extractSceneElements(sceneDescription);
    const consistencyAnchors = {
      lighting: extractedElements.lighting,
      colorPalette: 'Consistent with original scene colors',
      atmosphere: extractedElements.atmosphere,
      characterAppearance: extractedElements.subject !== 'the main subject' ? extractedElements.subject : null,
      environmentKey: extractedElements.environment
    };

    // STEP 5.5: ENHANCED VIDEO PROMPTS for 10-second videos
    // Use ENHANCED_VIDEO_PROMPT_BUILDER for motion-dense, temporally-progressive prompts
    const is10SecondVideo = clipDuration >= 10;
    if (is10SecondVideo) {
      console.log(`[creationWizardDecomposeSceneToShots] Enhancing video prompts for 10-second clips using ENHANCED_VIDEO_PROMPT_BUILDER`);

      const sceneContext = {
        visualPrompt: sceneDescription,
        narration: narration,
        styleBible: styleBible
      };

      shotsWithPrompts.forEach((shot, idx) => {
        const isFirst = idx === 0;
        const isLast = idx === shotsWithPrompts.length - 1;

        const shotData = {
          ...shot,
          isFirst,
          isLast,
          actionText: shot.narrativeBeat?.action || shot.videoPrompt,
          arcPosition: isFirst ? 'establish' : isLast ? 'resolve' :
                       idx < shotsWithPrompts.length / 3 ? 'develop' :
                       idx < (shotsWithPrompts.length * 2) / 3 ? 'build' : 'peak'
        };

        // Generate enhanced motion-dense prompt
        const enhancedPrompt = ENHANCED_VIDEO_PROMPT_BUILDER.generateEnhancedPrompt(
          shotData,
          sceneContext,
          clipDuration
        );

        // Update the shot's video prompt
        shot.videoPrompt = enhancedPrompt;

        console.log(`[creationWizardDecomposeSceneToShots] Shot ${idx + 1} enhanced prompt (${enhancedPrompt.length} chars): ${enhancedPrompt.substring(0, 100)}...`);
      });
    }

    // STEP 5.55: HOLLYWOOD CHOREOGRAPHY for enriched scenes
    // Apply 4-beat choreography when scene has choreography enrichment data
    const hasChoreography = HOLLYWOOD_CHOREOGRAPHER.shouldUseChoreography(scene);
    if (hasChoreography) {
      console.log(`[creationWizardDecomposeSceneToShots] Applying Hollywood Choreography for enriched scene ${scene.id}`);

      try {
        // Generate choreography for each shot
        const choreographedShots = HOLLYWOOD_CHOREOGRAPHER.generateMultiShotChoreography(
          scene,
          shotsWithPrompts.length,
          previousSceneContext,  // Previous scene for continuity
          null,  // Next scene (not available here)
          styleBible || {}
        );

        // Apply choreographed prompts to shots
        shotsWithPrompts.forEach((shot, idx) => {
          const choreography = choreographedShots[idx];
          if (choreography && choreography.prompt) {
            // Enhance the video prompt with choreography data
            const existingPrompt = shot.videoPrompt || '';

            // Build enhanced prompt with choreography
            shot.videoPrompt = `${choreography.prompt}\n\n[ORIGINAL SCENE CONTEXT]\n${existingPrompt}`;

            // Add choreography metadata
            shot.hollywoodChoreography = {
              hasEnrichment: choreography.hasEnrichment,
              captureFrame: choreography.captureFrame,
              sceneArc: choreography.choreographyData?.sceneInfo?.sceneArc,
              shotIndex: choreography.shotIndex,
              totalShots: choreography.totalShots
            };

            console.log(`[creationWizardDecomposeSceneToShots] Shot ${idx + 1} choreographed (${choreography.hasEnrichment ? 'enriched' : 'fallback'}): capture at beat ${choreography.captureFrame?.beat}`);
          }
        });

        console.log(`[creationWizardDecomposeSceneToShots] Hollywood Choreography applied to ${shotsWithPrompts.length} shots`);
      } catch (choreographyError) {
        console.error(`[creationWizardDecomposeSceneToShots] Hollywood Choreography failed, continuing without:`, choreographyError.message);
        // Continue without choreography - fallback to existing prompts
      }
    } else {
      console.log(`[creationWizardDecomposeSceneToShots] Scene ${scene.id} lacks choreography enrichment, skipping Hollywood Choreographer`);
    }

    // STEP 5.6: HOLLYWOOD SCENE CLOSURE for final shot
    // Use SCENE_CLOSURE_ENGINE to ensure meaningful narrative resolution
    if (shotsWithPrompts.length > 0) {
      const finalShotIndex = shotsWithPrompts.length - 1;
      const finalShot = shotsWithPrompts[finalShotIndex];

      // Build scene context for closure detection
      const closureSceneContext = {
        visualPrompt: sceneDescription,
        narration: narration,
        sceneAction: sceneActionData.sceneAction || ''
      };

      // Build scene position info
      const scenePosition = {
        sceneIndex: sceneIndex || 0,
        totalScenes: totalScenes || 1
      };

      // Mark the shot as final
      finalShot.isLast = true;

      // Apply Hollywood closure enhancement
      SCENE_CLOSURE_ENGINE.enhanceFinalShot(
        finalShot,
        closureSceneContext,
        scenePosition,
        clipDuration
      );

      console.log(`[creationWizardDecomposeSceneToShots] Applied ${finalShot.closureType || 'default'} closure to final shot`);
    }

    // STEP 5.7: CROSS-SHOT INTELLIGENCE - Connect shots narratively
    // Use CROSS_SHOT_INTELLIGENCE to ensure shots build progressively
    const crossShotSceneContext = {
      visualPrompt: sceneDescription,
      narration: narration,
      sceneAction: sceneActionData.sceneAction || ''
    };

    const intelligentShots = CROSS_SHOT_INTELLIGENCE.processSceneShots(
      shotsWithPrompts,
      crossShotSceneContext
    );

    // Get narrative flow summary for logging
    const narrativeFlow = CROSS_SHOT_INTELLIGENCE.getNarrativeFlowSummary(intelligentShots);
    console.log(`[creationWizardDecomposeSceneToShots] ${narrativeFlow}`);

    // STEP 5.8: VISUAL CONTINUITY - Lock visual anchors across shots
    // Based on Qwen workflow principle: "preserve core identifiers, vary only action/camera"
    const visualContinuityContext = {
      visualPrompt: sceneDescription,
      styleBible: styleBible
    };

    const continuityShots = VISUAL_CONTINUITY_ENGINE.processSceneWithContinuity(
      intelligentShots,
      visualContinuityContext
    );

    // Get continuity summary for logging
    const continuitySummary = VISUAL_CONTINUITY_ENGINE.getContinuitySummary(continuityShots);
    console.log(`[creationWizardDecomposeSceneToShots] ${continuitySummary}`);

    // STEP 5.9: OPENING SCENE WORLD-FIRST STRUCTURE
    // Apply Hollywood "World-First" structure to opening scenes
    // Rule: Opening scenes must establish WORLD before CHARACTER
    let worldFirstShots = continuityShots;
    const currentSceneIndex = sceneIndex ?? 0;

    if (OPENING_SCENE_HANDLER.isOpeningScene(scene, currentSceneIndex)) {
      console.log(`[creationWizardDecomposeSceneToShots] Scene ${scene.id} is OPENING SCENE - applying World-First structure`);

      try {
        // Apply World-First enhancement to shots
        worldFirstShots = OPENING_SCENE_HANDLER.applyWorldFirstStructure(
          continuityShots,
          scene,
          currentSceneIndex
        );

        // Validate World-First compliance
        const validation = OPENING_SCENE_HANDLER.validateWorldFirst(worldFirstShots, currentSceneIndex);
        if (!validation.valid) {
          console.warn(`[creationWizardDecomposeSceneToShots] World-First warnings:`, validation.warnings);
        }

        console.log(`[creationWizardDecomposeSceneToShots] World-First structure applied: ${worldFirstShots.length} shots enhanced for cinematic opening`);
      } catch (openingError) {
        console.error(`[creationWizardDecomposeSceneToShots] World-First enhancement failed:`, openingError.message);
        // Continue with original shots
        worldFirstShots = continuityShots;
      }
    }

    // STEP 5.95: CINEMATIC PHYSICS ENHANCEMENT
    // Add Hollywood-level force/momentum physics to video prompts
    // Based on 2025 AI filmmaking research: "Describe forces, not appearances"
    const physicsEnhancedShots = worldFirstShots.map((shot, idx) => {
      try {
        // Get intensity from choreography or default based on position
        const choreographyIntensity = shot.hollywoodChoreography?.captureFrame?.intensity;
        const positionIntensity = idx === 0 ? 0.4 : idx === worldFirstShots.length - 1 ? 0.6 : 0.5 + (idx * 0.1);
        const intensity = choreographyIntensity ?? positionIntensity;

        // Generate physics enhancement
        const physicsEnhancement = CINEMATIC_PHYSICS_ENGINE.generatePhysicsEnhancement(
          scene,
          shot,
          intensity
        );

        // Get physics summary for metadata
        const physicsSummary = CINEMATIC_PHYSICS_ENGINE.getPhysicsSummary(scene, shot, intensity);

        // CRITICAL: Add CONDENSED physics hint to video prompt (keeps quality, stays short)
        // Full enhancement stored as metadata for reference
        const existingVideoPrompt = shot.videoPrompt || '';
        const condensedPhysicsHint = CINEMATIC_PHYSICS_ENGINE.generateCondensedHint(scene, shot, intensity);
        const enhancedVideoPrompt = condensedPhysicsHint
          ? `${existingVideoPrompt} ${condensedPhysicsHint}`
          : existingVideoPrompt;

        return {
          ...shot,
          videoPrompt: enhancedVideoPrompt, // Add condensed hint (max ~120 chars)
          cinematicPhysics: {
            applied: true,
            intensity,
            fullEnhancement: physicsEnhancement, // Store full text as metadata
            condensedHint: condensedPhysicsHint,
            ...physicsSummary
          }
        };
      } catch (physicsError) {
        console.warn(`[creationWizardDecomposeSceneToShots] Physics enhancement failed for shot ${idx + 1}:`, physicsError.message);
        return {
          ...shot,
          cinematicPhysics: { applied: false, error: physicsError.message }
        };
      }
    });

    console.log(`[creationWizardDecomposeSceneToShots] Cinematic Physics applied to ${physicsEnhancedShots.filter(s => s.cinematicPhysics?.applied).length}/${physicsEnhancedShots.length} shots`);

    // STEP 5.96: CHARACTER REFERENCE ENHANCEMENT
    // Add character consistency hints to video prompts
    // Based on 2025 AI filmmaking: "Character consistency is the #1 priority"
    const characterEnhancedShots = physicsEnhancedShots.map((shot, idx) => {
      try {
        // Generate character enhancement
        const characterEnhancement = CHARACTER_REFERENCE_ENGINE.generateCharacterEnhancement(
          scene,
          shot,
          characterBible || []
        );

        // Get character summary for metadata
        const characterSummary = CHARACTER_REFERENCE_ENGINE.getCharacterSummary(
          characterBible || [],
          scene,
          shot
        );

        // CRITICAL: Add CONDENSED character hint to video prompt (keeps consistency, stays short)
        // Full enhancement stored as metadata for reference
        const existingVideoPrompt = shot.videoPrompt || '';
        const condensedCharacterHint = CHARACTER_REFERENCE_ENGINE.generateCondensedHint(scene, shot, characterBible || []);
        const enhancedVideoPrompt = condensedCharacterHint
          ? `${existingVideoPrompt} ${condensedCharacterHint}`
          : existingVideoPrompt;

        return {
          ...shot,
          videoPrompt: enhancedVideoPrompt, // Add condensed hint (max ~100 chars)
          characterReference: {
            applied: characterEnhancement.length > 0,
            fullEnhancement: characterEnhancement, // Store full text as metadata
            condensedHint: condensedCharacterHint,
            ...characterSummary
          }
        };
      } catch (charError) {
        console.warn(`[creationWizardDecomposeSceneToShots] Character enhancement failed for shot ${idx + 1}:`, charError.message);
        return {
          ...shot,
          characterReference: { applied: false, error: charError.message }
        };
      }
    });

    console.log(`[creationWizardDecomposeSceneToShots] Character Reference applied to ${characterEnhancedShots.filter(s => s.characterReference?.applied).length}/${characterEnhancedShots.length} shots`);

    // STEP 5.97: AUDIO BEAT MAPPING
    // Add sound design hints to video prompts for AI audio generation
    // Based on 2025 AI filmmaking: Native audio generation is now expected
    const audioEnhancedShots = characterEnhancedShots.map((shot, idx) => {
      try {
        // Calculate beat index for 4-beat system (each shot maps to a beat phase)
        const totalShots = characterEnhancedShots.length;
        const beatIndex = Math.min(Math.floor(idx / (totalShots / 4)), 3);

        // Generate audio enhancement
        const audioEnhancement = AUDIO_BEAT_ENGINE.generateAudioEnhancement(
          scene,
          shot,
          beatIndex
        );

        // Get audio summary for metadata
        const audioSummary = AUDIO_BEAT_ENGINE.getAudioSummary(scene, shot, beatIndex);

        // CRITICAL: Add CONDENSED audio hint to video prompt (keeps mood/sound context, stays short)
        // Full enhancement stored as metadata for reference
        const existingVideoPrompt = shot.videoPrompt || '';
        const condensedAudioHint = AUDIO_BEAT_ENGINE.generateCondensedHint(scene, shot, beatIndex);
        const enhancedVideoPrompt = condensedAudioHint
          ? `${existingVideoPrompt} ${condensedAudioHint}`
          : existingVideoPrompt;

        return {
          ...shot,
          videoPrompt: enhancedVideoPrompt, // Add condensed hint (max ~80 chars)
          audioBeat: {
            applied: true,
            beatIndex,
            fullEnhancement: audioEnhancement, // Store full text as metadata
            condensedHint: condensedAudioHint,
            ...audioSummary
          }
        };
      } catch (audioError) {
        console.warn(`[creationWizardDecomposeSceneToShots] Audio enhancement failed for shot ${idx + 1}:`, audioError.message);
        return {
          ...shot,
          audioBeat: { applied: false, error: audioError.message }
        };
      }
    });

    console.log(`[creationWizardDecomposeSceneToShots] Audio Beat Mapping applied to ${audioEnhancedShots.filter(s => s.audioBeat?.applied).length}/${audioEnhancedShots.length} shots`);

    // STEP 5.98: BEAT TIMELINE SYNTHESIS
    // Create temporal choreography for each shot - the Hollywood differentiator
    // This synthesizes physics, character, and audio data into a precise timeline
    const timelineEnhancedShots = audioEnhancedShots.map((shot, idx) => {
      try {
        // Generate the beat timeline prompt using all collected data
        const timelinePrompt = BEAT_TIMELINE_ENGINE.generateTimelinePrompt(
          shot,
          scene,
          shot.cinematicPhysics || {},
          shot.characterReference || {},
          shot.audioBeat || {}
        );

        // Get timeline metadata
        const timeline = BEAT_TIMELINE_ENGINE.generateTimeline(
          shot,
          scene,
          shot.cinematicPhysics || {},
          shot.characterReference || {},
          shot.audioBeat || {}
        );

        // Add timeline to video prompt (this is the key enhancement)
        const existingVideoPrompt = shot.videoPrompt || '';
        const enhancedVideoPrompt = timelinePrompt
          ? `${existingVideoPrompt}\n\n${timelinePrompt}`
          : existingVideoPrompt;

        return {
          ...shot,
          videoPrompt: enhancedVideoPrompt,
          beatTimeline: {
            applied: true,
            progressionType: timeline.metadata.progressionType,
            primaryCharacter: timeline.metadata.primaryCharacter,
            primaryMaterial: timeline.metadata.primaryMaterial,
            peakIntensity: timeline.metadata.peakIntensity,
            beats: Object.keys(timeline.beats).map(beatName => ({
              name: beatName,
              timing: timeline.beats[beatName].timing,
              intensity: timeline.beats[beatName].intensity
            }))
          }
        };
      } catch (timelineError) {
        console.warn(`[creationWizardDecomposeSceneToShots] Timeline enhancement failed for shot ${idx + 1}:`, timelineError.message);
        return {
          ...shot,
          beatTimeline: { applied: false, error: timelineError.message }
        };
      }
    });

    console.log(`[creationWizardDecomposeSceneToShots] Beat Timeline applied to ${timelineEnhancedShots.filter(s => s.beatTimeline?.applied).length}/${timelineEnhancedShots.length} shots`);

    // STEP 6: Normalize shots with all required fields
    // Includes imagePrompt, videoPrompt, narrativeBeat, captureSuggestion, crossShotIntelligence, and visualContinuity
    // IMPORTANT: Use timelineEnhancedShots (with World-First + Physics + Character + Audio + Timeline) for final normalization
    const normalizedShots = timelineEnhancedShots.map((shot, idx) => {
      const isLast = idx === timelineEnhancedShots.length - 1;
      const isFirst = idx === 0;
      const beatData = storyBeats ? storyBeats[idx] : null;

      return {
        id: `${scene.id}_shot_${idx + 1}`,
        sceneId: scene.id,
        shotIndex: shot.shotIndex || idx + 1,
        shotType: shot.shotType,
        shotTypeName: shot.shotTypeName,
        duration: shot.duration,
        cameraMovement: shot.cameraMovement,
        // Separate prompts for image and video generation
        imagePrompt: shot.imagePrompt,    // For Imagen - visual composition
        videoPrompt: shot.videoPrompt,    // For Minimax - ACTION focused
        prompt: shot.prompt,              // Legacy - backward compatibility
        // Narrative beat info for progressive action
        narrativeBeat: shot.narrativeBeat || null,
        purpose: shot.purpose,
        focusElement: shot.focusElement,
        transition: shot.transition || 'cut',
        // NEW: Capture point suggestions (Upgrade 2)
        captureSuggestion: isLast ? null : (beatData?.captureSuggestion || {
          timing: `${clipDuration - 2}-${clipDuration} seconds`,
          reason: 'End of shot action sequence',
          stability: 'medium',
          timingSeconds: { start: clipDuration - 2, end: clipDuration, recommended: clipDuration - 1 },
          stabilityColor: '#3b82f6'
        }),
        // NEW: Hollywood scene closure (Phase 3)
        closureType: isLast ? (shot.closureType || null) : null,
        hasHollywoodClosure: isLast ? (shot.hasHollywoodClosure || false) : false,
        closureSummary: isLast && shot.closureType ?
          SCENE_CLOSURE_ENGINE.getClosureSummary(shot.closureType) : null,
        // NEW: Cross-shot narrative intelligence (Phase 4)
        crossShotIntelligence: shot.crossShotIntelligence || null,
        intensity: shot.crossShotIntelligence?.intensity || null,
        momentumDirection: shot.crossShotIntelligence?.momentumDirection || null,
        narrativePosition: shot.crossShotIntelligence?.narrativePosition || null,
        // NEW: Visual continuity (Qwen-inspired)
        visualContinuity: shot.visualContinuity || null,
        lastFrameCapture: shot.visualContinuity?.lastFrameCapture || null,
        anchorCategories: shot.visualContinuity?.anchorCategories || [],
        // NEW: Hollywood choreography (4-beat system)
        hollywoodChoreography: shot.hollywoodChoreography || null,
        hasChoreography: !!shot.hollywoodChoreography?.hasEnrichment,
        captureFrame: shot.hollywoodChoreography?.captureFrame || null,
        // NEW: Opening scene World-First structure
        openingStructure: shot.openingStructure || null,
        isWorldFirst: shot.openingStructure?.isWorldFirst || false,
        // NEW: Cinematic Physics (force/momentum layer)
        cinematicPhysics: shot.cinematicPhysics || null,
        hasPhysicsEnhancement: shot.cinematicPhysics?.applied || false,
        // NEW: Character Reference (visual consistency)
        characterReference: shot.characterReference || null,
        hasCharacterConsistency: shot.characterReference?.applied || false,
        charactersInShot: shot.characterReference?.characterNames || [],
        // NEW: Audio Beat Mapping (sound design)
        audioBeat: shot.audioBeat || null,
        hasAudioMapping: shot.audioBeat?.applied || false,
        audioDialogue: shot.audioBeat?.hasDialogue || false,
        audioAmbience: shot.audioBeat?.ambienceType || null,
        // BEAT TIMELINE - temporal choreography
        beatTimeline: shot.beatTimeline || null,
        hasTimeline: shot.beatTimeline?.applied || false,
        timelineProgression: shot.beatTimeline?.progressionType || null,
        // Generation status
        // SHOT-SCENE IMAGE SYNC: Shot 1 automatically inherits scene's main image
        // This ensures when user regenerates scene image, Shot 1 stays in sync
        imageUrl: isFirst && scene.imageUrl ? scene.imageUrl : null,
        syncedWithScene: isFirst && scene.imageUrl ? true : false,
        videoUrl: null,
        status: isFirst && scene.imageUrl ? 'image_synced' : 'pending'
      };
    });

    // Log usage with decomposition method
    await db.collection('apiUsage').add({
      userId: uid,
      type: 'shot_decomposition',
      model: decompositionMethod === 'ai-powered' ? 'gpt-4o-mini' : 'local_engine',
      method: decompositionMethod,
      sceneId: scene.id,
      shotCount: normalizedShots.length,
      sceneActionLength: sceneAction?.length || 0,
      inputTokens: decompositionMethod === 'ai-powered' ? 500 : 0, // Estimated
      outputTokens: decompositionMethod === 'ai-powered' ? 1000 : 0, // Estimated
      timestamp: admin.firestore.FieldValue.serverTimestamp()
    });

    // Compute scene end state from last shot if not from AI
    if (!sceneEndState && normalizedShots.length > 0) {
      const lastShot = normalizedShots[normalizedShots.length - 1];
      sceneEndState = lastShot.narrativeBeat?.endState ||
                      lastShot.captureSuggestion?.captureDescription ||
                      'Scene concluded';
    }

    return {
      success: true,
      sceneId: scene.id,
      sceneType: sceneType,
      totalDuration: sceneDuration,
      shotCount: normalizedShots.length,
      shots: normalizedShots,
      consistencyAnchors: consistencyAnchors,
      decompositionMethod: decompositionMethod,
      // Cross-scene continuity data (Upgrade 3)
      crossSceneContinuity: {
        sceneEndState: sceneEndState,
        suggestedNextSceneTransition: suggestedNextSceneTransition,
        // This data should be passed as previousSceneContext to the next scene
        forNextScene: {
          endState: sceneEndState,
          sceneId: scene.id,
          transitionType: suggestedNextSceneTransition
        }
      },
      usage: {
        promptTokens: decompositionMethod === 'ai-powered' ? 500 : 0,
        completionTokens: decompositionMethod === 'ai-powered' ? 1000 : 0,
        method: decompositionMethod
      },
      // Hollywood choreography status
      hollywoodChoreography: {
        applied: hasChoreography,
        enrichmentSource: hasChoreography ? 'script_generation' : 'none',
        shotsWithChoreography: normalizedShots.filter(s => s.hasChoreography).length
      },
      // Opening Scene World-First status
      openingSceneEnforcement: {
        isOpeningScene: OPENING_SCENE_HANDLER.isOpeningScene(scene, currentSceneIndex),
        worldFirstApplied: worldFirstShots !== continuityShots,
        shotsWithWorldFirst: normalizedShots.filter(s => s.isWorldFirst).length,
        openingStructure: normalizedShots.slice(0, 4).map(s => s.openingStructure?.role || null).filter(Boolean)
      },
      // Shot-Scene Image Sync status
      shotSceneSync: {
        shot1SyncedWithScene: normalizedShots[0]?.syncedWithScene || false,
        sceneImageUrl: scene.imageUrl || null,
        shot1ImageUrl: normalizedShots[0]?.imageUrl || null
      },
      // Cinematic Physics Enhancement status
      cinematicPhysics: {
        applied: physicsEnhancedShots.some(s => s.cinematicPhysics?.applied),
        shotsWithPhysics: normalizedShots.filter(s => s.hasPhysicsEnhancement).length,
        physicsTypes: {
          hasObjectPhysics: normalizedShots.some(s => s.cinematicPhysics?.hasObjectPhysics),
          hasEnvironmentalForces: normalizedShots.some(s => s.cinematicPhysics?.hasEnvironmentalForces),
          hasMaterialPhysics: normalizedShots.some(s => s.cinematicPhysics?.hasMaterialPhysics),
          hasContactPhysics: normalizedShots.some(s => s.cinematicPhysics?.hasContactPhysics),
          hasCauseEffectChains: normalizedShots.some(s => s.cinematicPhysics?.causeEffectChains > 0)
        }
      },
      // Character Reference System status
      characterReferenceSystem: {
        applied: characterEnhancedShots.some(s => s.characterReference?.applied),
        shotsWithCharacterConsistency: normalizedShots.filter(s => s.hasCharacterConsistency).length,
        totalCharacters: characterBible?.length || 0,
        characterAnchorsExtracted: normalizedShots.some(s => s.characterReference?.totalCharacters > 0),
        charactersTracked: [...new Set(normalizedShots.flatMap(s => s.charactersInShot || []))]
      },
      // Audio Beat Mapping status
      audioBeatMapping: {
        applied: audioEnhancedShots.some(s => s.audioBeat?.applied),
        shotsWithAudio: normalizedShots.filter(s => s.hasAudioMapping).length,
        hasDialogue: normalizedShots.some(s => s.audioDialogue),
        lipSyncRequired: normalizedShots.some(s => s.audioBeat?.lipSyncRequired),
        ambienceTypes: [...new Set(normalizedShots.map(s => s.audioAmbience).filter(Boolean))],
        musicMoods: [...new Set(normalizedShots.map(s => s.audioBeat?.musicMood).filter(Boolean))]
      },
      // Beat Timeline Synthesis status - the Hollywood differentiator
      beatTimelineSynthesis: {
        applied: timelineEnhancedShots.some(s => s.beatTimeline?.applied),
        shotsWithTimeline: normalizedShots.filter(s => s.hasTimeline).length,
        progressionTypes: [...new Set(normalizedShots.map(s => s.timelineProgression).filter(Boolean))],
        primaryCharacters: [...new Set(normalizedShots.map(s => s.beatTimeline?.primaryCharacter).filter(Boolean))],
        peakIntensities: normalizedShots.map(s => s.beatTimeline?.peakIntensity).filter(Boolean)
      },
      // Shot Sequence Validation status
      shotSequenceValidation: SHOT_SEQUENCE_VALIDATOR.getSequenceSummary(normalizedShots),
      // Keyframe Quality Check status
      keyframeQuality: {
        averageScore: Math.round(normalizedShots.reduce((sum, shot) => {
          const quality = KEYFRAME_QUALITY_ENGINE.getKeyframeSummary(shot, scene);
          return sum + quality.score;
        }, 0) / (normalizedShots.length || 1)),
        shotsAnalyzed: normalizedShots.length,
        shotGrades: normalizedShots.map(shot => {
          const quality = KEYFRAME_QUALITY_ENGINE.getKeyframeSummary(shot, scene);
          return { shotId: shot.id, grade: quality.grade, score: quality.score };
        })
      }
    };

  } catch (error) {
    console.error('[creationWizardDecomposeSceneToShots] Error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to decompose scene'));
  }
});

/**
 * creationWizardGenerateAllShotsForScene - Generates images for all shots in a decomposed scene
 * Maintains visual consistency by using the first shot as anchor
 * NOW SUPPORTS visualStyleMode for VISUAL_STYLE_DNA integration
 */
exports.creationWizardGenerateAllShotsForScene = functions
  .runWith({
    timeoutSeconds: 540, // 9 minutes for multiple image generation
    memory: '2GB'
  })
  .https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const {
    sceneId,
    shots,              // Array of shot objects from decomposition
    consistencyAnchors, // Visual consistency requirements
    model,              // Image generation model
    aspectRatio,
    characterReference, // Optional character reference image
    visualStyleMode     // VISUAL_STYLE_DNA mode (photorealistic, cinematic, etc.)
  } = data;

  // Log visual style mode for debugging
  console.log(`[creationWizardGenerateAllShotsForScene] Using visual style mode: ${visualStyleMode || 'photorealistic (default)'}`);
  const effectiveStyleMode = visualStyleMode || 'photorealistic';

  if (!shots || !Array.isArray(shots) || shots.length === 0) {
    throw new functions.https.HttpsError('invalid-argument', 'Shots array required');
  }

  const results = [];
  let anchorImageUrl = null;

  try {
    console.log(`[creationWizardGenerateAllShotsForScene] Starting generation of ${shots.length} shots for scene ${sceneId}`);

    for (let i = 0; i < shots.length; i++) {
      const shot = shots[i];

      try {
        // =========================================================
        // USE NANOBANANA_PROMPT_BUILDER for narrative-style prompts
        // This creates photorealistic, well-composed images
        // =========================================================

        // Build context from shot data and consistency anchors
        const promptContext = {
          visualPrompt: shot.prompt || shot.imagePrompt || '',
          environment: consistencyAnchors?.environmentKey || '',
          mood: shot.mood || shot.narrativeBeat?.mood || '',
          atmosphere: consistencyAnchors?.atmosphere || '',
          subject: consistencyAnchors?.characterAppearance || 'the subject',
          action: shot.narrativeBeat?.action || shot.action || ''
        };

        // Extract character bible from shot data if available
        const shotCharacterBible = shot.characterBible || null;

        // Extract style bible from consistency anchors
        const shotStyleBible = consistencyAnchors ? {
          enabled: true,
          lighting: consistencyAnchors.lighting,
          colorGrade: consistencyAnchors.colorPalette,
          atmosphere: consistencyAnchors.atmosphere
        } : null;

        // Build the enhanced narrative prompt using NANOBANANA_PROMPT_BUILDER
        // NOW WITH VISUAL_STYLE_DNA integration for jaw-dropping results
        let enhancedPrompt = NANOBANANA_PROMPT_BUILDER.buildShotPrompt(
          {
            shotType: shot.shotType || 'medium',
            subject: promptContext.subject,
            action: promptContext.action,
            mood: promptContext.mood
          },
          promptContext,
          shotCharacterBible,
          shotStyleBible,
          effectiveStyleMode  // VISUAL_STYLE_DNA mode
        );

        // Append the original prompt content for specifics
        if (shot.prompt && shot.prompt.trim()) {
          enhancedPrompt += `\n\nSPECIFIC SCENE DETAILS: ${shot.prompt}`;
        }

        console.log(`[creationWizardGenerateAllShotsForScene] Shot ${i + 1} prompt (first 200 chars): ${enhancedPrompt.substring(0, 200)}...`);

        // Call image generation
        const generateFn = exports.generateCreativeImage;

        // Prepare request
        const requestData = {
          prompt: enhancedPrompt,
          model: model || 'nanobanana-pro',
          quantity: 1,
          aspectRatio: aspectRatio || '16:9',
          quality: 'hd'
        };

        // Add character reference if available
        if (characterReference) {
          requestData.characterReference = characterReference;
        }

        // For shots after the first, use the anchor image as style reference
        // This helps maintain visual consistency
        if (i > 0 && anchorImageUrl) {
          // Note: Style reference would need base64 conversion
          // For now, we rely on prompt consistency
          enhancedPrompt = `[Match visual style of previous shot exactly]\n\n${enhancedPrompt}`;
          requestData.prompt = enhancedPrompt;
        }

        // Generate the image (call internal function)
        const imageResult = await generateCreativeImageInternal(uid, requestData);

        if (imageResult.success && imageResult.images && imageResult.images.length > 0) {
          let imageUrl = imageResult.images[0].url;

          // UPSCALE: Enhance image quality using FAL AuraSR (Fix 3)
          try {
            const upscaleResult = await upscaleImageInternal(imageUrl, aspectRatio || '16:9', 'hd');
            if (upscaleResult.success) {
              console.log(`[creationWizardGenerateAllShotsForScene] Shot ${i + 1} upscaled to HD`);
              imageUrl = upscaleResult.url;
            } else if (upscaleResult.skipped) {
              console.log(`[creationWizardGenerateAllShotsForScene] Shot ${i + 1} upscale skipped: ${upscaleResult.reason || 'not configured'}`);
            }
          } catch (upscaleError) {
            console.warn(`[creationWizardGenerateAllShotsForScene] Shot ${i + 1} upscale failed, using original:`, upscaleError.message);
          }

          // Store anchor image from first shot
          if (i === 0) {
            anchorImageUrl = imageUrl;
          }

          results.push({
            shotId: shot.id,
            shotIndex: shot.shotIndex,
            status: 'ready',
            imageUrl: imageUrl,
            prompt: shot.prompt,
            duration: shot.duration
          });

          console.log(`[creationWizardGenerateAllShotsForScene] Shot ${i + 1}/${shots.length} generated successfully`);
        } else {
          results.push({
            shotId: shot.id,
            shotIndex: shot.shotIndex,
            status: 'failed',
            error: 'No image returned',
            prompt: shot.prompt
          });
        }

        // Small delay between generations to avoid rate limiting
        if (i < shots.length - 1) {
          await new Promise(resolve => setTimeout(resolve, 2000));
        }

      } catch (shotError) {
        console.error(`[creationWizardGenerateAllShotsForScene] Shot ${i + 1} failed:`, shotError);
        results.push({
          shotId: shot.id,
          shotIndex: shot.shotIndex,
          status: 'failed',
          error: shotError.message || 'Generation failed',
          prompt: shot.prompt
        });
      }
    }

    const successCount = results.filter(r => r.status === 'ready').length;

    return {
      success: successCount > 0,
      sceneId,
      totalShots: shots.length,
      successCount,
      failedCount: shots.length - successCount,
      anchorImageUrl,
      shots: results
    };

  } catch (error) {
    console.error('[creationWizardGenerateAllShotsForScene] Error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to generate shots'));
  }
});

/**
 * Internal helper for image generation (avoids auth re-check)
 */
async function generateCreativeImageInternal(uid, requestData) {
  const { prompt, model, quantity, aspectRatio, quality, characterReference } = requestData;

  // Similar to generateCreativeImage but without auth check
  const geminiApiKey = functions.config().gemini?.key;
  if (!geminiApiKey) {
    throw new Error('Gemini API key not configured');
  }

  const ai = new GoogleGenAI({ apiKey: geminiApiKey });
  const storage = admin.storage().bucket();
  const timestamp = Date.now();
  const generatedImages = [];

  // Determine model - Updated to use correct NanoBanana models
  // gemini-3-pro-image-preview = Nano Banana Pro (best quality, face preservation)
  // gemini-2.5-flash-image = Nano Banana (faster, good quality)
  const geminiImageModelMap = {
    'nanobanana-pro': 'gemini-3-pro-image-preview',
    'nanobanana': 'gemini-2.5-flash-image'
  };
  const geminiModelId = geminiImageModelMap[model] || 'gemini-3-pro-image-preview';

  // Build content parts
  const contentParts = [];

  // Add character reference if provided
  if (characterReference && characterReference.base64) {
    contentParts.push({
      inlineData: {
        mimeType: characterReference.mimeType || 'image/png',
        data: characterReference.base64
      }
    });
  }

  // Build enhanced prompt - No need for aspect ratio in text, use imageConfig instead
  contentParts.push({ text: prompt });

  // Map aspect ratio string to Gemini format
  const aspectRatioMap = {
    '16:9': '16:9',
    '9:16': '9:16',
    '1:1': '1:1',
    '4:3': '4:3',
    '3:4': '3:4',
    '21:9': '21:9'
  };
  const geminiAspectRatio = aspectRatioMap[aspectRatio] || '16:9';

  try {
    console.log(`[generateCreativeImageInternal] Model: ${geminiModelId}, Aspect: ${geminiAspectRatio}`);

    const result = await ai.models.generateContent({
      model: geminiModelId,
      contents: [{ role: 'user', parts: contentParts }],
      config: {
        responseModalities: ['IMAGE', 'TEXT'],
        imageConfig: {
          aspectRatio: geminiAspectRatio
        }
      }
    });

    const candidates = result.candidates || (result.response && result.response.candidates);
    if (candidates && candidates.length > 0) {
      const parts = candidates[0].content?.parts || candidates[0].parts || [];
      for (const part of parts) {
        const inlineData = part.inlineData || part.inline_data;
        if (inlineData && (inlineData.data || inlineData.bytesBase64Encoded)) {
          const imageBytes = inlineData.data || inlineData.bytesBase64Encoded;
          const rawBuffer = Buffer.from(imageBytes, 'base64');

          // Process image
          const { buffer: processedBuffer, width, height } = await enforceImageDimensions(rawBuffer, aspectRatio, 'hd');

          const fileName = `creative-studio/${uid}/${timestamp}-shot-${Date.now()}.png`;
          const file = storage.file(fileName);

          await file.save(processedBuffer, {
            metadata: { contentType: 'image/png' }
          });

          await file.makePublic();
          const publicUrl = `https://storage.googleapis.com/${storage.name}/${fileName}`;

          generatedImages.push({
            url: publicUrl,
            fileName,
            width,
            height
          });
          break;
        }
      }
    }

    return {
      success: generatedImages.length > 0,
      images: generatedImages
    };

  } catch (error) {
    console.error('[generateCreativeImageInternal] Error:', error);
    throw error;
  }
}

// Export the shot decomposition engine
exports.SHOT_DECOMPOSITION_ENGINE = SHOT_DECOMPOSITION_ENGINE;

// =============================================================================
// PHASE 12B: SHOT VIDEO GENERATION & ASSEMBLY
// Generate videos for each shot and assemble into scene sequences
// =============================================================================

/**
 * creationWizardGenerateShotVideo - Generate video for a single shot using Minimax
 *
 * Uses the shot's image as first frame (Image-to-Video) for consistency
 * Supports S2V-01 model for character-consistent video when characterReference is provided
 *
 * SMART MODEL SELECTION:
 * - If hasCharacters=true AND characterReference provided â†’ S2V-01 (character consistent)
 * - If hasCharacters=false OR no characterReference â†’ video-01/T2V-01 (standard)
 */
exports.creationWizardGenerateShotVideo = functions
  .runWith({
    timeoutSeconds: 120,
    memory: '512MB'
  })
  .https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const {
    sceneId,
    shotId,
    shotIndex,
    imageUrl,           // First frame image (from shot generation)
    prompt,             // Video motion prompt
    duration = '10s',   // Shot duration - 10s default for richer action
    cameraMovement,     // Camera movement for this shot
    model = 'hailuo-2.3',
    // NEW: Character consistency parameters
    hasCharacters = false,              // Does this shot contain characters?
    characterReference = null,          // Character portrait URL from Character Bible
    primaryCharacterName = null,        // Name of primary character for logging
    // NEW: Style enforcement
    styleSettings = null                // { style: 'photorealistic', colorGrade: '...', etc. }
  } = data;

  if (!imageUrl) {
    throw new functions.https.HttpsError('invalid-argument', 'Shot image URL is required for video generation');
  }

  if (!prompt || prompt.trim().length < 5) {
    throw new functions.https.HttpsError('invalid-argument', 'Video prompt is required');
  }

  const minimaxKey = process.env.MINIMAX_API_KEY || functions.config().minimax?.key;
  if (!minimaxKey) {
    throw new functions.https.HttpsError('failed-precondition', 'Minimax API key not configured');
  }

  try {
    // ==========================================
    // MODEL SELECTION
    // ==========================================
    // NOTE: S2V-01 (character consistency) is currently disabled due to API issues
    const useCharacterConsistency = false; // Disabled: hasCharacters && characterReference;

    // Use MiniMax-Hailuo-2.3 which supports both 6s and 10s durations
    // See: https://platform.minimax.io/docs/api-reference/video-generation-t2v
    const selectedModel = 'MiniMax-Hailuo-2.3';

    console.log(`[creationWizardGenerateShotVideo] Model: ${selectedModel} (duration: ${duration})`);
    console.log(`[creationWizardGenerateShotVideo] Has Characters: ${hasCharacters}, Character Reference: ${characterReference ? 'YES' : 'NO'}`);
    if (primaryCharacterName) {
      console.log(`[creationWizardGenerateShotVideo] Primary Character: ${primaryCharacterName}`);
    }

    // Build video prompt with camera movement
    let videoPrompt = prompt.trim();

    // ==========================================
    // STYLE ENFORCEMENT
    // ==========================================
    // If style is photorealistic, enforce it strongly in the prompt
    if (styleSettings?.style) {
      const isPhotorealistic = styleSettings.style.toLowerCase().includes('photorealistic') ||
                               styleSettings.style.toLowerCase().includes('realistic') ||
                               styleSettings.style.toLowerCase().includes('live-action');

      if (isPhotorealistic) {
        // Prepend strong style enforcement for photorealistic content
        videoPrompt = `[STYLE: Photorealistic, live-action cinematography, real human faces and bodies, NOT animated, NOT cartoon, NOT CGI characters] ${videoPrompt}`;
        console.log(`[creationWizardGenerateShotVideo] Style Enforcement: PHOTOREALISTIC`);
      } else if (styleSettings.style) {
        videoPrompt = `[STYLE: ${styleSettings.style}] ${videoPrompt}`;
        console.log(`[creationWizardGenerateShotVideo] Style Enforcement: ${styleSettings.style}`);
      }
    }

    // ENHANCED: Apply MINIMAX_PROMPT_OPTIMIZER for better video quality (Fix 4)
    // This is I2V (image-to-video) since we have imageUrl
    videoPrompt = MINIMAX_PROMPT_OPTIMIZER.enhance(videoPrompt, cameraMovement, true);

    // Map shot camera movement to Minimax format
    const cameraMovementMap = {
      'static': 'Static shot',
      'slow_push': 'Slow push in',
      'slow_pull': 'Slow pull out',
      'tracking': 'Tracking shot',
      'pan': 'Pan',
      'tilt': 'Tilt',
      'crane_up': 'Crane up',
      'crane_down': 'Crane down',
      'handheld': 'Handheld movement',
      'orbit': 'Orbit around subject'
    };

    const movement = cameraMovementMap[cameraMovement] || 'Subtle movement';
    videoPrompt = `[${movement}] ${videoPrompt}`;

    console.log(`[creationWizardGenerateShotVideo] Scene ${sceneId}, Shot ${shotIndex}: Starting video generation`);
    console.log(`[creationWizardGenerateShotVideo] Image URL: ${imageUrl}`);
    console.log(`[creationWizardGenerateShotVideo] Prompt: ${videoPrompt.substring(0, 150)}...`);

    // ==========================================
    // BUILD API PAYLOAD
    // ==========================================
    // Convert duration string ('6s' or '10s') to integer for Minimax API
    const durationSeconds = duration === '6s' ? 6 : 10;

    const payload = {
      model: selectedModel,
      prompt: videoPrompt,
      first_frame_image: imageUrl,
      prompt_optimizer: true, // Enable prompt optimization for video-01
      duration: durationSeconds // CRITICAL: Must include duration or API defaults to 5s
    };

    // NOTE: S2V-01 subject_reference is disabled until properly tested
    // if (useCharacterConsistency) {
    //   payload.subject_reference = [characterReference];
    // }

    console.log(`[creationWizardGenerateShotVideo] Sending to Minimax API with model: ${selectedModel}, duration: ${durationSeconds}s...`);
    console.log(`[creationWizardGenerateShotVideo] Payload keys:`, Object.keys(payload));

    const minimaxResponse = await axios.post(
      'https://api.minimax.io/v1/video_generation',
      payload,
      {
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${minimaxKey}`
        },
        timeout: 60000
      }
    );

    console.log(`[creationWizardGenerateShotVideo] Minimax response:`, JSON.stringify(minimaxResponse.data, null, 2));

    const taskId = minimaxResponse.data.task_id;
    const baseResp = minimaxResponse.data.base_resp;

    if (!taskId) {
      console.error(`[creationWizardGenerateShotVideo] No task_id returned! Full response:`, minimaxResponse.data);
      throw new Error(`Minimax API error: No task_id returned. Response: ${JSON.stringify(minimaxResponse.data)}`);
    }

    if (baseResp && baseResp.status_code !== 0) {
      console.error(`[creationWizardGenerateShotVideo] API error:`, baseResp);
      throw new Error(`Minimax API error: ${baseResp.status_msg || 'Unknown error'} (code: ${baseResp.status_code})`);
    }

    console.log(`[creationWizardGenerateShotVideo] Task started successfully: ${taskId} (Model: ${selectedModel})`);

    // Log usage with model info
    await db.collection('apiUsage').add({
      userId: uid,
      type: 'shot_video_generation',
      model: selectedModel,
      usedCharacterConsistency: useCharacterConsistency,
      primaryCharacter: primaryCharacterName || null,
      sceneId,
      shotId,
      shotIndex,
      duration,
      timestamp: admin.firestore.FieldValue.serverTimestamp()
    });

    return {
      success: true,
      taskId,
      sceneId,
      shotId,
      shotIndex,
      status: 'processing',
      provider: 'minimax',
      // Return model info for UI display
      modelUsed: selectedModel,
      usedCharacterConsistency: useCharacterConsistency,
      characterName: primaryCharacterName || null
    };

  } catch (error) {
    console.error('[creationWizardGenerateShotVideo] Error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to start shot video generation'));
  }
});

/**
 * creationWizardGenerateAllShotVideos - Generate videos for all shots in a scene
 *
 * Sequentially generates videos for each shot, then returns the complete sequence
 */
exports.creationWizardGenerateAllShotVideos = functions
  .runWith({
    timeoutSeconds: 540, // 9 minutes for multiple video generations
    memory: '1GB'
  })
  .https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const {
    sceneId,
    shots,              // Array of shot objects with imageUrl, prompt, etc.
    model = 'hailuo-2.3',
    duration = '10s'    // 10s default for richer action sequences
  } = data;

  if (!shots || !Array.isArray(shots) || shots.length === 0) {
    throw new functions.https.HttpsError('invalid-argument', 'Shots array is required');
  }

  const minimaxKey = process.env.MINIMAX_API_KEY || functions.config().minimax?.key;
  if (!minimaxKey) {
    throw new functions.https.HttpsError('failed-precondition', 'Minimax API key not configured');
  }

  const results = [];

  try {
    console.log(`[creationWizardGenerateAllShotVideos] Starting video generation for ${shots.length} shots in scene ${sceneId}`);

    for (let i = 0; i < shots.length; i++) {
      const shot = shots[i];

      if (!shot.imageUrl) {
        results.push({
          shotId: shot.id,
          shotIndex: i,
          status: 'error',
          error: 'No image URL - generate image first'
        });
        continue;
      }

      try {
        // Build video prompt with camera movement
        let videoPrompt = shot.prompt || 'Cinematic movement, subtle animation';

        const cameraMovementMap = {
          'static': 'Static shot',
          'slow_push': 'Slow push in',
          'slow_pull': 'Slow pull out',
          'tracking': 'Tracking shot',
          'pan': 'Pan',
          'tilt': 'Tilt',
          'crane_up': 'Crane up',
          'crane_down': 'Crane down',
          'handheld': 'Handheld movement',
          'orbit': 'Orbit around subject'
        };

        const movement = cameraMovementMap[shot.cameraMovement] || 'Subtle movement';
        videoPrompt = `[${movement}] ${videoPrompt}`;

        // Convert duration string ('6s' or '10s') to integer for Minimax API
        const durationSeconds = duration === '6s' ? 6 : 10;

        // Use MiniMax-Hailuo-2.3 which supports both 6s and 10s durations
        const selectedModel = 'MiniMax-Hailuo-2.3';

        // Call Minimax API
        const payload = {
          model: selectedModel,
          prompt: videoPrompt,
          first_frame_image: shot.imageUrl,
          prompt_optimizer: true,
          duration: durationSeconds // CRITICAL: Must include duration or API defaults to 5s
        };

        console.log(`[creationWizardGenerateAllShotVideos] Shot ${i + 1} payload: model=${selectedModel}, duration=${durationSeconds}s`);

        const minimaxResponse = await axios.post(
          'https://api.minimax.io/v1/video_generation',
          payload,
          {
            headers: {
              'Content-Type': 'application/json',
              'Authorization': `Bearer ${minimaxKey}`
            },
            timeout: 60000
          }
        );

        const taskId = minimaxResponse.data.task_id;
        const baseResp = minimaxResponse.data.base_resp;

        if (baseResp && baseResp.status_code !== 0) {
          throw new Error(baseResp.status_msg || 'Minimax error');
        }

        results.push({
          shotId: shot.id,
          shotIndex: i,
          taskId,
          status: 'processing',
          duration: shot.duration || 2
        });

        console.log(`[creationWizardGenerateAllShotVideos] Shot ${i + 1}/${shots.length} started: taskId=${taskId}`);

        // Small delay between API calls to avoid rate limiting
        if (i < shots.length - 1) {
          await new Promise(resolve => setTimeout(resolve, 1000));
        }

      } catch (shotError) {
        console.error(`[creationWizardGenerateAllShotVideos] Shot ${i + 1} failed:`, shotError);
        results.push({
          shotId: shot.id,
          shotIndex: i,
          status: 'error',
          error: shotError.message || 'Generation failed'
        });
      }
    }

    // Log usage
    const successCount = results.filter(r => r.status === 'processing').length;
    await db.collection('apiUsage').add({
      userId: uid,
      type: 'batch_shot_video_generation',
      sceneId,
      totalShots: shots.length,
      successfulStarts: successCount,
      timestamp: admin.firestore.FieldValue.serverTimestamp()
    });

    return {
      success: successCount > 0,
      sceneId,
      totalShots: shots.length,
      startedCount: successCount,
      failedCount: shots.length - successCount,
      shots: results
    };

  } catch (error) {
    console.error('[creationWizardGenerateAllShotVideos] Error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to generate shot videos'));
  }
});

/**
 * creationWizardCheckShotVideoStatus - Check status of a shot video generation
 */
exports.creationWizardCheckShotVideoStatus = functions.https.onCall(async (data, context) => {
  await verifyAuth(context);
  const { taskId, shotId, shotIndex } = data;

  if (!taskId) {
    throw new functions.https.HttpsError('invalid-argument', 'Task ID is required');
  }

  const minimaxKey = process.env.MINIMAX_API_KEY || functions.config().minimax?.key;
  if (!minimaxKey) {
    throw new functions.https.HttpsError('failed-precondition', 'Minimax API key not configured');
  }

  try {
    const statusResponse = await axios.get(
      `https://api.minimax.io/v1/query/video_generation?task_id=${taskId}`,
      {
        headers: { 'Authorization': `Bearer ${minimaxKey}` },
        timeout: 30000
      }
    );

    const statusData = statusResponse.data;
    console.log('[creationWizardCheckShotVideoStatus] Minimax response:', JSON.stringify(statusData, null, 2));

    // Handle different response formats
    const status = statusData.status || statusData.base_resp?.status_msg;
    const fileId = statusData.file_id;

    // Map Minimax status to our status
    // Minimax statuses: Queueing, Processing, Success, Fail
    let mappedStatus = 'processing';
    let videoUrl = null;

    if (status === 'Success' || status === 'Finished') {
      mappedStatus = 'ready';

      // Get video URL - check multiple possible locations
      videoUrl = statusData.video_url || statusData.download_url;

      // If no direct URL, try file_id retrieval
      if (!videoUrl && fileId) {
        try {
          console.log('[creationWizardCheckShotVideoStatus] Fetching file with ID:', fileId);
          const fileResponse = await axios.get(
            `https://api.minimax.io/v1/files/retrieve?file_id=${fileId}`,
            {
              headers: { 'Authorization': `Bearer ${minimaxKey}` },
              timeout: 30000
            }
          );
          console.log('[creationWizardCheckShotVideoStatus] File response:', JSON.stringify(fileResponse.data, null, 2));
          videoUrl = fileResponse.data.file?.download_url || fileResponse.data.download_url;
        } catch (fileError) {
          console.error('[creationWizardCheckShotVideoStatus] Error fetching file:', fileError.message);
        }
      }

      // If we have a video URL, download and upload to Firebase Storage for persistence
      if (videoUrl) {
        try {
          console.log('[creationWizardCheckShotVideoStatus] Downloading video from:', videoUrl);
          const videoResponse = await axios.get(videoUrl, {
            responseType: 'arraybuffer',
            timeout: 120000 // 2 minutes for video download
          });

          const videoBuffer = Buffer.from(videoResponse.data);
          const fileName = `shot_videos/${taskId}_${Date.now()}.mp4`;
          const file = admin.storage().bucket().file(fileName);

          await file.save(videoBuffer, {
            metadata: {
              contentType: 'video/mp4',
              metadata: {
                taskId,
                shotId,
                source: 'minimax'
              }
            }
          });

          // Make the file publicly accessible
          await file.makePublic();

          // Get the public URL
          videoUrl = `https://storage.googleapis.com/${admin.storage().bucket().name}/${fileName}`;
          console.log('[creationWizardCheckShotVideoStatus] Video uploaded to Firebase:', videoUrl);

        } catch (uploadError) {
          console.error('[creationWizardCheckShotVideoStatus] Error uploading video:', uploadError.message);
          // Keep the original Minimax URL as fallback
        }
      }
    } else if (status === 'Fail' || status === 'Failed' || status === 'Error') {
      mappedStatus = 'error';
      console.log('[creationWizardCheckShotVideoStatus] Video generation failed:', statusData);
    } else if (status === 'Queueing' || status === 'Processing' || status === 'Pending') {
      mappedStatus = 'processing';
    }

    console.log(`[creationWizardCheckShotVideoStatus] Mapped status: ${status} -> ${mappedStatus}, videoUrl: ${videoUrl ? 'yes' : 'no'}`);

    return {
      success: true,
      taskId,
      shotId,
      shotIndex,
      status: mappedStatus,
      videoUrl,
      rawStatus: status
    };

  } catch (error) {
    console.error('[creationWizardCheckShotVideoStatus] Error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to check shot video status'));
  }
});

/**
 * uploadBase64Image - Upload a base64 encoded image to Firebase Storage
 * Used for frame transfer functionality in video wizard
 */
exports.uploadBase64Image = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const { imageBase64, mimeType = 'image/png', filename } = data;

  if (!imageBase64) {
    throw new functions.https.HttpsError('invalid-argument', 'Image data is required');
  }

  try {
    console.log(`[uploadBase64Image] Uploading image for user: ${uid}, filename: ${filename}`);

    // Decode base64 to buffer
    const imageBuffer = Buffer.from(imageBase64, 'base64');

    // Validate image size (max 10MB)
    if (imageBuffer.length > 10 * 1024 * 1024) {
      throw new functions.https.HttpsError('invalid-argument', 'Image too large (max 10MB)');
    }

    // Generate unique filename
    const timestamp = Date.now();
    const extension = mimeType.includes('jpeg') || mimeType.includes('jpg') ? 'jpg' : 'png';
    const safeFilename = filename ? filename.replace(/[^a-zA-Z0-9_.-]/g, '_') : `frame_${timestamp}`;
    const storagePath = `wizard_frames/${uid}/${timestamp}_${safeFilename}.${extension}`;

    // Upload to Firebase Storage
    const bucket = admin.storage().bucket();
    const file = bucket.file(storagePath);

    await file.save(imageBuffer, {
      metadata: {
        contentType: mimeType,
        metadata: {
          uploadedBy: uid,
          originalFilename: filename || 'frame',
          uploadTimestamp: timestamp.toString()
        }
      }
    });

    // Make file publicly accessible
    await file.makePublic();

    // Generate public URL
    const publicUrl = `https://storage.googleapis.com/${bucket.name}/${storagePath}`;

    console.log(`[uploadBase64Image] Successfully uploaded to: ${publicUrl}`);

    return {
      success: true,
      url: publicUrl,
      filename: storagePath,
      size: imageBuffer.length
    };

  } catch (error) {
    console.error('[uploadBase64Image] Error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to upload image'));
  }
});

/**
 * extractVideoFrame - Extract a frame from a video at a specific timestamp
 *
 * Since Firebase Storage videos may have CORS issues for canvas capture,
 * this function extracts a frame server-side and returns a usable URL.
 *
 * For simplicity, this extracts the last frame (or near-end frame) as that's
 * the most common use case for shot-to-shot continuity.
 */
exports.extractVideoFrame = functions
  .runWith({
    timeoutSeconds: 120,
    memory: '1GB'
  })
  .https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const { videoUrl, timestamp = 'last' } = data;

  if (!videoUrl) {
    throw new functions.https.HttpsError('invalid-argument', 'Video URL is required');
  }

  try {
    console.log(`[extractVideoFrame] Extracting frame from: ${videoUrl}, timestamp: ${timestamp}`);

    // For Firebase Storage videos, we'll use a different approach:
    // Download the video and extract a frame using canvas in a headless environment
    // Since we can't use ffmpeg easily in Cloud Functions, we'll use a workaround

    // Option 1: If the video is from our storage, we can generate a signed URL with CORS
    if (videoUrl.includes('storage.googleapis.com') || videoUrl.includes('firebasestorage.googleapis.com')) {
      // Extract the file path from the URL
      const bucket = admin.storage().bucket();

      // Parse the URL to get the file path
      let filePath;
      if (videoUrl.includes('storage.googleapis.com')) {
        // Format: https://storage.googleapis.com/bucket-name/path/to/file.mp4
        const urlParts = videoUrl.split('.com/')[1];
        const bucketAndPath = urlParts.split('/');
        filePath = bucketAndPath.slice(1).join('/');
      } else {
        // Firebase Storage URL format
        const match = videoUrl.match(/o\/(.+?)\?/);
        if (match) {
          filePath = decodeURIComponent(match[1]);
        }
      }

      if (filePath) {
        console.log(`[extractVideoFrame] File path: ${filePath}`);

        // Generate a signed URL with CORS-friendly headers
        const file = bucket.file(filePath);
        const [signedUrl] = await file.getSignedUrl({
          action: 'read',
          expires: Date.now() + 3600000, // 1 hour
          responseDisposition: 'inline',
          responseType: 'video/mp4'
        });

        console.log(`[extractVideoFrame] Generated signed URL for CORS-friendly access`);

        // Return the signed URL - the client can use this to load the video with CORS
        return {
          success: true,
          signedVideoUrl: signedUrl,
          message: 'Use this signed URL to reload the video for frame capture'
        };
      }
    }

    // Fallback: Return an error asking user to try again
    throw new Error('Could not process video URL format');

  } catch (error) {
    console.error('[extractVideoFrame] Error:', error);
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to extract video frame'));
  }
});

/**
 * creationWizardAssembleSceneVideo - Creates a scene video sequence from shot videos
 *
 * Note: This creates a video sequence manifest that can be:
 * 1. Played back sequentially by a video player
 * 2. Concatenated during export using ffmpeg
 * 3. Processed by a cloud video editing service
 *
 * For actual video concatenation, we would need ffmpeg or a video editing API.
 * This function prepares the sequence data for those operations.
 */
exports.creationWizardAssembleSceneVideo = functions
  .runWith({
    timeoutSeconds: 60,
    memory: '256MB'
  })
  .https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const {
    sceneId,
    projectId,
    shots,              // Array of { shotId, videoUrl, duration, transition }
    transitions = 'cut' // Default transition between shots
  } = data;

  if (!shots || !Array.isArray(shots) || shots.length === 0) {
    throw new functions.https.HttpsError('invalid-argument', 'Shots array with video URLs is required');
  }

  // Validate all shots have video URLs
  const shotsWithVideos = shots.filter(s => s.videoUrl);
  if (shotsWithVideos.length === 0) {
    throw new functions.https.HttpsError('invalid-argument', 'At least one shot must have a video URL');
  }

  try {
    // Calculate total duration
    const totalDuration = shotsWithVideos.reduce((sum, shot) => sum + (shot.duration || 2), 0);

    // Build scene video sequence manifest
    const sceneSequence = {
      sceneId,
      projectId: projectId || null,
      createdAt: new Date().toISOString(),
      createdBy: uid,
      totalDuration,
      shotCount: shotsWithVideos.length,
      defaultTransition: transitions,
      shots: shotsWithVideos.map((shot, idx) => ({
        index: idx,
        shotId: shot.shotId,
        shotType: shot.shotType || 'medium',
        videoUrl: shot.videoUrl,
        duration: shot.duration || 2,
        startTime: shotsWithVideos.slice(0, idx).reduce((sum, s) => sum + (s.duration || 2), 0),
        transition: shot.transition || transitions,
        transitionDuration: 0.5 // Half second transition
      })),
      // Playback configuration
      playback: {
        mode: 'sequential', // sequential | loop | shuffle
        autoPlay: true,
        preload: true
      },
      // Export configuration (for when user exports the video)
      exportConfig: {
        format: 'mp4',
        codec: 'h264',
        quality: 'high',
        fps: 30
      }
    };

    // Store the scene sequence in Firestore for later retrieval
    const sequenceRef = await db.collection('sceneVideoSequences').add({
      ...sceneSequence,
      userId: uid,
      status: 'ready',
      timestamp: admin.firestore.FieldValue.serverTimestamp()
    });

    console.log(`[creationWizardAssembleSceneVideo] Created sequence ${sequenceRef.id} for scene ${sceneId} with ${shotsWithVideos.length} shots`);

    return {
      success: true,
      sequenceId: sequenceRef.id,
      sceneId,
      totalDuration,
      shotCount: shotsWithVideos.length,
      sequence: sceneSequence,
      // Provide a preview playlist for immediate playback
      playlist: shotsWithVideos.map(shot => ({
        url: shot.videoUrl,
        duration: shot.duration || 2
      }))
    };

  } catch (error) {
    console.error('[creationWizardAssembleSceneVideo] Error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to assemble scene video'));
  }
});

/**
 * creationWizardGetSceneVideoSequence - Retrieve a saved scene video sequence
 */
exports.creationWizardGetSceneVideoSequence = functions.https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const { sequenceId, sceneId } = data;

  try {
    let sequence = null;

    if (sequenceId) {
      // Get by sequence ID
      const doc = await db.collection('sceneVideoSequences').doc(sequenceId).get();
      if (doc.exists && doc.data().userId === uid) {
        sequence = { id: doc.id, ...doc.data() };
      }
    } else if (sceneId) {
      // Get latest sequence for a scene
      const snapshot = await db.collection('sceneVideoSequences')
        .where('userId', '==', uid)
        .where('sceneId', '==', sceneId)
        .orderBy('timestamp', 'desc')
        .limit(1)
        .get();

      if (!snapshot.empty) {
        const doc = snapshot.docs[0];
        sequence = { id: doc.id, ...doc.data() };
      }
    }

    if (!sequence) {
      return { success: false, message: 'Sequence not found' };
    }

    return {
      success: true,
      sequence
    };

  } catch (error) {
    console.error('[creationWizardGetSceneVideoSequence] Error:', error);
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to get scene sequence'));
  }
});

// =============================================================================
// PHASE 12C: BATCH OPERATIONS & CROSS-SCENE CONSISTENCY
// Decompose all scenes at once with unified visual style
// =============================================================================

/**
 * creationWizardBatchDecomposeScenes - Decompose all scenes into shots at once
 *
 * Creates a unified visual consistency profile that applies across ALL scenes,
 * ensuring the entire video maintains a cohesive look.
 */
exports.creationWizardBatchDecomposeScenes = functions
  .runWith({
    timeoutSeconds: 540, // 9 minutes for processing all scenes
    memory: '1GB'
  })
  .https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const {
    scenes,             // Array of scene objects { id, visualPrompt, narration, duration }
    targetShotCount,    // Default shots per scene
    clipDuration: inputClipDuration, // Video clip duration (6 or 10 seconds from Minimax)
    genre,
    productionMode,
    styleBible,         // Global style bible for consistency
    characterBible,     // Character definitions
    globalConsistency   // { enabled: true, anchorSceneId: null } - use first scene as anchor
  } = data;

  // MINIMAX CONSTRAINT: Clip duration must be exactly 6 or 10 seconds - DEFAULT TO 10s
  const clipDuration = inputClipDuration === 6 ? 6 : 10;

  if (!scenes || !Array.isArray(scenes) || scenes.length === 0) {
    throw new functions.https.HttpsError('invalid-argument', 'Scenes array is required');
  }

  console.log(`[creationWizardBatchDecomposeScenes] Starting batch decomposition of ${scenes.length} scenes, clipDuration: ${clipDuration}s`);

  try {
    // Step 1: Generate a GLOBAL visual consistency profile
    // This ensures ALL scenes share the same visual DNA
    let globalVisualProfile = null;

    if (globalConsistency?.enabled !== false) {
      console.log('[creationWizardBatchDecomposeScenes] Generating global visual profile...');

      const profilePrompt = `Analyze these scene descriptions and create a UNIFIED visual style profile that will maintain consistency across ALL scenes.

SCENES:
${scenes.map((s, i) => `Scene ${i + 1}: ${s.visualPrompt || s.visual || ''}`).join('\n')}

GENRE: ${genre || 'cinematic'}
PRODUCTION MODE: ${productionMode || 'premium'}

${styleBible ? `STYLE BIBLE: ${JSON.stringify(styleBible)}` : ''}
${characterBible && characterBible.length > 0 ? `CHARACTERS: ${JSON.stringify(characterBible)}` : ''}

Create a unified visual profile that ALL shots across ALL scenes must follow:

Return JSON:
{
  "visualDNA": {
    "cinematicStyle": "The overall cinematic approach (e.g., 'neo-noir thriller', 'warm documentary')",
    "colorPalette": {
      "primary": "Main color (e.g., 'deep teal')",
      "secondary": "Accent color (e.g., 'warm amber')",
      "shadows": "Shadow tone (e.g., 'cool blue-black')",
      "highlights": "Highlight tone (e.g., 'golden')"
    },
    "lighting": {
      "style": "Overall lighting approach (e.g., 'high contrast chiaroscuro')",
      "keyLight": "Main light character (e.g., 'hard side light')",
      "fillRatio": "Fill to key ratio (e.g., '1:4 dramatic')",
      "practicals": "Practical lights in scene (e.g., 'neon signs, screens')"
    },
    "atmosphere": {
      "mood": "Emotional quality (e.g., 'tense, anticipatory')",
      "texture": "Visual texture (e.g., 'film grain, subtle haze')",
      "depth": "Depth of field approach (e.g., 'shallow DOF isolating subjects')"
    },
    "cameraLanguage": {
      "movement": "Overall camera philosophy (e.g., 'deliberate, controlled movements')",
      "angles": "Preferred angles (e.g., 'low angles for power, eye-level for intimacy')",
      "lensChoice": "Lens character (e.g., 'anamorphic wide, slight distortion')"
    }
  },
  "sceneTransitions": {
    "style": "How scenes should flow (e.g., 'match cuts on movement, dissolves for time')",
    "pacing": "Overall rhythm (e.g., 'building tension, accelerating cuts')"
  },
  "consistencyRules": [
    "Rule 1 for maintaining consistency",
    "Rule 2...",
    "Rule 3..."
  ]
}`;

      const profileCompletion = await openai.chat.completions.create({
        model: 'gpt-4o',
        messages: [
          { role: 'system', content: 'You are a cinematographer creating a unified visual style guide for a video project. Output valid JSON only.' },
          { role: 'user', content: profilePrompt }
        ],
        response_format: { type: 'json_object' },
        temperature: 0.7,
        max_tokens: 1500
      });

      try {
        globalVisualProfile = JSON.parse(profileCompletion.choices[0].message.content.trim());
        console.log('[creationWizardBatchDecomposeScenes] Global visual profile created');
      } catch (e) {
        console.warn('[creationWizardBatchDecomposeScenes] Failed to parse visual profile, continuing without');
      }
    }

    // Step 2: Decompose each scene with the global profile
    const results = [];
    const defaultShots = targetShotCount || 3;

    // Cross-scene continuity tracking (Upgrade 3)
    let previousSceneEndState = null;
    let previousSceneId = null;
    let previousSceneTransition = null;

    for (let i = 0; i < scenes.length; i++) {
      const scene = scenes[i];
      const sceneDescription = scene.visualPrompt || scene.visual || '';
      const sceneDuration = scene.duration || 6;

      try {
        // Analyze scene type
        const sceneType = SHOT_DECOMPOSITION_ENGINE.analyzeSceneType(sceneDescription);

        // Build cross-scene context from previous scene (Upgrade 3)
        const crossSceneContext = previousSceneEndState ? {
          previousSceneEndState,
          previousSceneId,
          transitionType: previousSceneTransition || scene.transition?.type || 'cut'
        } : null;

        if (crossSceneContext) {
          console.log(`[creationWizardBatchDecomposeScenes] Scene ${i + 1} cross-scene: continuing from "${previousSceneEndState?.substring(0, 50)}..."`);
        }

        // Calculate shot count using Hollywood formula: ceil(sceneDuration / clipDuration)
        const hollywoodShotCount = Math.ceil(sceneDuration / clipDuration);
        const actualShotCount = Math.min(Math.max(targetShotCount || hollywoodShotCount, 2), 10);

        // Generate sequence with EXACT Minimax clip duration
        const baseSequence = SHOT_DECOMPOSITION_ENGINE.generateShotSequence(sceneType, actualShotCount, sceneDuration, clipDuration);

        // Build prompt with global consistency requirements
        const systemPrompt = `You are a Hollywood cinematographer decomposing Scene ${i + 1} of ${scenes.length} into shots.

CRITICAL: This scene is part of a LARGER VIDEO. ALL shots must match the global visual style.

${globalVisualProfile ? `
GLOBAL VISUAL DNA (MUST FOLLOW):
${JSON.stringify(globalVisualProfile.visualDNA, null, 2)}

CONSISTENCY RULES:
${globalVisualProfile.consistencyRules?.map((r, idx) => `${idx + 1}. ${r}`).join('\n') || 'Maintain visual consistency across all shots'}
` : ''}

Output JSON with shots array containing detailed prompts that EXPLICITLY include:
1. The global color palette
2. The lighting style
3. The atmosphere/mood
4. Camera language

Each shot prompt must be 150+ words and include these visual DNA elements.`;

        // Build cross-scene section for prompt (Upgrade 3)
        const crossScenePromptSection = crossSceneContext ? `
CROSS-SCENE CONTINUITY (IMPORTANT):
Previous scene (Scene ${i}) ended with: "${crossSceneContext.previousSceneEndState}"
Transition type to this scene: ${crossSceneContext.transitionType}

The FIRST SHOT of this scene should:
- Flow naturally from the previous scene's end state
- Create visual continuity (match cut, dissolve, or continuation as appropriate)
- The first shot's opening should acknowledge where we came from
` : '';

        const userPrompt = `Decompose Scene ${i + 1} into ${actualShotCount} shots:

SCENE DESCRIPTION:
${sceneDescription}

DURATION: ${sceneDuration}s
SCENE TYPE: ${sceneType}
${crossScenePromptSection}
SHOT SEQUENCE TEMPLATE:
${JSON.stringify(baseSequence, null, 2)}

${scene.narration ? `NARRATION: "${scene.narration}"` : ''}
${scene.sceneAction ? `SCENE ACTION: "${scene.sceneAction}"` : ''}

Return JSON:
{
  "shots": [
    {
      "shotIndex": 1,
      "shotType": "wide|medium|closeup|etc",
      "cameraMovement": "movement type",
      "prompt": "DETAILED prompt (150+ words) including global visual DNA...",
      "purpose": "narrative purpose",
      "endState": "How this shot ends (position/pose) - for frame-chain capture"
    }
  ],
  "sceneConsistencyNotes": "How this scene connects to others visually",
  "sceneEndState": "How the FINAL shot of this scene ends - will be passed to next scene for continuity"
}

IMPORTANT: Do NOT include duration in your response - durations are fixed at ${clipDuration || 10} seconds per shot.`;

        const completion = await openai.chat.completions.create({
          model: 'gpt-4o',
          messages: [
            { role: 'system', content: systemPrompt },
            { role: 'user', content: userPrompt }
          ],
          response_format: { type: 'json_object' },
          temperature: 0.7,
          max_tokens: 3000
        });

        const shotData = JSON.parse(completion.choices[0].message.content.trim());

        // MINIMAX CONSTRAINT: Each shot must be exactly 6s or 10s - DEFAULT TO 10s
        const validClipDuration = clipDuration === 6 ? 6 : 10;

        // Normalize shots - FORCE duration to be exactly clipDuration
        // Include endState for frame-chain capture (Upgrade 3)
        const normalizedShots = (shotData.shots || []).map((shot, idx) => ({
          id: `${scene.id}_shot_${idx + 1}`,
          sceneId: scene.id,
          shotIndex: shot.shotIndex || idx + 1,
          shotType: shot.shotType || baseSequence[idx]?.shotType || 'medium',
          duration: validClipDuration, // ALWAYS use Minimax clip duration (6 or 10)
          cameraMovement: shot.cameraMovement || baseSequence[idx]?.cameraMovement || 'static',
          prompt: shot.prompt,
          purpose: shot.purpose || 'scene coverage',
          endState: shot.endState || null, // For frame-chain capture
          status: 'pending',
          imageUrl: null,
          videoUrl: null
        }));

        // Extract scene end state for cross-scene continuity (Upgrade 3)
        const sceneEndState = shotData.sceneEndState ||
          (normalizedShots.length > 0 ? normalizedShots[normalizedShots.length - 1].endState : null);

        results.push({
          sceneId: scene.id,
          sceneIndex: i,
          status: 'ready',
          sceneType,
          totalDuration: sceneDuration,
          shotCount: normalizedShots.length,
          shots: normalizedShots,
          consistencyNotes: shotData.sceneConsistencyNotes || null,
          // Cross-scene continuity data (Upgrade 3)
          crossSceneContinuity: {
            previousSceneId: crossSceneContext?.previousSceneId || null,
            transitionFromPrevious: crossSceneContext?.transitionType || null,
            sceneEndState: sceneEndState,
            suggestedNextTransition: shotData.suggestedNextTransition || 'cut'
          }
        });

        // Update cross-scene tracking for next iteration (Upgrade 3)
        previousSceneEndState = sceneEndState;
        previousSceneId = scene.id;
        previousSceneTransition = shotData.suggestedNextTransition || scene.transition?.type || 'cut';

        console.log(`[creationWizardBatchDecomposeScenes] Scene ${i + 1}/${scenes.length} decomposed: ${normalizedShots.length} shots`);
        if (sceneEndState) {
          console.log(`[creationWizardBatchDecomposeScenes] Scene ${i + 1} ends with: "${sceneEndState.substring(0, 60)}..."`);
        }

        // Small delay between scenes
        if (i < scenes.length - 1) {
          await new Promise(resolve => setTimeout(resolve, 500));
        }

      } catch (sceneError) {
        console.error(`[creationWizardBatchDecomposeScenes] Scene ${i + 1} failed:`, sceneError);
        results.push({
          sceneId: scene.id,
          sceneIndex: i,
          status: 'error',
          error: sceneError.message || 'Decomposition failed',
          shots: []
        });
      }
    }

    // Log usage
    const successCount = results.filter(r => r.status === 'ready').length;
    const totalShots = results.reduce((sum, r) => sum + (r.shots?.length || 0), 0);

    await db.collection('apiUsage').add({
      userId: uid,
      type: 'batch_shot_decomposition',
      scenesProcessed: scenes.length,
      scenesSuccessful: successCount,
      totalShotsCreated: totalShots,
      timestamp: admin.firestore.FieldValue.serverTimestamp()
    });

    // Build cross-scene continuity chain summary (Upgrade 3)
    const continuityChain = results
      .filter(r => r.status === 'ready' && r.crossSceneContinuity?.sceneEndState)
      .map(r => ({
        sceneId: r.sceneId,
        sceneIndex: r.sceneIndex,
        endState: r.crossSceneContinuity.sceneEndState,
        nextTransition: r.crossSceneContinuity.suggestedNextTransition
      }));

    return {
      success: successCount > 0,
      totalScenes: scenes.length,
      successCount,
      failedCount: scenes.length - successCount,
      totalShots,
      globalVisualProfile,
      scenes: results,
      // Cross-scene continuity summary (Upgrade 3)
      crossSceneContinuity: {
        enabled: true,
        chainComplete: continuityChain.length === successCount,
        chain: continuityChain,
        description: 'Each scene ends in a capturable state that flows into the next scene'
      }
    };

  } catch (error) {
    console.error('[creationWizardBatchDecomposeScenes] Error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to batch decompose scenes'));
  }
});

/**
 * creationWizardBatchGenerateShotImages - Generate images for all shots across all scenes
 *
 * Uses the first generated image as a style anchor for subsequent images.
 * NOW SUPPORTS visualStyleMode for VISUAL_STYLE_DNA integration
 */
exports.creationWizardBatchGenerateShotImages = functions
  .runWith({
    timeoutSeconds: 540,
    memory: '2GB'
  })
  .https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const {
    decomposedScenes,   // Array of { sceneId, shots: [...] }
    model,              // Image model
    aspectRatio,
    characterReference, // Optional character reference image (base64) for face consistency
    characterBible,     // Optional character bible with names/descriptions for prompt building
    locationBible,      // Optional location bible with location references for background consistency
    globalVisualProfile, // From batch decomposition
    visualStyleMode     // VISUAL_STYLE_DNA mode (photorealistic, cinematic, etc.)
  } = data;

  // Log visual style mode for debugging
  console.log(`[creationWizardBatchGenerateShotImages] Using visual style mode: ${visualStyleMode || 'photorealistic (default)'}`);
  const effectiveStyleMode = visualStyleMode || 'photorealistic';

  if (!decomposedScenes || !Array.isArray(decomposedScenes) || decomposedScenes.length === 0) {
    throw new functions.https.HttpsError('invalid-argument', 'Decomposed scenes array is required');
  }

  const geminiApiKey = functions.config().gemini?.key;
  if (!geminiApiKey) {
    throw new functions.https.HttpsError('failed-precondition', 'Gemini API key not configured');
  }

  const ai = new GoogleGenAI({ apiKey: geminiApiKey });
  const storage = admin.storage().bucket();
  const timestamp = Date.now();

  // Log character and location consistency parameters
  console.log(`[creationWizardBatchGenerateShotImages] Character Reference: ${characterReference?.base64 ? 'YES (base64 present)' : 'NO'}`);
  console.log(`[creationWizardBatchGenerateShotImages] Character Bible: ${characterBible?.enabled ? `${characterBible.characters?.length || 0} characters` : 'disabled'}`);
  console.log(`[creationWizardBatchGenerateShotImages] Location Bible: ${locationBible?.enabled ? `${locationBible.locations?.length || 0} locations` : 'disabled'}`);

  // Flatten all shots
  const allShots = [];
  decomposedScenes.forEach(scene => {
    (scene.shots || []).forEach(shot => {
      allShots.push({
        ...shot,
        sceneId: scene.sceneId
      });
    });
  });

  console.log(`[creationWizardBatchGenerateShotImages] Starting batch generation of ${allShots.length} shots across ${decomposedScenes.length} scenes`);

  const results = [];
  let anchorImageBuffer = null; // First image becomes the style anchor

  for (let i = 0; i < allShots.length; i++) {
    const shot = allShots[i];

    try {
      // =========================================================
      // USE NANOBANANA_PROMPT_BUILDER for narrative-style prompts
      // This creates photorealistic, well-composed images
      // =========================================================

      // Build context from shot data and global visual profile
      const promptContext = {
        visualPrompt: shot.prompt || shot.imagePrompt || '',
        environment: shot.environment || '',
        mood: shot.mood || globalVisualProfile?.visualDNA?.atmosphere?.mood || '',
        atmosphere: globalVisualProfile?.visualDNA?.atmosphere?.mood || '',
        subject: shot.subject || 'the subject',
        action: shot.narrativeBeat?.action || shot.action || ''
      };

      // Build style bible from global visual profile
      const batchStyleBible = globalVisualProfile?.visualDNA ? {
        enabled: true,
        style: globalVisualProfile.visualDNA.cinematicStyle || 'cinematic',
        lighting: globalVisualProfile.visualDNA.lighting?.style || 'professional cinematic lighting',
        colorGrade: globalVisualProfile.visualDNA.colorPalette?.primary || 'cinematic color grade',
        atmosphere: globalVisualProfile.visualDNA.atmosphere?.mood || 'engaging'
      } : null;

      // Build the enhanced narrative prompt using NANOBANANA_PROMPT_BUILDER
      // Pass characterBible for character descriptions in prompts (names, attire, physical features)
      // Character reference image (base64) is passed separately to Gemini for face consistency
      // Build the enhanced narrative prompt using NANOBANANA_PROMPT_BUILDER
      // NOW WITH VISUAL_STYLE_DNA integration for jaw-dropping results
      let enhancedPrompt = NANOBANANA_PROMPT_BUILDER.buildShotPrompt(
        {
          shotType: shot.shotType || 'medium',
          subject: promptContext.subject,
          action: promptContext.action,
          mood: promptContext.mood
        },
        promptContext,
        characterBible, // Pass character bible for descriptions in prompt
        batchStyleBible,
        effectiveStyleMode  // VISUAL_STYLE_DNA mode
      );

      // Append original prompt content for scene-specific details
      if (shot.prompt && shot.prompt.trim()) {
        enhancedPrompt += `\n\nSPECIFIC SCENE DETAILS: ${shot.prompt}`;
      }

      console.log(`[creationWizardBatchGenerateShotImages] Shot ${i + 1}/${allShots.length} prompt (first 200 chars): ${enhancedPrompt.substring(0, 200)}...`);

      // Build content parts for Gemini API
      const contentParts = [];

      // =========================================================
      // LOCATION REFERENCE FIRST (establishes the background/environment)
      // This ensures consistent environments across shots
      // =========================================================
      let locationRef = null;
      if (locationBible?.enabled && locationBible.locations?.length > 0) {
        // Find location for this scene
        locationRef = locationBible.locations.find(loc => {
          const includeInAll = !loc.appliedToScenes || loc.appliedToScenes.length === 0;
          const includeInScene = loc.appliedToScenes && loc.appliedToScenes.includes(shot.sceneId);
          return (includeInAll || includeInScene) &&
                 loc.referenceImageBase64 &&
                 loc.referenceImageStatus === 'ready';
        });

        if (locationRef && locationRef.referenceImageBase64) {
          contentParts.push({
            inlineData: {
              mimeType: locationRef.referenceImageMimeType || 'image/png',
              data: locationRef.referenceImageBase64
            }
          });
          // Build location instruction with all metadata
          const locationInstruction = [
            `LOCATION/ENVIRONMENT REFERENCE (Image 1):`,
            `Use this exact environment as the background setting.`,
            `Location: ${locationRef.name}`,
            locationRef.description ? `Description: ${locationRef.description}` : '',
            `Time: ${locationRef.timeOfDay || 'day'}`,
            locationRef.weather && locationRef.weather !== 'clear' ? `Weather: ${locationRef.weather}` : '',
            locationRef.mood ? `Mood: ${locationRef.mood}` : '',
            locationRef.lightingStyle ? `Lighting: ${locationRef.lightingStyle}` : '',
            `Match the exact colors, textures, and atmosphere from this reference image.`,
            ``
          ].filter(line => line).join('\n');

          contentParts.push({ text: locationInstruction });
          console.log(`[creationWizardBatchGenerateShotImages] Added location reference: "${locationRef.name}" for Scene ${shot.sceneId}`);
        }
      }

      // =========================================================
      // CHARACTER REFERENCE SECOND (subjects placed in the environment)
      // Gemini uses this for face/appearance consistency
      // =========================================================
      if (characterReference?.base64) {
        const charImageNum = locationRef ? '(Image 2)' : '(Image 1)';
        contentParts.push({
          inlineData: {
            mimeType: characterReference.mimeType || 'image/png',
            data: characterReference.base64
          }
        });
        // Add instruction to use the reference
        contentParts.push({
          text: `CHARACTER REFERENCE ${charImageNum}: Use this character's face, features, skin tone, and appearance for the main character. ${locationRef ? 'Place them naturally within the environment above. ' : ''}Maintain facial consistency and match the lighting conditions.\n\n`
        });
        console.log(`[creationWizardBatchGenerateShotImages] Added character reference image for face consistency`);
      }

      // Add the main prompt text (combined with location context if available)
      let finalPrompt = enhancedPrompt;
      if (locationRef) {
        // Add explicit framing instructions when using location reference
        finalPrompt = `[IMPORTANT: Generate the CHARACTER as the main subject - FULL BODY or at minimum WAIST-UP visible, FACING THE CAMERA. Use the location reference as the background environment only.]\n\n${enhancedPrompt}`;
      }

      // Add character visibility instructions if we have both location and character
      if (locationRef && characterReference?.base64) {
        finalPrompt += `\n\n[CRITICAL FRAMING: The CHARACTER must be the HERO of the shot - clearly visible, well-lit, and properly framed. DO NOT cut off the head or face. The character should be prominent in the composition, not hidden or obscured by the environment.]`;
      }

      contentParts.push({
        text: finalPrompt
      });

      // Generate image - Use correct NanoBanana models
      // gemini-3-pro-image-preview supports up to 4K resolution
      const geminiModelId = model === 'nanobanana' ? 'gemini-2.5-flash-image' : 'gemini-3-pro-image-preview';

      // Determine output resolution based on model
      // gemini-3-pro-image-preview supports 1K, 2K, 4K
      // gemini-2.5-flash-image supports up to 1K only
      const outputImageSize = model === 'nanobanana' ? undefined : '2K';

      console.log(`[creationWizardBatchGenerateShotImages] Generating shot ${i + 1}/${allShots.length} (Scene ${shot.sceneId}, Shot ${shot.shotIndex})`);
      console.log(`[creationWizardBatchGenerateShotImages] Using model: ${geminiModelId}, aspect ratio: 16:9, image_size: ${outputImageSize || 'default'}, parts: ${contentParts.length}`);

      const result = await ai.models.generateContent({
        model: geminiModelId,
        contents: [{ role: 'user', parts: contentParts }],
        config: {
          responseModalities: ['IMAGE', 'TEXT'],
          imageConfig: {
            aspectRatio: '16:9',  // Proper 16:9 for video frames
            ...(outputImageSize && { image_size: outputImageSize })  // HD resolution for Pro model
          }
        }
      });

      // Extract image
      const candidates = result.candidates || (result.response && result.response.candidates);
      if (candidates && candidates.length > 0) {
        const parts = candidates[0].content?.parts || candidates[0].parts || [];
        for (const part of parts) {
          const inlineData = part.inlineData || part.inline_data;
          if (inlineData && (inlineData.data || inlineData.bytesBase64Encoded)) {
            const imageBytes = inlineData.data || inlineData.bytesBase64Encoded;
            const rawBuffer = Buffer.from(imageBytes, 'base64');

            // Process image
            const { buffer: processedBuffer, width, height } = await enforceImageDimensions(rawBuffer, aspectRatio || '16:9', 'hd');

            // Save first image as anchor
            if (i === 0) {
              anchorImageBuffer = processedBuffer;
            }

            // Save to storage
            const fileName = `creative-studio/${uid}/${timestamp}-scene${shot.sceneId}-shot${shot.shotIndex}.png`;
            const file = storage.file(fileName);

            await file.save(processedBuffer, {
              metadata: { contentType: 'image/png' }
            });

            await file.makePublic();
            let publicUrl = `https://storage.googleapis.com/${storage.name}/${fileName}`;

            // UPSCALE: Enhance image quality using FAL AuraSR (Fix 3)
            try {
              const upscaleResult = await upscaleImageInternal(publicUrl, aspectRatio || '16:9', 'hd');
              if (upscaleResult.success) {
                console.log(`[creationWizardBatchGenerateShotImages] Shot ${i + 1} upscaled to HD`);
                publicUrl = upscaleResult.url;
              } else if (upscaleResult.skipped) {
                console.log(`[creationWizardBatchGenerateShotImages] Shot ${i + 1} upscale skipped`);
              }
            } catch (upscaleError) {
              console.warn(`[creationWizardBatchGenerateShotImages] Shot ${i + 1} upscale failed:`, upscaleError.message);
            }

            results.push({
              sceneId: shot.sceneId,
              shotId: shot.id,
              shotIndex: shot.shotIndex,
              status: 'ready',
              imageUrl: publicUrl,
              width,
              height
            });

            console.log(`[creationWizardBatchGenerateShotImages] Shot ${i + 1}/${allShots.length} saved: ${fileName}`);
            break;
          }
        }
      }

      if (!results.find(r => r.shotId === shot.id)) {
        results.push({
          sceneId: shot.sceneId,
          shotId: shot.id,
          shotIndex: shot.shotIndex,
          status: 'error',
          error: 'No image generated'
        });
      }

      // Rate limiting delay
      if (i < allShots.length - 1) {
        await new Promise(resolve => setTimeout(resolve, 2000));
      }

    } catch (shotError) {
      console.error(`[creationWizardBatchGenerateShotImages] Shot ${i + 1} failed:`, shotError);
      results.push({
        sceneId: shot.sceneId,
        shotId: shot.id,
        shotIndex: shot.shotIndex,
        status: 'error',
        error: shotError.message || 'Generation failed'
      });
    }
  }

  const successCount = results.filter(r => r.status === 'ready').length;

  // Log usage
  await db.collection('apiUsage').add({
    userId: uid,
    type: 'batch_shot_image_generation',
    totalShots: allShots.length,
    successCount,
    model: model || 'nanobanana-pro',
    timestamp: admin.firestore.FieldValue.serverTimestamp()
  });

  return {
    success: successCount > 0,
    totalShots: allShots.length,
    successCount,
    failedCount: allShots.length - successCount,
    shots: results
  };
});

/**
 * creationWizardExportMultiShotProject - Prepare multi-shot project for export
 *
 * Creates an export manifest that includes all shot videos organized by scene,
 * ready for final video assembly.
 */
exports.creationWizardExportMultiShotProject = functions
  .runWith({
    timeoutSeconds: 120,
    memory: '512MB'
  })
  .https.onCall(async (data, context) => {
  const uid = await verifyAuth(context);
  const {
    projectId,
    scenes,             // Array of { sceneId, shots: [...with videoUrl] }
    audioTrack,         // Optional background audio
    voiceover,          // Optional voiceover track
    outputFormat = 'mp4',
    quality = 'high'
  } = data;

  if (!scenes || !Array.isArray(scenes) || scenes.length === 0) {
    throw new functions.https.HttpsError('invalid-argument', 'Scenes with shot videos required');
  }

  try {
    // Build export manifest
    const exportManifest = {
      projectId: projectId || `project_${Date.now()}`,
      createdAt: new Date().toISOString(),
      createdBy: uid,
      format: outputFormat,
      quality,
      scenes: [],
      totalDuration: 0,
      totalShots: 0
    };

    let currentTime = 0;

    for (const scene of scenes) {
      const sceneShots = (scene.shots || []).filter(s => s.videoUrl);

      if (sceneShots.length === 0) continue;

      const sceneDuration = sceneShots.reduce((sum, shot) => sum + (shot.duration || 6), 0);

      exportManifest.scenes.push({
        sceneId: scene.sceneId,
        sceneIndex: exportManifest.scenes.length,
        startTime: currentTime,
        duration: sceneDuration,
        shots: sceneShots.map((shot, idx) => ({
          shotId: shot.id || shot.shotId,
          shotIndex: idx,
          shotType: shot.shotType,
          videoUrl: shot.videoUrl,
          duration: shot.duration || 6,
          startTime: currentTime + sceneShots.slice(0, idx).reduce((sum, s) => sum + (s.duration || 6), 0),
          transition: shot.transition || 'cut'
        }))
      });

      currentTime += sceneDuration;
      exportManifest.totalShots += sceneShots.length;
    }

    exportManifest.totalDuration = currentTime;

    // Add audio tracks if provided
    if (audioTrack) {
      exportManifest.audioTrack = {
        url: audioTrack.url,
        volume: audioTrack.volume || 0.3,
        fadeIn: audioTrack.fadeIn || 2,
        fadeOut: audioTrack.fadeOut || 2
      };
    }

    if (voiceover) {
      exportManifest.voiceover = {
        url: voiceover.url,
        volume: voiceover.volume || 1.0
      };
    }

    // Store export manifest
    const manifestRef = await db.collection('exportManifests').add({
      ...exportManifest,
      userId: uid,
      status: 'ready',
      timestamp: admin.firestore.FieldValue.serverTimestamp()
    });

    console.log(`[creationWizardExportMultiShotProject] Created export manifest ${manifestRef.id}: ${exportManifest.scenes.length} scenes, ${exportManifest.totalShots} shots, ${exportManifest.totalDuration}s`);

    return {
      success: true,
      manifestId: manifestRef.id,
      manifest: exportManifest,
      // Generate FFmpeg-compatible concat file content
      ffmpegConcatFile: exportManifest.scenes.flatMap(scene =>
        scene.shots.map(shot => `file '${shot.videoUrl}'\nduration ${shot.duration}`)
      ).join('\n'),
      // Preview playlist
      playlist: exportManifest.scenes.flatMap(scene =>
        scene.shots.map(shot => ({
          sceneId: scene.sceneId,
          shotType: shot.shotType,
          url: shot.videoUrl,
          duration: shot.duration
        }))
      )
    };

  } catch (error) {
    console.error('[creationWizardExportMultiShotProject] Error:', error);
    if (error instanceof functions.https.HttpsError) throw error;
    throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to create export manifest'));
  }
});

// =============================================================================
// PHASE 12D: SHOT TRANSITIONS, AUDIO DUCKING & VIDEO EFFECTS
// =============================================================================

/**
 * SHOT_TRANSITION_ENGINE
 *
 * Manages smooth transitions between shots within a scene.
 * Different transition types convey different emotional meanings:
 * - Cut: Direct, energetic, fast-paced (action, dialogue)
 * - Dissolve: Passage of time, dreamy, reflective
 * - Fade: Beginning/end of segments, dramatic pause
 * - Wipe: Scene change, geographic shift, playful
 */
const SHOT_TRANSITION_ENGINE = {
  // Transition type library
  transitionTypes: {
    'cut': {
      name: 'Cut',
      description: 'Instant transition, most common',
      duration: 0,
      ffmpegFilter: null, // No filter needed for cut
      emotionalTone: ['energetic', 'action', 'dialogue', 'fast-paced'],
      useFor: ['same location shots', 'dialogue exchanges', 'action sequences']
    },
    'dissolve': {
      name: 'Cross Dissolve',
      description: 'Gradual blend between shots',
      duration: 0.5,
      ffmpegFilter: 'xfade=transition=dissolve:duration={duration}:offset={offset}',
      emotionalTone: ['dreamy', 'reflective', 'passage of time', 'romantic'],
      useFor: ['time passage', 'memory sequences', 'emotional moments']
    },
    'fade_black': {
      name: 'Fade to Black',
      description: 'Fade out to black, then fade in',
      duration: 0.8,
      ffmpegFilter: 'xfade=transition=fadeblack:duration={duration}:offset={offset}',
      emotionalTone: ['dramatic', 'conclusive', 'mysterious', 'serious'],
      useFor: ['scene endings', 'dramatic pauses', 'chapter breaks']
    },
    'fade_white': {
      name: 'Fade to White',
      description: 'Fade out to white, then fade in',
      duration: 0.8,
      ffmpegFilter: 'xfade=transition=fadewhite:duration={duration}:offset={offset}',
      emotionalTone: ['ethereal', 'heavenly', 'revelation', 'flashback'],
      useFor: ['dream sequences', 'flashbacks', 'revelations']
    },
    'wipe_left': {
      name: 'Wipe Left',
      description: 'Next shot wipes in from right',
      duration: 0.4,
      ffmpegFilter: 'xfade=transition=wipeleft:duration={duration}:offset={offset}',
      emotionalTone: ['playful', 'energetic', 'forward motion'],
      useFor: ['location changes', 'montages', 'educational content']
    },
    'wipe_right': {
      name: 'Wipe Right',
      description: 'Next shot wipes in from left',
      duration: 0.4,
      ffmpegFilter: 'xfade=transition=wiperight:duration={duration}:offset={offset}',
      emotionalTone: ['playful', 'backward glance', 'contrast'],
      useFor: ['comparisons', 'before/after', 'location changes']
    },
    'slide_left': {
      name: 'Slide Left',
      description: 'Both shots slide together',
      duration: 0.3,
      ffmpegFilter: 'xfade=transition=slideleft:duration={duration}:offset={offset}',
      emotionalTone: ['dynamic', 'modern', 'tech-forward'],
      useFor: ['product showcases', 'feature reveals', 'modern content']
    },
    'zoom_in': {
      name: 'Zoom In',
      description: 'Zoom into next shot',
      duration: 0.4,
      ffmpegFilter: 'xfade=transition=smoothup:duration={duration}:offset={offset}',
      emotionalTone: ['focus', 'importance', 'detail'],
      useFor: ['detail reveals', 'emotional focus', 'key moments']
    },
    'blur': {
      name: 'Blur Transition',
      description: 'Blur out then blur in',
      duration: 0.6,
      ffmpegFilter: 'xfade=transition=pixelize:duration={duration}:offset={offset}',
      emotionalTone: ['disorientation', 'time warp', 'confusion'],
      useFor: ['dream sequences', 'intoxication', 'time travel']
    }
  },

  // Recommend transition based on shot types
  recommendTransition(fromShot, toShot, sceneContext = {}) {
    const fromType = fromShot?.shotType || 'medium';
    const toType = toShot?.shotType || 'medium';
    const mood = sceneContext.mood || 'neutral';

    // Special rules for shot type combinations
    const shotCombinations = {
      // Wide to close = dissolve (emotional buildup)
      'establishing_wide->closeup': 'dissolve',
      'wide->closeup': 'dissolve',
      'wide->extreme_closeup': 'dissolve',

      // Close to wide = cut (reveal)
      'closeup->wide': 'cut',
      'extreme_closeup->wide': 'cut',

      // Same shot type = cut (coverage)
      'medium->medium': 'cut',
      'closeup->closeup': 'cut',

      // Over shoulder to reverse = cut (dialogue)
      'over_shoulder->over_shoulder': 'cut',

      // POV transitions = dissolve
      'pov->medium': 'dissolve',
      'medium->pov': 'dissolve',

      // Insert to any = cut
      'insert->*': 'cut',
      '*->insert': 'cut'
    };

    // Check direct match
    const key = `${fromType}->${toType}`;
    if (shotCombinations[key]) {
      return shotCombinations[key];
    }

    // Check wildcard matches
    for (const [pattern, transition] of Object.entries(shotCombinations)) {
      if (pattern.includes('*')) {
        const [from, to] = pattern.split('->');
        if ((from === '*' || from === fromType) && (to === '*' || to === toType)) {
          return transition;
        }
      }
    }

    // Mood-based defaults
    const moodDefaults = {
      'action': 'cut',
      'energetic': 'cut',
      'fast': 'cut',
      'dialogue': 'cut',
      'dramatic': 'fade_black',
      'emotional': 'dissolve',
      'dreamy': 'dissolve',
      'reflective': 'dissolve',
      'mysterious': 'fade_black',
      'educational': 'wipe_left',
      'playful': 'slide_left'
    };

    return moodDefaults[mood] || 'cut';
  },

  // Generate FFmpeg filter for transition
  generateFFmpegFilter(transitionType, offset, customDuration = null) {
    const transition = this.transitionTypes[transitionType];
    if (!transition || !transition.ffmpegFilter) {
      return null; // Cut = no filter
    }

    const duration = customDuration || transition.duration;
    return transition.ffmpegFilter
      .replace('{duration}', duration.toFixed(2))
      .replace('{offset}', offset.toFixed(2));
  }
};

/**
 * AUDIO_DUCKING_ENGINE
 *
 * Automatically reduces background music volume when voiceover is present.
 * This creates professional-sounding audio mix without manual adjustment.
 */
const AUDIO_DUCKING_ENGINE = {
  // Ducking presets
  presets: {
    'subtle': {
      name: 'Subtle Ducking',
      duckLevel: 0.7, // Reduce to 70% during voice
      attackTime: 0.3, // How fast to duck (seconds)
      releaseTime: 0.5, // How fast to restore
      threshold: -30, // dB threshold to trigger
      description: 'Gentle reduction, music still prominent'
    },
    'standard': {
      name: 'Standard Ducking',
      duckLevel: 0.4, // Reduce to 40%
      attackTime: 0.2,
      releaseTime: 0.4,
      threshold: -25,
      description: 'Balanced mix, voice clearly dominant'
    },
    'aggressive': {
      name: 'Aggressive Ducking',
      duckLevel: 0.15, // Reduce to 15%
      attackTime: 0.1,
      releaseTime: 0.3,
      threshold: -20,
      description: 'Voice-forward, music barely audible during speech'
    },
    'podcast': {
      name: 'Podcast Style',
      duckLevel: 0.25,
      attackTime: 0.15,
      releaseTime: 0.35,
      threshold: -22,
      description: 'Optimized for spoken content'
    }
  },

  // Generate FFmpeg sidechain compressor filter
  generateFFmpegFilter(preset = 'standard', voiceTrack = '[voice]', musicTrack = '[music]') {
    const config = this.presets[preset] || this.presets['standard'];

    // Use FFmpeg sidechaincompress for ducking
    // This compresses the music signal when the voice signal exceeds threshold
    return {
      filter: `${musicTrack}${voiceTrack}sidechaincompress=threshold=${config.threshold}dB:ratio=10:attack=${config.attackTime * 1000}:release=${config.releaseTime * 1000}:level_sc=1[ducked_music]`,
      mixFilter: `[ducked_music]${voiceTrack}amix=inputs=2:duration=longest:dropout_transition=2:weights=${config.duckLevel} 1[final_audio]`,
      config
    };
  },

  // Simpler approach using volume automation
  generateVolumeAutomation(voiceSegments, musicDuration, preset = 'standard') {
    const config = this.presets[preset] || this.presets['standard'];
    const automationPoints = [];

    // Start at full volume
    automationPoints.push({ time: 0, volume: 1.0 });

    for (const segment of voiceSegments) {
      const startDuck = Math.max(0, segment.start - config.attackTime);
      const endDuck = Math.min(musicDuration, segment.end + config.releaseTime);

      // Ramp down before voice
      if (startDuck > automationPoints[automationPoints.length - 1].time) {
        automationPoints.push({ time: startDuck, volume: 1.0 });
      }
      automationPoints.push({ time: segment.start, volume: config.duckLevel });

      // Hold during voice
      automationPoints.push({ time: segment.end, volume: config.duckLevel });

      // Ramp up after voice
      automationPoints.push({ time: endDuck, volume: 1.0 });
    }

    // Ensure we end at the music duration
    if (automationPoints[automationPoints.length - 1].time < musicDuration) {
      automationPoints.push({ time: musicDuration, volume: 1.0 });
    }

    return automationPoints;
  },

  // Generate FFmpeg volume filter from automation points
  generateVolumeFilter(automationPoints) {
    // Convert to FFmpeg volume expression
    // volume='if(lt(t,1),1,if(lt(t,2),0.4,1))'
    const conditions = [];

    for (let i = 0; i < automationPoints.length - 1; i++) {
      const current = automationPoints[i];
      const next = automationPoints[i + 1];

      if (current.volume === next.volume) {
        // Constant volume segment
        conditions.push(`between(t,${current.time.toFixed(2)},${next.time.toFixed(2)})*${current.volume.toFixed(2)}`);
      } else {
        // Linear interpolation
        const slope = (next.volume - current.volume) / (next.time - current.time);
        conditions.push(`between(t,${current.time.toFixed(2)},${next.time.toFixed(2)})*(${current.volume.toFixed(2)}+${slope.toFixed(4)}*(t-${current.time.toFixed(2)}))`);
      }
    }

    return `volume='${conditions.join('+')}'`;
  }
};

/**
 * VIDEO_EFFECTS_ENGINE
 *
 * Applies visual effects to shots for cinematic quality.
 * Effects are applied as FFmpeg filter chains.
 */
const VIDEO_EFFECTS_ENGINE = {
  // Effect presets
  presets: {
    'cinematic': {
      name: 'Cinematic',
      description: 'Film-like color grading with slight desaturation',
      filters: {
        saturation: 0.9,
        contrast: 1.1,
        brightness: 0.02,
        gamma: 1.05,
        vignette: 0.3
      }
    },
    'vibrant': {
      name: 'Vibrant',
      description: 'Punchy colors for engaging content',
      filters: {
        saturation: 1.3,
        contrast: 1.15,
        brightness: 0.05,
        gamma: 1.0,
        vignette: 0
      }
    },
    'dramatic': {
      name: 'Dramatic',
      description: 'High contrast, moody look',
      filters: {
        saturation: 0.8,
        contrast: 1.3,
        brightness: -0.05,
        gamma: 0.95,
        vignette: 0.5
      }
    },
    'warm': {
      name: 'Warm Tones',
      description: 'Golden hour warmth',
      filters: {
        saturation: 1.1,
        contrast: 1.05,
        brightness: 0.03,
        gamma: 1.0,
        vignette: 0.2,
        colorTemperature: 6500 // Kelvin (warmer)
      }
    },
    'cool': {
      name: 'Cool Tones',
      description: 'Blue-tinted modern look',
      filters: {
        saturation: 0.95,
        contrast: 1.1,
        brightness: 0,
        gamma: 1.02,
        vignette: 0.15,
        colorTemperature: 4500 // Kelvin (cooler)
      }
    },
    'vintage': {
      name: 'Vintage',
      description: 'Retro film look with grain',
      filters: {
        saturation: 0.7,
        contrast: 0.95,
        brightness: 0.05,
        gamma: 1.1,
        vignette: 0.4,
        grain: 0.15
      }
    },
    'documentary': {
      name: 'Documentary',
      description: 'Natural, realistic look',
      filters: {
        saturation: 1.0,
        contrast: 1.05,
        brightness: 0,
        gamma: 1.0,
        vignette: 0
      }
    },
    'noir': {
      name: 'Film Noir',
      description: 'High contrast black & white',
      filters: {
        saturation: 0,
        contrast: 1.4,
        brightness: -0.02,
        gamma: 0.9,
        vignette: 0.6
      }
    }
  },

  // Individual effect filters
  effects: {
    saturation: (value) => value !== 1.0 ? `eq=saturation=${value.toFixed(2)}` : null,
    contrast: (value) => value !== 1.0 ? `eq=contrast=${value.toFixed(2)}` : null,
    brightness: (value) => value !== 0 ? `eq=brightness=${value.toFixed(2)}` : null,
    gamma: (value) => value !== 1.0 ? `eq=gamma=${value.toFixed(2)}` : null,
    vignette: (value) => value > 0 ? `vignette=PI/${(4/value).toFixed(1)}` : null,
    grain: (value) => value > 0 ? `noise=alls=${Math.round(value * 30)}:allf=t` : null,
    blur: (value) => value > 0 ? `boxblur=${value}:${value}` : null,
    sharpen: (value) => value > 0 ? `unsharp=5:5:${value.toFixed(1)}` : null
  },

  // Generate FFmpeg filter chain for a preset or custom effects
  generateFilterChain(presetName = null, customFilters = {}) {
    let filters = {};

    if (presetName && this.presets[presetName]) {
      filters = { ...this.presets[presetName].filters };
    }

    // Merge custom filters
    filters = { ...filters, ...customFilters };

    // Build filter chain
    const filterParts = [];

    // Combine eq filters (saturation, contrast, brightness, gamma)
    const eqParams = [];
    if (filters.saturation && filters.saturation !== 1.0) {
      eqParams.push(`saturation=${filters.saturation.toFixed(2)}`);
    }
    if (filters.contrast && filters.contrast !== 1.0) {
      eqParams.push(`contrast=${filters.contrast.toFixed(2)}`);
    }
    if (filters.brightness && filters.brightness !== 0) {
      eqParams.push(`brightness=${filters.brightness.toFixed(2)}`);
    }
    if (filters.gamma && filters.gamma !== 1.0) {
      eqParams.push(`gamma=${filters.gamma.toFixed(2)}`);
    }
    if (eqParams.length > 0) {
      filterParts.push(`eq=${eqParams.join(':')}`);
    }

    // Add other effects
    if (filters.vignette && filters.vignette > 0) {
      filterParts.push(`vignette=PI/${(4/filters.vignette).toFixed(1)}`);
    }
    if (filters.grain && filters.grain > 0) {
      filterParts.push(`noise=alls=${Math.round(filters.grain * 30)}:allf=t`);
    }
    if (filters.blur && filters.blur > 0) {
      filterParts.push(`boxblur=${filters.blur}:${filters.blur}`);
    }
    if (filters.sharpen && filters.sharpen > 0) {
      filterParts.push(`unsharp=5:5:${filters.sharpen.toFixed(1)}`);
    }

    return filterParts.length > 0 ? filterParts.join(',') : null;
  }
};

/**
 * TITLE_CARD_ENGINE
 *
 * Generates title cards and text overlays for scenes.
 */
const TITLE_CARD_ENGINE = {
  // Title card styles
  styles: {
    'minimal': {
      name: 'Minimal',
      background: 'black',
      textColor: 'white',
      fontFamily: 'sans-serif',
      fontSize: 72,
      animation: 'fade',
      duration: 2
    },
    'modern': {
      name: 'Modern',
      background: 'gradient:linear:#1a1a2e:#16213e',
      textColor: 'white',
      fontFamily: 'sans-serif',
      fontSize: 80,
      animation: 'slide_up',
      duration: 2.5,
      subtitle: true
    },
    'dramatic': {
      name: 'Dramatic',
      background: 'black',
      textColor: 'white',
      fontFamily: 'serif',
      fontSize: 90,
      animation: 'fade_slow',
      duration: 3,
      letterSpacing: 10
    },
    'chapter': {
      name: 'Chapter Card',
      background: 'rgba(0,0,0,0.8)',
      textColor: 'white',
      fontFamily: 'sans-serif',
      fontSize: 64,
      animation: 'fade',
      duration: 2,
      chapterNumber: true,
      overlay: true // Can overlay on video
    },
    'lower_third': {
      name: 'Lower Third',
      position: 'bottom',
      background: 'gradient:linear:rgba(0,0,0,0):rgba(0,0,0,0.8)',
      textColor: 'white',
      fontFamily: 'sans-serif',
      fontSize: 48,
      animation: 'slide_up',
      duration: 3,
      overlay: true
    }
  },

  // Generate title card FFmpeg filter
  generateTitleCard(text, style = 'minimal', options = {}) {
    const styleConfig = this.styles[style] || this.styles['minimal'];
    const duration = options.duration || styleConfig.duration;
    const width = options.width || 1920;
    const height = options.height || 1080;

    // For overlay titles, we generate a drawtext filter
    if (styleConfig.overlay) {
      const yPosition = styleConfig.position === 'bottom'
        ? `h-${styleConfig.fontSize * 2}`
        : styleConfig.position === 'top'
          ? styleConfig.fontSize
          : '(h-text_h)/2';

      return {
        type: 'overlay',
        filter: `drawtext=text='${text.replace(/'/g, "\\'")}':fontsize=${styleConfig.fontSize}:fontcolor=${styleConfig.textColor}:x=(w-text_w)/2:y=${yPosition}:enable='between(t,0,${duration})'`,
        duration
      };
    }

    // For full-screen title cards, we generate a video segment
    // This returns the configuration for creating a title frame
    return {
      type: 'fullscreen',
      config: {
        text,
        width,
        height,
        duration,
        style: styleConfig,
        options
      },
      // FFmpeg command to create title card video
      ffmpegArgs: [
        '-f', 'lavfi',
        '-i', `color=c=${styleConfig.background === 'black' ? 'black' : '0x1a1a2e'}:s=${width}x${height}:d=${duration}`,
        '-vf', `drawtext=text='${text.replace(/'/g, "\\'")}':fontsize=${styleConfig.fontSize}:fontcolor=${styleConfig.textColor}:x=(w-text_w)/2:y=(h-text_h)/2:enable='between(t,0.5,${duration-0.5})'`,
        '-c:v', 'libx264',
        '-pix_fmt', 'yuv420p',
        '-t', duration.toString()
      ]
    };
  },

  // Generate scene number overlay
  generateSceneNumber(sceneNumber, totalScenes, duration = 3) {
    const text = `Scene ${sceneNumber} of ${totalScenes}`;
    return this.generateTitleCard(text, 'chapter', { duration });
  }
};

// Export engines for use in other functions
exports.SHOT_TRANSITION_ENGINE = SHOT_TRANSITION_ENGINE;
exports.AUDIO_DUCKING_ENGINE = AUDIO_DUCKING_ENGINE;
exports.VIDEO_EFFECTS_ENGINE = VIDEO_EFFECTS_ENGINE;
exports.TITLE_CARD_ENGINE = TITLE_CARD_ENGINE;

/**
 * creationWizardConfigureShotTransitions
 *
 * Configure transitions between shots within a scene
 */
exports.creationWizardConfigureShotTransitions = functions
  .runWith({ timeoutSeconds: 60, memory: '256MB' })
  .https.onCall(async (data, context) => {
    console.log('[creationWizardConfigureShotTransitions] Starting...');

    const { projectId, sceneId, shots, autoMode = true, mood = 'neutral' } = data;

    if (!projectId || !sceneId) {
      throw new functions.https.HttpsError('invalid-argument', 'Project ID and Scene ID required');
    }

    try {
      const transitionsConfig = [];

      if (autoMode && shots && shots.length > 1) {
        // Auto-recommend transitions based on shot types
        for (let i = 0; i < shots.length - 1; i++) {
          const fromShot = shots[i];
          const toShot = shots[i + 1];

          const recommendedType = SHOT_TRANSITION_ENGINE.recommendTransition(
            fromShot,
            toShot,
            { mood }
          );

          const transitionInfo = SHOT_TRANSITION_ENGINE.transitionTypes[recommendedType];

          // Calculate offset (when transition starts)
          let cumulativeDuration = 0;
          for (let j = 0; j <= i; j++) {
            cumulativeDuration += shots[j].duration || 3;
          }
          const offset = cumulativeDuration - (transitionInfo.duration || 0);

          transitionsConfig.push({
            fromShotIndex: i,
            toShotIndex: i + 1,
            type: recommendedType,
            duration: transitionInfo.duration,
            offset,
            ffmpegFilter: SHOT_TRANSITION_ENGINE.generateFFmpegFilter(recommendedType, offset),
            reason: `${fromShot.shotType} â†’ ${toShot.shotType} with ${mood} mood`
          });
        }
      }

      // Store configuration in Firestore
      const configRef = admin.firestore()
        .collection('projects').doc(projectId)
        .collection('sceneTransitions').doc(sceneId);

      await configRef.set({
        sceneId,
        transitions: transitionsConfig,
        autoMode,
        mood,
        updatedAt: admin.firestore.FieldValue.serverTimestamp()
      });

      console.log(`[creationWizardConfigureShotTransitions] Configured ${transitionsConfig.length} transitions for scene ${sceneId}`);

      return {
        success: true,
        sceneId,
        transitions: transitionsConfig,
        availableTypes: Object.keys(SHOT_TRANSITION_ENGINE.transitionTypes).map(id => ({
          id,
          ...SHOT_TRANSITION_ENGINE.transitionTypes[id]
        }))
      };

    } catch (error) {
      console.error('[creationWizardConfigureShotTransitions] Error:', error);
      throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to configure transitions'));
    }
  });

/**
 * creationWizardConfigureAudioDucking
 *
 * Configure audio ducking for background music
 */
exports.creationWizardConfigureAudioDucking = functions
  .runWith({ timeoutSeconds: 60, memory: '256MB' })
  .https.onCall(async (data, context) => {
    console.log('[creationWizardConfigureAudioDucking] Starting...');

    const { projectId, preset = 'standard', voiceSegments = [], musicDuration } = data;

    if (!projectId) {
      throw new functions.https.HttpsError('invalid-argument', 'Project ID required');
    }

    try {
      const presetConfig = AUDIO_DUCKING_ENGINE.presets[preset] || AUDIO_DUCKING_ENGINE.presets['standard'];

      let duckingConfig = {
        preset,
        config: presetConfig
      };

      // If voice segments provided, generate volume automation
      if (voiceSegments.length > 0 && musicDuration) {
        const automationPoints = AUDIO_DUCKING_ENGINE.generateVolumeAutomation(
          voiceSegments,
          musicDuration,
          preset
        );

        const volumeFilter = AUDIO_DUCKING_ENGINE.generateVolumeFilter(automationPoints);

        duckingConfig.automationPoints = automationPoints;
        duckingConfig.ffmpegFilter = volumeFilter;
      }

      // Store configuration
      const configRef = admin.firestore()
        .collection('projects').doc(projectId)
        .collection('audioConfig').doc('ducking');

      await configRef.set({
        ...duckingConfig,
        updatedAt: admin.firestore.FieldValue.serverTimestamp()
      });

      console.log(`[creationWizardConfigureAudioDucking] Configured ${preset} ducking with ${voiceSegments.length} segments`);

      return {
        success: true,
        ducking: duckingConfig,
        availablePresets: Object.keys(AUDIO_DUCKING_ENGINE.presets).map(id => ({
          id,
          ...AUDIO_DUCKING_ENGINE.presets[id]
        }))
      };

    } catch (error) {
      console.error('[creationWizardConfigureAudioDucking] Error:', error);
      throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to configure audio ducking'));
    }
  });

/**
 * creationWizardApplyVideoEffects
 *
 * Apply video effects preset or custom effects to scenes
 */
exports.creationWizardApplyVideoEffects = functions
  .runWith({ timeoutSeconds: 60, memory: '256MB' })
  .https.onCall(async (data, context) => {
    console.log('[creationWizardApplyVideoEffects] Starting...');

    const { projectId, preset = null, customFilters = {}, applyToScenes = 'all' } = data;

    if (!projectId) {
      throw new functions.https.HttpsError('invalid-argument', 'Project ID required');
    }

    try {
      // Generate filter chain
      const filterChain = VIDEO_EFFECTS_ENGINE.generateFilterChain(preset, customFilters);

      const effectsConfig = {
        preset,
        customFilters,
        filterChain,
        applyToScenes,
        presetInfo: preset ? VIDEO_EFFECTS_ENGINE.presets[preset] : null
      };

      // Store configuration
      const configRef = admin.firestore()
        .collection('projects').doc(projectId)
        .collection('videoConfig').doc('effects');

      await configRef.set({
        ...effectsConfig,
        updatedAt: admin.firestore.FieldValue.serverTimestamp()
      });

      console.log(`[creationWizardApplyVideoEffects] Applied ${preset || 'custom'} effects, filter: ${filterChain || 'none'}`);

      return {
        success: true,
        effects: effectsConfig,
        availablePresets: Object.keys(VIDEO_EFFECTS_ENGINE.presets).map(id => ({
          id,
          name: VIDEO_EFFECTS_ENGINE.presets[id].name,
          description: VIDEO_EFFECTS_ENGINE.presets[id].description,
          filters: VIDEO_EFFECTS_ENGINE.presets[id].filters
        }))
      };

    } catch (error) {
      console.error('[creationWizardApplyVideoEffects] Error:', error);
      throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to apply video effects'));
    }
  });

/**
 * creationWizardGenerateTitleCards
 *
 * Generate title cards for scenes
 */
exports.creationWizardGenerateTitleCards = functions
  .runWith({ timeoutSeconds: 120, memory: '512MB' })
  .https.onCall(async (data, context) => {
    console.log('[creationWizardGenerateTitleCards] Starting...');

    const { projectId, scenes, style = 'modern', options = {} } = data;

    if (!projectId || !scenes || scenes.length === 0) {
      throw new functions.https.HttpsError('invalid-argument', 'Project ID and scenes required');
    }

    try {
      const titleCards = [];

      for (let i = 0; i < scenes.length; i++) {
        const scene = scenes[i];

        // Generate scene title card
        if (scene.title || scene.sceneTitle) {
          const card = TITLE_CARD_ENGINE.generateTitleCard(
            scene.title || scene.sceneTitle,
            style,
            {
              ...options,
              subtitle: scene.subtitle,
              width: options.width || 1920,
              height: options.height || 1080
            }
          );

          titleCards.push({
            sceneId: scene.sceneId || i + 1,
            sceneIndex: i,
            type: 'scene_title',
            ...card
          });
        }

        // Generate chapter card if requested
        if (options.includeChapterCards) {
          const chapterCard = TITLE_CARD_ENGINE.generateSceneNumber(
            i + 1,
            scenes.length,
            options.chapterDuration || 2
          );

          titleCards.push({
            sceneId: scene.sceneId || i + 1,
            sceneIndex: i,
            type: 'chapter_number',
            ...chapterCard
          });
        }
      }

      // Store configuration
      const configRef = admin.firestore()
        .collection('projects').doc(projectId)
        .collection('videoConfig').doc('titleCards');

      await configRef.set({
        titleCards,
        style,
        options,
        updatedAt: admin.firestore.FieldValue.serverTimestamp()
      });

      console.log(`[creationWizardGenerateTitleCards] Generated ${titleCards.length} title cards in ${style} style`);

      return {
        success: true,
        titleCards,
        availableStyles: Object.keys(TITLE_CARD_ENGINE.styles).map(id => ({
          id,
          ...TITLE_CARD_ENGINE.styles[id]
        }))
      };

    } catch (error) {
      console.error('[creationWizardGenerateTitleCards] Error:', error);
      throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to generate title cards'));
    }
  });

// ==========================================
// FIX CHARACTER FACES - NanoBananaPro Face Correction
// ==========================================
/**
 * creationWizardFixCharacterFaces
 *
 * Uses Gemini image generation to correct character faces in a captured frame
 * by using Character Bible portraits as reference images.
 *
 * Supports multi-character face correction in a single edit operation.
 *
 * @param {Object} data
 * @param {string} data.frameBase64 - The captured frame as base64 string (without data URL prefix)
 * @param {string} data.frameMimeType - MIME type of the frame (e.g., 'image/png')
 * @param {Array} data.characterReferences - Array of character reference objects:
 *   - name: Character name
 *   - description: Character description from Character Bible
 *   - base64: Reference portrait as base64 (without data URL prefix)
 *   - mimeType: MIME type of reference image
 * @param {string} data.aspectRatio - Target aspect ratio (e.g., '16:9')
 * @param {string} data.projectId - Project ID for storage path
 *
 * @returns {Object} { success: true, correctedImageUrl, correctedImageBase64 }
 */
exports.creationWizardFixCharacterFaces = functions
  .runWith({
    timeoutSeconds: 120, // Gemini image generation can take time
    memory: '1GB'
  })
  .https.onCall(async (data, context) => {
    console.log('[creationWizardFixCharacterFaces] Starting face correction...');

    // Authentication check
    if (!context.auth) {
      throw new functions.https.HttpsError('unauthenticated', 'User must be authenticated');
    }
    const uid = context.auth.uid;

    // Rate limit check
    checkRateLimit(uid, 'fixCharacterFaces', 10);

    // Extract parameters
    const {
      frameBase64,
      frameMimeType = 'image/png',
      characterReferences = [],
      aspectRatio = '16:9',
      projectId
    } = data;

    // Validate required parameters
    if (!frameBase64) {
      throw new functions.https.HttpsError('invalid-argument', 'frameBase64 is required');
    }
    if (!characterReferences || characterReferences.length === 0) {
      throw new functions.https.HttpsError('invalid-argument', 'At least one character reference is required');
    }

    // Validate character references have required fields
    for (let i = 0; i < characterReferences.length; i++) {
      const charRef = characterReferences[i];
      if (!charRef.name) {
        throw new functions.https.HttpsError('invalid-argument', `Character reference ${i + 1} missing name`);
      }
      if (!charRef.base64) {
        throw new functions.https.HttpsError('invalid-argument', `Character reference ${i + 1} (${charRef.name}) missing base64 image data`);
      }
    }

    console.log(`[creationWizardFixCharacterFaces] Processing ${characterReferences.length} character reference(s)`);
    characterReferences.forEach((c, idx) => {
      console.log(`  Character ${idx + 1}: ${c.name} - base64 length: ${c.base64?.length || 0}`);
    });

    try {
      // Initialize Gemini AI
      const geminiApiKey = functions.config().gemini?.key;
      if (!geminiApiKey) {
        throw new functions.https.HttpsError('failed-precondition',
          'Gemini API key not configured. Please contact support.');
      }

      const { GoogleGenAI } = require('@google/genai');
      const ai = new GoogleGenAI({ apiKey: geminiApiKey });

      // Use the same model as Creative Studio where face preservation works
      const geminiModelId = 'gemini-3-pro-image-preview';

      // ==========================================
      // BUILD CONTENT PARTS FOR GEMINI
      // ==========================================
      const contentParts = [];

      // 1. Add the original frame as the first image (scene to preserve)
      contentParts.push({
        inlineData: {
          mimeType: frameMimeType,
          data: frameBase64
        }
      });
      console.log('[creationWizardFixCharacterFaces] Added original frame as input');

      // 2. Add each character reference portrait
      characterReferences.forEach((charRef, idx) => {
        contentParts.push({
          inlineData: {
            mimeType: charRef.mimeType || 'image/png',
            data: charRef.base64
          }
        });
        console.log(`[creationWizardFixCharacterFaces] Added character reference ${idx + 1}: ${charRef.name}`);
      });

      // 3. Build detailed face correction prompt with LIGHTING PRESERVATION
      const characterDescriptions = characterReferences.map((c, idx) => {
        const charNum = idx + 2; // +2 because image 1 is the scene
        return `- Image ${charNum}: ${c.name}${c.description ? ` (${c.description})` : ''}`;
      }).join('\n');

      const faceFixPrompt = `
FACE CORRECTION TASK - LIGHTING CRITICAL:

You have been given ${characterReferences.length + 1} images:
- Image 1: The SCENE to preserve (composition, lighting, poses, background, clothing, everything EXCEPT faces)
${characterDescriptions}

=== CRITICAL LIGHTING PRESERVATION ===
BEFORE making ANY changes, ANALYZE Image 1's lighting:
1. Color temperature (warm/cool/neutral)
2. Light direction (front/side/back/above)
3. Light intensity (bright/dim/dramatic shadows)
4. Color grading (teal/orange, cool blues, warm ambers, etc.)
5. Atmospheric effects (fog, haze, smoke, neon glow, volumetric light)
6. Shadow depth and placement
7. Rim lighting or backlight effects
8. Any colored light sources (neon, fire, screens)

The corrected faces MUST match ALL these lighting characteristics EXACTLY.
The face should look like it was FILMED IN THAT SCENE, not pasted in from a studio.

YOUR TASK:
1. PRESERVE the EXACT scene from Image 1:
   - Same composition, poses, camera angle, background, clothing, body positions
   - Same color grading and color temperature
   - Same atmospheric effects (fog, haze, smoke if present)
   - Same lighting direction and shadow patterns
   - Same overall mood and cinematic look

2. REPLACE only the facial features with those from the character references:
   - Eyes, nose, mouth, face shape, skin tone FROM the reference
   - BUT the lighting ON the face must match Image 1's lighting EXACTLY
   - Shadows should fall in the same direction as Image 1
   - Skin should reflect the same color cast as Image 1
   - If Image 1 has blue/cyan lighting, the face must have that blue/cyan tint
   - If Image 1 has warm golden lighting, the face must have that warm glow

CRITICAL DO NOT:
- Do NOT flatten the lighting or make it look like studio lighting
- Do NOT remove atmospheric effects (fog, haze, smoke, dust particles)
- Do NOT change the color grading or color temperature
- Do NOT create a "pasted on" look where the face doesn't match the environment
- Do NOT make the face brighter or more evenly lit than the original

CRITICAL DO:
- Apply the SAME shadows to the new face that were on the original face
- Match any rim light or backlight effects from the original
- Preserve any color tints from environment lighting (neon, fire, etc.)
- Keep the atmospheric density consistent
- Maintain ${aspectRatio} aspect ratio

Output a single corrected image where the face blends SEAMLESSLY with the scene's lighting.
`.trim();

      contentParts.push({ text: faceFixPrompt });
      console.log('[creationWizardFixCharacterFaces] Built face correction prompt');

      // ==========================================
      // GENERATE CORRECTED IMAGE
      // ==========================================
      const MAX_RETRIES = 2;
      let correctedImageUrl = null;
      let correctedImageBase64 = null;
      let lastError = null;

      for (let retry = 0; retry <= MAX_RETRIES; retry++) {
        try {
          if (retry > 0) {
            console.log(`[creationWizardFixCharacterFaces] Retry ${retry}/${MAX_RETRIES}`);
            await new Promise(resolve => setTimeout(resolve, retry * 2000));
          }

          // Add aspect ratio and lighting preservation hint to prompt
          const aspectInstruction = `[LIGHTING MATCH CRITICAL: The corrected face MUST have the EXACT same lighting as the original scene - same shadows, same color temperature, same atmospheric effects. Generate in ${aspectRatio} aspect ratio. Cinematic quality. The face must look like it belongs in this specific lighting environment, NOT like it was shot in a studio.]\n\n`;
          const enhancedParts = contentParts.map((part, idx) => {
            if (part.text) {
              return { text: aspectInstruction + part.text };
            }
            return part;
          });

          // Determine aspect ratio format for Gemini
          const geminiAspectRatio = aspectRatio === '16:9' ? '16:9' :
                                     aspectRatio === '9:16' ? '9:16' :
                                     aspectRatio === '1:1' ? '1:1' : '16:9';

          console.log(`[creationWizardFixCharacterFaces] Calling Gemini model: ${geminiModelId}, image_size: 2K`);
          const startTime = Date.now();

          const result = await ai.models.generateContent({
            model: geminiModelId,
            contents: [{ role: 'user', parts: enhancedParts }],
            config: {
              responseModalities: ['IMAGE', 'TEXT'],
              imageConfig: {
                aspectRatio: geminiAspectRatio,
                image_size: '2K'  // HD resolution for quality face correction
              }
            }
          });

          console.log(`[creationWizardFixCharacterFaces] Gemini response in ${Date.now() - startTime}ms`);

          // Extract image from response
          const candidates = result.candidates || (result.response && result.response.candidates);
          if (candidates && candidates.length > 0) {
            const candidate = candidates[0];
            const parts = candidate.content?.parts || candidate.parts || [];

            for (const part of parts) {
              const inlineData = part.inlineData || part.inline_data;
              if (inlineData && (inlineData.data || inlineData.bytesBase64Encoded)) {
                // Found the corrected image
                const imageBytes = inlineData.data || inlineData.bytesBase64Encoded;
                const rawBuffer = Buffer.from(imageBytes, 'base64');

                console.log(`[creationWizardFixCharacterFaces] Raw corrected image: ${rawBuffer.length} bytes`);

                // Enforce exact dimensions based on aspect ratio
                const { buffer: processedBuffer, width: targetWidth, height: targetHeight, mimeType: processedMimeType } =
                  await enforceImageDimensions(rawBuffer, aspectRatio, 'hd');

                console.log(`[creationWizardFixCharacterFaces] Processed: ${processedBuffer.length} bytes, ${targetWidth}x${targetHeight}`);

                // Save to Cloud Storage
                const timestamp = Date.now();
                const fileName = projectId
                  ? `projects/${projectId}/face-corrections/${uid}/${timestamp}-fixed.png`
                  : `face-corrections/${uid}/${timestamp}-fixed.png`;

                const storage = admin.storage().bucket();
                const file = storage.file(fileName);

                await file.save(processedBuffer, {
                  metadata: {
                    contentType: processedMimeType,
                    metadata: {
                      characterCount: String(characterReferences.length),
                      characters: characterReferences.map(c => c.name).join(', '),
                      model: geminiModelId,
                      aspectRatio: aspectRatio,
                      width: String(targetWidth),
                      height: String(targetHeight),
                      generatedAt: new Date().toISOString()
                    }
                  }
                });

                // Make file publicly accessible
                await file.makePublic();
                correctedImageUrl = `https://storage.googleapis.com/${storage.name}/${fileName}`;
                correctedImageBase64 = processedBuffer.toString('base64');

                console.log(`[creationWizardFixCharacterFaces] Saved corrected image: ${fileName}`);
                break; // Got our image
              }
            }
          }

          if (correctedImageUrl) {
            break; // Success, exit retry loop
          }

          console.warn('[creationWizardFixCharacterFaces] No image in Gemini response, retrying...');

        } catch (genError) {
          lastError = genError.message || 'Unknown error';
          console.error(`[creationWizardFixCharacterFaces] Error on attempt ${retry + 1}:`, lastError);
        }
      }

      if (!correctedImageUrl) {
        const errorMsg = lastError
          ? `Face correction failed: ${lastError}`
          : 'Face correction failed: Gemini did not return an image. The content may have been filtered.';
        throw new functions.https.HttpsError('internal', errorMsg);
      }

      console.log('[creationWizardFixCharacterFaces] Face correction complete!');

      return {
        success: true,
        correctedImageUrl,
        correctedImageBase64,
        charactersCorrected: characterReferences.map(c => c.name)
      };

    } catch (error) {
      console.error('[creationWizardFixCharacterFaces] Error:', error);
      throw new functions.https.HttpsError('internal',
        sanitizeErrorMessage(error, 'Failed to fix character faces'));
    }
  });

/**
 * creationWizardGetPhase4Config
 *
 * Get all Phase 4 configuration options
 */
exports.creationWizardGetPhase4Config = functions
  .runWith({ timeoutSeconds: 30, memory: '256MB' })
  .https.onCall(async (data, context) => {
    console.log('[creationWizardGetPhase4Config] Starting...');

    try {
      return {
        success: true,
        config: {
          transitions: {
            types: Object.keys(SHOT_TRANSITION_ENGINE.transitionTypes).map(id => ({
              id,
              ...SHOT_TRANSITION_ENGINE.transitionTypes[id]
            }))
          },
          audioDucking: {
            presets: Object.keys(AUDIO_DUCKING_ENGINE.presets).map(id => ({
              id,
              ...AUDIO_DUCKING_ENGINE.presets[id]
            }))
          },
          videoEffects: {
            presets: Object.keys(VIDEO_EFFECTS_ENGINE.presets).map(id => ({
              id,
              name: VIDEO_EFFECTS_ENGINE.presets[id].name,
              description: VIDEO_EFFECTS_ENGINE.presets[id].description,
              filters: VIDEO_EFFECTS_ENGINE.presets[id].filters
            }))
          },
          titleCards: {
            styles: Object.keys(TITLE_CARD_ENGINE.styles).map(id => ({
              id,
              ...TITLE_CARD_ENGINE.styles[id]
            }))
          }
        }
      };

    } catch (error) {
      console.error('[creationWizardGetPhase4Config] Error:', error);
      throw new functions.https.HttpsError('internal', sanitizeErrorMessage(error, 'Failed to get config'));
    }
  });
